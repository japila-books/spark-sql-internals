== [[FileScanRDD]] FileScanRDD -- Input RDD of FileSourceScanExec Physical Operator

`FileScanRDD` is an `RDD` of link:spark-sql-InternalRow.adoc[internal binary rows] (i.e. `RDD[InternalRow]`) that is the one and only input RDD of link:spark-sql-SparkPlan-FileSourceScanExec.adoc[FileSourceScanExec] physical operator.

`FileScanRDD` is <<creating-instance, created>> exclusively when `FileSourceScanExec` physical operator is requested to link:spark-sql-SparkPlan-FileSourceScanExec.adoc#createBucketedReadRDD[createBucketedReadRDD] and link:spark-sql-SparkPlan-FileSourceScanExec.adoc#createNonBucketedReadRDD[createNonBucketedReadRDD] (which is when `FileSourceScanExec` is requested for the link:spark-sql-SparkPlan-FileSourceScanExec.adoc#inputRDD[input RDD] that `WholeStageCodegenExec` physical operator uses when link:spark-sql-SparkPlan-WholeStageCodegenExec.adoc#doExecute[executed]).

[source, scala]
----
val q = spark.read.text("README.md")

val sparkPlan = q.queryExecution.executedPlan
import org.apache.spark.sql.execution.FileSourceScanExec
val scan = sparkPlan.collectFirst { case exec: FileSourceScanExec => exec }.get
val inputRDD = scan.inputRDDs.head

val rdd = q.queryExecution.toRdd
scala> println(rdd.toDebugString)
(1) MapPartitionsRDD[1] at toRdd at <console>:26 []
 |  FileScanRDD[0] at toRdd at <console>:26 []

val fileScanRDD = q.queryExecution.toRdd.dependencies.head.rdd

// What FileSourceScanExec uses for the input RDD is exactly the first RDD in the lineage
assert(inputRDD == fileScanRDD)
----

[[internal-registries]]
.FileScanRDD's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| `ignoreCorruptFiles`
| [[ignoreCorruptFiles]] link:spark-sql-properties.adoc#spark.sql.files.ignoreCorruptFiles[spark.sql.files.ignoreCorruptFiles]

Used exclusively when `FileScanRDD` is requested to <<compute, compute a partition>>

| `ignoreMissingFiles`
| [[ignoreMissingFiles]] link:spark-sql-properties.adoc#spark.sql.files.ignoreMissingFiles[spark.sql.files.ignoreMissingFiles]

Used exclusively when `FileScanRDD` is requested to <<compute, compute a partition>>
|===

=== [[getPreferredLocations]] `getPreferredLocations` Method

[source, scala]
----
getPreferredLocations(split: RDDPartition): Seq[String]
----

NOTE: `getPreferredLocations` is part of the RDD Contract to...FIXME.

`getPreferredLocations`...FIXME

NOTE: `getPreferredLocations` is used when...FIXME

=== [[getPartitions]] `getPartitions` Method

[source, scala]
----
getPartitions: Array[RDDPartition]
----

NOTE: `getPartitions` is part of the RDD Contract to...FIXME.

`getPartitions`...FIXME

=== [[creating-instance]] Creating FileScanRDD Instance

`FileScanRDD` takes the following when created:

* [[sparkSession]] link:spark-sql-SparkSession.adoc[SparkSession]
* [[readFunction]] Read function that takes a link:spark-sql-PartitionedFile.adoc[PartitionedFile] and gives link:spark-sql-InternalRow.adoc[internal rows] back (i.e. `(PartitionedFile) => Iterator[InternalRow]`)
* [[filePartitions]] `FilePartitions`

=== [[compute]] Computing Partition (in TaskContext) -- `compute` Method

[source, scala]
----
compute(split: RDDPartition, context: TaskContext): Iterator[InternalRow]
----

NOTE: `compute` is part of Spark Core's `RDD` Contract to compute a partition (in a `TaskContext`).

`compute` creates a Scala https://www.scala-lang.org/api/2.11.12/#scala.collection.Iterator[Iterator] (of Java `Objects`) that...FIXME

`compute` then requests the input `TaskContext` to register a completion listener to be executed when a task completes (i.e. `addTaskCompletionListener`) that simply closes the iterator.

In the end, `compute` returns the iterator.
