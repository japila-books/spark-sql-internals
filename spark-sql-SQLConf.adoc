== [[SQLConf]] SQLConf -- Internal Configuration Store

`SQLConf` is an *internal key-value configuration store* for <<parameters, parameters and hints>> used in Spark SQL.

`SQLConf` offers methods to <<get, get>>, <<set, set>>, <<unset, unset>> or <<clear, clear>> values of configuration properties, but has also the <<accessor-methods, accessor methods>> to read the current value of a configuration property or hint.

You can access a session-specific `SQLConf` using link:spark-sql-SparkSession.adoc#sessionState[SessionState].

[source, scala]
----
scala> spark.version
res0: String = 2.3.0

scala> :type spark
org.apache.spark.sql.SparkSession

scala> :type spark.sessionState.conf
org.apache.spark.sql.internal.SQLConf

import spark.sessionState.conf

// accessing properties through accessor methods
scala> conf.numShufflePartitions
res1: Int = 200

// setting properties using aliases
import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS
conf.setConf(SHUFFLE_PARTITIONS, 2)
scala> conf.numShufflePartitions
res2: Int = 2

// unset aka reset properties to the default value
conf.unsetConf(SHUFFLE_PARTITIONS)
scala> conf.numShufflePartitions
res3: Int = 200

// You could also access the internal SQLConf using get
import org.apache.spark.sql.internal.SQLConf
val cc = SQLConf.get
scala> cc == conf
res4: Boolean = true
----

[NOTE]
====
`SQLConf` is an internal part of Spark SQL and is not meant to be used directly.

Spark SQL configuration is available through the user-facing interface `RuntimeConfig` that you can access using link:spark-sql-SparkSession.adoc#conf[SparkSession].

[source, scala]
----
scala> spark.version
res0: String = 2.3.0

scala> :type spark
org.apache.spark.sql.SparkSession

scala> :type spark.conf
org.apache.spark.sql.RuntimeConfig
----
====

[[accessor-methods]]
.SQLConf's Accessor Methods
[cols="1,1,1",options="header",width="100%"]
|===
| Name
| Parameter
| Description

| [[adaptiveExecutionEnabled]] `adaptiveExecutionEnabled`
| link:spark-sql-properties.adoc#spark.sql.adaptive.enabled[spark.sql.adaptive.enabled]
| Used exclusively when `EnsureRequirements` link:spark-sql-EnsureRequirements.adoc#withExchangeCoordinator[adds an ExchangeCoordinator] (for link:spark-sql-adaptive-query-execution.adoc[adaptive query execution])

| [[autoBroadcastJoinThreshold]] `autoBroadcastJoinThreshold`
| link:spark-sql-properties.adoc#spark.sql.autoBroadcastJoinThreshold[spark.sql.autoBroadcastJoinThreshold]
| Used exclusively in link:spark-sql-SparkStrategy-JoinSelection.adoc[JoinSelection] execution planning strategy

| [[autoSizeUpdateEnabled]] `autoSizeUpdateEnabled`
| link:spark-sql-properties.adoc#spark.sql.statistics.size.autoUpdate.enabled[spark.sql.statistics.size.autoUpdate.enabled]
a|

Used when:

1. `CommandUtils` is requested for link:spark-sql-CommandUtils.adoc#updateTableStats[updating existing table statistics]

1. `AlterTableAddPartitionCommand` is executed

| [[broadcastTimeout]] `broadcastTimeout`
| link:spark-sql-properties.adoc#spark.sql.broadcastTimeout[spark.sql.broadcastTimeout]
| Used exclusively in link:spark-sql-SparkPlan-BroadcastExchangeExec.adoc[BroadcastExchangeExec] (for broadcasting a table to executors).

| [[cboEnabled]] `cboEnabled`
| link:spark-sql-properties.adoc#spark.sql.cbo.enabled[spark.sql.cbo.enabled]
a|

Used in:

* link:spark-sql-Optimizer-ReorderJoin.adoc[ReorderJoin] logical plan optimization (and indirectly in `StarSchemaDetection` for `reorderStarJoins`)
* link:spark-sql-Optimizer-CostBasedJoinReorder.adoc[CostBasedJoinReorder] logical plan optimization

| [[columnBatchSize]] `columnBatchSize`
| link:spark-sql-properties.adoc#spark.sql.inMemoryColumnarStorage.batchSize[spark.sql.inMemoryColumnarStorage.batchSize]
| Used...FIXME

| [[dataFramePivotMaxValues]] `dataFramePivotMaxValues`
| link:spark-sql-properties.adoc#spark.sql.pivotMaxValues[spark.sql.pivotMaxValues]
| Used exclusively in link:spark-sql-RelationalGroupedDataset.adoc#pivot[pivot] operator.

| [[dataFrameRetainGroupColumns]] `dataFrameRetainGroupColumns`
| link:spark-sql-properties.adoc#spark.sql.retainGroupColumns[spark.sql.retainGroupColumns]
| Used exclusively in link:spark-sql-RelationalGroupedDataset.adoc[RelationalGroupedDataset] when creating the result `Dataset` (after `agg`, `count`, `mean`, `max`, `avg`, `min`, and `sum` operators).

| [[defaultSizeInBytes]] `defaultSizeInBytes`
| link:spark-sql-properties.adoc#spark.sql.defaultSizeInBytes[spark.sql.defaultSizeInBytes]
a|

Used when:

1. `DetermineTableStats` logical resolution rule could not compute the table size or <<spark.sql.statistics.fallBackToHdfs, spark.sql.statistics.fallBackToHdfs>> is turned off

1. `ExternalRDD`, link:spark-sql-LogicalRDD.adoc#computeStats[LogicalRDD] and `DataSourceV2Relation` are requested for statistics (i.e. `computeStats`)

1.  (Spark Structured Streaming) `StreamingRelation`, `StreamingExecutionRelation`, `StreamingRelationV2` and `ContinuousExecutionRelation` are requested for statistics (i.e. `computeStats`)

1. `DataSource` link:spark-sql-DataSource.adoc#resolveRelation[creates a HadoopFsRelation for FileFormat data source] (and builds a CatalogFileIndex when no table statistics are available)

1. `BaseRelation` is requested for link:spark-sql-BaseRelation.adoc#sizeInBytes[an estimated size of this relation] (in bytes)

| [[fallBackToHdfsForStatsEnabled]] `fallBackToHdfsForStatsEnabled`
| link:spark-sql-properties.adoc#spark.sql.statistics.fallBackToHdfs[spark.sql.statistics.fallBackToHdfs]
| Used exclusively when `DetermineTableStats` logical resolution rule is executed.

| [[histogramEnabled]] `histogramEnabled`
| link:spark-sql-properties.adoc#spark.sql.statistics.histogram.enabled[spark.sql.statistics.histogram.enabled]
| Used exclusively when `AnalyzeColumnCommand` logical command is link:spark-sql-LogicalPlan-AnalyzeColumnCommand.adoc#run[executed].

| [[histogramNumBins]] `histogramNumBins`
| link:spark-sql-properties.adoc#spark.sql.statistics.histogram.numBins[spark.sql.statistics.histogram.numBins]
| Used exclusively when `AnalyzeColumnCommand` is link:spark-sql-LogicalPlan-AnalyzeColumnCommand.adoc#run[executed] with link:spark-sql-properties.adoc#spark.sql.statistics.histogram.enabled[spark.sql.statistics.histogram.enabled] turned on (and link:spark-sql-LogicalPlan-AnalyzeColumnCommand.adoc#computePercentiles[calculates percentiles]).

| [[numShufflePartitions]] `numShufflePartitions`
| link:spark-sql-properties.adoc#spark.sql.shuffle.partitions[spark.sql.shuffle.partitions]
a|

Used in:

* Dataset's link:spark-sql-dataset-operators.adoc#repartition[repartition] operator (for a link:spark-sql-LogicalPlan-Repartition-RepartitionByExpression.adoc#RepartitionByExpression[RepartitionByExpression] logical operator)
* link:spark-sql-SparkSqlAstBuilder.adoc#withRepartitionByExpression[SparkSqlAstBuilder] (for a link:spark-sql-LogicalPlan-Repartition-RepartitionByExpression.adoc#RepartitionByExpression[RepartitionByExpression] logical operator)
* link:spark-sql-SparkStrategy-JoinSelection.adoc#canBuildLocalHashMap[JoinSelection] execution planning strategy
* link:spark-sql-LogicalPlan-RunnableCommand.adoc#SetCommand[SetCommand] logical command
* link:spark-sql-EnsureRequirements.adoc#defaultNumPreShufflePartitions[EnsureRequirements] physical plan optimization

| [[joinReorderEnabled]] `joinReorderEnabled`
| link:spark-sql-properties.adoc#spark.sql.cbo.joinReorder.enabled[spark.sql.cbo.joinReorder.enabled]
| Used exclusively in link:spark-sql-Optimizer-CostBasedJoinReorder.adoc[CostBasedJoinReorder] logical plan optimization

| [[limitScaleUpFactor]] `limitScaleUpFactor`
| link:spark-sql-properties.adoc#spark.sql.limit.scaleUpFactor[spark.sql.limit.scaleUpFactor]
| Used exclusively when a physical operator is requested link:spark-sql-SparkPlan.adoc#executeTake[the first n rows as an array].

| [[preferSortMergeJoin]] `preferSortMergeJoin`
| link:spark-sql-properties.adoc#spark.sql.join.preferSortMergeJoin[spark.sql.join.preferSortMergeJoin]
| Used exclusively in link:spark-sql-SparkStrategy-JoinSelection.adoc[JoinSelection] execution planning strategy to prefer sort merge join over shuffle hash join.

| [[runSQLonFile]] `runSQLonFile`
| link:spark-sql-properties.adoc#spark.sql.runSQLOnFiles[spark.sql.runSQLOnFiles]
a|

Used when:

* `ResolveRelations` does link:spark-sql-ResolveRelations.adoc#isRunningDirectlyOnFiles[isRunningDirectlyOnFiles]

* `ResolveSQLOnFile` does link:spark-sql-ResolveSQLOnFile.adoc#maybeSQLFile[maybeSQLFile]

| [[starSchemaDetection]] `starSchemaDetection`
| link:spark-sql-properties.adoc#spark.sql.cbo.starSchemaDetection[spark.sql.cbo.starSchemaDetection]
| Used exclusively in link:spark-sql-Optimizer-ReorderJoin.adoc[ReorderJoin] logical plan optimization (and indirectly in `StarSchemaDetection`)

| [[subexpressionEliminationEnabled]] `subexpressionEliminationEnabled`
| link:spark-sql-properties.adoc#spark.sql.subexpressionElimination.enabled[spark.sql.subexpressionElimination.enabled]
| Used exclusively when `SparkPlan` is requested for link:spark-sql-SparkPlan.adoc#subexpressionEliminationEnabled[subexpressionEliminationEnabled] flag.

| [[useCompression]] `useCompression`
| link:spark-sql-properties.adoc#spark.sql.inMemoryColumnarStorage.compressed[spark.sql.inMemoryColumnarStorage.compressed]
| Used...FIXME

| [[wholeStageEnabled]] `wholeStageEnabled`
| link:spark-sql-properties.adoc#spark.sql.codegen.wholeStage[spark.sql.codegen.wholeStage]
a| Used in:

* link:spark-sql-CollapseCodegenStages.adoc[CollapseCodegenStages] to control codegen
* link:spark-sql-ParquetFileFormat.adoc[ParquetFileFormat] to control row batch reading

| [[wholeStageFallback]] `wholeStageFallback`
| link:spark-sql-properties.adoc#spark.sql.codegen.fallback[spark.sql.codegen.fallback]
| Used exclusively when `WholeStageCodegenExec` is link:spark-sql-SparkPlan-WholeStageCodegenExec.adoc#doExecute[executed].

| [[wholeStageMaxNumFields]] `wholeStageMaxNumFields`
| link:spark-sql-properties.adoc#spark.sql.codegen.maxFields[spark.sql.codegen.maxFields]
a|

Used in:

* link:spark-sql-CollapseCodegenStages.adoc[CollapseCodegenStages] to control codegen
* link:spark-sql-ParquetFileFormat.adoc[ParquetFileFormat] to control row batch reading

| [[wholeStageSplitConsumeFuncByOperator]] `wholeStageSplitConsumeFuncByOperator`
| link:spark-sql-properties.adoc#spark.sql.codegen.splitConsumeFuncByOperator[spark.sql.codegen.splitConsumeFuncByOperator]
|

Used exclusively when `CodegenSupport` is requested to link:spark-sql-CodegenSupport.adoc#consume[consume]

| [[windowExecBufferSpillThreshold]] `windowExecBufferSpillThreshold`
| link:spark-sql-properties.adoc#spark.sql.windowExec.buffer.spill.threshold[spark.sql.windowExec.buffer.spill.threshold]
| Used exclusively when `WindowExec` unary physical operator is link:spark-sql-SparkPlan-WindowExec.adoc#doExecute[executed].

| [[useObjectHashAggregation]] `useObjectHashAggregation`
| link:spark-sql-properties.adoc#spark.sql.execution.useObjectHashAggregateExec[spark.sql.execution.useObjectHashAggregateExec]
| Used exclusively in `Aggregation` execution planning strategy when link:spark-sql-SparkStrategy-Aggregation.adoc#AggUtils-createAggregate[selecting a physical plan].
|===

=== [[get]] Getting Parameters and Hints

You can get the current parameters and hints using the following family of `get` methods.

[source, scala]
----
getConfString(key: String): String
getConf[T](entry: ConfigEntry[T], defaultValue: T): T
getConf[T](entry: ConfigEntry[T]): T
getConf[T](entry: OptionalConfigEntry[T]): Option[T]
getConfString(key: String, defaultValue: String): String
getAllConfs: immutable.Map[String, String]
getAllDefinedConfs: Seq[(String, String, String)]
----

=== [[set]] Setting Parameters and Hints

You can set parameters and hints using the following family of `set` methods.

[source, scala]
----
setConf(props: Properties): Unit
setConfString(key: String, value: String): Unit
setConf[T](entry: ConfigEntry[T], value: T): Unit
----

=== [[unset]] Unsetting Parameters and Hints

You can unset parameters and hints using the following family of `unset` methods.

[source, scala]
----
unsetConf(key: String): Unit
unsetConf(entry: ConfigEntry[_]): Unit
----

=== [[clear]] Clearing All Parameters and Hints

[source, scala]
----
clear(): Unit
----

You can use `clear` to remove all the parameters and hints in `SQLConf`.
