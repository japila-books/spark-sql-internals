{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"The Internals of Spark SQL (Apache Spark 3.4.0-rc5)","text":"<p>Welcome to The Internals of Spark SQL online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, an IT freelancer specializing in Apache Spark (incl. Spark SQL and Spark Structured Streaming), Delta Lake, Databricks, and Apache Kafka (incl. Kafka Streams and ksqlDB) with brief forays into a wider data engineering space (e.g., Trino, Dask and dbt, mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of Spark SQL as much as I have.</p> <p>Flannery O'Connor</p> <p>I write to discover what I know.</p> <p>\"The Internals Of\" series</p> <p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p> <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into Spark SQL \ud83d\udd25</p> <p>Last update: 2023-04-04</p>"},{"location":"AggUtils/","title":"AggUtils Utility","text":"<p><code>AggUtils</code> is an utility for Aggregation execution planning strategy.</p>"},{"location":"AggUtils/#planaggregatewithoutdistinct","title":"planAggregateWithoutDistinct <pre><code>planAggregateWithoutDistinct(\n  groupingExpressions: Seq[NamedExpression],\n  aggregateExpressions: Seq[AggregateExpression],\n  resultExpressions: Seq[NamedExpression],\n  child: SparkPlan): Seq[SparkPlan]\n</code></pre> <p><code>planAggregateWithoutDistinct</code> is a two-step physical operator generator.</p> <p><code>planAggregateWithoutDistinct</code> first creates an aggregate physical operator with <code>aggregateExpressions</code> in <code>Partial</code> mode (for partial aggregations).</p>  <p>Note</p> <p><code>requiredChildDistributionExpressions</code> for the aggregate physical operator for partial aggregation \"stage\" is empty.</p>  <p>In the end, <code>planAggregateWithoutDistinct</code> creates another aggregate physical operator (of the same type as before), but <code>aggregateExpressions</code> are now in <code>Final</code> mode (for final aggregations). The aggregate physical operator becomes the parent of the first aggregate operator.</p>  <p>Note</p> <p><code>requiredChildDistributionExpressions</code> for the parent aggregate physical operator for final aggregation \"stage\" are the Attributes of the <code>groupingExpressions</code>.</p>","text":""},{"location":"AggUtils/#planaggregatewithonedistinct","title":"planAggregateWithOneDistinct <pre><code>planAggregateWithOneDistinct(\n  groupingExpressions: Seq[NamedExpression],\n  functionsWithDistinct: Seq[AggregateExpression],\n  functionsWithoutDistinct: Seq[AggregateExpression],\n  resultExpressions: Seq[NamedExpression],\n  child: SparkPlan): Seq[SparkPlan]\n</code></pre> <p><code>planAggregateWithOneDistinct</code>...FIXME</p>","text":""},{"location":"AggUtils/#creating-physical-operator-for-aggregation","title":"Creating Physical Operator for Aggregation <pre><code>createAggregate(\n  requiredChildDistributionExpressions: Option[Seq[Expression]] = None,\n  groupingExpressions: Seq[NamedExpression] = Nil,\n  aggregateExpressions: Seq[AggregateExpression] = Nil,\n  aggregateAttributes: Seq[Attribute] = Nil,\n  initialInputBufferOffset: Int = 0,\n  resultExpressions: Seq[NamedExpression] = Nil,\n  child: SparkPlan): SparkPlan\n</code></pre> <p><code>createAggregate</code> creates one of the following physical operators based on the given AggregateExpressions (in the following order):</p> <ol> <li> <p>HashAggregateExec when all the aggBufferAttributes (of the AggregateFunctions of the given AggregateExpressions) are supported</p> </li> <li> <p>ObjectHashAggregateExec when the following all hold:</p> <ul> <li>spark.sql.execution.useObjectHashAggregateExec configuration property is enabled</li> <li>Aggregate expression supported</li> </ul> </li> <li> <p>SortAggregateExec</p> </li> </ol>  <p><code>createAggregate</code> is used when:</p> <ul> <li><code>AggUtils</code> is used to createStreamingAggregate, planAggregateWithoutDistinct, planAggregateWithOneDistinct</li> </ul>","text":""},{"location":"AggUtils/#planning-execution-of-streaming-aggregation","title":"Planning Execution of Streaming Aggregation <pre><code>planStreamingAggregation(\n  groupingExpressions: Seq[NamedExpression],\n  functionsWithoutDistinct: Seq[AggregateExpression],\n  resultExpressions: Seq[NamedExpression],\n  stateFormatVersion: Int,\n  child: SparkPlan): Seq[SparkPlan]\n</code></pre> <p><code>planStreamingAggregation</code>...FIXME</p>  <p><code>planStreamingAggregation</code> is used when:</p> <ul> <li><code>StatefulAggregationStrategy</code> (Spark Structured Streaming) execution planning strategy is requested to plan a logical plan of a streaming aggregation (a streaming query with Aggregate operator)</li> </ul>","text":""},{"location":"AggUtils/#creating-streaming-aggregate-physical-operator","title":"Creating Streaming Aggregate Physical Operator <pre><code>createStreamingAggregate(\n  requiredChildDistributionExpressions: Option[Seq[Expression]] = None,\n  groupingExpressions: Seq[NamedExpression] = Nil,\n  aggregateExpressions: Seq[AggregateExpression] = Nil,\n  aggregateAttributes: Seq[Attribute] = Nil,\n  initialInputBufferOffset: Int = 0,\n  resultExpressions: Seq[NamedExpression] = Nil,\n  child: SparkPlan): SparkPlan\n</code></pre> <p><code>createStreamingAggregate</code> creates an aggregate physical operator (with <code>isStreaming</code> flag enabled).</p>  <p>Note</p> <p><code>createStreamingAggregate</code> is exactly createAggregate with <code>isStreaming</code> flag enabled.</p>   <p><code>createStreamingAggregate</code> is used when:</p> <ul> <li><code>AggUtils</code> is requested to plan a regular and session-windowed streaming aggregation</li> </ul>","text":""},{"location":"AggregatingAccumulator/","title":"AggregatingAccumulator","text":"<p><code>AggregatingAccumulator</code> is an <code>AccumulatorV2</code> (Spark Core) for CollectMetricsExec physical operator.</p> <p><code>AggregatingAccumulator</code> accumulates InternalRows to produce an InternalRow.</p> <pre><code>AccumulatorV2[InternalRow, InternalRow]\n</code></pre>"},{"location":"AggregatingAccumulator/#creating-instance","title":"Creating Instance","text":"<p><code>AggregatingAccumulator</code> takes the following to be created:</p> <ul> <li> Buffer Schema (DataTypes) <li> Initial Values Expressions <li> Update Expressions <li> Merge Expressions <li> Result Expressions <li> ImperativeAggregates <li> TypedImperativeAggregates <li> SQLConf <p><code>AggregatingAccumulator</code> is created using apply and copyAndReset.</p>"},{"location":"AggregatingAccumulator/#creating-aggregatingaccumulator","title":"Creating AggregatingAccumulator <pre><code>apply(\n  functions: Seq[Expression],\n  inputAttributes: Seq[Attribute]): AggregatingAccumulator\n</code></pre> <p><code>apply</code>...FIXME</p>  <p><code>apply</code> is used when:</p> <ul> <li><code>CollectMetricsExec</code> physical operator is requested the accumulator</li> </ul>","text":""},{"location":"AggregatingAccumulator/#copyandreset","title":"copyAndReset <pre><code>copyAndReset(): AggregatingAccumulator\n</code></pre> <p><code>copyAndReset</code> is part of the <code>AccumulatorV2</code> (Spark Core) abstraction.</p>  <p><code>copyAndReset</code>...FIXME</p>","text":""},{"location":"Analyzer/","title":"Logical Query Plan Analyzer","text":"<p><code>Analyzer</code> (Spark Analyzer or Query Analyzer) is the logical query plan analyzer that validates and transforms an unresolved logical plan to an analyzed logical plan.</p> <p><code>Analyzer</code> is a RuleExecutor to transform logical operators (<code>RuleExecutor[LogicalPlan]</code>).</p> <pre><code>Analyzer: Unresolved Logical Plan ==&gt; Analyzed Logical Plan\n</code></pre> <p><code>Analyzer</code> is used by <code>QueryExecution</code> to resolve the managed <code>LogicalPlan</code> (and, as a sort of follow-up, assert that a structured query has already been properly analyzed, i.e. no failed or unresolved or somehow broken logical plan operators and expressions exist).</p>"},{"location":"Analyzer/#extendedresolutionrules-extension-point","title":"extendedResolutionRules Extension Point <pre><code>extendedResolutionRules: Seq[Rule[LogicalPlan]] = Nil\n</code></pre> <p><code>extendedResolutionRules</code> is an extension point for additional logical evaluation rules for Resolution batch. The rules are added at the end of the <code>Resolution</code> batch.</p> <p>Default: empty</p>  <p>Note</p> <p>SessionState uses its own <code>Analyzer</code> with custom extendedResolutionRules, postHocResolutionRules, and extendedCheckRules extension methods.</p>","text":""},{"location":"Analyzer/#posthocresolutionrules-extension-point","title":"postHocResolutionRules Extension Point <pre><code>postHocResolutionRules: Seq[Rule[LogicalPlan]] = Nil\n</code></pre> <p><code>postHocResolutionRules</code> is an extension point for rules in Post-Hoc Resolution batch if defined (that are executed in one pass, i.e. <code>Once</code> strategy).</p> <p>Default: empty</p>","text":""},{"location":"Analyzer/#batches","title":"Batches","text":""},{"location":"Analyzer/#hints","title":"Hints <p>Rules:</p> <ul> <li>ResolveJoinStrategyHints</li> <li>ResolveCoalesceHints</li> </ul> <p>Strategy: fixedPoint</p>","text":""},{"location":"Analyzer/#simple-sanity-check","title":"Simple Sanity Check <p>Rules:</p> <ul> <li>LookupFunctions</li> </ul> <p>Strategy: Once</p>","text":""},{"location":"Analyzer/#substitution","title":"Substitution <p>Rules:</p> <ul> <li>OptimizeUpdateFields</li> <li>CTESubstitution</li> <li>WindowsSubstitution</li> <li>EliminateUnions</li> <li>SubstituteUnresolvedOrdinals</li> </ul> <p>Strategy: fixedPoint</p>","text":""},{"location":"Analyzer/#resolution","title":"Resolution <p>Rules:</p> <ul> <li>ResolveTableValuedFunctions</li> <li>ResolveNamespace</li> <li>ResolveCatalogs</li> <li>ResolveInsertInto</li> <li>ResolveRelations</li> <li>ResolveTables</li> <li>ResolveReferences</li> <li>ResolveCreateNamedStruct</li> <li>ResolveDeserializer</li> <li>ResolveNewInstance</li> <li>ResolveUpCast</li> <li>ResolveGroupingAnalytics</li> <li>ResolvePivot</li> <li>ResolveOrdinalInOrderByAndGroupBy</li> <li>ResolveAggAliasInGroupBy</li> <li>ResolveMissingReferences</li> <li>ExtractGenerator</li> <li>ResolveGenerate</li> <li>ResolveFunctions</li> <li>ResolveAliases</li> <li>ResolveSubquery</li> <li>ResolveSubqueryColumnAliases</li> <li>ResolveWindowOrder</li> <li>ResolveWindowFrame</li> <li>ResolveNaturalAndUsingJoin</li> <li>ResolveOutputRelation</li> <li>ExtractWindowExpressions</li> <li>GlobalAggregates</li> <li>ResolveAggregateFunctions</li> <li>TimeWindowing</li> <li>ResolveInlineTables</li> <li>ResolveHigherOrderFunctions</li> <li>ResolveLambdaVariables</li> <li>ResolveTimeZone</li> <li>ResolveRandomSeed</li> <li>ResolveBinaryArithmetic</li> <li>Type Coercion Rules</li> <li>ResolveWithCTE</li> <li>extendedResolutionRules</li> </ul> <p>Strategy: fixedPoint</p>","text":""},{"location":"Analyzer/#post-hoc-resolution","title":"Post-Hoc Resolution <p>Rules:</p> <ul> <li>postHocResolutionRules</li> </ul> <p>Strategy: Once</p>","text":""},{"location":"Analyzer/#normalize-alter-table","title":"Normalize Alter Table <p>Rules:</p> <ul> <li>ResolveAlterTableChanges</li> </ul> <p>Strategy: Once</p>","text":""},{"location":"Analyzer/#remove-unresolved-hints","title":"Remove Unresolved Hints <p>Rules:</p> <ul> <li>RemoveAllHints</li> </ul> <p>Strategy: Once</p>","text":""},{"location":"Analyzer/#nondeterministic","title":"Nondeterministic <p>Rules:</p> <ul> <li>PullOutNondeterministic</li> </ul> <p>Strategy: Once</p>","text":""},{"location":"Analyzer/#udf","title":"UDF <p>Rules:</p> <ul> <li>HandleNullInputsForUDF</li> </ul> <p>Strategy: Once</p>","text":""},{"location":"Analyzer/#updatenullability","title":"UpdateNullability <p>Rules:</p> <ul> <li>UpdateAttributeNullability</li> </ul> <p>Strategy: Once</p>","text":""},{"location":"Analyzer/#subquery","title":"Subquery <p>Rules:</p> <ul> <li>UpdateOuterReferences</li> </ul> <p>Strategy: Once</p>","text":""},{"location":"Analyzer/#cleanup","title":"Cleanup <p>Rules:</p> <ul> <li>CleanupAliases</li> </ul> <p>Strategy: fixedPoint</p>","text":""},{"location":"Analyzer/#creating-instance","title":"Creating Instance <p><code>Analyzer</code> takes the following to be created:</p> <ul> <li> CatalogManager <li> SQLConf <li> Maximum number of iterations (of the FixedPoint rule batches)  <p><code>Analyzer</code> is created when <code>SessionState</code> is requested for the analyzer.</p> <p></p>","text":""},{"location":"Analyzer/#accessing-analyzer","title":"Accessing Analyzer <p><code>Analyzer</code> is available as the analyzer property of <code>SessionState</code>.</p> <pre><code>scala&gt; :type spark\norg.apache.spark.sql.SparkSession\n\nscala&gt; :type spark.sessionState.analyzer\norg.apache.spark.sql.catalyst.analysis.Analyzer\n</code></pre> <p>You can access the analyzed logical plan of a structured query using Dataset.explain basic action (with <code>extended</code> flag enabled) or SQL's <code>EXPLAIN EXTENDED</code> SQL command.</p> <pre><code>// sample structured query\nval inventory = spark\n  .range(5)\n  .withColumn(\"new_column\", 'id + 5 as \"plus5\")\n\n// Using explain operator (with extended flag enabled)\nscala&gt; inventory.explain(extended = true)\n== Parsed Logical Plan ==\n'Project [id#0L, ('id + 5) AS plus5#2 AS new_column#3]\n+- AnalysisBarrier\n      +- Range (0, 5, step=1, splits=Some(8))\n\n== Analyzed Logical Plan ==\nid: bigint, new_column: bigint\nProject [id#0L, (id#0L + cast(5 as bigint)) AS new_column#3L]\n+- Range (0, 5, step=1, splits=Some(8))\n\n== Optimized Logical Plan ==\nProject [id#0L, (id#0L + 5) AS new_column#3L]\n+- Range (0, 5, step=1, splits=Some(8))\n\n== Physical Plan ==\n*(1) Project [id#0L, (id#0L + 5) AS new_column#3L]\n+- *(1) Range (0, 5, step=1, splits=8)\n</code></pre> <p>Alternatively, you can access the analyzed logical plan using <code>QueryExecution</code> and its analyzed property  (that together with <code>numberedTreeString</code> method is a very good \"debugging\" tool).</p> <pre><code>val analyzedPlan = inventory.queryExecution.analyzed\nscala&gt; println(analyzedPlan.numberedTreeString)\n00 Project [id#0L, (id#0L + cast(5 as bigint)) AS new_column#3L]\n01 +- Range (0, 5, step=1, splits=Some(8))\n</code></pre>","text":""},{"location":"Analyzer/#fixedpoint","title":"FixedPoint <p><code>FixedPoint</code> with maxIterations for Hints, Substitution, Resolution and Cleanup batches.</p>","text":""},{"location":"Analyzer/#expandrelationname","title":"expandRelationName <pre><code>expandRelationName(\n  nameParts: Seq[String]): Seq[String]\n</code></pre> <p><code>expandRelationName</code>...FIXME</p> <p><code>expandRelationName</code> is used when <code>ResolveTables</code> and ResolveRelations logical analysis rules are executed.</p>","text":""},{"location":"Analyzer/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for the respective session-specific loggers to see what happens inside <code>Analyzer</code>:</p> <ul> <li> <p><code>org.apache.spark.sql.internal.SessionState$$anon$1</code></p> </li> <li> <p><code>org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1</code> for Hive support</p> </li> </ul> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code># with no Hive support\nlog4j.logger.org.apache.spark.sql.internal.SessionState$$anon$1=ALL\n\n# with Hive support enabled\nlog4j.logger.org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1=ALL\n</code></pre>  <p>Note</p> <p>The reason for such weird-looking logger names is that <code>analyzer</code> attribute is created as an anonymous subclass of <code>Analyzer</code> class in the respective <code>SessionStates</code>.</p>  <p>Refer to Logging.</p>","text":""},{"location":"BaseRelation/","title":"BaseRelation \u2014 Collection of Tuples with Schema","text":"<p><code>BaseRelation</code> is an abstraction of relations that are collections of tuples (rows) with a known schema.</p> <p><code>BaseRelation</code> represents an external data source with data to load datasets from or write to.</p> <p><code>BaseRelation</code> is \"created\" when <code>DataSource</code> is requested to resolve a relation.</p> <p><code>BaseRelation</code> is then transformed into a <code>DataFrame</code> when <code>SparkSession</code> is requested to create a DataFrame.</p> <p>Note</p> <p>\"Relation\" and \"table\" used to be synonyms, but Connector API in Spark 3 changed it with Table abstraction.</p>"},{"location":"BaseRelation/#contract","title":"Contract","text":""},{"location":"BaseRelation/#sqlcontext","title":"SQLContext <pre><code>sqlContext: SQLContext\n</code></pre> <p><code>SQLContext</code></p>","text":""},{"location":"BaseRelation/#schema","title":"Schema <pre><code>schema: StructType\n</code></pre> <p>Schema of the tuples of the relation</p>","text":""},{"location":"BaseRelation/#size","title":"Size <pre><code>sizeInBytes: Long\n</code></pre> <p>Estimated size of the relation (in bytes)</p> <p>Default: spark.sql.defaultSizeInBytes configuration property</p> <p><code>sizeInBytes</code> is used when <code>LogicalRelation</code> is requested for statistics (and they are not available in a catalog).</p>","text":""},{"location":"BaseRelation/#needs-conversion","title":"Needs Conversion <pre><code>needConversion: Boolean\n</code></pre> <p>Controls type conversion (whether or not JVM objects inside Rows needs to be converted to Catalyst types, e.g. <code>java.lang.String</code> to <code>UTF8String</code>)</p> <p>Default: <code>true</code></p>  <p>Note</p> <p>It is recommended to leave <code>needConversion</code> enabled (as is) for custom data sources (outside Spark SQL).</p>  <p>Used when DataSourceStrategy execution planning strategy is executed (and does the RDD conversion from <code>RDD[Row]</code> to <code>RDD[InternalRow]</code>).</p>","text":""},{"location":"BaseRelation/#unhandled-filters","title":"Unhandled Filters <pre><code>unhandledFilters(\n  filters: Array[Filter]): Array[Filter]\n</code></pre> <p>Filter predicates that the relation does not support (handle) natively</p> <p>Default: the input filters (as it is considered safe to double evaluate filters regardless whether they could be supported or not)</p> <p>Used when DataSourceStrategy execution planning strategy is executed (and selectFilters).</p>","text":""},{"location":"BaseRelation/#implementations","title":"Implementations","text":"<ul> <li>ConsoleRelation (Spark Structured Streaming)</li> <li>HadoopFsRelation</li> <li>JDBCRelation</li> <li>KafkaRelation</li> <li>KafkaSourceProvider</li> </ul>"},{"location":"BaseSessionStateBuilder/","title":"BaseSessionStateBuilder \u2014 Generic Builder of SessionState","text":"<p><code>BaseSessionStateBuilder</code> is an abstraction of builders that can produce a new BaseSessionStateBuilder to create a SessionState.</p> <p>spark.sql.catalogImplementation Configuration Property</p> <p><code>BaseSessionStateBuilder</code> and spark.sql.catalogImplementation configuration property allow for Hive and non-Hive Spark deployments.</p> <pre><code>assert(spark.sessionState.isInstanceOf[org.apache.spark.sql.internal.SessionState])\n</code></pre> <p><code>BaseSessionStateBuilder</code> holds properties that (together with newBuilder) are used to create a SessionState.</p>"},{"location":"BaseSessionStateBuilder/#contract","title":"Contract","text":""},{"location":"BaseSessionStateBuilder/#newbuilder","title":"newBuilder <pre><code>newBuilder: (SparkSession, Option[SessionState]) =&gt; BaseSessionStateBuilder\n</code></pre> <p>Produces a new <code>BaseSessionStateBuilder</code> for given SparkSession.md[SparkSession] and optional SessionState.md[SessionState]</p> <p>Used when <code>BaseSessionStateBuilder</code> is requested to &lt;&gt;","text":""},{"location":"BaseSessionStateBuilder/#implementations","title":"Implementations","text":"<ul> <li>HiveSessionStateBuilder</li> <li>SessionStateBuilder</li> </ul>"},{"location":"BaseSessionStateBuilder/#creating-instance","title":"Creating Instance","text":"<p><code>BaseSessionStateBuilder</code> takes the following to be created:</p> <ul> <li> SparkSession <li> Optional parent SessionState (default: undefined) <p><code>BaseSessionStateBuilder</code> is created when <code>SparkSession</code> is requested to instantiateSessionState.</p>"},{"location":"BaseSessionStateBuilder/#session-specific-registries","title":"Session-Specific Registries","text":"<p>The following registries are Scala lazy values which are created once and on demand (when accessed for the first time).</p>"},{"location":"BaseSessionStateBuilder/#analyzer","title":"Analyzer <pre><code>analyzer: Analyzer\n</code></pre> <p>Logical Analyzer</p>","text":""},{"location":"BaseSessionStateBuilder/#sessioncatalog","title":"SessionCatalog <pre><code>catalog: SessionCatalog\n</code></pre> <p>SessionCatalog</p>  <p>Note</p> <p>HiveSessionStateBuilder manages its own Hive-aware HiveSessionCatalog.</p>","text":""},{"location":"BaseSessionStateBuilder/#catalogmanager","title":"CatalogManager <pre><code>catalogManager: CatalogManager\n</code></pre> <p>CatalogManager that is created for the session-specific SQLConf, V2SessionCatalog and SessionCatalog.</p> <p><code>catalogManager</code> is used when:</p> <ul> <li> <p><code>BaseSessionStateBuilder</code> is requested for Analyzer and Optimizer</p> </li> <li> <p><code>HiveSessionStateBuilder</code> is requested for Analyzer</p> </li> </ul>","text":""},{"location":"BaseSessionStateBuilder/#sqlconf","title":"SQLConf <p>SQLConf</p>","text":""},{"location":"BaseSessionStateBuilder/#experimentalmethods","title":"ExperimentalMethods <p>ExperimentalMethods</p>","text":""},{"location":"BaseSessionStateBuilder/#functionregistry","title":"FunctionRegistry <p>FunctionRegistry</p>","text":""},{"location":"BaseSessionStateBuilder/#sessionresourceloader","title":"SessionResourceLoader <pre><code>resourceLoader: SessionResourceLoader\n</code></pre> <p><code>SessionResourceLoader</code></p>","text":""},{"location":"BaseSessionStateBuilder/#parserinterface","title":"ParserInterface <pre><code>sqlParser: ParserInterface\n</code></pre> <p>ParserInterface</p>","text":""},{"location":"BaseSessionStateBuilder/#tablefunctionregistry","title":"TableFunctionRegistry <pre><code>tableFunctionRegistry: TableFunctionRegistry\n</code></pre> <p>TableFunctionRegistry</p>  <p>When requested for the first time (as a <code>lazy val</code>), <code>tableFunctionRegistry</code> requests the parent SessionState (if available) to clone the tableFunctionRegistry or requests the SparkSessionExtensions to register the built-in function expressions.</p> <p><code>tableFunctionRegistry</code> is used when:</p> <ul> <li><code>HiveSessionStateBuilder</code> is requested for a HiveSessionCatalog</li> <li><code>BaseSessionStateBuilder</code> is requested for a SessionCatalog and a SessionState</li> </ul>","text":""},{"location":"BaseSessionStateBuilder/#v2sessioncatalog","title":"V2SessionCatalog <pre><code>v2SessionCatalog: V2SessionCatalog\n</code></pre> <p>V2SessionCatalog that is created for the session-specific SessionCatalog and  SQLConf.</p> <p><code>v2SessionCatalog</code> is used when <code>BaseSessionStateBuilder</code> is requested for the CatalogManager.</p>","text":""},{"location":"BaseSessionStateBuilder/#custom-operator-optimization-rules","title":"Custom Operator Optimization Rules <pre><code>customOperatorOptimizationRules: Seq[Rule[LogicalPlan]]\n</code></pre> <p>Custom operator optimization rules to add to the base Operator Optimization batch.</p> <p>When requested for the custom rules, <code>customOperatorOptimizationRules</code> simply requests the SparkSessionExtensions to buildOptimizerRules.</p> <p><code>customOperatorOptimizationRules</code> is used when <code>BaseSessionStateBuilder</code> is requested for an Optimizer.</p>","text":""},{"location":"BaseSessionStateBuilder/#sparksessionextensions","title":"SparkSessionExtensions <pre><code>extensions: SparkSessionExtensions\n</code></pre> <p>SparkSessionExtensions</p>","text":""},{"location":"BaseSessionStateBuilder/#executionlistenermanager","title":"ExecutionListenerManager <pre><code>listenerManager: ExecutionListenerManager\n</code></pre> <p>ExecutionListenerManager</p>","text":""},{"location":"BaseSessionStateBuilder/#optimizer","title":"Optimizer <pre><code>optimizer: Optimizer\n</code></pre> <p><code>optimizer</code> creates a SparkOptimizer for the CatalogManager, SessionCatalog and ExperimentalMethods.</p> <p>The <code>SparkOptimizer</code> uses the following extension methods:</p> <ul> <li>customEarlyScanPushDownRules for earlyScanPushDownRules</li> <li>customOperatorOptimizationRules for extendedOperatorOptimizationRules</li> </ul> <p><code>optimizer</code> is used when <code>BaseSessionStateBuilder</code> is requested to build a SessionState (as the optimizerBuilder function to build a logical query plan optimizer on demand).</p>","text":""},{"location":"BaseSessionStateBuilder/#sparkplanner","title":"SparkPlanner <pre><code>planner: SparkPlanner\n</code></pre> <p>SparkPlanner</p>","text":""},{"location":"BaseSessionStateBuilder/#streamingquerymanager","title":"StreamingQueryManager <pre><code>streamingQueryManager: StreamingQueryManager\n</code></pre> <p>Spark Structured Streaming's <code>StreamingQueryManager</code></p>","text":""},{"location":"BaseSessionStateBuilder/#udfregistration","title":"UDFRegistration <pre><code>udfRegistration: UDFRegistration\n</code></pre> <p>UDFRegistration</p>","text":""},{"location":"BaseSessionStateBuilder/#creating-clone-of-sessionstate","title":"Creating Clone of SessionState <pre><code>createClone: (SparkSession, SessionState) =&gt; SessionState\n</code></pre> <p><code>createClone</code> creates a SessionState using newBuilder followed by build.</p> <p><code>createClone</code> is used when <code>BaseSessionStateBuilder</code> is requested for a SessionState.</p>","text":""},{"location":"BaseSessionStateBuilder/#building-sessionstate","title":"Building SessionState <pre><code>build(): SessionState\n</code></pre> <p><code>build</code> creates a SessionState with the following:</p> <ul> <li>SparkSession.md#sharedState[SharedState] of the &lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt;  <p><code>build</code> is used when:</p> <ul> <li> <p><code>SparkSession</code> is requested for a SessionState (that in turn builds one using a class name based on spark.sql.catalogImplementation configuration property)</p> </li> <li> <p><code>BaseSessionStateBuilder</code> is requested to create a clone of a <code>SessionState</code></p> </li> </ul>","text":""},{"location":"BaseSessionStateBuilder/#getting-function-to-create-queryexecution-for-logicalplan","title":"Getting Function to Create QueryExecution For LogicalPlan <pre><code>createQueryExecution: LogicalPlan =&gt; QueryExecution\n</code></pre> <p><code>createQueryExecution</code> simply returns a function that takes a LogicalPlan and creates a QueryExecution with the SparkSession and the logical plan.</p> <p><code>createQueryExecution</code> is used when <code>BaseSessionStateBuilder</code> is requested to create a SessionState instance.</p>","text":""},{"location":"BaseSessionStateBuilder/#columnarrules-method","title":"columnarRules Method <pre><code>columnarRules: Seq[ColumnarRule]\n</code></pre> <p><code>columnarRules</code> requests the SparkSessionExtensions to buildColumnarRules.</p> <p><code>columnarRules</code> is used when <code>BaseSessionStateBuilder</code> is requested to build a SessionState instance.</p>","text":""},{"location":"BaseSessionStateBuilder/#customcheckrules","title":"customCheckRules <pre><code>customCheckRules: Seq[LogicalPlan =&gt; Unit]\n</code></pre> <p><code>customCheckRules</code> requests the SparkSessionExtensions to buildCheckRules on the SparkSession.</p> <p><code>customCheckRules</code> is used when:</p> <ul> <li><code>BaseSessionStateBuilder</code> is requested for an Analyzer</li> <li><code>HiveSessionStateBuilder</code> is requested for an Analyzer</li> </ul>","text":""},{"location":"BindReferences/","title":"BindReferences","text":"<p><code>BindReferences</code> utility allows to bind (resolve) one or multiple <code>AttributeReference</code>s.</p>"},{"location":"BindReferences/#binding-one-attributereference","title":"Binding One AttributeReference <pre><code>bindReference[A &lt;: Expression](\n  expression: A,\n  input: AttributeSeq,\n  allowFailures: Boolean = false): A\n</code></pre> <p>For every <code>AttributeReference</code> expression in the given <code>expression</code>, <code>bindReference</code> finds the <code>ExprId</code> in the <code>input</code> schema and creates a BoundReference expression.</p>  <p><code>bindReference</code> throws an <code>IllegalStateException</code> when an <code>AttributeReference</code> could not be found in the input schema:</p> <pre><code>Couldn't find [a] in [input]\n</code></pre>  <p><code>bindReference</code> is used when:</p> <ul> <li><code>ExpressionEncoder</code> is requested to resolveAndBind</li> <li><code>BindReferences</code> is used to bind multiple references</li> <li>others</li> </ul>","text":""},{"location":"BindReferences/#binding-multiple-attributereferences","title":"Binding Multiple AttributeReferences <pre><code>bindReferences[A &lt;: Expression](\n  expressions: Seq[A],\n  input: AttributeSeq): Seq[A]\n</code></pre> <p><code>bindReferences</code> bindReference of all the given <code>expressions</code> to the <code>input</code> schema.</p>","text":""},{"location":"BloomFilter/","title":"BloomFilter","text":"<p><code>BloomFilter</code> is an abstraction of bloom filters for the following:</p> <ul> <li>DataFrameStatFunctions.bloomFilter operator</li> <li>As an aggregation buffer in BloomFilterAggregate expression</li> <li>BloomFilterMightContain expression</li> </ul>"},{"location":"BloomFilter/#contract-subset","title":"Contract (Subset)","text":""},{"location":"BloomFilter/#mightcontain","title":"mightContain <pre><code>boolean mightContain(\n  Object item)\n</code></pre> <p>See BloomFilterImpl</p>  <p>Not Used</p> <p><code>mightContain</code> does not seem to be used (as mightContainLong seems to be used directly instead).</p>","text":""},{"location":"BloomFilter/#mightcontainlong","title":"mightContainLong <pre><code>boolean mightContainLong(\n  long item)\n</code></pre> <p>See BloomFilterImpl</p> <p>Used when:</p> <ul> <li><code>BloomFilterImpl</code> is requested to mightContain</li> <li><code>BloomFilterMightContain</code> is requested to eval and doGenCode</li> </ul>","text":""},{"location":"BloomFilter/#mightcontainstring","title":"mightContainString <pre><code>boolean mightContainString(\n  String item)\n</code></pre> <p>See BloomFilterImpl</p> <p>Used when:</p> <ul> <li><code>BloomFilterImpl</code> is requested to mightContain</li> </ul>","text":""},{"location":"BloomFilter/#implementations","title":"Implementations","text":"<ul> <li>BloomFilterImpl</li> </ul>"},{"location":"BloomFilter/#creating-bloomfilter","title":"Creating BloomFilter <pre><code>BloomFilter create(\n  long expectedNumItems)\nBloomFilter create(\n  long expectedNumItems,\n  double fpp)\nBloomFilter create(\n  long expectedNumItems,\n  long numBits)\n</code></pre> <p><code>create</code> creates a BloomFilterImpl for the given <code>expectedNumItems</code>.</p> <p>Unless the false positive probability is given, <code>create</code> uses DEFAULT_FPP value to determine the number of bits.</p>  <p><code>create</code> is used when:</p> <ul> <li><code>BloomFilterAggregate</code> is requested to create an aggregation buffer</li> <li><code>DataFrameStatFunctions</code> is requested to build a BloomFilter</li> </ul>","text":""},{"location":"BloomFilterImpl/","title":"BloomFilterImpl","text":"<p><code>BloomFilterImpl</code> is a BloomFilter.</p>"},{"location":"BloomFilterImpl/#creating-instance","title":"Creating Instance","text":"<p><code>BloomFilterImpl</code> takes the following to be created:</p> <ul> <li> numHashFunctions <li> Number of bits (to create a <code>BitArray</code> or <code>BitArray</code> directly) <p><code>BloomFilterImpl</code> is created when:</p> <ul> <li><code>BloomFilter</code> is requested to create a BloomFilter</li> </ul>"},{"location":"BloomFilterImpl/#mightcontainlong","title":"mightContainLong <pre><code>boolean mightContainLong(\n  long item)\n</code></pre> <p><code>mightContainLong</code> is part of the BloomFilter abstraction.</p>  <p><code>mightContainLong</code>...FIXME</p>","text":""},{"location":"CacheManager/","title":"CacheManager","text":"<p><code>CacheManager</code> is a registry of structured queries that are cached and supposed to be replaced with corresponding InMemoryRelation logical operators as their cached representation (when <code>QueryExecution</code> is requested for a logical query plan with cached data).</p>"},{"location":"CacheManager/#accessing-cachemanager","title":"Accessing CacheManager","text":"<p><code>CacheManager</code> is shared across SparkSessions through SharedState.</p> <pre><code>val spark: SparkSession = ...\nspark.sharedState.cacheManager\n</code></pre>"},{"location":"CacheManager/#datasetcache-and-persist-operators","title":"Dataset.cache and persist Operators","text":"<p>A structured query (as Dataset) can be cached and registered with <code>CacheManager</code> using Dataset.cache or Dataset.persist high-level operators.</p>"},{"location":"CacheManager/#cached-queries","title":"Cached Queries <pre><code>cachedData: LinkedList[CachedData]\n</code></pre> <p><code>CacheManager</code> uses the <code>cachedData</code> internal registry to manage cached structured queries as <code>CachedData</code> with InMemoryRelation leaf logical operators.</p> <p>A new <code>CachedData</code> is added when <code>CacheManager</code> is requested to:</p> <ul> <li>cacheQuery</li> <li>recacheByCondition</li> </ul> <p>A <code>CachedData</code> is removed when <code>CacheManager</code> is requested to:</p> <ul> <li>uncacheQuery</li> <li>recacheByCondition</li> </ul> <p>All <code>CachedData</code> are removed (cleared) when <code>CacheManager</code> is requested to clearCache</p>","text":""},{"location":"CacheManager/#re-caching-by-path","title":"Re-Caching By Path <pre><code>recacheByPath(\n  spark: SparkSession,\n  resourcePath: String): Unit\nrecacheByPath(\n  spark: SparkSession,\n  resourcePath: Path,\n  fs: FileSystem): Unit\n</code></pre> <p><code>recacheByPath</code>...FIXME</p> <p><code>recacheByPath</code> is used when:</p> <ul> <li><code>CatalogImpl</code> is requested to refreshByPath</li> <li>InsertIntoHadoopFsRelationCommand command is executed</li> </ul>","text":""},{"location":"CacheManager/#lookupandrefresh","title":"lookupAndRefresh <pre><code>lookupAndRefresh(\n  plan: LogicalPlan,\n  fs: FileSystem,\n  qualifiedPath: Path): Boolean\n</code></pre> <p><code>lookupAndRefresh</code>...FIXME</p>","text":""},{"location":"CacheManager/#refreshfileindexifnecessary","title":"refreshFileIndexIfNecessary <pre><code>refreshFileIndexIfNecessary(\n  fileIndex: FileIndex,\n  fs: FileSystem,\n  qualifiedPath: Path): Boolean\n</code></pre> <p><code>refreshFileIndexIfNecessary</code>...FIXME</p> <p><code>refreshFileIndexIfNecessary</code> is used when <code>CacheManager</code> is requested to lookupAndRefresh.</p>","text":""},{"location":"CacheManager/#looking-up-cacheddata","title":"Looking Up CachedData <pre><code>lookupCachedData(\n  query: Dataset[_]): Option[CachedData]\nlookupCachedData(\n  plan: LogicalPlan): Option[CachedData]\n</code></pre> <p><code>lookupCachedData</code>...FIXME</p> <p><code>lookupCachedData</code> is used when:</p> <ul> <li>Dataset.storageLevel basic action is used</li> <li><code>CatalogImpl</code> is requested to isCached</li> <li><code>CacheManager</code> is requested to cacheQuery and useCachedData</li> </ul>","text":""},{"location":"CacheManager/#un-caching-dataset","title":"Un-caching Dataset <pre><code>uncacheQuery(\n  query: Dataset[_],\n  cascade: Boolean,\n  blocking: Boolean = true): Unit\nuncacheQuery(\n  spark: SparkSession,\n  plan: LogicalPlan,\n  cascade: Boolean,\n  blocking: Boolean): Unit\n</code></pre> <p><code>uncacheQuery</code>...FIXME</p> <p><code>uncacheQuery</code> is used when:</p> <ul> <li>Dataset.unpersist basic action is used</li> <li><code>DropTableCommand</code> and TruncateTableCommand logical commands are executed</li> <li><code>CatalogImpl</code> is requested to uncache and refresh a table or view, dropTempView and dropGlobalTempView</li> </ul>","text":""},{"location":"CacheManager/#caching-query","title":"Caching Query <pre><code>cacheQuery(\n  query: Dataset[_],\n  tableName: Option[String] = None,\n  storageLevel: StorageLevel = MEMORY_AND_DISK): Unit\n</code></pre> <p><code>cacheQuery</code> adds the analyzed logical plan of the input Dataset to the cachedData internal registry of cached queries.</p> <p>Internally, <code>cacheQuery</code> requests the <code>Dataset</code> for the analyzed logical plan and creates a InMemoryRelation with the following:</p> <ul> <li>spark.sql.inMemoryColumnarStorage.compressed configuration property</li> <li>spark.sql.inMemoryColumnarStorage.batchSize configuration property</li> <li>Input <code>storageLevel</code> storage level</li> <li>Optimized physical query plan (after requesting <code>SessionState</code> to execute the analyzed logical plan)</li> <li>Input <code>tableName</code></li> <li>Statistics of the analyzed query plan</li> </ul> <p><code>cacheQuery</code> then creates a <code>CachedData</code> (for the analyzed query plan and the <code>InMemoryRelation</code>) and adds it to the cachedData internal registry.</p> <p>If the input <code>query</code> has already been cached, <code>cacheQuery</code> simply prints out the following WARN message to the logs and exits (i.e. does nothing but prints out the WARN message):</p> <pre><code>Asked to cache already cached data.\n</code></pre> <p><code>cacheQuery</code> is used when:</p> <ul> <li>Dataset.persist basic action is used</li> <li><code>CatalogImpl</code> is requested to cache and refresh a table or view in-memory</li> </ul>","text":""},{"location":"CacheManager/#clearing-cache","title":"Clearing Cache <pre><code>clearCache(): Unit\n</code></pre> <p><code>clearCache</code> takes every <code>CachedData</code> from the cachedData internal registry and requests it for the InMemoryRelation to access the CachedRDDBuilder. <code>clearCache</code> requests the <code>CachedRDDBuilder</code> to clearCache.</p> <p>In the end, <code>clearCache</code> removes all <code>CachedData</code> entries from the cachedData internal registry.</p> <p><code>clearCache</code> is used when <code>CatalogImpl</code> is requested to clear the cache.</p>","text":""},{"location":"CacheManager/#re-caching-query","title":"Re-Caching Query <pre><code>recacheByCondition(\n  spark: SparkSession,\n  condition: LogicalPlan =&gt; Boolean): Unit\n</code></pre> <p><code>recacheByCondition</code>...FIXME</p> <p><code>recacheByCondition</code> is used when <code>CacheManager</code> is requested to uncache a structured query, recacheByPlan, and recacheByPath.</p>","text":""},{"location":"CacheManager/#re-caching-by-logical-plan","title":"Re-Caching By Logical Plan <pre><code>recacheByPlan(\n  spark: SparkSession,\n  plan: LogicalPlan): Unit\n</code></pre> <p><code>recacheByPlan</code>...FIXME</p> <p><code>recacheByPlan</code> is used when InsertIntoDataSourceCommand logical command is executed.</p>","text":""},{"location":"CacheManager/#replacing-segments-of-logical-query-plan-with-cached-data","title":"Replacing Segments of Logical Query Plan With Cached Data <pre><code>useCachedData(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>useCachedData</code> traverses the given logical query plan down (parent operators first, children later) and replaces them with cached representation (i.e. InMemoryRelation) if found. <code>useCachedData</code> does this operator substitution for SubqueryExpression expressions, too.</p> <p><code>useCachedData</code> skips IgnoreCachedData commands (and leaves them unchanged).</p> <p><code>useCachedData</code> is used (recursively) when <code>QueryExecution</code> is requested for a logical query plan with cached data.</p>","text":""},{"location":"CacheManager/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.CacheManager</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.CacheManager=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"Catalog/","title":"Catalog \u2014 Metastore Management Interface","text":"<p><code>Catalog</code> is an abstraction of metadata catalogs for managing relational entities (e.g. database(s), tables, functions, table columns and temporary views).</p> <p><code>Catalog</code> is available using SparkSession.catalog property.</p> <pre><code>assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession])\nassert(spark.catalog.isInstanceOf[org.apache.spark.sql.catalog.Catalog])\n</code></pre>"},{"location":"Catalog/#contract","title":"Contract","text":""},{"location":"Catalog/#cachetable","title":"cacheTable <pre><code>cacheTable(\n  tableName: String): Unit\ncacheTable(\n  tableName: String,\n  storageLevel: StorageLevel): Unit\n</code></pre> <p>Caches a given table</p> <p>Used for SQL's CACHE TABLE and <code>AlterTableRenameCommand</code> command.</p>","text":""},{"location":"Catalog/#others","title":"Others","text":""},{"location":"Catalog/#implementations","title":"Implementations","text":"<ul> <li>CatalogImpl</li> </ul>"},{"location":"CatalogImpl/","title":"CatalogImpl","text":"<p><code>CatalogImpl</code> is a Catalog.</p> <p></p>"},{"location":"CatalogImpl/#creating-instance","title":"Creating Instance","text":"<p><code>CatalogImpl</code> takes the following to be created:</p> <ul> <li> SparkSession <p><code>CatalogImpl</code> is created when:</p> <ul> <li><code>SparkSession</code> is requested for the Catalog</li> </ul>"},{"location":"CatalogImpl/#listcolumns","title":"listColumns  Signature <pre><code>listColumns(\n  ident: Seq[String]): Dataset[Column]\nlistColumns(\n  tableName: String): Dataset[Column]\nlistColumns(\n  dbName: String,\n  tableName: String): Dataset[Column]\n</code></pre> <p><code>listColumns</code> is part of the Catalog abstraction.</p>  <p><code>listColumns</code>...FIXME</p> <p>In the end, <code>listColumns</code> makeDataset with the columns.</p>","text":""},{"location":"CatalogImpl/#gettable","title":"getTable  Signature <pre><code>getTable(\n  tableName: String): Table\ngetTable(\n  dbName: String,\n  tableName: String): Table\n</code></pre> <p><code>getTable</code> is part of the Catalog abstraction.</p>  <p><code>getTable</code>...FIXME</p>","text":""},{"location":"CatalogImpl/#looking-up-table","title":"Looking Up Table <pre><code>makeTable(\n  catalog: TableCatalog,\n  ident: Identifier): Option[Table]\n</code></pre> <p><code>makeTable</code>...FIXME</p>  <p><code>makeTable</code> is used when:</p> <ul> <li><code>CatalogImpl</code> is requested to listTables and getTable</li> </ul>","text":""},{"location":"CatalogImpl/#loadtable","title":"loadTable <pre><code>loadTable(\n  catalog: TableCatalog,\n  ident: Identifier): Option[Table]\n</code></pre> <p><code>loadTable</code>...FIXME</p>","text":""},{"location":"CatalogStatistics/","title":"CatalogStatistics","text":"<p><code>CatalogStatistics</code> represents the table- and column-level statistics (that are stored in SessionCatalog).</p>"},{"location":"CatalogStatistics/#creating-instance","title":"Creating Instance","text":"<p><code>CatalogStatistics</code> takes the following to be created:</p> <ul> <li> Physical total size (in bytes) <li> Estimated number of rows (row count) <li> Column statistics (column names and their CatalogColumnStats) <p><code>CatalogStatistics</code> is created when:</p> <ul> <li>AnalyzeColumnCommand and <code>AlterTableAddPartitionCommand</code> logical commands are executed (and alter table statistics in SessionCatalog)</li> <li><code>CommandUtils</code> is requested to update existing table statistics and current statistics (if changed)</li> <li><code>HiveExternalCatalog</code> is requested to convert Hive properties to Spark statistics</li> <li><code>HiveClientImpl</code> is requested to readHiveStats</li> <li>PruneHiveTablePartitions logical optimization is executed (and updates the table metadata)</li> <li>PruneFileSourcePartitions logical optimization is executed</li> </ul>"},{"location":"CatalogStatistics/#table-statistics","title":"Table Statistics","text":"<p><code>CatalogStatistics</code> is created with table statistics when:</p> <ul> <li><code>HiveExternalCatalog</code> is requested for table statistics (from a Hive metastore)</li> <li><code>HiveClientImpl</code> is requested to readHiveStats</li> </ul> <p><code>CatalogStatistics</code> is created to update table statistics for the following logical optimizations:</p> <ul> <li>PruneHiveTablePartitions</li> <li>PruneFileSourcePartitions</li> </ul> <p><code>CatalogStatistics</code> is created to alter table statistics (directly or indirectly using CommandUtils) for the following logical commands:</p> Logical Command SQL Statement AnalyzeColumnCommand ANALYZE TABLE FOR COLUMNS AnalyzePartitionCommand ANALYZE TABLE PARTITION AnalyzeTableCommand ANALYZE TABLE AnalyzeTablesCommand <code>ANALYZE TABLES</code> <code>AlterTableAddPartitionCommand</code> <code>ALTER TABLE ADD PARTITION</code> <code>AlterTableDropPartitionCommand</code> <code>ALTER TABLE DROP PARTITION</code> <code>AlterTableSetLocationCommand</code> <code>ALTER TABLE SET LOCATION</code> CreateDataSourceTableAsSelectCommand InsertIntoHadoopFsRelationCommand InsertIntoHiveTable LoadDataCommand TruncateTableCommand"},{"location":"CatalogStatistics/#catalogstatistics-and-statistics","title":"CatalogStatistics and Statistics","text":"<p><code>CatalogStatistics</code> are a \"subset\" of the statistics in Statistics (as there are no concepts of attributes and broadcast hints in Hive metastore).</p> <p><code>CatalogStatistics</code> are stored in a Hive metastore and are referred as Hive statistics while <code>Statistics</code> are Spark statistics.</p>"},{"location":"CatalogStatistics/#readable-textual-representation","title":"Readable Textual Representation <pre><code>simpleString: String\n</code></pre> <p><code>simpleString</code> is the following text (with the sizeInBytes and the optional rowCount if defined):</p> <pre><code>[sizeInBytes] bytes, [rowCount] rows\n</code></pre>  <p><code>simpleString</code> is used when:</p> <ul> <li><code>CatalogTablePartition</code> is requested to toLinkedHashMap</li> <li><code>CatalogTable</code> is requested to toLinkedHashMap</li> </ul>","text":""},{"location":"CatalogStatistics/#converting-metastore-statistics-to-spark-statistics","title":"Converting Metastore Statistics to Spark Statistics <pre><code>toPlanStats(\n  planOutput: Seq[Attribute],\n  cboEnabled: Boolean): Statistics\n</code></pre> <p><code>toPlanStats</code> converts the table statistics (from an external metastore) to Spark statistics.</p> <p>With cost-based optimization enabled and row count statistics available, <code>toPlanStats</code> creates a Statistics with the estimated total (output) size, row count and column statistics.</p> <p>Otherwise (when cost-based optimization is disabled), <code>toPlanStats</code> creates a Statistics with just the mandatory sizeInBytes.</p>  <p>Note</p> <p><code>toPlanStats</code> does the reverse of HiveExternalCatalog.statsToProperties.</p>   <p><code>toPlanStats</code> is used when:</p> <ul> <li>HiveTableRelation and LogicalRelation are requested for statistics</li> </ul>","text":""},{"location":"CatalogStatistics/#demo","title":"Demo <pre><code>// Using higher-level interface to access CatalogStatistics\n// Make sure that you ran ANALYZE TABLE (as described above)\nval db = spark.catalog.currentDatabase\nval tableName = \"t1\"\nval metadata = spark.sharedState.externalCatalog.getTable(db, tableName)\nval stats = metadata.stats\n</code></pre> <pre><code>scala&gt; :type stats\nOption[org.apache.spark.sql.catalyst.catalog.CatalogStatistics]\n</code></pre> <pre><code>val tid = spark.sessionState.sqlParser.parseTableIdentifier(tableName)\nval metadata = spark.sessionState.catalog.getTempViewOrPermanentTableMetadata(tid)\nval stats = metadata.stats\nassert(stats.isInstanceOf[Option[org.apache.spark.sql.catalyst.catalog.CatalogStatistics]])\n</code></pre> <pre><code>stats.map(_.simpleString).foreach(println)\n// 714 bytes, 2 rows\n</code></pre>","text":""},{"location":"CatalogStorageFormat/","title":"CatalogStorageFormat","text":"<p>[[creating-instance]] <code>CatalogStorageFormat</code> is the storage specification of a partition or a table, i.e. the metadata that includes the following:</p> <ul> <li>[[locationUri]] Location URI (Java URI)</li> <li>[[inputFormat]] Input format</li> <li>[[outputFormat]] Output format</li> <li>[[serde]] SerDe</li> <li>[[compressed]] <code>compressed</code> flag</li> <li>[[properties]] Properties (as <code>Map[String, String]</code>)</li> </ul> <p><code>CatalogStorageFormat</code> is &lt;&gt; when: <ul> <li> <p><code>HiveClientImpl</code> is requested for metadata of a table or table partition</p> </li> <li> <p><code>SparkSqlAstBuilder</code> is requested to parse Hive-specific CREATE TABLE or INSERT OVERWRITE DIRECTORY SQL statements</p> </li> </ul> <p>[[toString]] <code>CatalogStorageFormat</code> uses the following text representation (i.e. <code>toString</code>)...FIXME</p> <p>=== [[toLinkedHashMap]] Converting Storage Specification to LinkedHashMap -- <code>toLinkedHashMap</code> Method</p>"},{"location":"CatalogStorageFormat/#source-scala","title":"[source, scala]","text":""},{"location":"CatalogStorageFormat/#tolinkedhashmap-mutablelinkedhashmapstring-string","title":"toLinkedHashMap: mutable.LinkedHashMap[String, String]","text":"<p><code>toLinkedHashMap</code>...FIXME</p> <p><code>toLinkedHashMap</code> is used when:</p> <ul> <li><code>CatalogStorageFormat</code> is requested for a text representation</li> <li><code>CatalogTablePartition</code> is requested for toLinkedHashMap</li> <li><code>CatalogTable</code> is requested for toLinkedHashMap</li> <li>DescribeTableCommand is executed</li> </ul>"},{"location":"CatalogTable/","title":"CatalogTable","text":"<p><code>CatalogTable</code> is the specification (metadata) of a table in a SessionCatalog.</p>"},{"location":"CatalogTable/#creating-instance","title":"Creating Instance","text":"<p><code>CatalogTable</code> takes the following to be created:</p> <ul> <li> <code>TableIdentifier</code> <li>Table type</li> <li> CatalogStorageFormat <li> Schema (StructType) <li> Name of the table provider <li> Partition Columns <li>Bucketing specification</li> <li> Owner <li> Created Time <li> Last access time <li> Created By version <li> Table Properties <li>Statistics</li> <li> View Text <li> Comment <li> Unsupported Features (<code>Seq[String]</code>) <li> <code>tracksPartitionsInCatalog</code> flag (default: <code>false</code>) <li> <code>schemaPreservesCase</code> flag (default: <code>true</code>) <li> Ignored properties <li> View Original Text <p><code>CatalogTable</code> is created when:</p> <ul> <li><code>HiveClientImpl</code> is requested to convertHiveTableToCatalogTable</li> <li>InsertIntoHiveDirCommand is executed</li> <li><code>DataFrameWriter</code> is requested to create a table</li> <li><code>ResolveSessionCatalog</code> logical resolution rule is requested to buildCatalogTable</li> <li><code>CreateTableLikeCommand</code> is executed</li> <li><code>CreateViewCommand</code> is requested to prepareTable</li> <li><code>ViewHelper</code> utility is used to <code>prepareTemporaryView</code> and <code>prepareTemporaryViewStoringAnalyzedPlan</code></li> <li><code>V2SessionCatalog</code> is requested to createTable</li> <li><code>CatalogImpl</code> is requested to createTable</li> </ul>"},{"location":"CatalogTable/#bucketing-specification","title":"Bucketing Specification <pre><code>bucketSpec: Option[BucketSpec] = None\n</code></pre> <p><code>CatalogTable</code> can be given a BucketSpec when created. It is undefined (<code>None</code>) by default.</p> <p><code>BucketSpec</code> is given (using getBucketSpecFromTableProperties from a Hive metastore) when:</p> <ul> <li><code>HiveExternalCatalog</code> is requested to restoreHiveSerdeTable and restoreDataSourceTable</li> <li><code>HiveClientImpl</code> is requested to convertHiveTableToCatalogTable</li> </ul> <p><code>BucketSpec</code> is given when:</p> <ul> <li><code>DataFrameWriter</code> is requested to create a table (with getBucketSpec)</li> <li><code>ResolveSessionCatalog</code> logical resolution rule is requested to buildCatalogTable</li> <li><code>CreateTableLikeCommand</code> is executed (with a bucketed table)</li> <li><code>PreprocessTableCreation</code> logical resolution rule is requested to normalizeCatalogTable</li> <li><code>V2SessionCatalog</code> is requested to create a table</li> </ul> <p><code>BucketSpec</code> is used when:</p> <ul> <li><code>CatalogTable</code> is requested to toLinkedHashMap</li> <li><code>V1Table</code> is requested for the partitioning</li> <li>CreateDataSourceTableCommand is executed</li> <li>CreateDataSourceTableAsSelectCommand is requested to saveDataIntoTable</li> <li>others</li> </ul>  <p>Note</p> <ol> <li>Use DescribeTableCommand to review <code>BucketSpec</code></li> <li>Use ShowCreateTableCommand to review the Spark DDL syntax</li> <li>Use Catalog.listColumns to list all columns (incl. bucketing columns)</li> </ol>","text":""},{"location":"CatalogTable/#table-type","title":"Table Type <p><code>CatalogTable</code> is given a <code>CatalogTableType</code> when created:</p> <ul> <li><code>EXTERNAL</code> for external tables (EXTERNAL_TABLE in Hive)</li> <li><code>MANAGED</code> for managed tables (MANAGED_TABLE in Hive)</li> <li><code>VIEW</code> for views (VIRTUAL_VIEW in Hive)</li> </ul> <p><code>CatalogTableType</code> is included when a <code>TreeNode</code> is requested for a JSON representation for...FIXME</p>","text":""},{"location":"CatalogTable/#statistics","title":"Statistics <pre><code>stats: Option[CatalogStatistics] = None\n</code></pre> <p><code>CatalogTable</code> can be given a CatalogStatistics when created. It is undefined (<code>None</code>) by default.</p> <p><code>CatalogTable</code> can be displayed using the following commands (when executed with <code>EXTENDED</code> or <code>FORMATTED</code> clause):</p> <ul> <li>DescribeTableCommand (<code>DESCRIBE TABLE</code> SQL statement)</li> <li>DescribeColumnCommand (<code>DESCRIBE TABLE</code> with a column specified)</li> </ul> <p>The <code>CatalogStatistics</code> can be defined when:</p> <ul> <li><code>InMemoryCatalog</code> is requested to alterTableStats</li> <li><code>HiveExternalCatalog</code> is requested to restore a table metadata</li> <li><code>HiveClientImpl</code> is requested to convertHiveTableToCatalogTable</li> <li>PruneHiveTablePartitions logical optimization is executed (and requested to update a table metadata)</li> <li>PruneFileSourcePartitions logical optimization is executed</li> </ul> <p>The <code>CatalogStatistics</code> is used when:</p> <ul> <li><code>DataSource</code> is requested to resolve a Relation (of type FileFormat that uses a CatalogFileIndex)</li> <li><code>HiveTableRelation</code> is requested to computeStats (with spark.sql.cbo.enabled or spark.sql.cbo.planStats.enabled enabled)</li> <li><code>LogicalRelation</code> is requested to computeStats (with spark.sql.cbo.enabled or spark.sql.cbo.planStats.enabled enabled)</li> </ul> <p>The <code>CatalogStatistics</code> is updated (altered) when:</p> <ul> <li><code>AnalyzeColumnCommand</code> is requested to analyzeColumnInCatalog</li> <li><code>CommandUtils</code> is requested to updateTableStats, analyzeTable</li> <li><code>AlterTableAddPartitionCommand</code> is executed</li> </ul> <p><code>CatalogStatistics</code> is Statistics in toLinkedHashMap.</p>","text":""},{"location":"CatalogTable/#tolinkedhashmap","title":"toLinkedHashMap <pre><code>toLinkedHashMap: LinkedHashMap[String, String]\n</code></pre> <p><code>toLinkedHashMap</code>...FIXME</p>  <p><code>toLinkedHashMap</code> is used when:</p> <ul> <li><code>CatalogTable</code> is requested to toString and simpleString</li> <li>DescribeTableCommand is executed (and describeFormattedTableInfo)</li> </ul>","text":""},{"location":"CatalogTable/#demo-accessing-table-metadata","title":"Demo: Accessing Table Metadata","text":""},{"location":"CatalogTable/#catalog","title":"Catalog","text":"<pre><code>val q = spark.catalog.listTables.filter($\"name\" === \"t1\")\n</code></pre> <pre><code>scala&gt; q.show\n+----+--------+-----------+---------+-----------+\n|name|database|description|tableType|isTemporary|\n+----+--------+-----------+---------+-----------+\n|  t1| default|       null|  MANAGED|      false|\n+----+--------+-----------+---------+-----------+\n</code></pre>"},{"location":"CatalogTable/#sessioncatalog","title":"SessionCatalog","text":"<pre><code>import org.apache.spark.sql.catalyst.catalog.SessionCatalog\nval sessionCatalog = spark.sessionState.catalog\nassert(sessionCatalog.isInstanceOf[SessionCatalog])\n</code></pre> <pre><code>val t1Tid = spark.sessionState.sqlParser.parseTableIdentifier(\"t1\")\nval t1Metadata = sessionCatalog.getTempViewOrPermanentTableMetadata(t1Tid)\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.catalog.CatalogTable\nassert(t1Metadata.isInstanceOf[CatalogTable])\n</code></pre>"},{"location":"CatalogTablePartition/","title":"CatalogTablePartition","text":"<p><code>CatalogTablePartition</code> is the partition specification of a table, i.e. the metadata of the partitions of a table.</p> <p><code>CatalogTablePartition</code> is &lt;&gt; when: <ul> <li><code>HiveClientImpl</code> is requested to hive/HiveClientImpl.md#fromHivePartition[retrieve a table partition metadata]</li> </ul> <p><code>CatalogTablePartition</code> can hold the &lt;&gt; that...FIXME <p>[[simpleString]] The readable text representation of a <code>CatalogTablePartition</code> (aka <code>simpleString</code>) is...FIXME</p> <p>NOTE: <code>simpleString</code> is used exclusively when <code>ShowTablesCommand</code> is executed (with a partition specification).</p> <p>[[toString]] <code>CatalogTablePartition</code> uses the following text representation (i.e. <code>toString</code>)...FIXME</p>"},{"location":"CatalogTablePartition/#creating-instance","title":"Creating Instance","text":"<p><code>CatalogTablePartition</code> takes the following when created:</p> <ul> <li>[[spec]] Partition specification</li> <li>[[storage]] CatalogStorageFormat</li> <li>[[parameters]] Parameters (default: an empty collection)</li> <li>[[stats]] Table statistics (default: <code>None</code>)</li> </ul> <p>=== [[toLinkedHashMap]] Converting Partition Specification to LinkedHashMap -- <code>toLinkedHashMap</code> Method</p>"},{"location":"CatalogTablePartition/#source-scala","title":"[source, scala]","text":""},{"location":"CatalogTablePartition/#tolinkedhashmap-mutablelinkedhashmapstring-string","title":"toLinkedHashMap: mutable.LinkedHashMap[String, String]","text":"<p><code>toLinkedHashMap</code> converts the partition specification to a collection of pairs (<code>LinkedHashMap[String, String]</code>) with the following fields and their values:</p> <ul> <li>Partition Values with the &lt;&gt; <li>Storage specification (of the given CatalogStorageFormat)</li> <li>Partition Parameters with the &lt;&gt; (if not empty) <li>Partition Statistics with the &lt;&gt; (if available)"},{"location":"CatalogTablePartition/#note","title":"[NOTE]","text":"<p><code>toLinkedHashMap</code> is used when:</p> <ul> <li><code>DescribeTableCommand</code> logical command is &lt;&gt; (with the DescribeTableCommand.md#isExtended[isExtended] flag on and a non-empty DescribeTableCommand.md#partitionSpec[partitionSpec])."},{"location":"CatalogTablePartition/#catalogtablepartition-is-requested-for-either-a-or-a-text-representation","title":"* <code>CatalogTablePartition</code> is requested for either a &lt;&gt; or a &lt;&gt; text representation <p>=== [[location]] <code>location</code> Method</p>","text":""},{"location":"CatalogTablePartition/#source-scala_1","title":"[source, scala]","text":""},{"location":"CatalogTablePartition/#location-uri","title":"location: URI","text":"<p><code>location</code> simply returns the location URI of the CatalogStorageFormat or throws an <code>AnalysisException</code>:</p> <pre><code>Partition [[specString]] did not specify locationUri\n</code></pre> <p>NOTE: <code>location</code> is used when...FIXME</p>"},{"location":"CatalogUtils/","title":"CatalogUtils","text":""},{"location":"CatalogUtils/#normalizing-bucketspec","title":"Normalizing BucketSpec <pre><code>normalizeBucketSpec(\n  tableName: String,\n  tableCols: Seq[String],\n  bucketSpec: BucketSpec,\n  resolver: Resolver): BucketSpec\n</code></pre> <p><code>normalizeBucketSpec</code>...FIXME</p>  <p><code>normalizeBucketSpec</code> is used when:</p> <ul> <li>PreprocessTableCreation logical analysis rule is executed (on a bucketed table while appending data to a CreateTable)</li> </ul>","text":""},{"location":"CatalystSerde/","title":"CatalystSerde Utility","text":""},{"location":"CatalystSerde/#creating-deserializer","title":"Creating Deserializer <pre><code>deserialize[T : Encoder](\n  child: LogicalPlan): DeserializeToObject\n</code></pre> <p><code>deserialize</code> creates a DeserializeToObject logical operator for the given <code>child</code> logical plan.</p>  <p>Internally, <code>deserialize</code> creates an <code>UnresolvedDeserializer</code> for the deserializer for the type <code>T</code> first and passes it on to a <code>DeserializeToObject</code> with an <code>AttributeReference</code> (being the result of generateObjAttr).</p>  <p><code>deserialize</code> is used when:</p> <ul> <li><code>Dataset</code> is requested for a QueryExecution</li> <li><code>ExpressionEncoder</code> is requested to resolveAndBind</li> <li><code>MapPartitions</code> utility is used to apply</li> <li><code>MapElements</code> utility is used to <code>apply</code></li> <li>(Catalyst DSL) deserialize operator is used</li> </ul>","text":""},{"location":"CatalystSerde/#creating-serializer","title":"Creating Serializer <pre><code>serialize[T : Encoder](\n  child: LogicalPlan): SerializeFromObject\n</code></pre> <p><code>serialize</code> tries to find the ExpressionEncoder for the type <code>T</code> and requests it for serializer expressions.</p> <p>In the end, creates a <code>SerializeFromObject</code> logical operator with the serializer and the given <code>child</code> logical operator.</p>","text":""},{"location":"CatalystTypeConverters/","title":"CatalystTypeConverters Helper Object","text":"<p><code>CatalystTypeConverters</code> is a Scala object that is used to convert Scala types to Catalyst types and vice versa.</p>"},{"location":"CatalystTypeConverters/#creating-catalyst-converter","title":"Creating Catalyst Converter <pre><code>createToCatalystConverter(\n  dataType: DataType): Any =&gt; Any\n</code></pre> <p><code>createToCatalystConverter</code>...FIXME</p> <p><code>createToCatalystConverter</code> is used when:</p> <ul> <li>FIXME</li> </ul> <p>=== [[convertToCatalyst]] <code>convertToCatalyst</code> Method</p>","text":""},{"location":"CatalystTypeConverters/#source-scala","title":"[source, scala]","text":""},{"location":"CatalystTypeConverters/#converttocatalysta-any-any","title":"convertToCatalyst(a: Any): Any <p><code>convertToCatalyst</code>...FIXME</p> <p>NOTE: <code>convertToCatalyst</code> is used when...FIXME</p>","text":""},{"location":"CheckAnalysis/","title":"CheckAnalysis \u2014 Analysis Validation","text":"<p><code>CheckAnalysis</code>\u00a0is an extension of the PredicateHelper abstraction for logical analysis checkers that can check analysis phase.</p> <p><code>CheckAnalysis</code> defines extendedCheckRules extension point for extra analysis check rules.</p> <p>Note</p> <p>Only after analysis a logical query plan is correct and ready for execution.</p>"},{"location":"CheckAnalysis/#contract","title":"Contract","text":""},{"location":"CheckAnalysis/#isview","title":"isView <pre><code>isView(\n  nameParts: Seq[String]): Boolean\n</code></pre> <p>Used when:</p> <ul> <li><code>CheckAnalysis</code> is requested to check analysis (of a logical plan with <code>UnresolvedV2Relation</code>s)</li> </ul>","text":""},{"location":"CheckAnalysis/#implementations","title":"Implementations","text":"<ul> <li>Analyzer</li> </ul>"},{"location":"CheckAnalysis/#extendedcheckrules-extension-point","title":"extendedCheckRules Extension Point <pre><code>extendedCheckRules: Seq[LogicalPlan =&gt; Unit] = Nil\n</code></pre> <p><code>CheckAnalysis</code> allows implementations for extra analysis check rules using <code>extendedCheckRules</code> extension point.</p> <p>Used after the built-in check rules have been evaluated.</p>","text":""},{"location":"CheckAnalysis/#checking-analysis-phase","title":"Checking Analysis Phase <pre><code>checkAnalysis(\n  plan: LogicalPlan): Unit\n</code></pre> <p><code>checkAnalysis</code> checks (asserts) whether the given logical plan is correct using built-in and extended validation rules followed by marking it as analyzed.</p> <p><code>checkAnalysis</code> traverses the operators from their children first (up the operator chain).</p> <p><code>checkAnalysis</code> skips analysis if the plan has already been analyzed.</p>","text":""},{"location":"CheckAnalysis/#unresolved-operators","title":"Unresolved Operators <p><code>checkAnalysis</code> checks whether the plan has any logical plans unresolved. If so, <code>checkAnalysis</code> fails the analysis with the following error message:</p> <pre><code>unresolved operator [o.simpleString]\n</code></pre>","text":""},{"location":"CheckAnalysis/#logical-plan-analyzed","title":"Logical Plan Analyzed <p>In the end, <code>checkAnalysis</code> marks the entire logical plan as analyzed.</p>","text":""},{"location":"CheckAnalysis/#usage","title":"Usage <p><code>checkAnalysis</code>\u00a0is used when:</p> <ul> <li><code>Analyzer</code> is requested to executeAndCheck</li> <li>ResolveRelations logical resolution rule is executed (and resolveViews)</li> <li>ResolveAggregateFunctions logical resolution rule is executed</li> <li><code>CheckAnalysis</code> is requested to checkSubqueryExpression</li> <li>Catalyst DSL's analyze operator is used</li> <li><code>ExpressionEncoder</code> is requested to resolveAndBind</li> <li>RelationalGroupedDataset.as operator is used</li> </ul>","text":""},{"location":"CheckAnalysis/#checkshowpartitions","title":"checkShowPartitions <pre><code>checkShowPartitions(\n  showPartitions: ShowPartitions): Unit\n</code></pre> <p><code>checkShowPartitions</code> branches off based on the input <code>ShowPartitions</code>.</p> <p>For <code>ShowPartitions</code> with ResolvedTable child with a Table that is not SupportsPartitionManagement, <code>checkShowPartitions</code> failAnalysis with the following message:</p> <pre><code>SHOW PARTITIONS cannot run for a table which does not support partitioning\n</code></pre> <p>For <code>ShowPartitions</code> with ResolvedTable child with a Table that is SupportsPartitionManagement with no partitions, <code>checkShowPartitions</code> failAnalysis with the following message:</p> <pre><code>SHOW PARTITIONS is not allowed on a table that is not partitioned: [name]\n</code></pre>","text":""},{"location":"Column/","title":"Column","text":""},{"location":"ColumnarRule/","title":"ColumnarRule","text":"<p><code>ColumnarRule</code> is an abstraction to hold preColumnarTransitions and postColumnarTransitions user-defined rules to inject columnar physical operators to a query plan (using SparkSessionExtensions).</p> <p>Note</p> <p>ApplyColumnarRulesAndInsertTransitions physical optimization is used to execute <code>ColumnarRule</code>s.</p>"},{"location":"ColumnarRule/#contract","title":"Contract","text":""},{"location":"ColumnarRule/#precolumnartransitions","title":"preColumnarTransitions <pre><code>preColumnarTransitions: Rule[SparkPlan]\n</code></pre> <p>Used when ApplyColumnarRulesAndInsertTransitions physical optimization is executed.</p>","text":""},{"location":"ColumnarRule/#postcolumnartransitions","title":"postColumnarTransitions <pre><code>postColumnarTransitions: Rule[SparkPlan]\n</code></pre> <p>Used when ApplyColumnarRulesAndInsertTransitions physical optimization is executed.</p>","text":""},{"location":"CommandUtils/","title":"CommandUtils","text":"<p><code>CommandUtils</code> is a helper class for logical commands to manage table statistics.</p>"},{"location":"CommandUtils/#analyzing-table","title":"Analyzing Table <pre><code>analyzeTable(\n  sparkSession: SparkSession,\n  tableIdent: TableIdentifier,\n  noScan: Boolean): Unit\n</code></pre> <p><code>analyzeTable</code> requests the SessionCatalog for the table metadata.</p> <p><code>analyzeTable</code> branches off based on the type of the table: a view and the other types.</p>","text":""},{"location":"CommandUtils/#views","title":"Views <p>For <code>CatalogTableType.VIEW</code>s, <code>analyzeTable</code> requests the CacheManager to lookupCachedData. If available and the given <code>noScan</code> flag is disabled, <code>analyzeTable</code> requests the table to <code>count</code> the number of rows (that materializes the underlying columnar RDD).</p>","text":""},{"location":"CommandUtils/#other-table-types","title":"Other Table Types <p>For other types, <code>analyzeTable</code> calculateTotalSize for the table. With the given <code>noScan</code> flag disabled, <code>analyzeTable</code> creates a <code>DataFrame</code> for the table and <code>count</code>s the number of rows (that triggers a Spark job). In case the table stats have changed, <code>analyzeTable</code> requests the SessionCatalog to alterTableStats.</p>","text":""},{"location":"CommandUtils/#usage","title":"Usage <p><code>analyzeTable</code> is used when the following commands are executed:</p> <ul> <li>AnalyzeTableCommand</li> <li>AnalyzeTablesCommand</li> </ul>","text":""},{"location":"CommandUtils/#updating-existing-table-statistics","title":"Updating Existing Table Statistics <pre><code>updateTableStats(\n  sparkSession: SparkSession,\n  table: CatalogTable): Unit\n</code></pre> <p><code>updateTableStats</code> updates the table statistics of the input CatalogTable (only if the statistics are available in the metastore already).</p>  <p><code>updateTableStats</code> requests SessionCatalog to alterTableStats with the &lt;&gt; (when spark.sql.statistics.size.autoUpdate.enabled property is enabled) or empty statistics (that effectively removes the recorded statistics completely).  <p>Important</p> <p><code>updateTableStats</code> uses spark.sql.statistics.size.autoUpdate.enabled property to auto-update table statistics and can be expensive (and slow down data change commands) if the total number of files of a table is very large.</p>   <p><code>updateTableStats</code> is used when the following logical commands are executed:</p> <ul> <li><code>AlterTableAddPartitionCommand</code></li> <li><code>AlterTableDropPartitionCommand</code></li> <li><code>AlterTableSetLocationCommand</code></li> <li>CreateDataSourceTableAsSelectCommand</li> <li>InsertIntoHadoopFsRelationCommand</li> <li>InsertIntoHiveTable</li> <li><code>LoadDataCommand</code></li> <li><code>TruncateTableCommand</code></li> </ul>","text":""},{"location":"CommandUtils/#compareandgetnewstats","title":"compareAndGetNewStats <pre><code>compareAndGetNewStats(\n  oldStats: Option[CatalogStatistics],\n  newTotalSize: BigInt,\n  newRowCount: Option[BigInt]): Option[CatalogStatistics]\n</code></pre> <p><code>compareAndGetNewStats</code>...FIXME</p>  <p><code>compareAndGetNewStats</code> is used when:</p> <ul> <li>AnalyzePartitionCommand is executed</li> <li><code>CommandUtils</code> is requested to analyze a table</li> </ul>","text":""},{"location":"CommandUtils/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.command.CommandUtils</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.command.CommandUtils=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"CompressionCodecs/","title":"CompressionCodecs","text":"<p><code>CompressionCodecs</code> is a utility object...FIXME</p> <p>[[getCodecClassName]] [[shortCompressionCodecNames]] .Known Compression Codecs [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Alias | Fully-Qualified Class Name</p> <code>none</code> <code>uncompressed</code> <p>| <code>bzip2</code> | <code>org.apache.hadoop.io.compress.BZip2Codec</code></p> <p>| <code>deflate</code> | <code>org.apache.hadoop.io.compress.DeflateCodec</code></p> <p>| <code>gzip</code> | <code>org.apache.hadoop.io.compress.GzipCodec</code></p> <p>| <code>lz4</code> | <code>org.apache.hadoop.io.compress.Lz4Codec</code></p> <p>| <code>snappy</code> | <code>org.apache.hadoop.io.compress.SnappyCodec</code> |===</p> <p>=== [[setCodecConfiguration]] <code>setCodecConfiguration</code> Method</p>"},{"location":"CompressionCodecs/#source-scala","title":"[source, scala]","text":""},{"location":"CompressionCodecs/#setcodecconfigurationconf-configuration-codec-string-unit","title":"setCodecConfiguration(conf: Configuration, codec: String): Unit","text":"<p><code>setCodecConfiguration</code> sets compression-related configurations to the Hadoop <code>Configuration</code> per the input <code>codec</code>.</p> <p>NOTE: The input <code>codec</code> should be a fully-qualified class name, i.e. <code>org.apache.hadoop.io.compress.SnappyCodec</code>.</p> <p>If the input <code>codec</code> is defined (i.e. not <code>null</code>), <code>setCodecConfiguration</code> sets the following &lt;&gt;. <p>[[setCodecConfiguration-codec]] .Compression-Related Hadoop Configuration Properties (codec defined) [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Name | Value</p> <p>| <code>mapreduce.output.fileoutputformat.compress</code> | <code>true</code></p> <p>| <code>mapreduce.output.fileoutputformat.compress.type</code> | <code>BLOCK</code></p> <p>| <code>mapreduce.output.fileoutputformat.compress.codec</code> | The input <code>codec</code> name</p> <p>| <code>mapreduce.map.output.compress</code> | <code>true</code></p> <p>| <code>mapreduce.map.output.compress.codec</code> | The input <code>codec</code> name |===</p> <p>If the input <code>codec</code> is not defined (i.e. <code>null</code>), <code>setCodecConfiguration</code> sets the following &lt;&gt;. <p>[[setCodecConfiguration-codec-undefined]] .Compression-Related Hadoop Configuration Properties (codec not defined) [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Name | Value</p> <p>| <code>mapreduce.output.fileoutputformat.compress</code> | <code>false</code></p> <p>| <code>mapreduce.map.output.compress</code> | <code>false</code> |===</p> <p><code>setCodecConfiguration</code> is used when <code>CSVFileFormat</code>, <code>JsonFileFormat</code> and <code>TextFileFormat</code> are requested to <code>prepareWrite</code>.</p>"},{"location":"CreatableRelationProvider/","title":"CreatableRelationProvider","text":"<p><code>CreatableRelationProvider</code> is an abstraction of data source providers that can save data and create a BaseRelation.</p>"},{"location":"CreatableRelationProvider/#contract","title":"Contract","text":""},{"location":"CreatableRelationProvider/#createrelation","title":"createRelation <pre><code>createRelation(\n  sqlContext: SQLContext,\n  mode: SaveMode,\n  parameters: Map[String, String],\n  data: DataFrame): BaseRelation\n</code></pre> <p>Saves the given <code>DataFrame</code> to this data source (and creates a BaseRelation to represent the relation)</p> <p>The <code>SaveMode</code> specifies what should happen when the target relation (destination) already exists.</p> <p>Used when:</p> <ul> <li><code>DataSource</code> is requested to writeAndRead</li> <li><code>SaveIntoDataSourceCommand</code> logical command is requested to run</li> </ul>","text":""},{"location":"CreatableRelationProvider/#implementations","title":"Implementations","text":"<ul> <li><code>ConsoleSinkProvider</code></li> <li>JdbcRelationProvider</li> <li>KafkaSourceProvider</li> </ul>"},{"location":"CreateTableWriter/","title":"CreateTableWriter","text":"<p><code>CreateTableWriter</code> is an extension of the WriteConfigMethods abstraction for table writers.</p>"},{"location":"CreateTableWriter/#contract","title":"Contract","text":""},{"location":"CreateTableWriter/#create","title":"create","text":"<pre><code>create(): Unit\n</code></pre> <p>Creates a new table from the contents of the dataframe</p>"},{"location":"CreateTableWriter/#createorreplace","title":"createOrReplace","text":"<pre><code>createOrReplace(): Unit\n</code></pre> <p>Creates a new table or replaces an existing table with the contents of the dataframe</p>"},{"location":"CreateTableWriter/#partitionedby","title":"partitionedBy","text":"<pre><code>partitionedBy(\ncolumn: Column,\ncolumns: Column*): CreateTableWriter[T]\n</code></pre> <p>Defines partition(s) of the output table</p>"},{"location":"CreateTableWriter/#replace","title":"replace","text":"<pre><code>replace(): Unit\n</code></pre> <p>Replaces an existing table with the contents of the dataframe</p>"},{"location":"CreateTableWriter/#tableproperty","title":"tableProperty","text":"<pre><code>tableProperty(\nproperty: String,\nvalue: String): CreateTableWriter[T]\n</code></pre> <p>Adds a table property</p>"},{"location":"CreateTableWriter/#using","title":"using","text":"<pre><code>using(\nprovider: String): CreateTableWriter[T]\n</code></pre> <p>Specifies the provider for the underlying output data source</p>"},{"location":"CreateTableWriter/#implementations","title":"Implementations","text":"<ul> <li>DataFrameWriterV2</li> </ul>"},{"location":"DataFrame/","title":"DataFrame \u2014 Dataset of Rows with RowEncoder","text":"<p>Spark SQL introduces a tabular functional data abstraction called DataFrame. It is designed to ease developing Spark applications for processing large amount of structured tabular data on Spark infrastructure.</p> <p>DataFrame is a data abstraction or a domain-specific language (DSL) for working with structured and semi-structured data, i.e. datasets that you can specify a schema for.</p> <p><code>DataFrame</code> is a collection of Rows with a schema that is the result of executing a structured query (once it will have been executed).</p> <p>DataFrame uses the immutable, in-memory, resilient, distributed and parallel capabilities of spark-rdd.md[RDD], and applies a structure called schema to the data.</p>"},{"location":"DataFrame/#note","title":"[NOTE]","text":"<p>In Spark 2.0.0 <code>DataFrame</code> is a mere type alias for <code>Dataset[Row]</code>.</p>"},{"location":"DataFrame/#source-scala","title":"[source, scala]","text":""},{"location":"DataFrame/#type-dataframe-datasetrow","title":"type DataFrame = Dataset[Row]","text":""},{"location":"DataFrame/#see-httpsgithubcomapachesparkblobmastersqlcoresrcmainscalaorgapachesparksqlpackagescalal45orgapachesparkpackagescala","title":"See https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/package.scala#L45[org.apache.spark.package.scala].","text":"<p><code>DataFrame</code> is a distributed collection of tabular data organized into rows and named columns. It is conceptually equivalent to a table in a relational database with operations to project (<code>select</code>), <code>filter</code>, <code>intersect</code>, <code>join</code>, <code>group</code>, <code>sort</code>, <code>join</code>, <code>aggregate</code>, or <code>convert</code> to a RDD (consult https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame[DataFrame API])</p>"},{"location":"DataFrame/#source-scala_1","title":"[source, scala]","text":""},{"location":"DataFrame/#datagroupbyproduct_idsumscore","title":"data.groupBy('Product_ID).sum('Score)","text":"<p>Spark SQL borrowed the concept of DataFrame from http://pandas.pydata.org/pandas-docs/stable/dsintro.html[pandas' DataFrame] and made it immutable, parallel (one machine, perhaps with many processors and cores) and distributed (many machines, perhaps with many processors and cores).</p> <p>NOTE: Hey, big data consultants, time to help teams migrate the code from pandas' DataFrame into Spark's DataFrames (at least to PySpark's DataFrame) and offer services to set up large clusters!</p> <p>DataFrames in Spark SQL strongly rely on spark-rdd.md[the features of RDD] - it's basically a RDD exposed as structured DataFrame by appropriate operations to handle very big data from the day one. So, petabytes of data should not scare you (unless you're an administrator to create such clustered Spark environment - book-intro.md[contact me when you feel alone with the task]).</p>"},{"location":"DataFrame/#source-scala_2","title":"[source, scala]","text":"<p>val df = Seq((\"one\", 1), (\"one\", 1), (\"two\", 1))   .toDF(\"word\", \"count\")</p> <p>scala&gt; df.show +----+-----+ |word|count| +----+-----+ | one|    1| | one|    1| | two|    1| +----+-----+</p> <p>val counted = df.groupBy('word).count</p> <p>scala&gt; counted.show +----+-----+ |word|count| +----+-----+ | two|    1| | one|    2| +----+-----+</p> <p>You can create DataFrames by &lt;&gt;. You can also create DataFrames from scratch and build upon them (as in the above example). See https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame[DataFrame API]. You can read any format given you have appropriate Spark SQL extension of DataFrameReader to format the dataset appropriately. <p>CAUTION: FIXME Diagram of reading data from sources to create DataFrame</p> <p>You can execute queries over DataFrames using two approaches:</p> <ul> <li>&lt;&gt; - helps migrating from \"SQL databases\" world into the world of DataFrame in Spark SQL <li>&lt;&gt; - an API that helps ensuring proper syntax at compile time. <p><code>DataFrame</code> also allows you to do the following tasks:</p> <ul> <li>&lt;&gt; <p>DataFrames use the Catalyst logical query optimizer to produce efficient queries (and so they are supposed to be faster than corresponding RDD-based queries).</p> <p>NOTE: Your DataFrames can also be type-safe and moreover further improve their performance through specialized encoders that can significantly cut serialization and deserialization times.</p> <p>You can enforce types on generic rows and hence bring type safety (at compile time) by &lt;Dataset object&gt;&gt;. As of Spark 2.0 it is a preferred way of developing Spark applications."},{"location":"DataFrame/#features-of-dataframe","title":"Features of DataFrame","text":"<p>A <code>DataFrame</code> is a collection of \"generic\" Row instances (as <code>RDD[Row]</code>) and a schema.</p> <p>NOTE: Regardless of how you create a <code>DataFrame</code>, it will always be a pair of <code>RDD[Row]</code> and StructType.</p>"},{"location":"DataFrame/#sqlcontext-spark-and-spark-shell","title":"SQLContext, spark, and Spark shell","text":"<p>You use https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext[org.apache.spark.sql.SQLContext] to build DataFrames and execute SQL queries.</p> <p>The quickest and easiest way to work with Spark SQL is to use spark-shell.md[Spark shell] and <code>spark</code> object.</p> <pre><code>scala&gt; spark\nres1: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@60ae950f\n</code></pre> <p>As you may have noticed, <code>spark</code> in Spark shell is actually a  https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.hive.HiveContext[org.apache.spark.sql.hive.HiveContext] that integrates the Spark SQL execution engine with data stored in https://hive.apache.org/[Apache Hive].</p> <p>The Apache Hive\u2122 data warehouse software facilitates querying and managing large datasets residing in distributed storage.</p> <p>=== Creating DataFrames from Scratch</p> <p>Use Spark shell as described in spark-shell.md[Spark shell].</p> <p>==== Using toDF</p> <p>After you <code>import spark.implicits._</code> (which is done for you by Spark shell) you may apply <code>toDF</code> method to convert objects to DataFrames.</p>"},{"location":"DataFrame/#source-scala_3","title":"[source, scala]","text":"<p>scala&gt; val df = Seq(\"I am a DataFrame!\").toDF(\"text\") df: org.apache.spark.sql.DataFrame = [text: string]</p> <p>==== Creating DataFrame using Case Classes in Scala</p> <p>This method assumes the data comes from a Scala case class that will describe the schema.</p>"},{"location":"DataFrame/#source-scala_4","title":"[source, scala]","text":"<p>scala&gt; case class Person(name: String, age: Int) defined class Person</p> <p>scala&gt; val people = Seq(Person(\"Jacek\", 42), Person(\"Patryk\", 19), Person(\"Maksym\", 5)) people: Seq[Person] = List(Person(Jacek,42), Person(Patryk,19), Person(Maksym,5))</p> <p>scala&gt; val df = spark.createDataFrame(people) df: org.apache.spark.sql.DataFrame = [name: string, age: int]</p> <p>scala&gt; df.show +------+---+ |  name|age| +------+---+ | Jacek| 42| |Patryk| 19| |Maksym|  5| +------+---+</p> <p>==== Custom DataFrame Creation using createDataFrame</p> <p>https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext[SQLContext] offers a family of <code>createDataFrame</code> operations.</p> <pre><code>scala&gt; val lines = sc.textFile(\"Cartier+for+WinnersCurse.csv\")\nlines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at textFile at &lt;console&gt;:24\n\nscala&gt; val headers = lines.first\nheaders: String = auctionid,bid,bidtime,bidder,bidderrate,openbid,price\n\nscala&gt; import org.apache.spark.sql.types.{StructField, StringType}\nimport org.apache.spark.sql.types.{StructField, StringType}\n\nscala&gt; val fs = headers.split(\",\").map(f =&gt; StructField(f, StringType))\nfs: Array[org.apache.spark.sql.types.StructField] = Array(StructField(auctionid,StringType,true), StructField(bid,StringType,true), StructField(bidtime,StringType,true), StructField(bidder,StringType,true), StructField(bidderrate,StringType,true), StructField(openbid,StringType,true), StructField(price,StringType,true))\n\nscala&gt; import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructType\n\nscala&gt; val schema = StructType(fs)\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(auctionid,StringType,true), StructField(bid,StringType,true), StructField(bidtime,StringType,true), StructField(bidder,StringType,true), StructField(bidderrate,StringType,true), StructField(openbid,StringType,true), StructField(price,StringType,true))\n\nscala&gt; val noheaders = lines.filter(_ != header)\nnoheaders: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[10] at filter at &lt;console&gt;:33\n\nscala&gt; import org.apache.spark.sql.Row\nimport org.apache.spark.sql.Row\n\nscala&gt; val rows = noheaders.map(_.split(\",\")).map(a =&gt; Row.fromSeq(a))\nrows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[12] at map at &lt;console&gt;:35\n\nscala&gt; val auctions = spark.createDataFrame(rows, schema)\nauctions: org.apache.spark.sql.DataFrame = [auctionid: string, bid: string, bidtime: string, bidder: string, bidderrate: string, openbid: string, price: string]\n\nscala&gt; auctions.printSchema\nroot\n |-- auctionid: string (nullable = true)\n |-- bid: string (nullable = true)\n |-- bidtime: string (nullable = true)\n |-- bidder: string (nullable = true)\n |-- bidderrate: string (nullable = true)\n |-- openbid: string (nullable = true)\n |-- price: string (nullable = true)\n\nscala&gt; auctions.dtypes\nres28: Array[(String, String)] = Array((auctionid,StringType), (bid,StringType), (bidtime,StringType), (bidder,StringType), (bidderrate,StringType), (openbid,StringType), (price,StringType))\n\nscala&gt; auctions.show(5)\n+----------+----+-----------+-----------+----------+-------+-----+\n| auctionid| bid|    bidtime|     bidder|bidderrate|openbid|price|\n+----------+----+-----------+-----------+----------+-------+-----+\n|1638843936| 500|0.478368056|  kona-java|       181|    500| 1625|\n|1638843936| 800|0.826388889|     doc213|        60|    500| 1625|\n|1638843936| 600|3.761122685|       zmxu|         7|    500| 1625|\n|1638843936|1500|5.226377315|carloss8055|         5|    500| 1625|\n|1638843936|1600|   6.570625|    jdrinaz|         6|    500| 1625|\n+----------+----+-----------+-----------+----------+-------+-----+\nonly showing top 5 rows\n</code></pre> <p>=== Loading data from structured files</p> <p>==== Creating DataFrame from CSV file</p> <p>Let's start with an example in which schema inference relies on a custom case class in Scala.</p> <pre><code>scala&gt; val lines = sc.textFile(\"Cartier+for+WinnersCurse.csv\")\nlines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at textFile at &lt;console&gt;:24\n\nscala&gt; val header = lines.first\nheader: String = auctionid,bid,bidtime,bidder,bidderrate,openbid,price\n\nscala&gt; lines.count\nres3: Long = 1349\n\nscala&gt; case class Auction(auctionid: String, bid: Float, bidtime: Float, bidder: String, bidderrate: Int, openbid: Float, price: Float)\ndefined class Auction\n\nscala&gt; val noheader = lines.filter(_ != header)\nnoheader: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[53] at filter at &lt;console&gt;:31\n\nscala&gt; val auctions = noheader.map(_.split(\",\")).map(r =&gt; Auction(r(0), r(1).toFloat, r(2).toFloat, r(3), r(4).toInt, r(5).toFloat, r(6).toFloat))\nauctions: org.apache.spark.rdd.RDD[Auction] = MapPartitionsRDD[59] at map at &lt;console&gt;:35\n\nscala&gt; val df = auctions.toDF\ndf: org.apache.spark.sql.DataFrame = [auctionid: string, bid: float, bidtime: float, bidder: string, bidderrate: int, openbid: float, price: float]\n\nscala&gt; df.printSchema\nroot\n |-- auctionid: string (nullable = true)\n |-- bid: float (nullable = false)\n |-- bidtime: float (nullable = false)\n |-- bidder: string (nullable = true)\n |-- bidderrate: integer (nullable = false)\n |-- openbid: float (nullable = false)\n |-- price: float (nullable = false)\n\nscala&gt; df.show\n+----------+------+----------+-----------------+----------+-------+------+\n| auctionid|   bid|   bidtime|           bidder|bidderrate|openbid| price|\n+----------+------+----------+-----------------+----------+-------+------+\n|1638843936| 500.0|0.47836804|        kona-java|       181|  500.0|1625.0|\n|1638843936| 800.0| 0.8263889|           doc213|        60|  500.0|1625.0|\n|1638843936| 600.0| 3.7611227|             zmxu|         7|  500.0|1625.0|\n|1638843936|1500.0| 5.2263775|      carloss8055|         5|  500.0|1625.0|\n|1638843936|1600.0|  6.570625|          jdrinaz|         6|  500.0|1625.0|\n|1638843936|1550.0| 6.8929167|      carloss8055|         5|  500.0|1625.0|\n|1638843936|1625.0| 6.8931136|      carloss8055|         5|  500.0|1625.0|\n|1638844284| 225.0|  1.237419|dre_313@yahoo.com|         0|  200.0| 500.0|\n|1638844284| 500.0| 1.2524074|        njbirdmom|        33|  200.0| 500.0|\n|1638844464| 300.0| 1.8111342|          aprefer|        58|  300.0| 740.0|\n|1638844464| 305.0| 3.2126737|        19750926o|         3|  300.0| 740.0|\n|1638844464| 450.0| 4.1657987|         coharley|        30|  300.0| 740.0|\n|1638844464| 450.0| 6.7363195|        adammurry|         5|  300.0| 740.0|\n|1638844464| 500.0| 6.7364697|        adammurry|         5|  300.0| 740.0|\n|1638844464|505.78| 6.9881945|        19750926o|         3|  300.0| 740.0|\n|1638844464| 551.0| 6.9896526|        19750926o|         3|  300.0| 740.0|\n|1638844464| 570.0| 6.9931483|        19750926o|         3|  300.0| 740.0|\n|1638844464| 601.0| 6.9939003|        19750926o|         3|  300.0| 740.0|\n|1638844464| 610.0|  6.994965|        19750926o|         3|  300.0| 740.0|\n|1638844464| 560.0| 6.9953704|            ps138|         5|  300.0| 740.0|\n+----------+------+----------+-----------------+----------+-------+------+\nonly showing top 20 rows\n</code></pre> <p>==== Creating DataFrame from CSV files using spark-csv module</p> <p>You're going to use https://github.com/databricks/spark-csv[spark-csv] module to load data from a CSV data source that handles proper parsing and loading.</p> <p>NOTE: Support for CSV data sources is available by default in Spark 2.0.0. No need for an external module.</p> <p>Start the Spark shell using <code>--packages</code> option as follows:</p> <pre><code>\u279c  spark git:(master) \u2717 ./bin/spark-shell --packages com.databricks:spark-csv_2.11:1.2.0\nIvy Default Cache set to: /Users/jacek/.ivy2/cache\nThe jars for the packages stored in: /Users/jacek/.ivy2/jars\n:: loading settings :: url = jar:file:/Users/jacek/dev/oss/spark/assembly/target/scala-2.11/spark-assembly-1.5.0-SNAPSHOT-hadoop2.7.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.11 added as a dependency\n\nscala&gt; val df = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"Cartier+for+WinnersCurse.csv\")\ndf: org.apache.spark.sql.DataFrame = [auctionid: string, bid: string, bidtime: string, bidder: string, bidderrate: string, openbid: string, price: string]\n\nscala&gt; df.printSchema\nroot\n |-- auctionid: string (nullable = true)\n |-- bid: string (nullable = true)\n |-- bidtime: string (nullable = true)\n |-- bidder: string (nullable = true)\n |-- bidderrate: string (nullable = true)\n |-- openbid: string (nullable = true)\n |-- price: string (nullable = true)\n\n scala&gt; df.show\n +----------+------+-----------+-----------------+----------+-------+-----+\n | auctionid|   bid|    bidtime|           bidder|bidderrate|openbid|price|\n +----------+------+-----------+-----------------+----------+-------+-----+\n |1638843936|   500|0.478368056|        kona-java|       181|    500| 1625|\n |1638843936|   800|0.826388889|           doc213|        60|    500| 1625|\n |1638843936|   600|3.761122685|             zmxu|         7|    500| 1625|\n |1638843936|  1500|5.226377315|      carloss8055|         5|    500| 1625|\n |1638843936|  1600|   6.570625|          jdrinaz|         6|    500| 1625|\n |1638843936|  1550|6.892916667|      carloss8055|         5|    500| 1625|\n |1638843936|  1625|6.893113426|      carloss8055|         5|    500| 1625|\n |1638844284|   225|1.237418982|dre_313@yahoo.com|         0|    200|  500|\n |1638844284|   500|1.252407407|        njbirdmom|        33|    200|  500|\n |1638844464|   300|1.811134259|          aprefer|        58|    300|  740|\n |1638844464|   305|3.212673611|        19750926o|         3|    300|  740|\n |1638844464|   450|4.165798611|         coharley|        30|    300|  740|\n |1638844464|   450|6.736319444|        adammurry|         5|    300|  740|\n |1638844464|   500|6.736469907|        adammurry|         5|    300|  740|\n |1638844464|505.78|6.988194444|        19750926o|         3|    300|  740|\n |1638844464|   551|6.989652778|        19750926o|         3|    300|  740|\n |1638844464|   570|6.993148148|        19750926o|         3|    300|  740|\n |1638844464|   601|6.993900463|        19750926o|         3|    300|  740|\n |1638844464|   610|6.994965278|        19750926o|         3|    300|  740|\n |1638844464|   560| 6.99537037|            ps138|         5|    300|  740|\n +----------+------+-----------+-----------------+----------+-------+-----+\n only showing top 20 rows\n</code></pre> <p>==== [[read]] Reading Data from External Data Sources (read method)</p> <p>You can create DataFrames by loading data from structured files (JSON, Parquet, CSV), RDDs, tables in Hive, or external databases (JDBC) using https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext[SQLContext.read] method.</p>"},{"location":"DataFrame/#source-scala_5","title":"[source, scala]","text":""},{"location":"DataFrame/#read-dataframereader","title":"read: DataFrameReader","text":"<p><code>read</code> returns a DataFrameReader instance.</p> <p>Among the supported structured data (file) formats are (consult Specifying Data Format (format method) for <code>DataFrameReader</code>):</p> <ul> <li>JSON</li> <li>parquet</li> <li>JDBC</li> <li>ORC</li> <li>Tables in Hive and any JDBC-compliant database</li> <li>libsvm</li> </ul> <pre><code>val reader = spark.read\nr: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@59e67a18\n\nreader.parquet(\"file.parquet\")\nreader.json(\"file.json\")\nreader.format(\"libsvm\").load(\"sample_libsvm_data.txt\")\n</code></pre> <p>=== Querying DataFrame</p> <p>NOTE: Spark SQL offers a &lt;&gt;. <p>==== [[query-using-dsl]] Using Query DSL</p> <p>You can select specific columns using <code>select</code> method.</p> <p>NOTE: This variant (in which you use stringified column names) can only select existing columns, i.e. you cannot create new ones using select expressions.</p> <pre><code>scala&gt; predictions.printSchema\nroot\n |-- id: long (nullable = false)\n |-- topic: string (nullable = true)\n |-- text: string (nullable = true)\n |-- label: double (nullable = true)\n |-- words: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- features: vector (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = true)\n\nscala&gt; predictions.select(\"label\", \"words\").show\n+-----+-------------------+\n|label|              words|\n+-----+-------------------+\n|  1.0|     [hello, math!]|\n|  0.0| [hello, religion!]|\n|  1.0|[hello, phy, ic, !]|\n+-----+-------------------+\n</code></pre> <pre><code>scala&gt; auctions.groupBy(\"bidder\").count().show(5)\n+--------------------+-----+\n|              bidder|count|\n+--------------------+-----+\n|    dennisthemenace1|    1|\n|            amskymom|    5|\n| nguyenat@san.rr.com|    4|\n|           millyjohn|    1|\n|ykelectro@hotmail...|    2|\n+--------------------+-----+\nonly showing top 5 rows\n</code></pre> <p>In the following example you query for the top 5 of the most active bidders.</p> <p>Note the tiny <code>$</code> and <code>desc</code> together with the column name to sort the rows by.</p> <pre><code>scala&gt; auctions.groupBy(\"bidder\").count().sort($\"count\".desc).show(5)\n+------------+-----+\n|      bidder|count|\n+------------+-----+\n|    lass1004|   22|\n|  pascal1666|   19|\n|     freembd|   17|\n|restdynamics|   17|\n|   happyrova|   17|\n+------------+-----+\nonly showing top 5 rows\n\nscala&gt; import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.functions._\n\nscala&gt; auctions.groupBy(\"bidder\").count().sort(desc(\"count\")).show(5)\n+------------+-----+\n|      bidder|count|\n+------------+-----+\n|    lass1004|   22|\n|  pascal1666|   19|\n|     freembd|   17|\n|restdynamics|   17|\n|   happyrova|   17|\n+------------+-----+\nonly showing top 5 rows\n</code></pre> <pre><code>scala&gt; df.select(\"auctionid\").distinct.count\nres88: Long = 97\n\nscala&gt; df.groupBy(\"bidder\").count.show\n+--------------------+-----+\n|              bidder|count|\n+--------------------+-----+\n|    dennisthemenace1|    1|\n|            amskymom|    5|\n| nguyenat@san.rr.com|    4|\n|           millyjohn|    1|\n|ykelectro@hotmail...|    2|\n|   shetellia@aol.com|    1|\n|              rrolex|    1|\n|            bupper99|    2|\n|           cheddaboy|    2|\n|             adcc007|    1|\n|           varvara_b|    1|\n|            yokarine|    4|\n|          steven1328|    1|\n|              anjara|    2|\n|              roysco|    1|\n|lennonjasonmia@ne...|    2|\n|northwestportland...|    4|\n|             bosspad|   10|\n|        31strawberry|    6|\n|          nana-tyler|   11|\n+--------------------+-----+\nonly showing top 20 rows\n</code></pre> <p>==== [[query-using-sql]][[registerTempTable]] Using SQL</p> <p>Register a DataFrame as a named temporary table to run SQL.</p>"},{"location":"DataFrame/#sourcescala","title":"[source,scala]","text":"<p>scala&gt; df.registerTempTable(\"auctions\") // &lt;1&gt;</p> <p>scala&gt; val sql = spark.sql(\"SELECT count(*) AS count FROM auctions\") sql: org.apache.spark.sql.DataFrame = [count: bigint]</p> <p>&lt;1&gt; Register a temporary table so SQL queries make sense</p> <p>You can execute a SQL query on a DataFrame using <code>sql</code> operation, but before the query is executed it is optimized by Catalyst query optimizer. You can print the physical plan for a DataFrame using the <code>explain</code> operation.</p> <pre><code>scala&gt; sql.explain\n== Physical Plan ==\nTungstenAggregate(key=[], functions=[(count(1),mode=Final,isDistinct=false)], output=[count#148L])\n TungstenExchange SinglePartition\n  TungstenAggregate(key=[], functions=[(count(1),mode=Partial,isDistinct=false)], output=[currentCount#156L])\n   TungstenProject\n    Scan PhysicalRDD[auctionid#49,bid#50,bidtime#51,bidder#52,bidderrate#53,openbid#54,price#55]\n\nscala&gt; sql.show\n+-----+\n|count|\n+-----+\n| 1348|\n+-----+\n\nscala&gt; val count = sql.collect()(0).getLong(0)\ncount: Long = 1348\n</code></pre> <p>=== [[filter]] Filtering</p>"},{"location":"DataFrame/#source-scala_6","title":"[source, scala]","text":"<p>scala&gt; df.show +----+---------+-----+ |name|productId|score| +----+---------+-----+ | aaa|      100| 0.12| | aaa|      200| 0.29| | bbb|      200| 0.53| | bbb|      300| 0.42| +----+---------+-----+</p> <p>scala&gt; df.filter($\"name\".like(\"a%\")).show +----+---------+-----+ |name|productId|score| +----+---------+-----+ | aaa|      100| 0.12| | aaa|      200| 0.29| +----+---------+-----+</p> <p>=== Handling data in Avro format</p> <p>Use custom serializer using http://spark-packages.org/package/databricks/spark-avro[spark-avro].</p> <p>Run Spark shell with <code>--packages com.databricks:spark-avro_2.11:2.0.0</code> (see https://github.com/databricks/spark-avro/issues/85[2.0.0 artifact is not in any public maven repo] why <code>--repositories</code> is required).</p> <pre><code>./bin/spark-shell --packages com.databricks:spark-avro_2.11:2.0.0 --repositories \"http://dl.bintray.com/databricks/maven\"\n</code></pre> <p>And then...</p> <pre><code>val fileRdd = sc.textFile(\"README.md\")\nval df = fileRdd.toDF\n\nimport org.apache.spark.sql.SaveMode\n\nval outputF = \"test.avro\"\ndf.write.mode(SaveMode.Append).format(\"com.databricks.spark.avro\").save(outputF)\n</code></pre> <p>See https://spark.apache.org/docs/latest/api/java/index.html#org.apache.spark.sql.SaveMode[org.apache.spark.sql.SaveMode] (and perhaps https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SaveMode[org.apache.spark.sql.SaveMode] from Scala's perspective).</p> <pre><code>val df = spark.read.format(\"com.databricks.spark.avro\").load(\"test.avro\")\n</code></pre> <p>=== Example Datasets</p> <ul> <li>http://www.modelingonlineauctions.com/datasets[eBay online auctions]</li> <li>https://data.sfgov.org/Public-Safety/SFPD-Incidents-from-1-January-2003/tmnf-yvry[SFPD Crime Incident Reporting system]</li> </ul>"},{"location":"DataFrameReader/","title":"DataFrameReader","text":"<p><code>DataFrameReader[T]</code> is a high-level API for Spark SQL developers to describe the \"read path\" of a structured query (over rows of <code>T</code> type).</p> <p><code>DataFrameReader</code> is used to describe an input node in a data processing graph.</p> <p><code>DataFrameReader</code> is used to describe the input data source format to be used to \"load\" data from a data source (e.g. files, Hive tables, JDBC or <code>Dataset[String]</code>).</p> <p><code>DataFrameReader</code> merely describes a process of loading a data (load specification) and does not trigger a Spark job (until an action is called unlike DataFrameWriter).</p> <p><code>DataFrameReader</code> is available using SparkSession.read operator.</p>"},{"location":"DataFrameReader/#creating-instance","title":"Creating Instance","text":"<p><code>DataFrameReader</code> takes the following to be created:</p> <ul> <li> SparkSession"},{"location":"DataFrameReader/#demo","title":"Demo","text":"<pre><code>import org.apache.spark.sql.SparkSession\nassert(spark.isInstanceOf[SparkSession])\n\nval reader = spark.read\n\nimport org.apache.spark.sql.DataFrameReader\nassert(reader.isInstanceOf[DataFrameReader])\n</code></pre>"},{"location":"DataFrameReader/#format","title":"format <pre><code>format(\n  source: String): DataFrameReader\n</code></pre> <p><code>format</code> specifies the input data source format.</p> <p>Built-in data source formats:</p> <ul> <li><code>json</code></li> <li><code>csv</code></li> <li><code>parquet</code></li> <li><code>orc</code></li> <li><code>text</code></li> <li><code>jdbc</code></li> <li><code>libsvm</code></li> </ul> <p>Use spark.sql.sources.default configuration property to specify the default format.</p>","text":""},{"location":"DataFrameReader/#loading-data","title":"Loading Data <pre><code>load(): DataFrame\nload(\n  path: String): DataFrame\nload(\n  paths: String*): DataFrame\n</code></pre> <p><code>load</code> loads a dataset from a data source (with optional support for multiple <code>paths</code>) as an untyped DataFrame.</p> <p>Internally, <code>load</code> lookupDataSource for the data source format. <code>load</code> then branches off per its type (i.e. whether it is of <code>DataSourceV2</code> marker type or not).</p> <p>For a \"DataSource V2\" data source, <code>load</code>...FIXME</p> <p>Otherwise, if the source is not a \"DataSource V2\" data source, <code>load</code> loadV1Source.</p> <p><code>load</code> throws a <code>AnalysisException</code> when the source is <code>hive</code>.</p> <pre><code>Hive data source can only be used with tables, you can not read files of Hive data source directly.\n</code></pre>","text":""},{"location":"DataFrameReader/#loadv1source","title":"loadV1Source <pre><code>loadV1Source(\n  paths: String*): DataFrame\n</code></pre> <p><code>loadV1Source</code> creates a DataSource and requests it to resolve the underlying relation (as a BaseRelation).</p> <p>In the end, <code>loadV1Source</code> requests the SparkSession to create a DataFrame from the BaseRelation.</p>","text":""},{"location":"DataFrameStatFunctions/","title":"DataFrameStatFunctions","text":"<p><code>DataFrameStatFunctions</code> API gives the statistic functions to be used in a structured query.</p>"},{"location":"DataFrameStatFunctions/#bloomfilter","title":"bloomFilter <pre><code>bloomFilter(\n  colName: String,\n  expectedNumItems: Long,\n  fpp: Double): BloomFilter\nbloomFilter(\n  col: Column,\n  expectedNumItems: Long,\n  fpp: Double): BloomFilter\nbloomFilter(\n  colName: String,\n  expectedNumItems: Long,\n  numBits: Long): BloomFilter\nbloomFilter(\n  col: Column,\n  expectedNumItems: Long,\n  numBits: Long): BloomFilter\n</code></pre> <p><code>bloomFilter</code> builds a BloomFilter.</p>","text":""},{"location":"DataFrameStatFunctions/#building-bloomfilter","title":"Building BloomFilter <pre><code>buildBloomFilter(\n  col: Column, expectedNumItems: Long,\n  numBits: Long,\n  fpp: Double): BloomFilter\n</code></pre> <p><code>buildBloomFilter</code>...FIXME</p>","text":""},{"location":"DataFrameWriter/","title":"DataFrameWriter","text":"<p><code>DataFrameWriter[T]</code> is a high-level API for Spark SQL developers to describe \"write path\" of a structured query (over rows of <code>T</code> type).</p> <p><code>DataFrameWriter</code> is used to describe an output node in a data processing graph.</p> <p><code>DataFrameWriter</code> is used to describe the output data source format to be used to \"save\" data to a data source (e.g. files, Hive tables, JDBC or <code>Dataset[String]</code>).</p> <p><code>DataFrameWriter</code> ends description of a write specification and does trigger a Spark job (unlike DataFrameWriter).</p> <p><code>DataFrameWriter</code> is available using Dataset.write operator.</p>"},{"location":"DataFrameWriter/#creating-instance","title":"Creating Instance","text":"<p><code>DataFrameWriter</code> takes the following to be created:</p> <ul> <li> Dataset"},{"location":"DataFrameWriter/#demo","title":"Demo","text":"<pre><code>assert(df.isInstanceOf[Dataset[_]])\n\nval writer = df.write\n\nimport org.apache.spark.sql.DataFrameWriter\nassert(writer.isInstanceOf[DataFrameWriter])\n</code></pre>"},{"location":"DataFrameWriter/#dataframe","title":"DataFrame <p>When created, <code>DataFrameWriter</code> converts the Dataset to a DataFrame.</p>","text":""},{"location":"DataFrameWriter/#name-of-data-source","title":"Name of Data Source <pre><code>source: String\n</code></pre> <p><code>source</code> is a short name (alias) or a fully-qualified class name to identify the data source to write data to.</p> <p><code>source</code> can be specified using <code>format</code> method:</p> <pre><code>format(\n  source: String): DataFrameWriter[T]\n</code></pre> <p>Default: spark.sql.sources.default configuration property</p>","text":""},{"location":"DataFrameWriter/#insertinto","title":"insertInto <pre><code>insertInto(\n  tableName: String): Unit\n</code></pre> <p><code>insertInto</code> requests the DataFrame for the SparkSession.</p> <p><code>insertInto</code> tries to look up the TableProvider for the data source.</p> <p><code>insertInto</code> requests the ParserInterface to parse the <code>tableName</code> identifier (possibly multi-part).</p> <p>In the end, <code>insertInto</code> uses the modern or the legacy insert paths based on...FIXME</p> <p></p> <p><code>insertInto</code> asserts that write is not bucketed (with insertInto operation name).</p>  <p>Note</p> <p>saveAsTable and insertInto are structurally alike.</p>","text":""},{"location":"DataFrameWriter/#modern-insert-path-catalogplugin","title":"Modern Insert Path (CatalogPlugin) <pre><code>insertInto(\n  catalog: CatalogPlugin,\n  ident: Identifier): Unit\n</code></pre> <p><code>insertInto</code>...FIXME</p>","text":""},{"location":"DataFrameWriter/#legacy-insert-path-tableidentifier","title":"Legacy Insert Path (TableIdentifier) <pre><code>insertInto(\n  tableIdent: TableIdentifier): Unit\n</code></pre> <p><code>insertInto</code>...FIXME</p>","text":""},{"location":"DataFrameWriter/#analysisexception","title":"AnalysisException <p><code>insertInto</code> throws an <code>AnalysisException</code> when the partitioningColumns are defined:</p> <pre><code>insertInto() can't be used together with partitionBy(). Partition columns have already been defined for the table. It is not necessary to use partitionBy().\n</code></pre>","text":""},{"location":"DataFrameWriter/#saveastable","title":"saveAsTable <pre><code>saveAsTable(\n  tableName: String): Unit\n</code></pre> <p><code>saveAsTable</code> requests the DataFrame for the SparkSession.</p> <p><code>saveAsTable</code> tries to look up the TableProvider for the data source.</p> <p><code>saveAsTable</code> requests the ParserInterface to parse the <code>tableName</code> identifier (possibly multi-part).</p> <p>In the end, <code>saveAsTable</code> uses the modern or the legacy save paths based on...FIXME</p>  <p>Note</p> <p>saveAsTable and insertInto are structurally alike.</p>","text":""},{"location":"DataFrameWriter/#modern-saveastable-with-tablecatalog","title":"Modern saveAsTable with TableCatalog <pre><code>saveAsTable(\n  catalog: TableCatalog,\n  ident: Identifier,\n  nameParts: Seq[String]): Unit\n</code></pre>","text":""},{"location":"DataFrameWriter/#legacy-saveastable-with-tableidentifier","title":"Legacy saveAsTable with TableIdentifier <pre><code>saveAsTable(\n  tableIdent: TableIdentifier): Unit\n</code></pre> <p><code>saveAsTable</code> saves the content of a <code>DataFrame</code> to the <code>tableName</code> table.</p>","text":""},{"location":"DataFrameWriter/#analysisexception_1","title":"AnalysisException <p><code>saveAsTable</code> throws an <code>AnalysisException</code> when no catalog could handle the table identifier:</p> <pre><code>Couldn't find a catalog to handle the identifier [tableName].\n</code></pre>","text":""},{"location":"DataFrameWriter/#demo_1","title":"Demo <pre><code>val ids = spark.range(5)\nids.write.\n  option(\"path\", \"/tmp/five_ids\").\n  saveAsTable(\"five_ids\")\n\n// Check out if saveAsTable as five_ids was successful\nval q = spark.catalog.listTables.filter($\"name\" === \"five_ids\")\nscala&gt; q.show\n+--------+--------+-----------+---------+-----------+\n|    name|database|description|tableType|isTemporary|\n+--------+--------+-----------+---------+-----------+\n|five_ids| default|       null| EXTERNAL|      false|\n+--------+--------+-----------+---------+-----------+\n</code></pre>","text":""},{"location":"DataFrameWriter/#writing-out-data-save","title":"Writing Out Data (save) <pre><code>save(): Unit\nsave(\n  path: String): Unit\n</code></pre> <p>Saves a <code>DataFrame</code> (the result of executing a structured query) to a data source.</p> <p>Internally, <code>save</code> uses <code>DataSource</code> to look up the class of the requested data source (for the source option and the SQLConf).</p>  <p>Note</p> <p><code>save</code> uses SparkSession to access the SessionState and in turn the SQLConf.</p> <pre><code>val df: DataFrame = ???\ndf.sparkSession.sessionState.conf\n</code></pre>  <p><code>save</code>...FIXME</p> <p><code>save</code> throws an <code>AnalysisException</code> when requested to save to Hive data source (the source is <code>hive</code>):</p> <pre><code>Hive data source can only be used with tables, you can not write files of Hive data source directly.\n</code></pre> <p><code>save</code> throws an <code>AnalysisException</code> when bucketing is used (the numBuckets or sortColumnNames options are defined):</p> <pre><code>'[operation]' does not support bucketing right now\n</code></pre>","text":""},{"location":"DataFrameWriter/#saveinternal","title":"saveInternal <pre><code>saveInternal(\n  path: Option[String]): Unit\n</code></pre> <p><code>saveInternal</code>...FIXME</p>","text":""},{"location":"DataFrameWriter/#looking-up-tableprovider","title":"Looking up TableProvider <pre><code>lookupV2Provider(): Option[TableProvider]\n</code></pre> <p><code>lookupV2Provider</code> tries to look up a TableProvider for the source.</p> <p><code>lookupV2Provider</code> explicitly excludes FileDataSourceV2-based data sources (due to SPARK-28396).</p> <p><code>lookupV2Provider</code> is used when:</p> <ul> <li><code>DataFrameWriter</code> is requested to save, insertInto and saveAsTable</li> </ul>","text":""},{"location":"DataFrameWriter/#save-mode","title":"Save Mode <pre><code>mode(\n  saveMode: SaveMode): DataFrameWriter[T]\nmode(\n  saveMode: String): DataFrameWriter[T]\n</code></pre> <p><code>mode</code> defines the behaviour of save when an external file or table Spark writes to already exists.</p>    Name Behaviour      Append Records are appended to an existing data    ErrorIfExists Exception is thrown if the target exists    Ignore Do not save the records and not change the existing data in any way    Overwrite Existing data is overwritten by new records","text":""},{"location":"DataFrameWriter/#creating-bucketspec","title":"Creating BucketSpec <pre><code>getBucketSpec: Option[BucketSpec]\n</code></pre> <p><code>getBucketSpec</code> creates a new BucketSpec for numBuckets if defined (with bucketColumnNames and sortColumnNames).</p>  IllegalArgumentException <p><code>getBucketSpec</code> throws an <code>IllegalArgumentException</code> when numBuckets are not defined but sortColumnNames are.</p> <pre><code>sortBy must be used together with bucketBy\n</code></pre>   <p><code>getBucketSpec</code> is used when:</p> <ul> <li><code>DataFrameWriter</code> is requested to assertNotBucketed, createTable, partitioningAsV2</li> </ul>","text":""},{"location":"DataFrameWriter/#partitioningasv2","title":"partitioningAsV2 <pre><code>partitioningAsV2: Seq[Transform]\n</code></pre> <p><code>partitioningAsV2</code> creates Transforms based on the partitioningColumns (<code>IdentityTransform</code>s) and getBucketSpec (a <code>BucketTransform</code>), if defined.</p>  <p><code>partitioningAsV2</code> is used when:</p> <ul> <li><code>DataFrameWriter</code> is requested to saveInternal, saveAsTable, checkPartitioningMatchesV2Table</li> </ul>","text":""},{"location":"DataFrameWriter/#executing-logical-command-for-writing-to-data-source-v1","title":"Executing Logical Command for Writing to Data Source V1 <pre><code>saveToV1Source(): Unit\n</code></pre> <p><code>saveToV1Source</code> creates a DataSource (for the source class name, the partitioningColumns and the extraOptions) and requests it for the logical command for writing (with the mode and the analyzed logical plan of the structured query).</p>  <p>Note</p> <p>While requesting the analyzed logical plan of the structured query, <code>saveToV1Source</code> triggers execution of logical commands.</p>  <p>In the end, <code>saveToV1Source</code> runs the logical command for writing.</p>  <p>Note</p> <p>The logical command for writing can be one of the following:</p> <ul> <li> <p>A SaveIntoDataSourceCommand for CreatableRelationProviders</p> </li> <li> <p>An InsertIntoHadoopFsRelationCommand for FileFormats</p> </li> </ul>  <p><code>saveToV1Source</code> is used when <code>DataFrameWriter</code> is requested to save the rows of a structured query (a DataFrame) to a data source.</p>","text":""},{"location":"DataFrameWriter/#executing-logical-commands","title":"Executing Logical Command(s) <pre><code>runCommand(\n  session: SparkSession,\n  name: String)(\n  command: LogicalPlan): Unit\n</code></pre> <p><code>runCommand</code> uses the given SparkSession to access the SessionState that is in turn requested to execute the logical command (that creates a QueryExecution).</p> <p><code>runCommand</code> records the current time (start time) and uses the <code>SQLExecution</code> helper object to execute the action (under a new execution id) that simply requests the <code>QueryExecution</code> for the RDD[InternalRow] (and triggers execution of logical commands).</p>  <p>Tip</p> <p>Use web UI's SQL tab to see the execution or a <code>SparkListener</code> to be notified when the execution is started and finished. The <code>SparkListener</code> should intercept <code>SparkListenerSQLExecutionStart</code> and <code>SparkListenerSQLExecutionEnd</code> events.</p>  <p><code>runCommand</code> records the current time (end time).</p> <p>In the end, <code>runCommand</code> uses the input <code>SparkSession</code> to access the ExecutionListenerManager and requests it to onSuccess (with the input <code>name</code>, the <code>QueryExecution</code> and the duration).</p> <p>In case of any exceptions, <code>runCommand</code> requests the <code>ExecutionListenerManager</code> to onFailure (with the exception) and (re)throws it.</p> <p><code>runCommand</code> is used when:</p> <ul> <li><code>DataFrameWriter</code> is requested to save the rows of a structured query (a DataFrame) to a data source (and indirectly executing a logical command for writing to a data source V1), insert the rows of a structured streaming (a DataFrame) into a table and create a table (that is used exclusively for saveAsTable)</li> </ul>","text":""},{"location":"DataFrameWriter/#creating-table","title":"Creating Table <pre><code>createTable(\n  tableIdent: TableIdentifier): Unit\n</code></pre> <p><code>createTable</code> builds a CatalogStorageFormat per extraOptions.</p> <p><code>createTable</code> assumes the table being external when location URI of <code>CatalogStorageFormat</code> is defined, and managed otherwise.</p> <p><code>createTable</code> creates a CatalogTable (with the bucketSpec per getBucketSpec).</p> <p>In the end, <code>createTable</code> creates a CreateTable logical command (with the <code>CatalogTable</code>, mode and the logical query plan of the dataset) and runs it.</p> <p><code>createTable</code> is used when:</p> <ul> <li><code>DataFrameWriter</code> is requested to saveAsTable</li> </ul>","text":""},{"location":"DataFrameWriterV2/","title":"DataFrameWriterV2","text":"<p><code>DataFrameWriterV2</code> is an API for Spark SQL developers to describe how to write a Dataset to an external storage using the DataSource V2.</p> <p><code>DataFrameWriterV2</code> is a CreateTableWriter (and thus a WriteConfigMethods).</p>"},{"location":"DataFrameWriterV2/#demo","title":"Demo","text":"<pre><code>val nums = spark.range(5)\nscala&gt; :type nums\norg.apache.spark.sql.Dataset[Long]\n\nval writerV2 = nums.writeTo(\"t1\")\nscala&gt; :type writerV2\norg.apache.spark.sql.DataFrameWriterV2[Long]\n</code></pre>"},{"location":"DataFrameWriterV2/#creating-instance","title":"Creating Instance","text":"<p><code>DataFrameWriterV2</code> takes the following to be created:</p> <ul> <li>Name of the target table (multi-part table identifier)</li> <li>Dataset</li> </ul> <p><code>DataFrameWriterV2</code> is created when:</p> <ul> <li>Dataset.writeTo operator is used</li> </ul>"},{"location":"DataFrameWriterV2/#append","title":"append <pre><code>append(): Unit\n</code></pre> <p><code>append</code> loads the table from the catalog and identifier (based on the table name).</p> <p>When found, <code>append</code> creates an AppendData logical command (by name) with a DataSourceV2Relation logical operator (for the catalog and the identifier).</p> <p>In the end, <code>append</code> runs the command with append execution name.</p>  <p>Tip</p> <p>Execution is announced as a SparkListenerSQLExecutionEnd.</p>  <p><code>append</code> throws a <code>NoSuchTableException</code> when the table could not be found.</p> <pre><code>scala&gt; spark.range(5).writeTo(\"my.catalog.t1\").append\norg.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table my.catalog.t1 not found;\n  at org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:162)\n  ... 47 elided\n</code></pre>","text":""},{"location":"DataFrameWriterV2/#create","title":"create <pre><code>create(): Unit\n</code></pre> <p><code>create</code> is part of the CreateTableWriter abstraction.</p> <p><code>create</code>...FIXME</p>","text":""},{"location":"DataFrameWriterV2/#createorreplace","title":"createOrReplace <pre><code>createOrReplace(): Unit\n</code></pre> <p><code>createOrReplace</code> is part of the CreateTableWriter abstraction.</p> <p><code>createOrReplace</code>...FIXME</p>","text":""},{"location":"DataFrameWriterV2/#option","title":"option <pre><code>option(\n  key: String,\n  value: String): DataFrameWriterV2[T]\n</code></pre> <p><code>option</code> is part of the WriteConfigMethods abstraction.</p> <p><code>option</code>...FIXME</p>","text":""},{"location":"DataFrameWriterV2/#options","title":"options <pre><code>options(\n  options: Map[String, String]): DataFrameWriterV2[T]\n</code></pre> <p><code>options</code> is part of the WriteConfigMethods abstraction.</p> <p><code>options</code>...FIXME</p>","text":""},{"location":"DataFrameWriterV2/#overwrite","title":"overwrite <pre><code>overwrite(\n  condition: Column): Unit\n</code></pre> <p><code>overwrite</code>...FIXME</p>","text":""},{"location":"DataFrameWriterV2/#overwritepartitions","title":"overwritePartitions <pre><code>overwritePartitions(): Unit\n</code></pre> <p><code>overwritePartitions</code>...FIXME</p>","text":""},{"location":"DataFrameWriterV2/#partitionedby","title":"partitionedBy <pre><code>partitionedBy(\n  column: Column,\n  columns: Column*): CreateTableWriter[T]\n</code></pre> <p><code>partitionedBy</code> is part of the CreateTableWriter abstraction.</p> <p><code>partitionedBy</code>...FIXME</p>","text":""},{"location":"DataFrameWriterV2/#replace","title":"replace <pre><code>replace(): Unit\n</code></pre> <p><code>replace</code> is part of the CreateTableWriter abstraction.</p> <p><code>replace</code>...FIXME</p>","text":""},{"location":"DataFrameWriterV2/#tableproperty","title":"tableProperty <pre><code>tableProperty(\n  property: String,\n  value: String): CreateTableWriter[T]\n</code></pre> <p><code>tableProperty</code> is part of the CreateTableWriter abstraction.</p> <p><code>tableProperty</code>...FIXME</p>","text":""},{"location":"DataFrameWriterV2/#using","title":"using <pre><code>using(\n  provider: String): CreateTableWriter[T]\n</code></pre> <p><code>using</code> is part of the CreateTableWriter abstraction.</p> <p><code>using</code>...FIXME</p>","text":""},{"location":"DataFrameWriterV2/#private-methods","title":"Private Methods","text":""},{"location":"DataFrameWriterV2/#internalreplace","title":"internalReplace <pre><code>internalReplace(\n  orCreate: Boolean): Unit\n</code></pre> <p><code>internalReplace</code>...FIXME</p>","text":""},{"location":"DataFrameWriterV2/#executing-logical-command","title":"Executing Logical Command <pre><code>runCommand(\n  name: String)(\n  command: LogicalPlan): Unit\n</code></pre> <p><code>runCommand</code>...FIXME</p>","text":""},{"location":"DataSource/","title":"DataSource \u2014 Pluggable Data Provider Framework","text":"<p><code>DataSource</code> paves the way for Pluggable Data Provider Framework in Spark SQL.</p> <p>Together with the provider interfaces, <code>DataSource</code> allows Spark SQL integrators to use external data systems as data sources and sinks in structured queries in Spark SQL (incl. Spark Structured Streaming).</p>"},{"location":"DataSource/#provider-interfaces","title":"Provider Interfaces","text":"<ul> <li>CreatableRelationProvider</li> <li>FileFormat</li> <li>RelationProvider</li> <li>SchemaRelationProvider</li> <li>StreamSinkProvider (Spark Structured Streaming)</li> <li>StreamSourceProvider (Spark Structured Streaming)</li> </ul>"},{"location":"DataSource/#accessing-datasource","title":"Accessing DataSource","text":"<p><code>DataSource</code> is available using DataFrameReader.</p> <pre><code>val people = spark\n  .read // Batch reading\n  .format(\"csv\")\n  .load(\"people.csv\")\n</code></pre> <pre><code>val messages = spark\n  .readStream // Streamed reading\n  .format(\"kafka\")\n  .option(\"subscribe\", \"topic\")\n  .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n  .load\n</code></pre>"},{"location":"DataSource/#creating-instance","title":"Creating Instance","text":"<p><code>DataSource</code> takes the following to be created:</p> <ul> <li> SparkSession <li> Fully-qualified class name or an alias of the data source provider (aka data source format) <li> Data Paths (default: empty) <li> User-specified schema (default: undefined) <li> Names of the partition columns (default: empty) <li> Bucketing specification (default: undefined) <li> Options (default: empty) <li> CatalogTable (default: undefined) <p>Note</p> <p>Only a SparkSession and a fully-qualified class name of the data source provider are required to create an instance of <code>DataSource</code>.</p> <p><code>DataSource</code> is created when:</p> <ul> <li> <p><code>HiveMetastoreCatalog</code> is requested to convert a HiveTableRelation to a LogicalRelation over a HadoopFsRelation</p> </li> <li> <p><code>DataFrameReader</code> is requested to load data from a data source (Data Source V1)</p> </li> <li> <p><code>DataFrameWriter</code> is requested to save to a data source (Data Source V1)</p> </li> <li> <p>CreateDataSourceTableCommand, CreateDataSourceTableAsSelectCommand, <code>InsertIntoDataSourceDirCommand</code>, CreateTempViewUsing commands are executed</p> </li> <li> <p>FindDataSourceTable and ResolveSQLOnFile logical evaluation rules are executed</p> </li> <li> <p>For Spark Structured Streaming's <code>FileStreamSource</code>, <code>DataStreamReader</code> and <code>DataStreamWriter</code></p> </li> </ul>"},{"location":"DataSource/#data-source-resolution","title":"Data Source Resolution","text":"<p><code>DataSource</code> is given an alias or a fully-qualified class name of the data source provider. <code>DataSource</code> uses the name to load the Java class. In the end, <code>DataSource</code> uses the Java class to resolve a relation to represent the data source in logical plans.</p>"},{"location":"DataSource/#resolving-relation","title":"Resolving Relation <pre><code>resolveRelation(\n  checkFilesExist: Boolean = true): BaseRelation\n</code></pre> <p><code>resolveRelation</code> resolves (creates) a BaseRelation.</p> <p>Internally, <code>resolveRelation</code> creates an instance of the class (of the provider) and branches off based on type and whether the user-defined schema was specified or not.</p>    Provider Behaviour     SchemaRelationProvider Executes SchemaRelationProvider.createRelation with the provided schema   RelationProvider Executes RelationProvider.createRelation   FileFormat Creates a HadoopFsRelation    <p><code>resolveRelation</code> is used when:</p> <ul> <li> <p><code>DataSource</code> is requested to write and read the result of a structured query (only when the class is a FileFormat)</p> </li> <li> <p><code>DataFrameReader</code> is requested to load data from a data source that supports multiple paths</p> </li> <li> <p><code>TextInputCSVDataSource</code> and <code>TextInputJsonDataSource</code> are requested to infer schema</p> </li> <li> <p>CreateDataSourceTableCommand logical command is executed</p> </li> <li> <p>CreateTempViewUsing logical command is executed</p> </li> <li> <p><code>FindDataSourceTable</code> is requested to readDataSourceTable</p> </li> <li> <p><code>ResolveSQLOnFile</code> is requested to convert a logical plan (when the class is a FileFormat)</p> </li> <li> <p><code>HiveMetastoreCatalog</code> is requested to convert a HiveTableRelation to a LogicalRelation over a HadoopFsRelation</p> </li> </ul>","text":""},{"location":"DataSource/#creating-logical-command-for-writing-for-creatablerelationprovider-and-fileformat-data-sources","title":"Creating Logical Command for Writing (for CreatableRelationProvider and FileFormat Data Sources) <pre><code>planForWriting(\n  mode: SaveMode,\n  data: LogicalPlan): LogicalPlan\n</code></pre> <p><code>planForWriting</code> creates an instance of the providingClass and branches off per type as follows:</p> <ul> <li> <p>For a CreatableRelationProvider, <code>planForWriting</code> creates a SaveIntoDataSourceCommand (with the input <code>data</code> and <code>mode</code> and the <code>CreatableRelationProvider</code> data source)</p> </li> <li> <p>For a FileFormat, <code>planForWriting</code> planForWritingFileFormat (with the <code>FileFormat</code> format and the input <code>mode</code> and <code>data</code>)</p> </li> <li> <p>For other types, <code>planForWriting</code> simply throws a <code>RuntimeException</code>:</p> <pre><code>[providingClass] does not allow create table as select.\n</code></pre> </li> </ul> <p><code>planForWriting</code> is used when:</p> <ul> <li><code>DataFrameWriter</code> is requested to save (to a data source V1</li> <li><code>InsertIntoDataSourceDirCommand</code> logical command is executed</li> </ul>","text":""},{"location":"DataSource/#writing-data-to-data-source-per-save-mode-followed-by-reading-rows-back-as-baserelation","title":"Writing Data to Data Source (per Save Mode) Followed by Reading Rows Back (as BaseRelation) <pre><code>writeAndRead(\n  mode: SaveMode,\n  data: LogicalPlan,\n  outputColumnNames: Seq[String],\n  physicalPlan: SparkPlan): BaseRelation\n</code></pre> <p><code>writeAndRead</code>...FIXME</p>  <p>Note</p> <p><code>writeAndRead</code> is also known as Create Table As Select (CTAS) query.</p>  <p><code>writeAndRead</code> is used when CreateDataSourceTableAsSelectCommand logical command is executed.</p>","text":""},{"location":"DataSource/#planning-for-writing-to-fileformat-based-data-source","title":"Planning for Writing (to FileFormat-Based Data Source) <pre><code>planForWritingFileFormat(\n  format: FileFormat,\n  mode: SaveMode,\n  data: LogicalPlan): InsertIntoHadoopFsRelationCommand\n</code></pre> <p><code>planForWritingFileFormat</code> takes the paths and the <code>path</code> option (from the caseInsensitiveOptions) together and (assuming that there is only one path available among the paths combined) creates a fully-qualified HDFS-compatible output path for writing.</p>  <p>Note</p> <p><code>planForWritingFileFormat</code> uses Hadoop HDFS's Hadoop Path to requests for the FileSystem that owns it (using a Hadoop Configuration).</p>  <p><code>planForWritingFileFormat</code> validates partition columns in the given partitionColumns.</p> <p>In the end, <code>planForWritingFileFormat</code> returns a new InsertIntoHadoopFsRelationCommand.</p> <p><code>planForWritingFileFormat</code> throws an <code>IllegalArgumentException</code> when there are more than one path specified:</p> <pre><code>Expected exactly one path to be specified, but got: [allPaths]\n</code></pre> <p><code>planForWritingFileFormat</code> is used when <code>DataSource</code> is requested for the following:</p> <ul> <li> <p>Writing data to a data source followed by \"reading\" rows back (for CreateDataSourceTableAsSelectCommand logical command)</p> </li> <li> <p>Creating a logical command for writing (for <code>InsertIntoDataSourceDirCommand</code> logical command and DataFrameWriter.save operator with DataSource V1 data sources)</p> </li> </ul>","text":""},{"location":"DataSource/#data-source-class","title":"Data Source Class <pre><code>providingClass: Class[_]\n</code></pre> <p>java.lang.Class that was loaded for the given data source provider</p> <p><code>providingClass</code> is used when:</p> <ul> <li><code>InsertIntoDataSourceDirCommand</code> logical command is executed (to ensure working with a FileFormat-based data source)</li> <li>ResolveSQLOnFile logical evaluation rule is executed (to ensure working with a FileFormat-based data source)</li> <li><code>DataSource</code> is requested for providingInstance</li> </ul>","text":""},{"location":"DataSource/#data-source-instance","title":"Data Source Instance <pre><code>providingInstance(): Any\n</code></pre> <p><code>providingInstance</code> simply creates an instance of the Java class of the data source.</p> <p><code>providingInstance</code> is used when:</p> <ul> <li><code>DataSource</code> is requested to <code>sourceSchema</code>, <code>createSource</code>, <code>createSink</code>, resolve a relation, write and read and plan for writing</li> </ul>","text":""},{"location":"DataSource/#utilities","title":"Utilities","text":""},{"location":"DataSource/#looking-up-tableprovider","title":"Looking up TableProvider <pre><code>lookupDataSourceV2(\n  provider: String,\n  conf: SQLConf): Option[TableProvider]\n</code></pre> <p><code>lookupDataSourceV2</code> uses the spark.sql.sources.useV1SourceList configuration property for the data sources for which to use V1 version.</p> <p><code>lookupDataSourceV2</code> loads up the class of the input <code>provider</code>.</p> <p><code>lookupDataSourceV2</code> branches off based on the type of the data source and returns (in that order):</p> <ol> <li><code>None</code> for a DataSourceRegister with the short name among the \"useV1SourceList\" data source names</li> <li>A TableProvider when the canonical name of the class is not among the \"useV1SourceList\" data source names</li> <li><code>None</code> for other cases</li> </ol> <p><code>lookupDataSourceV2</code> is used when:</p> <ul> <li><code>DataFrameReader</code> is requested to load a DataFrame</li> <li><code>DataFrameWriter</code> is requested to look up a TableProvider</li> <li>ResolveSessionCatalog logical extended resolution rule is executed</li> </ul>","text":""},{"location":"DataSource/#loading-java-class-of-data-source-provider","title":"Loading Java Class Of Data Source Provider <pre><code>lookupDataSource(\n  provider: String,\n  conf: SQLConf): Class[_]\n</code></pre> <p><code>lookupDataSource</code> first finds the given <code>provider</code> in the backwardCompatibilityMap internal registry, and falls back to the <code>provider</code> name itself when not found.</p>  <p>Note</p> <p>The <code>provider</code> argument can be either an alias (a simple name, e.g. <code>parquet</code>) or a fully-qualified class name (e.g. <code>org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat</code>).</p>  <p><code>lookupDataSource</code> then uses the given SQLConf to decide on the class name of the provider for ORC and Avro data sources as follows:</p> <ul> <li> <p>For <code>orc</code> provider and native, <code>lookupDataSource</code> uses...FIXME</p> </li> <li> <p>For <code>orc</code> provider and hive, <code>lookupDataSource</code> uses <code>org.apache.spark.sql.hive.orc.OrcFileFormat</code></p> </li> <li> <p>For <code>com.databricks.spark.avro</code> and spark.sql.legacy.replaceDatabricksSparkAvro.enabled configuration enabled (default), <code>lookupDataSource</code> uses the built-in (but external) Avro data source module</p> </li> </ul> <p><code>lookupDataSource</code> uses <code>DefaultSource</code> as the class name as another provider name variant (i.e. <code>[provider1].DefaultSource</code>).</p> <p><code>lookupDataSource</code> uses Java's ServiceLoader service-provider loading facility to find all data source providers of type DataSourceRegister on the Spark CLASSPATH.</p> <p><code>lookupDataSource</code> tries to find the <code>DataSourceRegister</code> provider classes (by their alias) that match the provider name (case-insensitive, e.g. <code>parquet</code> or <code>kafka</code>).</p> <p>If a single <code>DataSourceRegister</code> provider class is found, <code>lookupDataSource</code> simply returns the instance of the data source provider.</p> <p>If no <code>DataSourceRegister</code> provider class could be found by the short name (alias), <code>lookupDataSource</code> tries to load the provider name to be a fully-qualified class name. If not successful, <code>lookupDataSource</code> tries to load the other provider name (aka DefaultSource) instead.</p>  <p>Note</p> <p>DataFrameWriter.format and DataFrameReader.format methods accept the name of the data source provider to use as an alias or a fully-qualified class name.</p>  <pre><code>import org.apache.spark.sql.execution.datasources.DataSource\nval source = \"parquet\"\nval cls = DataSource.lookupDataSource(source, spark.sessionState.conf)\n</code></pre> <p><code>lookupDataSource</code> is used when:</p> <ul> <li><code>SparkSession</code> is requested to executeCommand</li> <li><code>CreateTableLikeCommand</code> and AlterTableAddColumnsCommand runnable commands are executed</li> <li><code>DataSource</code> is requested for providingClass and to lookupDataSourceV2</li> <li>PreprocessTableCreation posthoc logical resolution rule is executed</li> <li><code>DataStreamReader</code> (Spark Structured Streaming) is requested to <code>load</code></li> <li><code>DataStreamWriter</code> (Spark Structured Streaming) is requested to <code>start</code> a streaming query</li> </ul>","text":""},{"location":"DataSource/#creating-inmemoryfileindex","title":"Creating InMemoryFileIndex <pre><code>createInMemoryFileIndex(\n  globbedPaths: Seq[Path]): InMemoryFileIndex\n</code></pre> <p><code>createInMemoryFileIndex</code> creates an InMemoryFileIndex with the following:</p>    Property Value     Root Paths The given <code>globbedPaths</code>   Parameters Options   User-defined schema User-specified schema   FileStatusCache FileStatusCache     <p><code>createInMemoryFileIndex</code> is used when <code>DataSource</code> is requested for the following:</p> <ul> <li>Resolve a BaseRelation (for non-streaming FileFormat-based data sources)</li> <li>Source Schema (for FileFormat-based data sources)</li> </ul>","text":""},{"location":"DataSourceRDD/","title":"DataSourceRDD","text":"<p><code>DataSourceRDD</code> is an RDD of InternalRows (<code>RDD[InternalRow]</code>) that acts as a thin adapter between Spark SQL's DataSource V2 and Spark Core's RDD API.</p> <p><code>DataSourceRDD</code> is used as an input RDD of the following physical operators:</p> <ul> <li>BatchScanExec</li> <li><code>MicroBatchScanExec</code> (Spark Structured Streaming)</li> </ul> <p><code>DataSourceRDD</code> uses DataSourceRDDPartition for the partitions (that is a mere wrapper of the InputPartitions).</p>"},{"location":"DataSourceRDD/#creating-instance","title":"Creating Instance","text":"<p><code>DataSourceRDD</code> takes the following to be created:</p> <ul> <li> <code>SparkContext</code> (Spark Core) <li>InputPartitions</li> <li> PartitionReaderFactory <li>columnarReads flag</li> <li> Custom SQLMetrics <p><code>DataSourceRDD</code> is created when:</p> <ul> <li><code>BatchScanExec</code> physical operator is requested for an input RDD</li> <li><code>MicroBatchScanExec</code> (Spark Structured Streaming) physical operator is requested for an <code>inputRDD</code></li> </ul>"},{"location":"DataSourceRDD/#inputPartitions","title":"InputPartitions","text":"<pre><code>inputPartitions: Seq[Seq[InputPartition]]\n</code></pre> <p><code>DataSourceRDD</code> is given a collection of InputPartitions when created.</p> <p>The <code>InputPartition</code>s are used to create RDD partitions (one for every collection of InputPartitions in the <code>inputPartitions</code> collection)</p> <p>Note</p> <p>Number of RDD partitions is exactly the number of elements in the <code>inputPartitions</code> collection.</p> <p>The <code>InputPartition</code>s are the filtered partitions in BatchScanExec.</p>"},{"location":"DataSourceRDD/#columnarReads","title":"columnarReads","text":"<p><code>DataSourceRDD</code> is given <code>columnarReads</code> flag when created.</p> <p><code>columnarReads</code> is used to determine the type of scan (row-based or columnar) when computing a partition.</p> <p><code>columnarReads</code> is enabled (using supportsColumnar) when the PartitionReaderFactory can support columnar scans.</p>"},{"location":"DataSourceRDD/#getPartitions","title":"RDD Partitions","text":"Signature <pre><code>getPartitions: Array[Partition]\n</code></pre> <p><code>getPartitions</code> is part of <code>RDD</code> (Spark Core) abstraction.</p> <p><code>getPartitions</code> creates one DataSourceRDDPartition for every collection of InputPartitions in the given inputPartitions.</p>"},{"location":"DataSourceRDD/#getPreferredLocations","title":"Preferred Locations For Partition","text":"Signature <pre><code>getPreferredLocations(\nsplit: Partition): Seq[String]\n</code></pre> <p><code>getPreferredLocations</code> is part of <code>RDD</code> (Spark Core) abstraction.</p> <p><code>getPreferredLocations</code> assumes that the given <code>split</code> partition is a DataSourceRDDPartition.</p> <p><code>getPreferredLocations</code> requests the given DataSourceRDDPartition for the InputPartition that is then requested for the preferred locations.</p>"},{"location":"DataSourceRDD/#compute","title":"Computing Partition","text":"Signature <pre><code>compute(\nsplit: Partition,\ncontext: TaskContext): Iterator[T]\n</code></pre> <p><code>compute</code> is part of <code>RDD</code> (Spark Core) abstraction.</p> <p><code>compute</code> assumes that the given <code>Partition</code> is a DataSourceRDDPartition (or throws a <code>SparkException</code>).</p> <p>DataSourceRDDPartition and InputPartitions</p> <p><code>DataSourceRDDPartition</code> can have many inputPartitions.</p> <p><code>compute</code> requests the PartitionReaderFactory to createColumnarReader or createReader based on columnarReads flag.</p>"},{"location":"DataSourceRDDPartition/","title":"DataSourceRDDPartition","text":"<p><code>DataSourceRDDPartition</code> is a <code>Partition</code> (Apache Spark) of DataSourceRDD.</p>"},{"location":"DataSourceRDDPartition/#creating-instance","title":"Creating Instance","text":"<p><code>DataSourceRDDPartition</code> takes the following to be created:</p> <ul> <li> Partition Index <li> InputPartitions <p><code>DataSourceRDDPartition</code> is created when:</p> <ul> <li><code>DataSourceRDD</code> is requested for the partitions</li> </ul>"},{"location":"DataSourceRegister/","title":"DataSourceRegister","text":"<p><code>DataSourceRegister</code> is an abstraction of data sources to be available under shortName alias (so it can be looked up by the alias not a fully-qualified class name)</p>"},{"location":"DataSourceRegister/#contract","title":"Contract","text":""},{"location":"DataSourceRegister/#short-name","title":"Short Name <pre><code>shortName(): String\n</code></pre> <p>Short name (alias) of the data source</p> <p>Used when:</p> <ul> <li><code>DataSource</code> utility is used to lookup a data source</li> </ul>","text":""},{"location":"DataSourceRegister/#data-source-discovery","title":"Data Source Discovery","text":"<p><code>DataSourceRegister</code> should register itself in <code>META-INF/services/org.apache.spark.sql.sources.DataSourceRegister</code> file for Java's ServiceLoader to discover the service.</p>"},{"location":"Dataset-untyped-transformations/","title":"Dataset API \u2014 Untyped Transformations","text":"<p>Untyped transformations are part of the Dataset API for transforming a <code>Dataset</code> to a DataFrame, a Column, a RelationalGroupedDataset, a DataFrameNaFunctions or a DataFrameStatFunctions (and hence untyped).</p> <p>Note</p> <p>Untyped transformations are the methods in the <code>Dataset</code> Scala class that are grouped in <code>untypedrel</code> group name, i.e. <code>@group untypedrel</code>.</p>"},{"location":"Dataset/","title":"Dataset","text":"<p><code>Dataset[T]</code> is a strongly-typed data structure that represents a structured query over rows of <code>T</code> type.</p> <p><code>Dataset</code> is created using SQL or Dataset high-level declarative \"languages\".</p> <p></p> <p>It is fair to say that <code>Dataset</code> is a Spark SQL developer-friendly layer over the following two low-level entities:</p> <ol> <li> <p>QueryExecution (with the parsed yet unanalyzed LogicalPlan of a structured query)</p> </li> <li> <p>Encoder (of the type of the records for fast serialization and deserialization to and from InternalRow)</p> </li> </ol>"},{"location":"Dataset/#creating-instance","title":"Creating Instance","text":"<p><code>Dataset</code> takes the following when created:</p> <ul> <li> SparkSession <li> QueryExecution <li> Encoder of <code>T</code> <p>Note</p> <p><code>Dataset</code> can be created using LogicalPlan when executed using SessionState.</p> <p>When created, <code>Dataset</code> requests QueryExecution to assert analyzed phase is successful.</p> <p><code>Dataset</code> is created when:</p> <ul> <li> <p>Dataset.apply (for a LogicalPlan and a SparkSession with the Encoder in a Scala implicit scope)</p> </li> <li> <p>Dataset.ofRows (for a LogicalPlan and a SparkSession)</p> </li> <li> <p>Dataset.toDF untyped transformation is used</p> </li> <li> <p>Dataset.select, Dataset.randomSplit and Dataset.mapPartitions typed transformations are used</p> </li> <li> <p>KeyValueGroupedDataset.agg operator is used (that requests <code>KeyValueGroupedDataset</code> to aggUntyped)</p> </li> <li> <p>SparkSession.emptyDataset and SparkSession.range operators are used</p> </li> <li> <p><code>CatalogImpl</code> is requested to makeDataset (when requested to list databases, tables, functions and columns)</p> </li> </ul>"},{"location":"Dataset/#observe","title":"observe <pre><code>observe(\n  observation: Observation,\n  expr: Column,\n  exprs: Column*): Dataset[T]\nobserve(\n  name: String,\n  expr: Column,\n  exprs: Column*): Dataset[T]\n</code></pre> <p>When executed with <code>name</code> argument, <code>observe</code> creates a typed <code>Dataset</code> with a CollectMetrics logical operator.</p> <p>For an Observation argument, <code>observe</code> requests the <code>Observation</code> to observe this <code>Dataset</code> (that in turn uses the <code>name</code>-argument <code>observe</code>).</p>  <p>Requirements</p> <ol> <li>An <code>Observation</code> can be used with a <code>Dataset</code> only once</li> <li><code>Observation</code> does not support streaming <code>Dataset</code>s.</li> </ol>","text":""},{"location":"Dataset/#repartition","title":"repartition <pre><code>repartition(\n  partitionExprs: Column*): Dataset[T] // (1)!\nrepartition(\n  numPartitions: Int): Dataset[T] // (2)!\nrepartition(\n  numPartitions: Int,\n  partitionExprs: Column*): Dataset[T]  // (3)!\n</code></pre> <ol> <li>An alias of repartitionByExpression with undefined <code>numPartitions</code></li> <li>Creates a Repartition logical operator with <code>shuffle</code> enabled</li> <li>An alias of repartitionByExpression</li> </ol>","text":""},{"location":"Dataset/#repartitionbyexpression","title":"repartitionByExpression <pre><code>repartitionByExpression(\n  numPartitions: Option[Int],\n  partitionExprs: Seq[Column]): Dataset[T]\n</code></pre> <p><code>repartitionByExpression</code> withTypedPlan with a new RepartitionByExpression (with the given <code>numPartitions</code> and <code>partitionExprs</code>, and the logicalPlan).</p>","text":""},{"location":"Dataset/#example-number-of-partitions-only","title":"Example: Number of Partitions Only <pre><code>val numsRepd = nums.repartition(numPartitions = 4)\n</code></pre> <pre><code>assert(numsRepd.rdd.getNumPartitions == 4, \"Number of partitions should be 4\")\n</code></pre> <pre><code>scala&gt; numsRepd.explain(extended = true)\n== Parsed Logical Plan ==\nRepartition 4, true\n+- RepartitionByExpression [id#4L ASC NULLS FIRST]\n   +- Range (0, 10, step=1, splits=Some(16))\n\n== Analyzed Logical Plan ==\nid: bigint\nRepartition 4, true\n+- RepartitionByExpression [id#4L ASC NULLS FIRST]\n   +- Range (0, 10, step=1, splits=Some(16))\n\n== Optimized Logical Plan ==\nRepartition 4, true\n+- Range (0, 10, step=1, splits=Some(16))\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Exchange RoundRobinPartitioning(4), REPARTITION_BY_NUM, [id=#75]\n   +- Range (0, 10, step=1, splits=16)\n</code></pre>","text":""},{"location":"Dataset/#repartitionbyrange","title":"repartitionByRange <pre><code>repartitionByRange(\n  partitionExprs: Column*): Dataset[T]\nrepartitionByRange(\n  numPartitions: Int,\n  partitionExprs: Column*): Dataset[T]\nrepartitionByRange(\n    numPartitions: Option[Int],\n    partitionExprs: Seq[Column]): Dataset[T] // (1)!\n</code></pre> <ol> <li>A private method</li> </ol> <p><code>repartitionByRange</code> creates a SortOrders with Ascending sorting direction (ascending nulls first) for the given <code>partitionExprs</code> with no sorting specified.</p> <p>In the end, <code>repartitionByRange</code> creates a Dataset with a RepartitionByExpression (with the <code>SortOrder</code>s, the logicalPlan and the given <code>numPartitions</code>).</p>","text":""},{"location":"Dataset/#example-partition-expressions-only","title":"Example: Partition Expressions Only <pre><code>val nums = spark.range(10).repartitionByRange($\"id\".asc)\n</code></pre> <pre><code>scala&gt; println(nums.queryExecution.logical.numberedTreeString)\n00 'RepartitionByExpression ['id ASC NULLS FIRST]\n01 +- Range (0, 10, step=1, splits=Some(16))\n</code></pre>  <p>Note</p> <p>Adaptive Query Execution is enabled.</p>  <pre><code>scala&gt; println(nums.queryExecution.toRdd.getNumPartitions)\n1\n</code></pre> <pre><code>scala&gt; println(nums.queryExecution.toRdd.toDebugString)\n(1) SQLExecutionRDD[17] at toRdd at &lt;console&gt;:24 []\n |  ShuffledRowRDD[16] at toRdd at &lt;console&gt;:24 []\n +-(16) MapPartitionsRDD[15] at toRdd at &lt;console&gt;:24 []\n    |   MapPartitionsRDD[11] at toRdd at &lt;console&gt;:24 []\n    |   MapPartitionsRDD[10] at toRdd at &lt;console&gt;:24 []\n    |   ParallelCollectionRDD[9] at toRdd at &lt;console&gt;:24 []\n</code></pre>","text":""},{"location":"Dataset/#example-number-of-partitions-and-partition-expressions","title":"Example: Number of Partitions and Partition Expressions <pre><code>val q = spark.range(10).repartitionByRange(numPartitions = 5, $\"id\")\n</code></pre> <pre><code>scala&gt; println(q.queryExecution.logical.numberedTreeString)\n00 'RepartitionByExpression ['id ASC NULLS FIRST], 5\n01 +- Range (0, 10, step=1, splits=Some(16))\n</code></pre> <pre><code>scala&gt; println(q.queryExecution.toRdd.getNumPartitions)\n5\n</code></pre> <pre><code>scala&gt; println(q.queryExecution.toRdd.toDebugString)\n(5) SQLExecutionRDD[8] at toRdd at &lt;console&gt;:24 []\n |  ShuffledRowRDD[7] at toRdd at &lt;console&gt;:24 []\n +-(16) MapPartitionsRDD[6] at toRdd at &lt;console&gt;:24 []\n    |   MapPartitionsRDD[2] at toRdd at &lt;console&gt;:24 []\n    |   MapPartitionsRDD[1] at toRdd at &lt;console&gt;:24 []\n    |   ParallelCollectionRDD[0] at toRdd at &lt;console&gt;:24 []\n</code></pre>","text":""},{"location":"Dataset/#logicalplan","title":"LogicalPlan <pre><code>logicalPlan: LogicalPlan\n</code></pre> <p><code>Dataset</code> initializes an internal LogicalPlan when created.</p> <p><code>logicalPlan</code> requests the QueryExecution for a LogicalPlan (with commands executed per the command execution mode).</p> <p>With SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED enabled, <code>logicalPlan</code>...FIXME. Otherwise, <code>logicalPlan</code> returns the <code>LogicalPlan</code> intact.</p>","text":""},{"location":"Dataset/#lazy-values","title":"Lazy Values <p>The following are Scala lazy values of <code>Dataset</code>. Using lazy values guarantees that the code to initialize them is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>","text":""},{"location":"Dataset/#rdd","title":"RDD <pre><code>rdd: RDD[T]\n</code></pre> <p><code>rdd</code> ...FIXME</p> <p><code>rdd</code> is used when:</p> <ul> <li><code>Dataset</code> is requested for an RDD and withNewRDDExecutionId</li> </ul>","text":""},{"location":"Dataset/#queryexecution","title":"QueryExecution <pre><code>rddQueryExecution: QueryExecution\n</code></pre> <p><code>rddQueryExecution</code> creates a deserializer for the <code>T</code> type and the logical query plan of this <code>Dataset</code>.</p> <p>In other words, <code>rddQueryExecution</code> simply adds a new DeserializeToObject unary logical operator as the parent of the current logical query plan (that in turn becomes a child operator).</p> <p>In the end, <code>rddQueryExecution</code> requests the SparkSession for the SessionState to create a QueryExecution for the query plan with the top-most <code>DeserializeToObject</code>.</p> <p><code>rddQueryExecution</code> is used when:</p> <ul> <li><code>Dataset</code> is requested for an RDD and withNewRDDExecutionId</li> </ul>","text":""},{"location":"Dataset/#withnewrddexecutionid","title":"withNewRDDExecutionId <pre><code>withNewRDDExecutionId[U](\n  body: =&gt; U): U\n</code></pre> <p><code>withNewRDDExecutionId</code> executes the input <code>body</code> action (that produces the result of type <code>U</code>) under a new execution id (with the QueryExecution).</p> <p><code>withNewRDDExecutionId</code> requests the QueryExecution for the SparkPlan to resetMetrics.</p> <p><code>withNewRDDExecutionId</code> is used when the following <code>Dataset</code> operators are used:</p> <ul> <li>reduce, foreach and foreachPartition</li> </ul>","text":""},{"location":"Dataset/#writeto","title":"writeTo <pre><code>writeTo(\n  table: String): DataFrameWriterV2[T]\n</code></pre> <p><code>writeTo</code> creates a DataFrameWriterV2 for the input table and this <code>Dataset</code>.</p>","text":""},{"location":"Dataset/#write","title":"write <pre><code>write: DataFrameWriter[T]\n</code></pre> <p><code>write</code> creates a DataFrameWriter for this <code>Dataset</code>.</p>","text":""},{"location":"Dataset/#withtypedplan","title":"withTypedPlan <pre><code>withTypedPlan[U : Encoder](\n  logicalPlan: LogicalPlan): Dataset[U]\n</code></pre>  Final Method <p><code>withTypedPlan</code> is annotated with @inline annotation that requests the Scala compiler to try especially hard to inline it</p>  <p><code>withTypedPlan</code>...FIXME</p>","text":""},{"location":"Dataset/#withsetoperator","title":"withSetOperator <pre><code>withSetOperator[U: Encoder](\n  logicalPlan: LogicalPlan): Dataset[U]\n</code></pre>  Final Method <p><code>withSetOperator</code> is annotated with @inline annotation that requests the Scala compiler to try especially hard to inline it</p>  <p><code>withSetOperator</code>...FIXME</p>","text":""},{"location":"Dataset/#apply","title":"apply <pre><code>apply[T: Encoder](\n  sparkSession: SparkSession,\n  logicalPlan: LogicalPlan): Dataset[T]\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is used when:</p> <ul> <li><code>Dataset</code> is requested to withTypedPlan and withSetOperator</li> </ul>","text":""},{"location":"Dataset/#executing-action-under-new-execution-id","title":"Executing Action Under New Execution ID <pre><code>withAction[U](\n  name: String,\n  qe: QueryExecution)(\n  action: SparkPlan =&gt; U)\n</code></pre> <p><code>withAction</code> creates a new execution ID to execute the given <code>action</code> with the optimized physical query plan (of the given QueryExecution).</p> <p><code>withAction</code> resets the performance metrics.</p>  <p><code>withAction</code> is used to execute the following <code>Dataset</code> actions:</p>    Action Name     isEmpty <code>isEmpty</code>   checkpoint <code>checkpoint</code> or <code>localCheckpoint</code>   head <code>head</code>   tail <code>tail</code>   collect <code>collect</code>   collectAsList <code>collectAsList</code>   toLocalIterator <code>toLocalIterator</code>   count <code>count</code>   collectToPython <code>collectToPython</code>   tailToPython <code>tailToPython</code>   collectAsArrowToR <code>collectAsArrowToR</code>   collectAsArrowToPython <code>collectAsArrowToPython</code>","text":""},{"location":"Dataset/#collectasarrowtopython","title":"collectAsArrowToPython <pre><code>collectAsArrowToPython: Array[Any]\n</code></pre> <p><code>collectAsArrowToPython</code>...FIXME</p>  <p><code>collectAsArrowToPython</code> is used when:</p> <ul> <li><code>PandasConversionMixin</code> (PySpark) is requested to <code>_collect_as_arrow</code></li> </ul>","text":""},{"location":"Dataset/#collecttopython","title":"collectToPython <pre><code>collectToPython(): Array[Any]\n</code></pre> <p><code>collectToPython</code>...FIXME</p>  <p><code>collectToPython</code> is used when:</p> <ul> <li><code>DataFrame</code> (PySpark) is requested to <code>collect</code></li> </ul>","text":""},{"location":"Encoder/","title":"Encoder","text":"<p><code>Encoder[T]</code> is an abstraction of converters that can convert JVM objects (of type <code>T</code>) to and from the internal Spark SQL representation (InternalRow).</p> <p><code>Encoder</code>s are <code>Serializable</code> (Java).</p> <p><code>Encoder</code> is the fundamental concept in the Serialization and Deserialization (SerDe) Framework. Spark SQL uses the SerDe framework for IO to make it efficient time- and space-wise.</p> <p>Note</p> <p>Spark has borrowed the idea from the Hive SerDe library so it might be worthwhile to get familiar with Hive a little bit, too.</p> <p><code>Encoder</code> is also called \"a container of serde expressions in Dataset\".</p> <p><code>Encoder</code> is a part of Datasets (to serialize and deserialize the records of this dataset).</p> <p><code>Encoder</code> knows the schema of the records and that is how they offer significantly faster serialization and deserialization (comparing to the default Java or Kryo serializers).</p> <p>Custom <code>Encoder</code>s are created using Encoders utility. <code>Encoder</code>s for common Scala types and their product types are already available in implicits object.</p> <pre><code>val spark = SparkSession.builder.getOrCreate()\nimport spark.implicits._\n</code></pre> <p>Tip</p> <p>The default encoders are already imported in <code>spark-shell</code>.</p> <p><code>Encoder</code>s map columns (of your dataset) to fields (of your JVM object) by name. It is by <code>Encoder</code>s that you can bridge JVM objects to data sources (CSV, JDBC, Parquet, Avro, JSON, Cassandra, Elasticsearch, memsql) and vice versa.</p>"},{"location":"Encoder/#contract","title":"Contract","text":""},{"location":"Encoder/#classtag","title":"ClassTag <pre><code>clsTag: ClassTag[T]\n</code></pre> <p><code>ClassTag</code> (Scala) for creating Arrays of <code>T</code>s</p> <p>Used when:</p> <ul> <li><code>AppendColumns</code> utility is used to create a <code>AppendColumns</code></li> <li><code>MapElements</code> utility is used to create a <code>MapElements</code></li> <li><code>TypedFilter</code> utility is used to create a <code>TypedFilter</code></li> <li><code>TypedColumn</code> is requested to withInputType</li> </ul>","text":""},{"location":"Encoder/#schema","title":"Schema <pre><code>schema: StructType\n</code></pre> <p>Schema of encoding this type of object as a Row</p>","text":""},{"location":"Encoder/#implementations","title":"Implementations","text":"<ul> <li>ExpressionEncoder</li> </ul>"},{"location":"Encoder/#demo","title":"Demo","text":"<pre><code>// The domain object for your records in a large dataset\ncase class Person(id: Long, name: String)\n\nimport org.apache.spark.sql.Encoders\n\nscala&gt; val personEncoder = Encoders.product[Person]\npersonEncoder: org.apache.spark.sql.Encoder[Person] = class[id[0]: bigint, name[0]: string]\n\nscala&gt; personEncoder.schema\nres0: org.apache.spark.sql.types.StructType = StructType(StructField(id,LongType,false), StructField(name,StringType,true))\n\nscala&gt; personEncoder.clsTag\nres1: scala.reflect.ClassTag[Person] = Person\n\nimport org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n\nscala&gt; val personExprEncoder = personEncoder.asInstanceOf[ExpressionEncoder[Person]]\npersonExprEncoder: org.apache.spark.sql.catalyst.encoders.ExpressionEncoder[Person] = class[id[0]: bigint, name[0]: string]\n\n// ExpressionEncoders may or may not be flat\nscala&gt; personExprEncoder.flat\nres2: Boolean = false\n\n// The Serializer part of the encoder\nscala&gt; personExprEncoder.serializer\nres3: Seq[org.apache.spark.sql.catalyst.expressions.Expression] = List(assertnotnull(input[0, Person, true], top level non-flat input object).id AS id#0L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Person, true], top level non-flat input object).name, true) AS name#1)\n\n// The Deserializer part of the encoder\nscala&gt; personExprEncoder.deserializer\nres4: org.apache.spark.sql.catalyst.expressions.Expression = newInstance(class Person)\n\nscala&gt; personExprEncoder.namedExpressions\nres5: Seq[org.apache.spark.sql.catalyst.expressions.NamedExpression] = List(assertnotnull(input[0, Person, true], top level non-flat input object).id AS id#2L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Person, true], top level non-flat input object).name, true) AS name#3)\n\n// A record in a Dataset[Person]\n// A mere instance of Person case class\n// There could be a thousand of Person in a large dataset\nval jacek = Person(0, \"Jacek\")\n\n// Serialize a record to the internal representation, i.e. InternalRow\nscala&gt; val row = personExprEncoder.toRow(jacek)\nrow: org.apache.spark.sql.catalyst.InternalRow = [0,0,1800000005,6b6563614a]\n\n// Spark uses InternalRows internally for IO\n// Let's deserialize it to a JVM object, i.e. a Scala object\nimport org.apache.spark.sql.catalyst.dsl.expressions._\n\n// in spark-shell there are competing implicits\n// That's why DslSymbol is used explicitly in the following line\nscala&gt; val attrs = Seq(DslSymbol('id).long, DslSymbol('name).string)\nattrs: Seq[org.apache.spark.sql.catalyst.expressions.AttributeReference] = List(id#8L, name#9)\n\nscala&gt; val jacekReborn = personExprEncoder.resolveAndBind(attrs).fromRow(row)\njacekReborn: Person = Person(0,Jacek)\n\n// Are the jacek instances same?\nscala&gt; jacek == jacekReborn\nres6: Boolean = true\n</code></pre> <pre><code>val toRow = personExprEncoder.createSerializer()\ntoRow(jacek)\n</code></pre> <pre><code>val fromRow = personExprEncoder.resolveAndBind().createDeserializer()\nval jacekReborn = fromRow(row)\n</code></pre>"},{"location":"Encoder/#further-reading-and-watching","title":"Further Reading and Watching","text":"<ul> <li>(video) Modern Spark DataFrame and Dataset (Intermediate Tutorial) by Adam Breindel</li> </ul>"},{"location":"Encoders/","title":"Encoders Utility","text":""},{"location":"Encoders/#demo","title":"Demo","text":"<pre><code>import org.apache.spark.sql.Encoders\nval encoder = Encoders.LOCALDATE\n</code></pre> <pre><code>scala&gt; :type encoder\norg.apache.spark.sql.Encoder[java.time.LocalDate]\n</code></pre>"},{"location":"Encoders/#creating-generic-expressionencoder-using-java-serialization","title":"Creating Generic ExpressionEncoder using Java Serialization <pre><code>javaSerialization[T: ClassTag]: Encoder[T]\njavaSerialization[T](\n  clazz: Class[T]): Encoder[T]\n</code></pre> <p><code>javaSerialization</code> creates a generic ExpressionEncoder (with <code>useKryo</code> flag off).</p>","text":""},{"location":"Encoders/#creating-generic-expressionencoder-using-kryo-serialization","title":"Creating Generic ExpressionEncoder using Kryo Serialization <pre><code>kryo[T: ClassTag]: Encoder[T]\nkryo[T](\n  clazz: Class[T]): Encoder[T]\n</code></pre> <p><code>kryo</code> creates a generic ExpressionEncoder (with <code>useKryo</code> flag on).</p>","text":""},{"location":"Encoders/#creating-generic-expressionencoder","title":"Creating Generic ExpressionEncoder <pre><code>genericSerializer[T: ClassTag](\n  useKryo: Boolean): Encoder[T]\n</code></pre> <p><code>genericSerializer</code> creates an ExpressionEncoder with the following:</p>    Attribute Catalyst Expression     <code>objSerializer</code> EncodeUsingSerializer with BoundReference (<code>ordinal</code> = 0, <code>dataType</code> = ObjectType)   <code>objDeserializer</code> DecodeUsingSerializer with <code>GetColumnByOrdinal</code> leaf expression    <p><code>genericSerializer</code>\u00a0is used when:</p> <ul> <li><code>Encoders</code> utility is used for a generic encoder using Kryo and Java serialization</li> </ul>","text":""},{"location":"EquivalentExpressions/","title":"EquivalentExpressions","text":"<p>[[internal-registries]] .EquivalentExpressions's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| [[equivalenceMap]] <code>equivalenceMap</code> | Equivalent sets of expressions, i.e. semantically equal expressions/Expression.md[expressions] by their <code>Expr</code> \"representative\"</p> <p>Used when...FIXME |===</p> <p>=== [[addExprTree]] <code>addExprTree</code> Method</p>"},{"location":"EquivalentExpressions/#source-scala","title":"[source, scala]","text":""},{"location":"EquivalentExpressions/#addexprtreeexpr-expression-unit","title":"addExprTree(expr: Expression): Unit","text":"<p><code>addExprTree</code>...FIXME</p> <p>NOTE: <code>addExprTree</code> is used when <code>CodegenContext</code> is requested to subexpressionElimination or subexpressionEliminationForWholeStageCodegen.</p> <p>=== [[addExpr]] <code>addExpr</code> Method</p>"},{"location":"EquivalentExpressions/#source-scala_1","title":"[source, scala]","text":""},{"location":"EquivalentExpressions/#addexprexpr-expression-boolean","title":"addExpr(expr: Expression): Boolean","text":"<p><code>addExpr</code>...FIXME</p>"},{"location":"EquivalentExpressions/#note","title":"[NOTE]","text":"<p><code>addExpr</code> is used when:</p> <ul> <li><code>EquivalentExpressions</code> is requested to &lt;&gt;"},{"location":"EquivalentExpressions/#physicalaggregation-is-requested-to-physicalaggregationmdunapplydestructure-an-aggregate-logical-operator","title":"* <code>PhysicalAggregation</code> is requested to PhysicalAggregation.md#unapply[destructure an Aggregate logical operator]","text":"<p>=== [[getAllEquivalentExprs]] Getting Equivalent Sets Of Expressions -- <code>getAllEquivalentExprs</code> Method</p>"},{"location":"EquivalentExpressions/#source-scala_2","title":"[source, scala]","text":""},{"location":"EquivalentExpressions/#getallequivalentexprs-seqseqexpression","title":"getAllEquivalentExprs: Seq[Seq[Expression]]","text":"<p><code>getAllEquivalentExprs</code> takes the values of all the &lt;&gt;. <p><code>getAllEquivalentExprs</code> is used when <code>CodegenContext</code> is requested to subexpressionElimination or subexpressionEliminationForWholeStageCodegen.</p>"},{"location":"ExecutionListenerBus/","title":"ExecutionListenerBus","text":"<p><code>ExecutionListenerBus</code> is a <code>ListenerBus</code> (Spark Core) that notifies registered QueryExecutionListeners aboutSparkListenerSQLExecutionEnd events.</p> <p><code>ExecutionListenerBus</code> is a <code>SparkListener</code> (Spark Core).</p>"},{"location":"ExecutionListenerBus/#creating-instance","title":"Creating Instance","text":"<p><code>ExecutionListenerBus</code> takes the following to be created:</p> <ul> <li> ExecutionListenerManager <li> SparkSession or Session ID <p><code>ExecutionListenerBus</code> is created when:</p> <ul> <li><code>ExecutionListenerManager</code> is created</li> </ul>"},{"location":"ExecutionListenerBus/#intercepting-other-events","title":"Intercepting Other Events <pre><code>onOtherEvent(\n  event: SparkListenerEvent): Unit\n</code></pre> <p><code>onOtherEvent</code> is part of the <code>SparkListenerInterface</code> (Spark Core) abstraction.</p>  <p><code>onOtherEvent</code> post the given SparkListenerSQLExecutionEnd to all registered QueryExecutionListeners.</p>","text":""},{"location":"ExecutionListenerBus/#notifying-queryexecutionlistener-about-sparklistenersqlexecutionend","title":"Notifying QueryExecutionListener about SparkListenerSQLExecutionEnd <pre><code>doPostEvent(\n  listener: QueryExecutionListener,\n  event: SparkListenerSQLExecutionEnd): Unit\n</code></pre> <p><code>doPostEvent</code> is part of the <code>ListenerBus</code> (Spark Core) abstraction.</p>  <p><code>doPostEvent</code>...FIXME</p>","text":""},{"location":"ExecutionListenerManager/","title":"ExecutionListenerManager","text":"<p><code>ExecutionListenerManager</code> is a frontend (facade) of ExecutionListenerBus to manage QueryExecutionListeners (in a SparkSession).</p>"},{"location":"ExecutionListenerManager/#creating-instance","title":"Creating Instance","text":"<p><code>ExecutionListenerManager</code> takes the following to be created:</p> <ul> <li> SparkSession <li> SQLConf <li>loadExtensions flag</li> <p><code>ExecutionListenerManager</code> is created\u00a0when:</p> <ul> <li><code>BaseSessionStateBuilder</code> is requested for the session ExecutionListenerManager (while <code>SessionState</code> is built)</li> <li><code>ExecutionListenerManager</code> is requested to clone</li> </ul>"},{"location":"ExecutionListenerManager/#executionlistenerbus","title":"ExecutionListenerBus <p><code>ExecutionListenerManager</code> creates an ExecutionListenerBus when created with the following:</p> <ul> <li>This <code>ExecutionListenerManager</code></li> <li>SparkSession</li> </ul> <p>The <code>ExecutionListenerBus</code> is used for the following:</p> <ul> <li>Register a QueryExecutionListener</li> <li>Unregister a QueryExecutionListener</li> <li>Unregister all QueryExecutionListeners</li> <li>clone</li> </ul>","text":""},{"location":"ExecutionListenerManager/#accessing-executionlistenermanager","title":"Accessing ExecutionListenerManager <p><code>ExecutionListenerManager</code> is available as SparkSession.listenerManager (and SessionState.listenerManager).</p> <pre><code>scala&gt; :type spark.listenerManager\norg.apache.spark.sql.util.ExecutionListenerManager\n</code></pre> <pre><code>scala&gt; :type spark.sessionState.listenerManager\norg.apache.spark.sql.util.ExecutionListenerManager\n</code></pre>","text":""},{"location":"ExecutionListenerManager/#sparksqlqueryexecutionlisteners","title":"spark.sql.queryExecutionListeners <p><code>ExecutionListenerManager</code> is given <code>loadExtensions</code> flag when created.</p> <p>When enabled, <code>ExecutionListenerManager</code> registers the QueryExecutionListeners that are configured using the spark.sql.queryExecutionListeners configuration property.</p>","text":""},{"location":"ExecutionListenerManager/#removing-all-queryexecutionlisteners","title":"Removing All QueryExecutionListeners <pre><code>clear(): Unit\n</code></pre>","text":""},{"location":"ExecutionListenerManager/#registering-queryexecutionlistener","title":"Registering QueryExecutionListener <pre><code>register(\n  listener: QueryExecutionListener): Unit\n</code></pre> <p><code>register</code> requests the ExecutionListenerBus to register the given QueryExecutionListener.</p>  <p><code>register</code> is used when:</p> <ul> <li><code>Observation</code> is requested to register</li> <li><code>ExecutionListenerManager</code> is created and cloned</li> </ul>","text":""},{"location":"ExecutionListenerManager/#de-registering-queryexecutionlistener","title":"De-registering QueryExecutionListener <pre><code>unregister(\n  listener: QueryExecutionListener): Unit\n</code></pre>","text":""},{"location":"ExecutionListenerManager/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.util.ExecutionListenerManager</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.util.ExecutionListenerManager=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"ExperimentalMethods/","title":"ExperimentalMethods","text":"<p><code>ExperimentalMethods</code> holds extra &lt;&gt; and &lt;&gt; that are used in SparkOptimizer and SparkPlanner, respectively. <p>[[attributes]] .ExperimentalMethods' Attributes [width=\"100%\",cols=\"1m,2\",options=\"header\"] |=== | Name | Description</p> <p>| extraOptimizations a| [[extraOptimizations]] Collection of catalyst/Rule.md[rules] to optimize spark-sql-LogicalPlan.md[LogicalPlans] (i.e. <code>Rule[LogicalPlan]</code> objects)</p>"},{"location":"ExperimentalMethods/#source-scala","title":"[source, scala]","text":""},{"location":"ExperimentalMethods/#extraoptimizations-seqrulelogicalplan","title":"extraOptimizations: Seq[Rule[LogicalPlan]]","text":"<p>Used when <code>SparkOptimizer</code> is requested for the User Provided Optimizers</p> <p>| extraStrategies a| [[extraStrategies]] Collection of SparkStrategies</p>"},{"location":"ExperimentalMethods/#source-scala_1","title":"[source, scala]","text":""},{"location":"ExperimentalMethods/#extrastrategies-seqstrategy","title":"extraStrategies: Seq[Strategy]","text":"<p>Used when <code>SessionState</code> is requested for the SessionState.md#planner[SparkPlanner] |===</p> <p><code>ExperimentalMethods</code> is available as the &lt;&gt; property of a <code>SparkSession</code>."},{"location":"ExperimentalMethods/#source-scala_2","title":"[source, scala]","text":"<p>scala&gt; :type spark org.apache.spark.sql.SparkSession</p> <p>scala&gt; :type spark.experimental org.apache.spark.sql.ExperimentalMethods</p> <p>=== Example</p>"},{"location":"ExperimentalMethods/#source-scala_3","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.rules.Rule import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan</p> <p>object SampleRule extends Rule[LogicalPlan] {   def apply(p: LogicalPlan): LogicalPlan = p }</p> <p>scala&gt; :type spark org.apache.spark.sql.SparkSession</p> <p>spark.experimental.extraOptimizations = Seq(SampleRule)</p> <p>// extraOptimizations is used in Spark Optimizer val rule = spark.sessionState.optimizer.batches.flatMap(.rules).filter( == SampleRule).head scala&gt; rule.ruleName res0: String = SampleRule</p>"},{"location":"ExplainUtils/","title":"ExplainUtils","text":"<p><code>ExplainUtils</code> is a utility to process a query plan (when <code>QueryExecution</code> is requested for a simple (basic) text representation for <code>formatted</code> explain mode).</p>"},{"location":"ExplainUtils/#demo","title":"Demo","text":"<pre><code>val q = spark.range(5).join(spark.range(10), Seq(\"id\"), \"inner\")\n</code></pre> <pre><code>scala&gt; q.explain(mode = \"formatted\")\n== Physical Plan ==\nAdaptiveSparkPlan (6)\n+- Project (5)\n   +- BroadcastHashJoin Inner BuildLeft (4)\n      :- BroadcastExchange (2)\n      :  +- Range (1)\n      +- Range (3)\n\n\n(1) Range\nOutput [1]: [id#0L]\nArguments: Range (0, 5, step=1, splits=Some(16))\n\n(2) BroadcastExchange\nInput [1]: [id#0L]\nArguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#16]\n\n(3) Range\nOutput [1]: [id#2L]\nArguments: Range (0, 10, step=1, splits=Some(16))\n\n(4) BroadcastHashJoin\nLeft keys [1]: [id#0L]\nRight keys [1]: [id#2L]\nJoin condition: None\n\n(5) Project\nOutput [1]: [id#0L]\nInput [2]: [id#0L, id#2L]\n\n(6) AdaptiveSparkPlan\nOutput [1]: [id#0L]\nArguments: isFinalPlan=false\n</code></pre> <p>Note that the AdaptiveSparkPlan physical operator has isFinalPlan flag <code>false</code> (and you can see part of the final output).</p> <p>Execute Adaptive Query Execution optimization.</p> <pre><code>q.take(0)\n</code></pre> <p>The isFinalPlan flag should now be <code>true</code>.</p> <pre><code>scala&gt; q.explain(mode = \"formatted\")\n== Physical Plan ==\nAdaptiveSparkPlan (10)\n+- == Final Plan ==\n   * Project (6)\n   +- * BroadcastHashJoin Inner BuildLeft (5)\n      :- BroadcastQueryStage (3)\n      :  +- BroadcastExchange (2)\n      :     +- * Range (1)\n      +- * Range (4)\n+- == Initial Plan ==\n   Project (9)\n   +- BroadcastHashJoin Inner BuildLeft (8)\n      :- BroadcastExchange (7)\n      :  +- Range (1)\n      +- Range (4)\n\n\n(1) Range [codegen id : 1]\nOutput [1]: [id#0L]\nArguments: Range (0, 5, step=1, splits=Some(16))\n\n(2) BroadcastExchange\nInput [1]: [id#0L]\nArguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#72]\n\n(3) BroadcastQueryStage\nOutput [1]: [id#0L]\nArguments: 0\n\n(4) Range\nOutput [1]: [id#2L]\nArguments: Range (0, 10, step=1, splits=Some(16))\n\n(5) BroadcastHashJoin [codegen id : 2]\nLeft keys [1]: [id#0L]\nRight keys [1]: [id#2L]\nJoin condition: None\n\n(6) Project [codegen id : 2]\nOutput [1]: [id#0L]\nInput [2]: [id#0L, id#2L]\n\n(7) BroadcastExchange\nInput [1]: [id#0L]\nArguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#16]\n\n(8) BroadcastHashJoin\nLeft keys [1]: [id#0L]\nRight keys [1]: [id#2L]\nJoin condition: None\n\n(9) Project\nOutput [1]: [id#0L]\nInput [2]: [id#0L, id#2L]\n\n(10) AdaptiveSparkPlan\nOutput [1]: [id#0L]\nArguments: isFinalPlan=true\n</code></pre>"},{"location":"ExplainUtils/#processing-query-plan","title":"Processing Query Plan <pre><code>processPlan[T &lt;: QueryPlan[T]](\n  plan: T,\n  append: String =&gt; Unit): Unit\n</code></pre> <p><code>processPlan</code>...FIXME</p> <p><code>processPlan</code> is used when:</p> <ul> <li><code>QueryExecution</code> is requested to simpleString</li> </ul>","text":""},{"location":"ExplainUtils/#processplanskippingsubqueries","title":"processPlanSkippingSubqueries <pre><code>processPlanSkippingSubqueries[T &lt;: QueryPlan[T]](\n  plan: T,\n  append: String =&gt; Unit,\n  collectedOperators: BitSet): Unit\n</code></pre> <p><code>processPlanSkippingSubqueries</code>...FIXME</p>","text":""},{"location":"ExplainUtils/#collectoperatorswithid","title":"collectOperatorsWithID <pre><code>collectOperatorsWithID(\n  plan: QueryPlan[_],\n  operators: ArrayBuffer[QueryPlan[_]],\n  collectedOperators: BitSet): Unit\n</code></pre> <p><code>collectOperatorsWithID</code>...FIXME</p>","text":""},{"location":"ExplainUtils/#removetags","title":"removeTags <pre><code>removeTags(\n  plan: QueryPlan[_]): Unit\n</code></pre> <p><code>removeTags</code>...FIXME</p>","text":""},{"location":"ExplainUtils/#generateoperatorids","title":"generateOperatorIDs <pre><code>generateOperatorIDs(\n  plan: QueryPlan[_],\n  startOperatorID: Int): Int\n</code></pre> <p><code>generateOperatorIDs</code>...FIXME</p>","text":""},{"location":"ExpressionEncoder/","title":"ExpressionEncoder","text":"<p><code>ExpressionEncoder[T]</code> is the only built-in Encoder.</p> <p>Important</p> <p><code>ExpressionEncoder</code> is the only supported Encoder which is enforced when <code>Dataset</code> is created (even though <code>Dataset</code> data structure accepts a bare <code>Encoder[T]</code>).</p>"},{"location":"ExpressionEncoder/#creating-instance","title":"Creating Instance","text":"<p><code>ExpressionEncoder</code> takes the following to be created:</p> <ul> <li> Expression for object serialization <li> Expression for object deserialization <li> <code>ClassTag[T]</code> (Scala) <p><code>ExpressionEncoder</code> is created\u00a0when:</p> <ul> <li><code>Encoders</code> utility is used for genericSerializer</li> <li><code>ExpressionEncoder</code> utility is used to create one, javaBean and tuple</li> <li><code>RowEncoder</code> utility is used to create one</li> </ul>"},{"location":"ExpressionEncoder/#serializer","title":"Serializer <pre><code>serializer: Seq[NamedExpression]\n</code></pre> <p><code>ExpressionEncoder</code> creates the <code>serializer</code> (to be NamedExpressions) when created.</p>","text":""},{"location":"ExpressionEncoder/#encoders-utility","title":"Encoders Utility <p>Encoders utility contains the <code>ExpressionEncoder</code> for Scala and Java primitive types, e.g. <code>boolean</code>, <code>long</code>, <code>String</code>, <code>java.sql.Date</code>, <code>java.sql.Timestamp</code>, <code>Array[Byte]</code>.</p>","text":""},{"location":"ExpressionEncoder/#demo","title":"Demo <pre><code>import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\nval stringEncoder = ExpressionEncoder[String]\n</code></pre> <pre><code>scala&gt; val row = stringEncoder.toRow(\"hello world\")\nrow: org.apache.spark.sql.catalyst.InternalRow = [0,100000000b,6f77206f6c6c6568,646c72]\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.expressions.UnsafeRow\n</code></pre> <pre><code>scala&gt; val unsafeRow = row match { case ur: UnsafeRow =&gt; ur }\nunsafeRow: org.apache.spark.sql.catalyst.expressions.UnsafeRow = [0,100000000b,6f77206f6c6c6568,646c72]\n</code></pre>","text":""},{"location":"ExpressionEncoder/#creating-expressionencoder","title":"Creating ExpressionEncoder <pre><code>apply[T : TypeTag](): ExpressionEncoder[T]\n</code></pre> <p><code>apply</code> creates an <code>ExpressionEncoder</code> with the following:</p> <ul> <li>ScalaReflection.serializerForType</li> <li>ScalaReflection.deserializerForType</li> </ul>","text":""},{"location":"ExpressionEncoder/#creating-expressionencoder-for-scala-tuples","title":"Creating ExpressionEncoder for Scala Tuples <pre><code>tuple[T](\n  e: ExpressionEncoder[T]): ExpressionEncoder[Tuple1[T]]\ntuple[T1, T2](\n  e1: ExpressionEncoder[T1],\n  e2: ExpressionEncoder[T2]): ExpressionEncoder[(T1, T2)]\ntuple[T1, T2, T3](\n  e1: ExpressionEncoder[T1],\n  e2: ExpressionEncoder[T2],\n  e3: ExpressionEncoder[T3]): ExpressionEncoder[(T1, T2, T3)]\ntuple[T1, T2, T3, T4](\n  e1: ExpressionEncoder[T1],\n  e2: ExpressionEncoder[T2],\n  e3: ExpressionEncoder[T3],\n  e4: ExpressionEncoder[T4]): ExpressionEncoder[(T1, T2, T3, T4)]\ntuple[T1, T2, T3, T4, T5](\n  e1: ExpressionEncoder[T1],\n  e2: ExpressionEncoder[T2],\n  e3: ExpressionEncoder[T3],\n  e4: ExpressionEncoder[T4],\n  e5: ExpressionEncoder[T5]): ExpressionEncoder[(T1, T2, T3, T4, T5)]\ntuple(\n  encoders: Seq[ExpressionEncoder[_]]): ExpressionEncoder[_]\n</code></pre> <p><code>tuple</code>...FIXME</p> <p><code>tuple</code> is used when:</p> <ul> <li><code>Dataset</code> is requested to selectUntyped, select, joinWith</li> <li><code>KeyValueGroupedDataset</code> is requested to aggUntyped</li> <li><code>Encoders</code> utility is used to tuple</li> <li><code>ReduceAggregator</code> is requested for <code>bufferEncoder</code></li> </ul>","text":""},{"location":"ExpressionEncoder/#creating-expressionencoder-for-java-bean","title":"Creating ExpressionEncoder for Java Bean <pre><code>javaBean[T](\n  beanClass: Class[T]): ExpressionEncoder[T]\n</code></pre> <p><code>javaBean</code>...FIXME</p> <p><code>javaBean</code> is used when:</p> <ul> <li><code>Encoders</code> utility is used to bean</li> </ul>","text":""},{"location":"ExpressionEncoder/#resolveandbind","title":"resolveAndBind <pre><code>resolveAndBind(\n  attrs: Seq[Attribute] = schema.toAttributes,\n  analyzer: Analyzer = SimpleAnalyzer): ExpressionEncoder[T]\n</code></pre> <p><code>resolveAndBind</code> creates a deserializer for a LocalRelation with the given Attributes (to create a dummy query plan).</p> <p><code>resolveAndBind</code>...FIXME</p>  <p><code>resolveAndBind</code> is used when:</p> <ul> <li><code>Dataset</code> is requested for resolvedEnc</li> <li>others</li> </ul>","text":""},{"location":"ExpressionEncoder/#demo_1","title":"Demo","text":"<pre><code>case class Person(id: Long, name: String)\nimport org.apache.spark.sql.Encoders\nval schema = Encoders.product[Person].schema\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.encoders.{RowEncoder, ExpressionEncoder}\nimport org.apache.spark.sql.Row\nval encoder: ExpressionEncoder[Row] = RowEncoder.apply(schema).resolveAndBind()\nval deserializer = encoder.deserializer\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.InternalRow\nval input = InternalRow(1, \"Jacek\")\n</code></pre> <pre><code>scala&gt; deserializer.eval(input)\njava.lang.UnsupportedOperationException: Only code-generated evaluation is supported\n  at org.apache.spark.sql.catalyst.expressions.objects.CreateExternalRow.eval(objects.scala:1105)\n  ... 54 elided\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\nval ctx = new CodegenContext\nval code = deserializer.genCode(ctx).code\n</code></pre>"},{"location":"ExternalAppendOnlyUnsafeRowArray/","title":"ExternalAppendOnlyUnsafeRowArray","text":"<p><code>ExternalAppendOnlyUnsafeRowArray</code> is an append-only array of UnsafeRows.</p> <p><code>ExternalAppendOnlyUnsafeRowArray</code> keeps rows in memory until the spill threshold is reached that triggers disk spilling.</p>"},{"location":"ExternalAppendOnlyUnsafeRowArray/#creating-instance","title":"Creating Instance","text":"<p><code>ExternalAppendOnlyUnsafeRowArray</code> takes the following to be created:</p> <ul> <li> <code>TaskMemoryManager</code> (Apache Spark) <li> <code>BlockManager</code> (Apache Spark) <li> <code>SerializerManager</code> (Apache Spark) <li> <code>TaskContext</code> (Apache Spark) <li> Initial size (default: <code>1024</code>) <li> Page size (in bytes) <li>numRowsInMemoryBufferThreshold</li> <li>numRowsSpillThreshold</li> <p><code>ExternalAppendOnlyUnsafeRowArray</code> is created when:</p> <ul> <li><code>SortMergeJoinScanner</code> is requested for bufferedMatches</li> <li><code>UnsafeCartesianRDD</code> is requested to <code>compute</code></li> <li><code>UpdatingSessionsIterator</code> is requested to <code>startNewSession</code></li> <li><code>WindowExec</code> physical operator is requested to doExecute (and creates an internal buffer for window frames)</li> <li><code>WindowInPandasExec</code> (PySpark) is requested to <code>doExecute</code></li> </ul>"},{"location":"ExternalAppendOnlyUnsafeRowArray/#numrowsinmemorybufferthreshold","title":"numRowsInMemoryBufferThreshold <p><code>numRowsInMemoryBufferThreshold</code> is used for the following:</p> <ul> <li>initialSizeOfInMemoryBuffer</li> <li>add</li> </ul>","text":""},{"location":"ExternalAppendOnlyUnsafeRowArray/#numrowsspillthreshold","title":"numRowsSpillThreshold <p><code>numRowsSpillThreshold</code> is used for the following:</p> <ul> <li>Create an UnsafeExternalSorter (after the numRowsInMemoryBufferThreshold is reached)</li> </ul>","text":""},{"location":"ExternalAppendOnlyUnsafeRowArray/#numrows-counter","title":"numRows Counter <p><code>ExternalAppendOnlyUnsafeRowArray</code> uses <code>numRows</code> internal counter for the number of rows added.</p>","text":""},{"location":"ExternalAppendOnlyUnsafeRowArray/#length","title":"length <pre><code>length: Int\n</code></pre> <p><code>length</code> returns the numRows.</p> <p><code>length</code> is used when:</p> <ul> <li><code>SortMergeJoinExec</code> physical operator is requested to doExecute (for <code>LeftSemi</code>, <code>LeftAnti</code> and <code>ExistenceJoin</code> joins)</li> <li><code>FrameLessOffsetWindowFunctionFrame</code> is requested to <code>doWrite</code></li> <li><code>OffsetWindowFunctionFrameBase</code> is requested to <code>findNextRowWithNonNullInput</code></li> <li><code>SlidingWindowFunctionFrame</code> is requested to <code>write</code></li> <li><code>UnboundedFollowingWindowFunctionFrame</code> is requested to <code>write</code> and <code>currentUpperBound</code></li> <li><code>UnboundedOffsetWindowFunctionFrame</code> is requested to <code>prepare</code></li> <li><code>UnboundedPrecedingWindowFunctionFrame</code> is requested to <code>prepare</code></li> <li><code>UnboundedWindowFunctionFrame</code> is requested to <code>prepare</code></li> </ul>","text":""},{"location":"ExternalAppendOnlyUnsafeRowArray/#inmemorybuffer","title":"inMemoryBuffer <p><code>ExternalAppendOnlyUnsafeRowArray</code> creates an <code>inMemoryBuffer</code> internal array of UnsafeRows when created and the initialSizeOfInMemoryBuffer is greater than <code>0</code>.</p> <p>A new <code>UnsafeRow</code> can be added to <code>inMemoryBuffer</code> in add (up to the numRowsInMemoryBufferThreshold).</p> <p><code>inMemoryBuffer</code> is cleared in clear or add (when the number of <code>UnsafeRow</code>s is above the numRowsInMemoryBufferThreshold).</p>","text":""},{"location":"ExternalAppendOnlyUnsafeRowArray/#initialsizeofinmemorybuffer","title":"initialSizeOfInMemoryBuffer <p><code>ExternalAppendOnlyUnsafeRowArray</code> uses <code>initialSizeOfInMemoryBuffer</code> internal value as the number of UnsafeRows in the inMemoryBuffer.</p> <p><code>initialSizeOfInMemoryBuffer</code> is at most <code>128</code> and can be configured using thenumRowsInMemoryBufferThreshold (if smaller).</p>","text":""},{"location":"ExternalAppendOnlyUnsafeRowArray/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray=ALL\n</code></pre> <p>Refer to Logging</p>","text":""},{"location":"ExternalCatalog/","title":"ExternalCatalog","text":"<p><code>ExternalCatalog</code> is an abstraction of external system catalogs (aka metadata registry or metastore) of permanent relational entities (i.e., databases, tables, partitions, and functions).</p> <p><code>ExternalCatalog</code> is available as ephemeral (in-memory) or persistent (hive-aware).</p>"},{"location":"ExternalCatalog/#contract","title":"Contract","text":""},{"location":"ExternalCatalog/#getpartition","title":"getPartition <pre><code>getPartition(\n  db: String,\n  table: String,\n  spec: TablePartitionSpec): CatalogTablePartition\n</code></pre> <p>CatalogTablePartition of a given table (in a database)</p> <p>See:</p> <ul> <li>HiveExternalCatalog</li> </ul> <p>Used when:</p> <ul> <li><code>ExternalCatalogWithListener</code> is requested to <code>getPartition</code></li> <li><code>SessionCatalog</code> is requested to getPartition</li> </ul>","text":""},{"location":"ExternalCatalog/#getpartitionoption","title":"getPartitionOption <pre><code>getPartitionOption(\n  db: String,\n  table: String,\n  spec: TablePartitionSpec): Option[CatalogTablePartition]\n</code></pre> <p>CatalogTablePartition of a given table (in a database)</p> <p>See:</p> <ul> <li>HiveExternalCatalog</li> </ul> <p>Used when:</p> <ul> <li><code>ExternalCatalogWithListener</code> is requested to <code>getPartitionOption</code></li> <li><code>InsertIntoHiveTable</code> is requested to processInsert</li> </ul>","text":""},{"location":"ExternalCatalog/#gettable","title":"getTable <pre><code>getTable(\n  db: String,\n  table: String): CatalogTable\n</code></pre> <p>CatalogTable of a given table (in a database)</p> <p>See:</p> <ul> <li>HiveExternalCatalog</li> </ul> <p>Used when:</p> <ul> <li><code>ExternalCatalogWithListener</code> is requested to <code>getTable</code></li> <li><code>SessionCatalog</code> is requested to alterTableDataSchema, getTableRawMetadata and lookupRelation</li> </ul>","text":""},{"location":"ExternalCatalog/#gettablesbyname","title":"getTablesByName <pre><code>getTablesByName(\n  db: String,\n  tables: Seq[String]): Seq[CatalogTable]\n</code></pre> <p>CatalogTables of the given tables (in a database)</p> <p>See:</p> <ul> <li>HiveExternalCatalog</li> </ul> <p>Used when:</p> <ul> <li><code>ExternalCatalogWithListener</code> is requested to <code>getTablesByName</code></li> <li><code>SessionCatalog</code> is requested to getTablesByName</li> </ul>","text":""},{"location":"ExternalCatalog/#listpartitionsbyfilter","title":"listPartitionsByFilter <pre><code>listPartitionsByFilter(\n  db: String,\n  table: String,\n  predicates: Seq[Expression],\n  defaultTimeZoneId: String): Seq[CatalogTablePartition]\n</code></pre> <p>CatalogTablePartitions</p> <p>See:</p> <ul> <li>HiveExternalCatalog</li> </ul> <p>Used when:</p> <ul> <li><code>ExternalCatalogWithListener</code> is requested to <code>getTablesByName</code></li> <li><code>SessionCatalog</code> is requested to listPartitionsByFilter</li> </ul>","text":""},{"location":"ExternalCatalog/#implementations","title":"Implementations","text":"<ul> <li><code>ExternalCatalogWithListener</code></li> <li>HiveExternalCatalog</li> <li>InMemoryCatalog</li> </ul>"},{"location":"ExternalCatalog/#accessing-externalcatalog","title":"Accessing ExternalCatalog","text":"<p><code>ExternalCatalog</code> is available as externalCatalog of SharedState (in <code>SparkSession</code>).</p> <pre><code>scala&gt; :type spark\norg.apache.spark.sql.SparkSession\n\nscala&gt; :type spark.sharedState.externalCatalog\norg.apache.spark.sql.catalyst.catalog.ExternalCatalog\n</code></pre>"},{"location":"ExternalCatalogWithListener/","title":"ExternalCatalogWithListener","text":"<p><code>ExternalCatalogWithListener</code> is...FIXME</p>"},{"location":"ExtractEquiJoinKeys/","title":"ExtractEquiJoinKeys Scala Extractor","text":"<p><code>ExtractEquiJoinKeys</code> is a Scala extractor to destructure a Join logical operator into a tuple of the following elements:</p> <ol> <li> <p>Join type</p> </li> <li> <p>Left and right keys (for non-empty join keys in the condition of the <code>Join</code> operator)</p> </li> <li> <p>Optional join condition (a Catalyst expression) that could be used as a new join condition</p> </li> <li> <p>The left and the right logical operators</p> </li> <li> <p><code>JoinHint</code></p> </li> </ol> <p><code>unapply</code> gives <code>None</code> (aka nothing) when no join keys were found or the logical plan is not a Join logical operator.</p>"},{"location":"ExtractEquiJoinKeys/#demo","title":"Demo","text":"<pre><code>val left = Seq((0, 1, \"zero\"), (1, 2, \"one\")).toDF(\"k1\", \"k2\", \"name\")\nval right = Seq((0, 0, \"0\"), (1, 1, \"1\")).toDF(\"k1\", \"k2\", \"name\")\n</code></pre> <p>The following join query gives no data result but is enough for demo purposes.</p> <pre><code>val q = left\n.join(right, Seq(\"k1\", \"k2\", \"name\"))\n.where(left(\"k1\") &gt; 3)\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.plans.logical.Join\nval plan = q.queryExecution.analyzed\n</code></pre> <pre><code>val join = plan.collectFirst { case j: Join =&gt; j }.get\nassert(join.condition.isDefined)\n</code></pre> <p>Enable DEBUG logging level</p> <pre><code>import org.apache.log4j.{Level, Logger}\nLogger.getLogger(\"org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\").setLevel(Level.DEBUG)\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\nval joinParts = ExtractEquiJoinKeys.unapply(join)\n</code></pre> <pre><code>21/05/03 19:16:07 DEBUG ExtractEquiJoinKeys: Considering join on: Some((((k1#10 = k1#39) AND (k2#11 = k2#40)) AND (name#12 = name#41)))\n21/05/03 19:16:07 DEBUG ExtractEquiJoinKeys: leftKeys:List(k1#10, k2#11, name#12) | rightKeys:List(k1#39, k2#40, name#41)\njoinParts: Option[org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys.ReturnType] =\nSome((Inner,List(k1#10, k2#11, name#12),List(k1#39, k2#40, name#41),None,Project [_1#3 AS k1#10, _2#4 AS k2#11, _3#5 AS name#12]\n+- LocalRelation [_1#3, _2#4, _3#5]\n,Project [_1#32 AS k1#39, _2#33 AS k2#40, _3#34 AS name#41]\n+- LocalRelation [_1#32, _2#33, _3#34]\n,))\n</code></pre>"},{"location":"ExtractEquiJoinKeys/#destructuring-join-logical-plan","title":"Destructuring Join Logical Plan <pre><code>type ReturnType =\n  (JoinType,\n   Seq[Expression],\n   Seq[Expression],\n   Option[Expression],\n   LogicalPlan,\n   LogicalPlan,\n   JoinHint)\n\nunapply(\n  join: Join): Option[ReturnType]\n</code></pre> <p><code>unapply</code> prints out the following DEBUG message to the logs:</p> <pre><code>Considering join on: [condition]\n</code></pre> <p><code>unapply</code> then splits <code>condition</code> at <code>And</code> expression points (if there are any) to have a list of predicate expressions.</p> <p><code>unapply</code> finds EqualTo and EqualNullSafe binary predicates to collect the join keys (for the left and right side).</p> <p><code>unapply</code> takes the expressions that...FIXME...to build <code>otherPredicates</code>.</p> <p>In the end, <code>unapply</code> splits the pairs of join keys into collections of left and right join keys. <code>unapply</code> prints out the following DEBUG message to the logs:</p> <pre><code>leftKeys:[leftKeys] | rightKeys:[rightKeys]\n</code></pre> <p><code>unapply</code> is used when:</p> <ul> <li><code>JoinEstimation</code> is requested to estimateInnerOuterJoin</li> <li>JoinSelection and LogicalQueryStageStrategy execution planning strategies are executed</li> <li><code>NormalizeFloatingNumbers</code> and PartitionPruning logical optimizations are executed</li> </ul>","text":""},{"location":"ExtractEquiJoinKeys/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"ExtractJoinWithBuckets/","title":"ExtractJoinWithBuckets Scala Extractor","text":""},{"location":"ExtractJoinWithBuckets/#destructuring-basejoinexec","title":"Destructuring BaseJoinExec <pre><code>unapply(\n  plan: SparkPlan): Option[(BaseJoinExec, Int, Int)]\n</code></pre> <p><code>unapply</code> makes sure that the given SparkPlan is a BaseJoinExec and applicable.</p> <p>If so, <code>unapply</code> getBucketSpec for the <code>left</code> and <code>right</code> join child operators.</p> <p><code>unapply</code>...FIXME</p> <p><code>unapply</code> is used when:</p> <ul> <li>CoalesceBucketsInJoin physical optimization is executed</li> </ul>","text":""},{"location":"ExtractJoinWithBuckets/#isapplicable","title":"isApplicable <pre><code>isApplicable(\n  j: BaseJoinExec): Boolean\n</code></pre> <p><code>isApplicable</code> is <code>true</code> when the following all hold:</p> <ol> <li> <p>The given BaseJoinExec physical operator is either a SortMergeJoinExec or a ShuffledHashJoinExec</p> </li> <li> <p>The <code>left</code> side of the join has a FileSourceScanExec operator</p> </li> <li> <p>The <code>right</code> side of the join has a FileSourceScanExec operator</p> </li> <li> <p>satisfiesOutputPartitioning on the leftKeys and the outputPartitioning of the left join operator</p> </li> <li> <p>satisfiesOutputPartitioning on the rightKeys and the outputPartitioning of the right join operator</p> </li> </ol>","text":""},{"location":"ExtractJoinWithBuckets/#hasscanoperation","title":"hasScanOperation <pre><code>hasScanOperation(\n  plan: SparkPlan): Boolean\n</code></pre> <p><code>hasScanOperation</code> holds <code>true</code> for SparkPlan physical operators that are FileSourceScanExecs (possibly as the children of FilterExecs and ProjectExecs).</p>","text":""},{"location":"ExtractJoinWithBuckets/#satisfiesoutputpartitioning","title":"satisfiesOutputPartitioning <pre><code>satisfiesOutputPartitioning(\n  keys: Seq[Expression],\n  partitioning: Partitioning): Boolean\n</code></pre> <p><code>satisfiesOutputPartitioning</code> holds <code>true</code> for HashPartitioning partitionings that match the given join keys (their number and equivalence).</p>","text":""},{"location":"ExtractJoinWithBuckets/#bucket-spec-of-filesourcescanexec-operator","title":"Bucket Spec of FileSourceScanExec Operator <pre><code>getBucketSpec(\n  plan: SparkPlan): Option[BucketSpec]\n</code></pre> <p><code>getBucketSpec</code> finds the FileSourceScanExec operator (in the given SparkPlan) with a non-empty bucket spec but an empty optionalNumCoalescedBuckets. When found, <code>getBucketSpec</code> returns the non-empty bucket spec.</p>","text":""},{"location":"ExtractSingleColumnNullAwareAntiJoin/","title":"ExtractSingleColumnNullAwareAntiJoin Scala Extractor","text":"<p><code>ExtractSingleColumnNullAwareAntiJoin</code> is a Scala extractor to destructure a Join logical operator into a tuple of the following elements:</p> <ol> <li>Streamed Side Keys (Catalyst expressions)</li> <li>Build Side Keys (Catalyst expressions)</li> </ol> <p><code>ExtractSingleColumnNullAwareAntiJoin</code> is used to support single-column NULL-aware anti joins (described in the VLDB paper) that will almost certainly be planned as a very time-consuming Broadcast Nested Loop join (<code>O(M*N)</code> calculation). If it's a single column case this expensive calculation could be optimized into <code>O(M)</code> using hash lookup instead of loop lookup. Refer to SPARK-32290.</p>"},{"location":"ExtractSingleColumnNullAwareAntiJoin/#sparksqloptimizenullawareantijoin","title":"spark.sql.optimizeNullAwareAntiJoin <p><code>ExtractSingleColumnNullAwareAntiJoin</code> uses the spark.sql.optimizeNullAwareAntiJoin configuration property.</p>","text":""},{"location":"ExtractSingleColumnNullAwareAntiJoin/#destructuring-join-logical-plan","title":"Destructuring Join Logical Plan <pre><code>type ReturnType =\n// streamedSideKeys, buildSideKeys\n  (Seq[Expression], Seq[Expression])\n\nunapply(\n  join: Join): Option[ReturnType]\n</code></pre> <p><code>unapply</code> matches Join logical operators with LeftAnti join type and the following condition:</p> <pre><code>Or(EqualTo(a=b), IsNull(EqualTo(a=b)))\n</code></pre> <p><code>unapply</code>...FIXME</p> <p><code>unapply</code> is used when:</p> <ul> <li><code>AQEPropagateEmptyRelation</code> adaptive logical optimization is executed</li> <li><code>JoinSelection</code> execution planning strategy is executed</li> <li><code>LogicalQueryStageStrategy</code> execution planning strategy is executed</li> </ul>","text":""},{"location":"FileRelation/","title":"FileRelation","text":"<p><code>FileRelation</code> is an abstraction of relations that are backed by files.</p>"},{"location":"FileRelation/#contract","title":"Contract","text":""},{"location":"FileRelation/#inputfiles","title":"inputFiles <pre><code>inputFiles: Array[String]\n</code></pre> <p>The files that this relation will read when scanning</p> <p>Used when:</p> <ul> <li><code>Dataset</code> is requested for the inputFiles</li> </ul>","text":""},{"location":"FileRelation/#implementations","title":"Implementations","text":"<ul> <li>HadoopFsRelation</li> </ul>"},{"location":"Filter/","title":"Data Source Filter Predicate","text":"<p><code>Filter</code> is the &lt;&gt; for &lt;&gt; that can be pushed down to a relation (aka data source). <p><code>Filter</code> is used when:</p> <ul> <li> <p>(Data Source API V1) <code>BaseRelation</code> is requested for unhandled filter predicates (and hence <code>BaseRelation</code> implementations, i.e. JDBCRelation)</p> </li> <li> <p>(Data Source API V1) <code>PrunedFilteredScan</code> is requested for build a scan (and hence <code>PrunedFilteredScan</code> implementations, i.e. JDBCRelation)</p> </li> <li> <p><code>FileFormat</code> is requested to buildReader (and hence <code>FileFormat</code> implementations, i.e. <code>OrcFileFormat</code>, <code>CSVFileFormat</code>, <code>JsonFileFormat</code>, <code>TextFileFormat</code> and Spark MLlib's <code>LibSVMFileFormat</code>)</p> </li> <li> <p><code>FileFormat</code> is requested to build a Data Reader with partition column values appended (and hence <code>FileFormat</code> implementations, i.e. <code>OrcFileFormat</code>, ParquetFileFormat)</p> </li> <li> <p><code>RowDataSourceScanExec</code> is RowDataSourceScanExec.md#creating-instance[created] (for a DataSourceScanExec.md#simpleString[simple text representation (in a query plan tree)])</p> </li> <li> <p><code>DataSourceStrategy</code> execution planning strategy is requested to pruneFilterProject (when executed for LogicalRelation.md[LogicalRelation] logical operators with a PrunedFilteredScan or a PrunedScan)</p> </li> <li> <p><code>DataSourceStrategy</code> execution planning strategy is requested to selectFilters</p> </li> <li> <p><code>JDBCRDD</code> is created and requested to scanTable</p> </li> </ul> <p>[[contract]] [source, scala]</p> <p>package org.apache.spark.sql.sources</p> <p>abstract class Filter {   // only required methods that have no implementation   // the others follow   def references: Array[String] }</p> <p>.Filter Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| <code>references</code> a| [[references]] Column references, i.e. list of column names that are referenced by a filter</p> <p>Used when:</p> <ul> <li> <p><code>Filter</code> is requested to &lt;&gt; <li> <p>&lt;&gt;, &lt;&gt; and &lt;&gt; filters are requested for the &lt;&gt; |=== <p>=== [[findReferences]] Finding Column References in Any Value -- <code>findReferences</code> Method</p>"},{"location":"Filter/#source-scala","title":"[source, scala]","text":""},{"location":"Filter/#findreferencesvalue-any-arraystring","title":"findReferences(value: Any): Array[String]","text":"<p><code>findReferences</code> takes the &lt;&gt; from the <code>value</code> filter is it is one or returns an empty array."},{"location":"FunctionRegistry/","title":"FunctionRegistry","text":"<p><code>FunctionRegistry</code> is an extension of the FunctionRegistryBase abstraction for function registries with functions that produce a result of Expression type.</p>"},{"location":"FunctionRegistry/#implementations","title":"Implementations","text":"<ul> <li><code>EmptyFunctionRegistry</code></li> <li>SimpleFunctionRegistry</li> </ul>"},{"location":"FunctionRegistry/#accessing-functionregistry","title":"Accessing FunctionRegistry","text":"<p><code>FunctionRegistry</code> is available using SessionState.functionRegistry.</p> <pre><code>import org.apache.spark.sql.catalyst.analysis.FunctionRegistry\nassert(spark.sessionState.functionRegistry.isInstanceOf[FunctionRegistry])\n</code></pre>"},{"location":"FunctionRegistry/#built-in-functions","title":"Built-In Functions <pre><code>builtin: SimpleFunctionRegistry\n</code></pre> <p><code>builtin</code> creates a new SimpleFunctionRegistry and registers all the built-in function expressions.</p>  <p><code>builtin</code> is used when:</p> <ul> <li><code>FunctionRegistry</code> utility is used for functionSet</li> <li><code>SessionCatalog</code> is requested to isTemporaryFunction and reset</li> <li><code>DropFunctionCommand</code> and <code>RefreshFunctionCommand</code> commands are executed</li> <li><code>BaseSessionStateBuilder</code> is requested for a FunctionRegistry</li> </ul>","text":""},{"location":"FunctionRegistry/#logicalplan","title":"logicalPlan <pre><code>type TableFunctionBuilder = Seq[Expression] =&gt; LogicalPlan\nlogicalPlan[T &lt;: LogicalPlan : ClassTag](\n  name: String): (String, (ExpressionInfo, TableFunctionBuilder))\n</code></pre> <p><code>logicalPlan</code> builds info and builder for the given <code>name</code> table function and returns a tuple of the following:</p> <ul> <li>The given name</li> <li>The info</li> <li>A function that uses the builder to build a LogicalPlan for a given Expressions</li> </ul>  <p><code>logicalPlan</code> is used when:</p> <ul> <li>TableFunctionRegistry is created (and registers range table function)</li> </ul>","text":""},{"location":"FunctionRegistry/#generator","title":"generator <pre><code>type TableFunctionBuilder = Seq[Expression] =&gt; LogicalPlan\ngenerator[T &lt;: Generator : ClassTag](\n  name: String,\n  outer: Boolean = false): (String, (ExpressionInfo, TableFunctionBuilder))\n</code></pre> <p><code>generator</code>...FIXME</p>  <p><code>generator</code> is used when:</p> <ul> <li>TableFunctionRegistry is created (and registers generate table functions)</li> </ul>","text":""},{"location":"FunctionRegistryBase/","title":"FunctionRegistryBase","text":"<p><code>FunctionRegistryBase[T]</code> is an abstraction of function registries for registering functions (that produce a result of type <code>T</code>).</p>"},{"location":"FunctionRegistryBase/#contract-subset","title":"Contract (Subset)","text":""},{"location":"FunctionRegistryBase/#lookupfunction","title":"lookupFunction <pre><code>lookupFunction(\n  name: FunctionIdentifier): Option[ExpressionInfo]\n</code></pre> <p>Looks up the <code>ExpressionInfo</code> metadata of a user-defined function by the given <code>name</code></p> <p>See SimpleFunctionRegistryBase</p> <p>Used when:</p> <ul> <li><code>FunctionRegistryBase</code> is requested to check if a function exists</li> <li><code>SessionCatalog</code> is requested to lookupBuiltinOrTempFunction, lookupBuiltinOrTempTableFunction, lookupPersistentFunction</li> </ul>","text":""},{"location":"FunctionRegistryBase/#registering-named-user-defined-function","title":"Registering Named User-Defined Function <pre><code>registerFunction(\n  name: FunctionIdentifier,\n  info: ExpressionInfo,\n  builder: Seq[Expression] =&gt; T): Unit\nregisterFunction(\n  name: FunctionIdentifier,\n  builder: Seq[Expression] =&gt; T,\n  source: String): Unit // (1)!\n</code></pre> <ol> <li>A final method that relays to the abstract <code>registerFunction</code></li> </ol> <p>Registers a user-defined function (written in Python, Scala or Java) under the given <code>name</code></p> <p>See:</p> <ul> <li>SimpleFunctionRegistryBase</li> </ul> <p>Used when:</p> <ul> <li><code>FunctionRegistryBase</code> is requested to createOrReplaceTempFunction</li> <li><code>SessionCatalog</code> is requested to registerFunction, reset</li> <li><code>SparkSessionExtensions</code> is requested to registerFunctions, registerTableFunctions</li> </ul>","text":""},{"location":"FunctionRegistryBase/#implementations","title":"Implementations","text":"<ul> <li><code>EmptyFunctionRegistryBase</code></li> <li>FunctionRegistry</li> <li>SimpleFunctionRegistryBase</li> <li>TableFunctionRegistry</li> </ul>"},{"location":"FunctionRegistryBase/#createorreplacetempfunction","title":"createOrReplaceTempFunction <pre><code>createOrReplaceTempFunction(\n  name: String,\n  builder: Seq[Expression] =&gt; T,\n  source: String): Unit\n</code></pre> <p><code>createOrReplaceTempFunction</code> registers a named function (with a <code>FunctionIdentifier</code> for the given name).</p>  <p>source Argument</p>    source Call Site     <code>java_udf</code> UDFRegistration.register   <code>python_udf</code> UDFRegistration.registerPython   <code>scala_udf</code> UDFRegistration.register    <p><code>source</code> is used to create an <code>ExpressionInfo</code> for registering a named function.</p>   <p><code>createOrReplaceTempFunction</code> is used when:</p> <ul> <li><code>UDFRegistration</code> is requested to register a user-defined function written in Python or Scala</li> </ul>","text":""},{"location":"GlobalTempViewManager/","title":"GlobalTempViewManager -- Management Interface of Global Temporary Views","text":"<p><code>GlobalTempViewManager</code> is the &lt;&gt; to manage global temporary views (that <code>SessionCatalog</code> uses when requested to create, alter or drop global temporary views). <p>Strictly speaking, <code>GlobalTempViewManager</code> simply &lt;&gt; the names of the global temporary views registered (and the corresponding &lt;&gt;) and has no interaction with other services in Spark SQL. <p><code>GlobalTempViewManager</code> is available as SharedState.md#globalTempViewManager[globalTempViewManager] property of a <code>SharedState</code>.</p> <p>.GlobalTempViewManager and SparkSession image::images/spark-sql-GlobalTempViewManager.png[align=\"center\"]</p>"},{"location":"GlobalTempViewManager/#source-scala","title":"[source, scala]","text":"<p>scala&gt; :type spark org.apache.spark.sql.SparkSession</p> <p>scala&gt; :type spark.sharedState.globalTempViewManager org.apache.spark.sql.catalyst.catalog.GlobalTempViewManager</p> <p>[[methods]] .GlobalTempViewManager API [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| &lt;&gt; a|"},{"location":"GlobalTempViewManager/#source-scala_1","title":"[source, scala]","text":""},{"location":"GlobalTempViewManager/#clear-unit","title":"clear(): Unit","text":"<p>| &lt;&gt; a|"},{"location":"GlobalTempViewManager/#source-scala_2","title":"[source, scala]","text":"<p>create(   name: String,   viewDefinition: LogicalPlan,   overrideIfExists: Boolean): Unit</p> <p>Registers (creates) a global temporary view (as a spark-sql-LogicalPlan.md[LogicalPlan]) by name</p> <p>Used when <code>SessionCatalog</code> is requested to createGlobalTempView</p> <p>| &lt;&gt; a|"},{"location":"GlobalTempViewManager/#source-scala_3","title":"[source, scala]","text":"<p>get(   name: String): Option[LogicalPlan]</p> <p>Finds the global view definition (as a spark-sql-LogicalPlan.md[LogicalPlan]) for the given name if available</p> <p>Used when <code>SessionCatalog</code> is requested to getGlobalTempView, getTempViewOrPermanentTableMetadata, lookupRelation, isTemporaryTable, refreshTable</p> <p>| &lt;&gt; a|"},{"location":"GlobalTempViewManager/#source-scala_4","title":"[source, scala]","text":""},{"location":"GlobalTempViewManager/#listviewnamespattern-string-seqstring","title":"listViewNames(pattern: String): Seq[String]","text":"<p>| &lt;&gt; a|"},{"location":"GlobalTempViewManager/#source-scala_5","title":"[source, scala]","text":""},{"location":"GlobalTempViewManager/#removename-string-boolean","title":"remove(name: String): Boolean","text":"<p>| &lt;&gt; a|"},{"location":"GlobalTempViewManager/#source-scala_6","title":"[source, scala]","text":""},{"location":"GlobalTempViewManager/#renameoldname-string-newname-string-boolean","title":"rename(oldName: String, newName: String): Boolean","text":"<p>| &lt;&gt; a|"},{"location":"GlobalTempViewManager/#source-scala_7","title":"[source, scala]","text":""},{"location":"GlobalTempViewManager/#updatename-string-viewdefinition-logicalplan-boolean","title":"update(name: String, viewDefinition: LogicalPlan): Boolean","text":"<p>|===</p> <p><code>GlobalTempViewManager</code> is &lt;&gt; exclusively when <code>SharedState</code> is requested for &lt;&gt; (for the very first time only as it is cached). <p>[[database]] [[creating-instance]] <code>GlobalTempViewManager</code> takes the name of the database when created.</p> <p>.Creating GlobalTempViewManager image::images/spark-sql-GlobalTempViewManager-creating-instance.png[align=\"center\"]</p> <p>[[internal-registries]] .GlobalTempViewManager's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| viewDefinitions | [[viewDefinitions]] Registry of global temporary view definitions as &lt;&gt; per view name. |=== <p>=== [[clear]] <code>clear</code> Method</p>"},{"location":"GlobalTempViewManager/#source-scala_8","title":"[source, scala]","text":""},{"location":"GlobalTempViewManager/#clear-unit_1","title":"clear(): Unit","text":"<p><code>clear</code> simply removes all the entries in the &lt;&gt; internal registry. <p>NOTE: <code>clear</code> is used when <code>SessionCatalog</code> is requested to reset (that happens to be exclusively in the Spark SQL internal tests).</p> <p>=== [[create]] Creating (Registering) Global Temporary View (Definition) -- <code>create</code> Method</p>"},{"location":"GlobalTempViewManager/#source-scala_9","title":"[source, scala]","text":"<p>create(   name: String,   viewDefinition: LogicalPlan,   overrideIfExists: Boolean): Unit</p> <p><code>create</code> simply registers (adds) the input &lt;&gt; under the input <code>name</code>. <p><code>create</code> throws an <code>AnalysisException</code> when the input <code>overrideIfExists</code> flag is off and the &lt;&gt; internal registry contains the input <code>name</code>. <pre><code>Temporary view '[table]' already exists\n</code></pre> <p>NOTE: <code>create</code> is used when <code>SessionCatalog</code> is requested to createGlobalTempView (when &lt;&gt; and &lt;&gt; logical commands are executed). <p>=== [[get]] Retrieving Global View Definition Per Name -- <code>get</code> Method</p>"},{"location":"GlobalTempViewManager/#source-scala_10","title":"[source, scala]","text":""},{"location":"GlobalTempViewManager/#getname-string-optionlogicalplan","title":"get(name: String): Option[LogicalPlan]","text":"<p><code>get</code> simply returns the &lt;&gt; that was registered under the <code>name</code> if it defined. <p>NOTE: <code>get</code> is used when <code>SessionCatalog</code> is requested to getGlobalTempView, getTempViewOrPermanentTableMetadata, lookupRelation, isTemporaryTable or refreshTable.</p> <p>=== [[listViewNames]] Listing Global Temporary Views For Pattern -- <code>listViewNames</code> Method</p>"},{"location":"GlobalTempViewManager/#source-scala_11","title":"[source, scala]","text":""},{"location":"GlobalTempViewManager/#listviewnamespattern-string-seqstring_1","title":"listViewNames(pattern: String): Seq[String]","text":"<p><code>listViewNames</code> simply gives a list of the global temporary views with names matching the input <code>pattern</code>.</p> <p><code>listViewNames</code> is used when <code>SessionCatalog</code> is requested to listTables</p> <p>=== [[remove]] Removing (De-Registering) Global Temporary View -- <code>remove</code> Method</p>"},{"location":"GlobalTempViewManager/#source-scala_12","title":"[source, scala]","text":""},{"location":"GlobalTempViewManager/#removename-string-boolean_1","title":"remove(name: String): Boolean","text":"<p><code>remove</code> simply tries to remove the <code>name</code> from the &lt;&gt; internal registry and returns <code>true</code> when removed or <code>false</code> otherwise. <p><code>remove</code> is used when <code>SessionCatalog</code> is requested to drop a global temporary view or table.</p> <p>=== [[rename]] <code>rename</code> Method</p>"},{"location":"GlobalTempViewManager/#source-scala_13","title":"[source, scala]","text":""},{"location":"GlobalTempViewManager/#renameoldname-string-newname-string-boolean_1","title":"rename(oldName: String, newName: String): Boolean","text":"<p><code>rename</code>...FIXME</p> <p>NOTE: <code>rename</code> is used when...FIXME</p> <p>=== [[update]] <code>update</code> Method</p>"},{"location":"GlobalTempViewManager/#source-scala_14","title":"[source, scala]","text":""},{"location":"GlobalTempViewManager/#updatename-string-viewdefinition-logicalplan-boolean_1","title":"update(name: String, viewDefinition: LogicalPlan): Boolean","text":"<p><code>update</code>...FIXME</p> <p><code>update</code> is used when <code>SessionCatalog</code> is requested to alter a global temporary view.</p>"},{"location":"HashMapGenerator/","title":"HashMapGenerator","text":"<p><code>HashMapGenerator</code> is an abstraction of HashMap generators that can generate an append-only row-based hash map for extremely fast key-value lookups while evaluating aggregates.</p>"},{"location":"HashMapGenerator/#contract","title":"Contract","text":""},{"location":"HashMapGenerator/#generateequals","title":"generateEquals <pre><code>generateEquals(): String\n</code></pre> <p>Used when:</p> <ul> <li><code>HashMapGenerator</code> is requested to generate a Java code</li> </ul>","text":""},{"location":"HashMapGenerator/#generatefindorinsert","title":"generateFindOrInsert <pre><code>generateFindOrInsert(): String\n</code></pre> <p>Used when:</p> <ul> <li><code>HashMapGenerator</code> is requested to generate a Java code</li> </ul>","text":""},{"location":"HashMapGenerator/#generaterowiterator","title":"generateRowIterator <pre><code>generateRowIterator(): String\n</code></pre> <p>Used when:</p> <ul> <li><code>HashMapGenerator</code> is requested to generate a Java code</li> </ul>","text":""},{"location":"HashMapGenerator/#initializeaggregatehashmap","title":"initializeAggregateHashMap <pre><code>initializeAggregateHashMap(): String\n</code></pre> <p>Used when:</p> <ul> <li><code>HashMapGenerator</code> is requested to generate a Java code</li> </ul>","text":""},{"location":"HashMapGenerator/#implementations","title":"Implementations","text":"<ul> <li><code>RowBasedHashMapGenerator</code></li> <li><code>VectorizedHashMapGenerator</code></li> </ul>"},{"location":"HashMapGenerator/#generating-java-code","title":"Generating Java Code <pre><code>generate(): String\n</code></pre> <p><code>generate</code> creates a source code of a Java class with the following (in that order):</p> <ol> <li>generatedClassName</li> <li>initializeAggregateHashMap</li> <li>generateFindOrInsert</li> <li>generateEquals</li> <li>generateHashFunction</li> <li>generateRowIterator</li> <li>generateClose</li> </ol>  <p><code>generate</code> is used when:</p> <ul> <li><code>HashAggregateExec</code> physical operator is requested to doProduceWithKeys</li> </ul>","text":""},{"location":"InMemoryCatalog/","title":"InMemoryCatalog","text":"<p><code>InMemoryCatalog</code> is...FIXME</p> <p>=== [[listPartitionsByFilter]] <code>listPartitionsByFilter</code> Method</p>"},{"location":"InMemoryCatalog/#source-scala","title":"[source, scala]","text":"<p>listPartitionsByFilter(   db: String,   table: String,   predicates: Seq[Expression],   defaultTimeZoneId: String): Seq[CatalogTablePartition]</p> <p><code>listPartitionsByFilter</code>...FIXME</p> <p><code>listPartitionsByFilter</code> is part of the ExternalCatalog abstraction.</p>"},{"location":"InsertableRelation/","title":"InsertableRelation","text":"<p><code>InsertableRelation</code> is an abstraction of relations with support for inserting or overwriting data.</p>"},{"location":"InsertableRelation/#contract","title":"Contract","text":""},{"location":"InsertableRelation/#inserting-data-into-or-overwriting-relation","title":"Inserting Data into or Overwriting Relation <pre><code>insert(\n  data: DataFrame,\n  overwrite: Boolean): Unit\n</code></pre> <p>Inserts or overwrites data (from the given DataFrame)</p> <p>Used when:</p> <ul> <li>InsertIntoDataSourceCommand logical command is executed</li> <li><code>SupportsV1Write</code> physical operator is executed</li> </ul>","text":""},{"location":"InsertableRelation/#implementations","title":"Implementations","text":"<ul> <li>JDBCRelation</li> </ul>"},{"location":"InternalRow/","title":"InternalRow","text":"<p><code>InternalRow</code> is an abstraction of binary rows.</p> <p><code>InternalRow</code> is also called Catalyst row or Spark SQL row.</p>"},{"location":"InternalRow/#contract","title":"Contract","text":""},{"location":"InternalRow/#copy","title":"copy <pre><code>copy(): InternalRow\n</code></pre>","text":""},{"location":"InternalRow/#numfields","title":"numFields <pre><code>numFields: Int\n</code></pre>","text":""},{"location":"InternalRow/#setnullat","title":"setNullAt <pre><code>setNullAt(\n  i: Int): Unit\n</code></pre>","text":""},{"location":"InternalRow/#update","title":"update <pre><code>update(\n  i: Int,\n  value: Any): Unit\n</code></pre> <p>Updates the <code>value</code> at column <code>i</code></p>","text":""},{"location":"InternalRow/#implementations","title":"Implementations","text":"<ul> <li>BaseGenericInternalRow</li> <li>ColumnarBatchRow</li> <li>ColumnarRow</li> <li>JoinedRow</li> <li>MutableColumnarRow</li> <li>UnsafeRow</li> </ul>"},{"location":"InternalRow/#serializable","title":"Serializable","text":"<p><code>InternalRow</code> is a <code>Serializable</code> (Java).</p>"},{"location":"InternalRow/#demo","title":"Demo","text":"<pre><code>// The type of your business objects\ncase class Person(id: Long, name: String)\n\n// The encoder for Person objects\nimport org.apache.spark.sql.Encoders\nval personEncoder = Encoders.product[Person]\n\n// The expression encoder for Person objects\nimport org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\nval personExprEncoder = personEncoder.asInstanceOf[ExpressionEncoder[Person]]\n\n// Convert Person objects to InternalRow\nscala&gt; val row = personExprEncoder.toRow(Person(0, \"Jacek\"))\nrow: org.apache.spark.sql.catalyst.InternalRow = [0,0,1800000005,6b6563614a]\n\n// How many fields are available in Person's InternalRow?\nscala&gt; row.numFields\nres0: Int = 2\n\n// Are there any NULLs in this InternalRow?\nscala&gt; row.anyNull\nres1: Boolean = false\n\n// You can create your own InternalRow objects\nimport org.apache.spark.sql.catalyst.InternalRow\n\nscala&gt; val ir = InternalRow(5, \"hello\", (0, \"nice\"))\nir: org.apache.spark.sql.catalyst.InternalRow = [5,hello,(0,nice)]\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.InternalRow\n\nscala&gt; InternalRow.empty\nres0: org.apache.spark.sql.catalyst.InternalRow = [empty row]\n\nscala&gt; InternalRow(0, \"string\", (0, \"pair\"))\nres1: org.apache.spark.sql.catalyst.InternalRow = [0,string,(0,pair)]\n\nscala&gt; InternalRow.fromSeq(Seq(0, \"string\", (0, \"pair\")))\nres2: org.apache.spark.sql.catalyst.InternalRow = [0,string,(0,pair)]\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.InternalRow\nimport org.apache.spark.unsafe.types.UTF8String\nval demoRow = InternalRow(UTF8String.fromString(\"demo\"))\n</code></pre> <pre><code>assert(demoRow.getString(0).equals(\"demo\"))\n</code></pre>"},{"location":"IntervalUtils/","title":"IntervalUtils","text":""},{"location":"IntervalUtils/#parsing-calendarinterval","title":"Parsing CalendarInterval <pre><code>fromIntervalString(\n  input: String): CalendarInterval\n</code></pre> <p><code>fromIntervalString</code>...FIXME</p> <p><code>fromIntervalString</code> is used when:</p> <ul> <li><code>TimeWindow</code> utility is used to getIntervalInMicroSeconds</li> <li><code>Dataset</code> is requested to withWatermark</li> </ul>","text":""},{"location":"JoinSelectionHelper/","title":"JoinSelectionHelper","text":""},{"location":"JoinSelectionHelper/#canBroadcastBySize","title":"canBroadcastBySize","text":"<pre><code>canBroadcastBySize(\nplan: LogicalPlan,\nconf: SQLConf): Boolean\n</code></pre> <p><code>canBroadcastBySize</code>...FIXME</p> <p><code>canBroadcastBySize</code> is used when:</p> <ul> <li>InjectRuntimeFilter logical optimization is executed (and injectInSubqueryFilter and isProbablyShuffleJoin)</li> <li><code>JoinSelectionHelper</code> is requested to getBroadcastBuildSide</li> <li>JoinSelection execution planning strategy is executed</li> </ul>"},{"location":"JoinSelectionHelper/#getBroadcastBuildSide","title":"getBroadcastBuildSide","text":"<pre><code>getBroadcastBuildSide(\nleft: LogicalPlan,\nright: LogicalPlan,\njoinType: JoinType,\nhint: JoinHint,\nhintOnly: Boolean,\nconf: SQLConf): Option[BuildSide]\n</code></pre> <p><code>getBroadcastBuildSide</code>...FIXME</p> <p><code>getBroadcastBuildSide</code> is used when:</p> <ul> <li><code>JoinSelectionHelper</code> is requested to canPlanAsBroadcastHashJoin</li> <li>JoinSelection execution planning strategy is executed</li> </ul>"},{"location":"JoinSelectionHelper/#canPlanAsBroadcastHashJoin","title":"canPlanAsBroadcastHashJoin","text":"<pre><code>canPlanAsBroadcastHashJoin(\njoin: Join,\nconf: SQLConf): Boolean\n</code></pre> <p><code>canPlanAsBroadcastHashJoin</code>...FIXME</p> <p><code>canPlanAsBroadcastHashJoin</code> is used when:</p> <ul> <li>PushDownLeftSemiAntiJoin logical optimization is executed</li> </ul>"},{"location":"JoinSelectionHelper/#hintToSortMergeJoin","title":"hintToSortMergeJoin","text":"<pre><code>hintToSortMergeJoin(\nhint: JoinHint): Boolean\n</code></pre> <p><code>hintToSortMergeJoin</code> is enabled (<code>true</code>) when either the left or the right side of the join contains SHUFFLE_MERGE hint.</p> <p><code>hintToSortMergeJoin</code> is used when:</p> <ul> <li>PushDownLeftSemiAntiJoin logical optimization is executed</li> </ul>"},{"location":"KnownSizeEstimation/","title":"KnownSizeEstimation","text":"<p><code>KnownSizeEstimation</code> is an abstraction of size estimators that can give a more precise size estimation.</p>"},{"location":"KnownSizeEstimation/#contract","title":"Contract","text":""},{"location":"KnownSizeEstimation/#estimated-size","title":"Estimated Size <pre><code>estimatedSize: Long\n</code></pre> <p><code>estimatedSize</code> is used when:</p> <ul> <li><code>SizeEstimator</code> is requested to <code>visitSingleObject</code></li> <li>BroadcastExchangeExec physical operator is requested for relationFuture</li> <li>BroadcastHashJoinExec physical operator is executed</li> <li>ShuffledHashJoinExec physical operator is requested to buildHashedRelation</li> </ul>","text":""},{"location":"KnownSizeEstimation/#implementations","title":"Implementations","text":"<ul> <li>HashedRelation</li> </ul>"},{"location":"Observation/","title":"Observation","text":"<p><code>Observation</code> is used to simplify observing named metrics in batch queries using Dataset.observe.</p> <pre><code>val observation = Observation(\"name\")\nval observed = ds.observe(observation, max($\"id\").as(\"max_id\"))\nobserved.count()\nval metrics = observation.get\n</code></pre> <pre><code>// Observe row count (rows) and highest id (maxid) in the Dataset while writing it\nval observation = Observation(\"my_metrics\")\nval observed_ds = ds.observe(observation, count(lit(1)).as(\"rows\"), max($\"id\").as(\"maxid\"))\nobserved_ds.write.parquet(\"ds.parquet\")\nval metrics = observation.get\n</code></pre> <p>[SPARK-34806][SQL] Add Observation helper for Dataset.observe</p> <p><code>Observation</code> was added in 3.3.1 (this commit).</p>"},{"location":"Observation/#creating-instance","title":"Creating Instance","text":"<p><code>Observation</code> takes the following to be created:</p> <ul> <li> Name (default: random UUID) <p><code>Observation</code> is created using apply factories.</p>"},{"location":"Observation/#creating-observation","title":"Creating Observation <pre><code>apply(): Observation\napply(name: String): Observation\n</code></pre> <p><code>apply</code> creates a Observation.</p>","text":""},{"location":"PhysicalAggregation/","title":"PhysicalAggregation -- Scala Extractor for Destructuring Aggregate Logical Operators","text":"<p><code>PhysicalAggregation</code> is a Scala extractor to &lt;&gt; into a four-element tuple with the following elements: <p>. Grouping expressions/NamedExpression.md[named expressions]</p> <p>. AggregateExpressions</p> <p>. Result expressions/NamedExpression.md[named expressions]</p> <p>. Child spark-sql-LogicalPlan.md[logical operator]</p> <p>[[ReturnType]] .ReturnType [source, scala]</p>"},{"location":"PhysicalAggregation/#seqnamedexpression-seqaggregateexpression-seqnamedexpression-logicalplan","title":"(Seq[NamedExpression], Seq[AggregateExpression], Seq[NamedExpression], LogicalPlan)","text":"<p>TIP: See the document about http://docs.scala-lang.org/tutorials/tour/extractor-objects.html[Scala extractor objects].</p> <p>=== [[unapply]] Destructuring Aggregate Logical Operator -- <code>unapply</code> Method</p>"},{"location":"PhysicalAggregation/#source-scala","title":"[source, scala]","text":"<p>type ReturnType =   (Seq[NamedExpression], Seq[AggregateExpression], Seq[NamedExpression], LogicalPlan)</p>"},{"location":"PhysicalAggregation/#unapplya-any-optionreturntype","title":"unapply(a: Any): Option[ReturnType]","text":"<p><code>unapply</code> destructures the input <code>a</code> Aggregate.md[Aggregate] logical operator into a four-element &lt;&gt;."},{"location":"PhysicalAggregation/#note","title":"[NOTE]","text":""},{"location":"PhysicalAggregation/#unapply-is-used-whenfixme","title":"<code>unapply</code> is used when...FIXME","text":""},{"location":"PhysicalOperation/","title":"PhysicalOperation -- Scala Extractor for Destructuring Logical Query Plans","text":"<p><code>PhysicalOperation</code> is a Scala extractor to &lt;&gt; into a tuple with the following elements: <p>. expressions/NamedExpression.md[Named expressions] (aka projects)</p> <p>. expressions/Expression.md[Expressions] (aka filters)</p> <p>. spark-sql-LogicalPlan.md[Logical operator] (aka leaf node)</p> <p>[[ReturnType]] .ReturnType [source, scala]</p>"},{"location":"PhysicalOperation/#seqnamedexpression-seqexpression-logicalplan","title":"(Seq[NamedExpression], Seq[Expression], LogicalPlan)","text":"<p>The following idiom is often used in <code>Strategy</code> implementations (e.g. hive/HiveTableScans.md#apply[HiveTableScans], InMemoryScans, DataSourceStrategy, &lt;&gt;):"},{"location":"PhysicalOperation/#source-scala","title":"[source, scala]","text":"<p>def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {   case PhysicalOperation(projections, predicates, plan) =&gt;     // do something   case _ =&gt; Nil }</p> <p>Whenever used to pattern match to a <code>LogicalPlan</code>, <code>PhysicalOperation</code>'s <code>unapply</code> is called.</p> <p>=== [[unapply]] <code>unapply</code> Method</p>"},{"location":"PhysicalOperation/#source-scala_1","title":"[source, scala]","text":"<p>type ReturnType = (Seq[NamedExpression], Seq[Expression], LogicalPlan)</p>"},{"location":"PhysicalOperation/#unapplyplan-logicalplan-optionreturntype","title":"unapply(plan: LogicalPlan): Option[ReturnType]","text":"<p><code>unapply</code>...FIXME</p> <p>NOTE: <code>unapply</code> is almost &lt;&gt; method itself (with some manipulations of the return value)."},{"location":"PhysicalOperation/#note","title":"[NOTE]","text":""},{"location":"PhysicalOperation/#unapply-is-used-whenfixme","title":"<code>unapply</code> is used when...FIXME","text":""},{"location":"PredicateHelper/","title":"PredicateHelper","text":""},{"location":"PredicateHelper/#islikelyselective","title":"isLikelySelective <pre><code>isLikelySelective(\n  e: Expression): Boolean\n</code></pre> <p><code>isLikelySelective</code> is <code>true</code> (enabled) for the following Expressions:</p> <ul> <li><code>Not</code> with an Expression that is likely to be selective</li> <li><code>And</code> with either Expression likely to be selective</li> <li><code>Or</code> with both Expressions likely to be selective</li> <li><code>StringRegexExpression</code><ul> <li><code>Like</code></li> <li><code>RLike</code></li> </ul> </li> <li><code>BinaryComparison</code><ul> <li><code>EqualNullSafe</code></li> <li><code>EqualTo</code></li> <li><code>GreaterThan</code></li> <li><code>GreaterThanOrEqual</code></li> <li><code>LessThan</code></li> <li><code>LessThanOrEqual</code></li> </ul> </li> <li>In</li> <li>InSet</li> <li><code>StringPredicate</code><ul> <li><code>Contains</code></li> <li><code>EndsWith</code></li> <li><code>StartsWith</code></li> </ul> </li> <li><code>BinaryPredicate</code></li> <li><code>MultiLikeBase</code><ul> <li><code>LikeAll</code></li> <li><code>NotLikeAll</code></li> <li><code>LikeAny</code></li> <li><code>NotLikeAny</code></li> </ul> </li> </ul>  <p><code>isLikelySelective</code> is used when:</p> <ul> <li><code>InjectRuntimeFilter</code> logical optimization is requested to isSelectiveFilterOverScan</li> <li><code>PartitionPruning</code> logical optimization is requested to hasSelectivePredicate</li> </ul>","text":""},{"location":"PrunedFilteredScan/","title":"PrunedFilteredScan \u2014 Relations with Column Pruning and Filter Pushdown","text":"<p><code>PrunedFilteredScan</code> is the &lt;&gt; of &lt;&gt; with support for &lt;&gt; (i.e. eliminating unneeded columns) and &lt;&gt; (i.e. filtering using selected predicates only). <p>[[contract]] [source, scala]</p> <p>package org.apache.spark.sql.sources</p> <p>trait PrunedFilteredScan {   def buildScan(requiredColumns: Array[String], filters: Array[Filter]): RDD[Row] }</p> <p>.PrunedFilteredScan Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Property | Description</p> <p>| <code>buildScan</code> | [[buildScan]] Building distributed data scan with column pruning and filter pushdown</p> <p>In other words, <code>buildScan</code> creates a <code>RDD[Row]</code> to represent a distributed data scan (i.e. scanning over data in a relation)</p> <p>Used exclusively when <code>DataSourceStrategy</code> execution planning strategy is requested to plan a LogicalRelation with a PrunedFilteredScan. |===</p> <p>Note</p> <p><code>PrunedFilteredScan</code> is a \"lighter\" and stable version of the <code>CatalystScan</code> abstraction.</p> <p>[[implementations]] NOTE: JDBCRelation is the one and only known implementation of the &lt;&gt; in Spark SQL. <p>[[example]] [source, scala]</p> <p>// Use :paste to define MyBaseRelation case class // BEGIN import org.apache.spark.sql.sources.PrunedFilteredScan import org.apache.spark.sql.sources.BaseRelation import org.apache.spark.sql.types.{StructField, StructType, StringType} import org.apache.spark.sql.SQLContext import org.apache.spark.sql.sources.Filter import org.apache.spark.rdd.RDD import org.apache.spark.sql.Row case class MyBaseRelation(sqlContext: SQLContext) extends BaseRelation with PrunedFilteredScan {   override def schema: StructType = StructType(StructField(\"a\", StringType) :: Nil)   def buildScan(requiredColumns: Array[String], filters: Array[Filter]): RDD[Row] = {     println(s\"&gt;&gt;&gt; [buildScan] requiredColumns = ${requiredColumns.mkString(\",\")}\")     println(s\"&gt;&gt;&gt; [buildScan] filters = ${filters.mkString(\",\")}\")     import sqlContext.implicits._     (0 to 4).toDF.rdd   } } // END val scan = MyBaseRelation(spark.sqlContext)</p> <p>import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan import org.apache.spark.sql.execution.datasources.LogicalRelation val plan: LogicalPlan = LogicalRelation(scan)</p> <p>scala&gt; println(plan.numberedTreeString) 00 Relation[a#1] MyBaseRelation(org.apache.spark.sql.SQLContext@4a57ad67)</p> <p>import org.apache.spark.sql.execution.datasources.DataSourceStrategy val strategy = DataSourceStrategy(spark.sessionState.conf)</p> <p>val sparkPlan = strategy(plan).head // &gt;&gt;&gt; [buildScan] requiredColumns = a // &gt;&gt;&gt; [buildScan] filters = scala&gt; println(sparkPlan.numberedTreeString) 00 Scan MyBaseRelation(org.apache.spark.sql.SQLContext@4a57ad67) [a#8] PushedFilters: [], ReadSchema: struct"},{"location":"PrunedScan/","title":"PrunedScan","text":"<p><code>PrunedScan</code> is...FIXME</p>"},{"location":"PushDownUtils/","title":"PushDownUtils","text":""},{"location":"PushDownUtils/#prunecolumns","title":"pruneColumns <pre><code>pruneColumns(\n  scanBuilder: ScanBuilder,\n  relation: DataSourceV2Relation,\n  projects: Seq[NamedExpression],\n  filters: Seq[Expression]): (Scan, Seq[AttributeReference])\n</code></pre> <p><code>pruneColumns</code>...FIXME</p>  <p><code>pruneColumns</code> is used when:</p> <ul> <li>GroupBasedRowLevelOperationScanPlanning logical optimization is executed (to transform ReplaceData logical operators over DataSourceV2Relations)</li> <li>V2ScanRelationPushDown logical optimization is executed (to pruneColumns of <code>ScanBuilderHolder</code>s)</li> </ul>","text":""},{"location":"PushDownUtils/#pushfilters","title":"pushFilters <pre><code>pushFilters(\n  scanBuilder: ScanBuilder,\n  filters: Seq[Expression]): (Either[Seq[sources.Filter], Seq[Predicate]], Seq[Expression])\n</code></pre> <p><code>pushFilters</code>...FIXME</p>  <p><code>pushFilters</code> is used when:</p> <ul> <li>GroupBasedRowLevelOperationScanPlanning logical optimization is executed (to pushFilters)</li> <li>V2ScanRelationPushDown logical optimization is executed (to pushDownFilters to <code>ScanBuilderHolder</code>s)</li> </ul>","text":""},{"location":"QueryExecution/","title":"QueryExecution \u2014 Structured Query Execution Pipeline","text":"<p><code>QueryExecution</code> is the execution pipeline (workflow) of a structured query.</p> <p><code>QueryExecution</code> is made up of execution stages (phases).</p> <p></p> <p><code>QueryExecution</code> is the result of executing a LogicalPlan in a SparkSession (and so you could create a <code>Dataset</code> from a logical operator or use the <code>QueryExecution</code> after executing a logical operator).</p> <pre><code>val plan: LogicalPlan = ...\nval qe = new QueryExecution(sparkSession, plan)\n</code></pre>"},{"location":"QueryExecution/#creating-instance","title":"Creating Instance","text":"<p><code>QueryExecution</code> takes the following to be created:</p> <ul> <li> SparkSession <li> Logical Query Plan <li>QueryPlanningTracker</li> <p><code>QueryExecution</code> is created when:</p> <ul> <li>Dataset.ofRows and Dataset.selectUntyped are executed</li> <li><code>KeyValueGroupedDataset</code> is requested to aggUntyped</li> <li><code>CommandUtils</code> utility is requested to computeColumnStats and computePercentiles</li> <li><code>BaseSessionStateBuilder</code> is requested to create a QueryExecution for a LogicalPlan</li> </ul>"},{"location":"QueryExecution/#queryplanningtracker","title":"QueryPlanningTracker <p><code>QueryExecution</code> can be given a QueryPlanningTracker when created.</p>","text":""},{"location":"QueryExecution/#accessing-queryexecution","title":"Accessing QueryExecution <p><code>QueryExecution</code> is part of <code>Dataset</code> using queryExecution attribute.</p> <pre><code>val ds: Dataset[Long] = ...\nds.queryExecution\n</code></pre>","text":""},{"location":"QueryExecution/#execution-pipeline-phases","title":"Execution Pipeline Phases","text":""},{"location":"QueryExecution/#analyzed-logical-plan","title":"Analyzed Logical Plan <p>Analyzed logical plan that has passed Logical Analyzer.</p>  <p>Tip</p> <p>Beside <code>analyzed</code>, you can use Dataset.explain basic action (with <code>extended</code> flag enabled) or SQL's <code>EXPLAIN EXTENDED</code> to see the analyzed logical plan of a structured query.</p>","text":""},{"location":"QueryExecution/#analyzed-logical-plan-with-cached-data","title":"Analyzed Logical Plan with Cached Data <p>Analyzed logical plan after <code>CacheManager</code> was requested to replace logical query segments with cached query plans.</p> <p><code>withCachedData</code> makes sure that the logical plan was analyzed and uses supported operations only.</p>","text":""},{"location":"QueryExecution/#optimized-logical-plan","title":"Optimized Logical Plan <p>Logical plan after executing the logical query plan optimizer on the withCachedData logical plan.</p>","text":""},{"location":"QueryExecution/#physical-plan","title":"Physical Plan <p>Physical plan (after SparkPlanner has planned the optimized logical plan).</p> <p><code>sparkPlan</code> is the first physical plan from the collection of all possible physical plans.</p>  <p>Note</p> <p>It is guaranteed that Catalyst's <code>QueryPlanner</code> (which <code>SparkPlanner</code> extends) will always generate at least one physical plan.</p>","text":""},{"location":"QueryExecution/#optimized-physical-plan","title":"Optimized Physical Plan <p>Optimized physical plan that is in the final optimized \"shape\" and therefore ready for execution, i.e. the physical sparkPlan with physical preparation rules applied.</p>","text":""},{"location":"QueryExecution/#rdd","title":"RDD <pre><code>toRdd: RDD[InternalRow]\n</code></pre> <p>Spark Core's execution graph of a distributed computation (<code>RDD</code> of internal binary rows) from the executedPlan after execution.</p> <p>The <code>RDD</code> is the top-level RDD of the DAG of RDDs (that represent physical operators).</p>  <p>Note</p> <p><code>toRdd</code> is a \"boundary\" between two Spark modules: Spark SQL and Spark Core.</p> <p>After you have executed <code>toRdd</code> (directly or not), you basically \"leave\" Spark SQL's Dataset world and \"enter\" Spark Core's RDD space.</p>  <p><code>toRdd</code> triggers a structured query execution (i.e. physical planning, but not execution of the plan) using SparkPlan.execute that recursively triggers execution of every child physical operator in the physical plan tree.</p>  <p>Note</p> <p>SparkSession.internalCreateDataFrame applies a schema to an <code>RDD[InternalRow]</code>.</p>   <p>Note</p> <p>Dataset.rdd gives the <code>RDD[InternalRow]</code> with internal binary rows deserialized to a concrete Scala type.</p>  <p>You can access the lazy attributes as follows:</p> <pre><code>val dataset: Dataset[Long] = ...\ndataset.queryExecution.executedPlan\n</code></pre> <p><code>QueryExecution</code> uses the Logical Query Optimizer and Tungsten for better structured query performance.</p> <p><code>QueryExecution</code> uses the input <code>SparkSession</code> to access the current SparkPlanner (through SessionState) when &lt;&gt;. It then computes a SparkPlan (a <code>PhysicalPlan</code> exactly) using the planner. It is available as the &lt;sparkPlan attribute&gt;&gt;.  <p>Note</p> <p>A variant of <code>QueryExecution</code> that Spark Structured Streaming uses for query planning is <code>IncrementalExecution</code>.</p> <p>Refer to IncrementalExecution\u2009\u2014\u2009QueryExecution of Streaming Datasets in the Spark Structured Streaming online gitbook.</p>","text":""},{"location":"QueryExecution/#sparkplanner","title":"SparkPlanner <p>SparkPlanner</p>","text":""},{"location":"QueryExecution/#text-representation-with-statistics","title":"Text Representation With Statistics <pre><code>stringWithStats: String\n</code></pre> <p><code>stringWithStats</code>...FIXME</p> <p><code>stringWithStats</code> is used when ExplainCommand logical command is executed (with <code>cost</code> flag enabled).</p>","text":""},{"location":"QueryExecution/#physical-query-optimizations","title":"Physical Query Optimizations <p>Physical Query Optimizations are Catalyst Rules for transforming physical operators (to be more efficient and optimized for execution). They are executed in the following order:</p> <ol> <li>InsertAdaptiveSparkPlan (if defined)</li> <li>CoalesceBucketsInJoin</li> <li>PlanDynamicPruningFilters</li> <li>PlanSubqueries</li> <li>RemoveRedundantProjects</li> <li>EnsureRequirements</li> <li>RemoveRedundantSorts</li> <li>DisableUnnecessaryBucketedScan</li> <li>ApplyColumnarRulesAndInsertTransitions</li> <li>CollapseCodegenStages</li> <li>ReuseExchange</li> <li>ReuseSubquery</li> </ol>","text":""},{"location":"QueryExecution/#preparations","title":"preparations <pre><code>preparations: Seq[Rule[SparkPlan]]\n</code></pre> <p><code>preparations</code> creates an InsertAdaptiveSparkPlan (with a new AdaptiveExecutionContext) that is added to the other preparations rules.</p> <p><code>preparations</code> is used when:</p> <ul> <li><code>QueryExecution</code> is requested for an optimized physical query plan</li> </ul>","text":""},{"location":"QueryExecution/#preparations-internal-utility","title":"preparations Internal Utility <pre><code>preparations(\n  sparkSession: SparkSession,\n  adaptiveExecutionRule: Option[InsertAdaptiveSparkPlan] = None): Seq[Rule[SparkPlan]]\n</code></pre> <p><code>preparations</code> is the Physical Query Optimizations.</p> <p><code>preparations</code> is used when:</p> <ul> <li><code>QueryExecution</code> is requested for the physical optimization rules (preparations) (with the <code>InsertAdaptiveSparkPlan</code> defined)</li> <li><code>QueryExecution</code> utility is requested to prepareExecutedPlan (with no <code>InsertAdaptiveSparkPlan</code>)</li> </ul>","text":""},{"location":"QueryExecution/#prepareexecutedplan-for-physical-operators","title":"prepareExecutedPlan for Physical Operators <pre><code>prepareExecutedPlan(\n  spark: SparkSession,\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>prepareExecutedPlan</code> applies the preparations physical query optimization rules (with no <code>InsertAdaptiveSparkPlan</code> optimization) to the physical plan.</p> <p><code>prepareExecutedPlan</code> is used when:</p> <ul> <li> <p><code>QueryExecution</code> utility is requested to prepareExecutedPlan for a logical operator</p> </li> <li> <p>PlanDynamicPruningFilters physical optimization is executed</p> </li> </ul>","text":""},{"location":"QueryExecution/#prepareexecutedplan-for-logical-operators","title":"prepareExecutedPlan for Logical Operators <pre><code>prepareExecutedPlan(\n  spark: SparkSession,\n  plan: LogicalPlan): SparkPlan\n</code></pre> <p><code>prepareExecutedPlan</code> is...FIXME</p> <p><code>prepareExecutedPlan</code> is used when PlanSubqueries physical optimization is executed.</p>","text":""},{"location":"QueryExecution/#applying-preparations-physical-query-optimization-rules-to-physical-plan","title":"Applying preparations Physical Query Optimization Rules to Physical Plan <pre><code>prepareForExecution(\n  preparations: Seq[Rule[SparkPlan]],\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>prepareForExecution</code> takes physical preparation rules and executes them one by one with the given SparkPlan.</p> <p><code>prepareForExecution</code> is used when:</p> <ul> <li><code>QueryExecution</code> is requested to prepare the physical plan for execution and prepareExecutedPlan</li> </ul>","text":""},{"location":"QueryExecution/#assertsupported-method","title":"assertSupported Method <pre><code>assertSupported(): Unit\n</code></pre> <p><code>assertSupported</code> requests <code>UnsupportedOperationChecker</code> to <code>checkForBatch</code>.</p> <p><code>assertSupported</code> is used when <code>QueryExecution</code> is requested for withCachedData logical plan.</p>","text":""},{"location":"QueryExecution/#creating-analyzed-logical-plan-and-checking-correctness","title":"Creating Analyzed Logical Plan and Checking Correctness <pre><code>assertAnalyzed(): Unit\n</code></pre> <p><code>assertAnalyzed</code> triggers initialization of analyzed (which is almost like executing it).</p> <p><code>assertAnalyzed</code> executes analyzed by accessing it and throwing the result away. Since <code>analyzed</code> is a lazy value in Scala, it will then get initialized for the first time and stays so forever.</p> <p><code>assertAnalyzed</code> then requests <code>Analyzer</code> to validate analysis of the logical plan (i.e. <code>analyzed</code>).</p>  <p>Note</p> <p><code>assertAnalyzed</code> uses SparkSession to access the current SessionState that it then uses to access the Analyzer.</p>  <p>In case of any <code>AnalysisException</code>, <code>assertAnalyzed</code> creates a new <code>AnalysisException</code> to make sure that it holds analyzed and reports it.</p>","text":""},{"location":"QueryExecution/#building-text-representation-with-cost-stats","title":"Building Text Representation with Cost Stats <pre><code>toStringWithStats: String\n</code></pre> <p><code>toStringWithStats</code> is a mere alias for completeString with <code>appendStats</code> flag enabled.</p> <p><code>toStringWithStats</code> is a custom toString with cost statistics.</p> <pre><code>val dataset = spark.range(20).limit(2)\n\n// toStringWithStats in action - note Optimized Logical Plan section with Statistics\nscala&gt; dataset.queryExecution.toStringWithStats\nres6: String =\n== Parsed Logical Plan ==\nGlobalLimit 2\n+- LocalLimit 2\n   +- Range (0, 20, step=1, splits=Some(8))\n\n== Analyzed Logical Plan ==\nid: bigint\nGlobalLimit 2\n+- LocalLimit 2\n   +- Range (0, 20, step=1, splits=Some(8))\n\n== Optimized Logical Plan ==\nGlobalLimit 2, Statistics(sizeInBytes=32.0 B, rowCount=2, isBroadcastable=false)\n+- LocalLimit 2, Statistics(sizeInBytes=160.0 B, isBroadcastable=false)\n   +- Range (0, 20, step=1, splits=Some(8)), Statistics(sizeInBytes=160.0 B, isBroadcastable=false)\n\n== Physical Plan ==\nCollectLimit 2\n+- *Range (0, 20, step=1, splits=Some(8))\n</code></pre> <p><code>toStringWithStats</code> is used when ExplainCommand logical command is executed (with <code>cost</code> attribute enabled).</p>","text":""},{"location":"QueryExecution/#extended-text-representation-with-logical-and-physical-plans","title":"Extended Text Representation with Logical and Physical Plans <pre><code>toString: String\n</code></pre> <p><code>toString</code> is a mere alias for completeString with <code>appendStats</code> flag disabled.</p>  <p>Note</p> <p><code>toString</code> is on the \"other\" side of toStringWithStats which has <code>appendStats</code> flag enabled.</p>  <p><code>toString</code> is part of Java's <code>Object</code> abstraction.</p>","text":""},{"location":"QueryExecution/#simple-basic-text-representation","title":"Simple (Basic) Text Representation <pre><code>simpleString: String // (1)\nsimpleString(\n  formatted: Boolean): String\n</code></pre> <ol> <li><code>formatted</code> is <code>false</code></li> </ol> <p><code>simpleString</code> requests the optimized physical plan for the text representation (of all nodes in the query tree) with <code>verbose</code> flag turned off.</p> <p>In the end, <code>simpleString</code> adds == Physical Plan == header to the text representation and redacts sensitive information.</p> <p><code>simpleString</code> is used when:</p> <ul> <li><code>QueryExecution</code> is requested to explainString</li> <li>others</li> </ul>","text":""},{"location":"QueryExecution/#demo","title":"Demo <pre><code>import org.apache.spark.sql.{functions =&gt; f}\nval q = spark.range(10).withColumn(\"rand\", f.rand())\nval output = q.queryExecution.simpleString\n</code></pre> <pre><code>scala&gt; println(output)\n== Physical Plan ==\n*(1) Project [id#53L, rand(-5226178239369056152) AS rand#55]\n+- *(1) Range (0, 10, step=1, splits=16)\n</code></pre>","text":""},{"location":"QueryExecution/#explainstring","title":"explainString <pre><code>explainString(\n  mode: ExplainMode): String\nexplainString(\n  mode: ExplainMode,\n  maxFields: Int,\n  append: String =&gt; Unit): Unit\n</code></pre> <p><code>explainString</code>...FIXME</p> <p><code>explainString</code> is used when:</p> <ul> <li><code>Dataset</code> is requested to explain</li> <li><code>SQLExecution</code> utility is used to withNewExecutionId</li> <li><code>AdaptiveSparkPlanExec</code> leaf physical operator is requested to onUpdatePlan</li> <li><code>ExplainCommand</code> logical command is executed</li> <li><code>debug</code> utility is used to <code>toFile</code></li> </ul>","text":""},{"location":"QueryExecution/#redacting-sensitive-information","title":"Redacting Sensitive Information <pre><code>withRedaction(\n  message: String): String\n</code></pre> <p><code>withRedaction</code> takes the value of spark.sql.redaction.string.regex configuration property (as the regular expression to point at sensitive information) and requests Spark Core's <code>Utils</code> to redact sensitive information in the input <code>message</code>.</p> <p>NOTE: Internally, Spark Core's <code>Utils.redact</code> uses Java's <code>Regex.replaceAllIn</code> to replace all matches of a pattern with a string.</p> <p>NOTE: <code>withRedaction</code> is used when <code>QueryExecution</code> is requested for the &lt;&gt;, &lt;&gt; and &lt;&gt; text representations.","text":""},{"location":"QueryExecution/#writeplans","title":"writePlans <pre><code>writePlans(\n   append: String =&gt; Unit,\n   maxFields: Int): Unit\n</code></pre> <p><code>writePlans</code>...FIXME</p> <p><code>writePlans</code> is used when...FIXME</p>","text":""},{"location":"QueryExecutionListener/","title":"QueryExecutionListener","text":"<p><code>QueryExecutionListener</code> is an abstraction of query execution listeners that can intercept onFailure and onSuccess events.</p>"},{"location":"QueryExecutionListener/#contract","title":"Contract","text":""},{"location":"QueryExecutionListener/#onfailure","title":"onFailure <pre><code>onFailure(\n  funcName: String,\n  qe: QueryExecution,\n  exception: Exception): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>ExecutionListenerBus</code> is requested to doPostEvent</li> </ul>","text":""},{"location":"QueryExecutionListener/#onsuccess","title":"onSuccess <pre><code>onSuccess(\n  funcName: String,\n  qe: QueryExecution,\n  durationNs: Long): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>ExecutionListenerBus</code> is requested to doPostEvent</li> </ul>","text":""},{"location":"QueryExecutionListener/#implementations","title":"Implementations","text":"<ul> <li><code>ObservationListener</code></li> </ul>"},{"location":"QueryPlanningTracker/","title":"QueryPlanningTracker","text":"<p><code>QueryPlanningTracker</code> is used to track the execution phases of a structured query.</p> Phase Description <code>parsing</code> <code>SparkSession</code> is requested to execute a SQL query <code>analysis</code> <code>QueryExecution</code> is requested for an analyzed query plan <code>optimization</code> <code>QueryExecution</code> is requested for an optimized query plan <code>planning</code> <code>QueryExecution</code> is requested for an physical and executed query plans"},{"location":"QueryPlanningTracker/#accessing-queryplanningtracker","title":"Accessing QueryPlanningTracker","text":"<p><code>QueryPlanningTracker</code> of a structured query is available using QueryExecution.</p> <pre><code>val df_ops = spark.range(1000).selectExpr(\"count(*)\")\nval tracker = df_ops.queryExecution.tracker\n\n// There are three execution phases tracked for structured queries using Dataset API\nassert(tracker.phases.keySet == Set(\"analysis\", \"optimization\", \"planning\"))\n</code></pre> <pre><code>val df_sql = sql(\"SELECT * FROM range(1000)\")\nval tracker = df_sql.queryExecution.tracker\n\n// There are four execution phases tracked for structured queries using SQL\nassert(tracker.phases.keySet == Set(\"parsing\", \"analysis\", \"optimization\", \"planning\"))\n</code></pre>"},{"location":"QueryPlanningTracker/#creating-instance","title":"Creating Instance","text":"<p><code>QueryPlanningTracker</code> takes no arguments to be created.</p> <p><code>QueryPlanningTracker</code> is created when:</p> <ul> <li> <p><code>SparkSession</code> is requested to execute a SQL query</p> </li> <li> <p><code>QueryExecution</code> is created</p> </li> </ul>"},{"location":"QueryPlanningTracker/#getting-queryplanningtracker","title":"Getting QueryPlanningTracker <pre><code>get: Option[QueryPlanningTracker]\n</code></pre> <p><code>get</code> utility allows to access the <code>QueryPlanningTracker</code> bound to the current thread (using a thread local variable facility).</p> <pre><code>import org.apache.spark.sql.catalyst.QueryPlanningTracker\n\nscala&gt; :type QueryPlanningTracker.get\nOption[org.apache.spark.sql.catalyst.QueryPlanningTracker]\n</code></pre>  <p><code>get</code> is used when:</p> <ul> <li><code>RuleExecutor</code> is requested to execute rules on a query plan</li> </ul>","text":""},{"location":"QueryPlanningTracker/#measuring-execution-phase","title":"Measuring Execution Phase <pre><code>measurePhase[T](\n  phase: String)(\n  f: =&gt; T): T\n</code></pre> <p><code>measurePhase</code> executes the given <code>f</code> executable block and records the start and end times in the phasesMap registry.</p> <p>If the given <code>phase</code> has already been recorded in the phasesMap registry, <code>measurePhase</code> replaces the end time.</p>  <p><code>measurePhase</code> is used when:</p> <ul> <li><code>SparkSession</code> is requested to execute a SQL query</li> <li><code>QueryExecution</code> is requested to executePhase</li> </ul>","text":""},{"location":"QueryPlanningTracker/#phasesmap-registry","title":"phasesMap Registry <pre><code>phasesMap: HashMap[String, PhaseSummary]\n</code></pre> <p><code>QueryPlanningTracker</code> creates <code>phasesMap</code> registry of phases and their start and end times (<code>PhaseSummary</code>) when created.</p> <p>A phase with a <code>PhaseSummary</code> is added (recorded) in measurePhase.</p> <p><code>phasesMap</code> is available using phases method.</p>","text":""},{"location":"QueryPlanningTracker/#execution-phases-summaries","title":"Execution Phases Summaries <pre><code>phases: Map[String, PhaseSummary]\n</code></pre> <p><code>phases</code> gives the phasesMap registry.</p>  <p>Note</p> <p><code>phases</code> sees to be used in tests only.</p>","text":""},{"location":"RelationProvider/","title":"RelationProvider \u2014 Relations with Pre-Defined Schema","text":"<p><code>RelationProvider</code> is an abstraction of providers for relational data sources with schema inference (schema discovery). In other words, the schema of a relation is pre-defined (fixed), and a user-specified schema is not allowed.</p> <p>SchemaRelationProvider</p> <p>Use SchemaRelationProvider for data source providers that require a user-defined schema.</p>"},{"location":"RelationProvider/#contract","title":"Contract","text":""},{"location":"RelationProvider/#creating-baserelation","title":"Creating BaseRelation <pre><code>createRelation(\n  sqlContext: SQLContext,\n  parameters: Map[String, String]): BaseRelation\n</code></pre> <p>Creates a BaseRelation</p> <p>Used when:</p> <ul> <li><code>DataSource</code> is requested to resolve a relation</li> </ul>","text":""},{"location":"RelationProvider/#implementations","title":"Implementations","text":"<ul> <li>JdbcRelationProvider</li> <li>KafkaSourceProvider</li> </ul>"},{"location":"RelationProvider/#creatablerelationprovider","title":"CreatableRelationProvider","text":"<p>It is a common pattern while developing a custom data source to use RelationProvider.createRelation with CreatableRelationProvider when requested for a relation (after writing out a structured query).</p>"},{"location":"Row/","title":"Row","text":"<p><code>Row</code> is a generic row object with an ordered collection of fields that can be accessed by an &lt;&gt; (aka generic access by ordinal), a name (aka native primitive access) or using &lt;&gt;. <p>NOTE: <code>Row</code> is also called Catalyst Row.</p> <p><code>Row</code> may have an optional &lt;&gt;. <p>The traits of <code>Row</code>:</p> <ul> <li><code>length</code> or <code>size</code> - <code>Row</code> knows the number of elements (columns).</li> <li><code>schema</code> - <code>Row</code> knows the schema</li> </ul> <p><code>Row</code> belongs to <code>org.apache.spark.sql.Row</code> package.</p>"},{"location":"Row/#source-scala","title":"[source, scala]","text":""},{"location":"Row/#import-orgapachesparksqlrow","title":"import org.apache.spark.sql.Row","text":"<p>=== [[apply]] Creating Row -- <code>apply</code> Factory Method</p> <p>CAUTION: FIXME</p> <p>=== [[field-access]][[get]][[apply-index]] Field Access by Index -- <code>apply</code> and <code>get</code> methods</p> <p>Fields of a <code>Row</code> instance can be accessed by index (starting from <code>0</code>) using <code>apply</code> or <code>get</code>.</p>"},{"location":"Row/#source-scala_1","title":"[source, scala]","text":"<p>scala&gt; val row = Row(1, \"hello\") row: org.apache.spark.sql.Row = [1,hello]</p> <p>scala&gt; row(1) res0: Any = hello</p> <p>scala&gt; row.get(1) res1: Any = hello</p> <p>NOTE: Generic access by ordinal (using <code>apply</code> or <code>get</code>) returns a value of type <code>Any</code>.</p> <p>=== [[getAs]] Get Field As Type -- <code>getAs</code> method</p> <p>You can query for fields with their proper types using <code>getAs</code> with an index</p>"},{"location":"Row/#source-scala_2","title":"[source, scala]","text":"<p>val row = Row(1, \"hello\")</p> <p>scala&gt; row.getAsInt res1: Int = 1</p> <p>scala&gt; row.getAsString res2: String = hello</p>"},{"location":"Row/#note","title":"[NOTE]","text":"<p>FIXME [source, scala]</p>"},{"location":"Row/#rowgetasstring","title":"row.getAsString","text":"<p>====</p> <p>=== [[schema]] Schema</p> <p>A <code>Row</code> instance can have a schema defined.</p> <p>NOTE: Unless you are instantiating <code>Row</code> yourself (using &lt;&gt;), a <code>Row</code> has always a schema. <p>Note</p> <p>It is RowEncoder to take care of assigning a schema to a <code>Row</code> when <code>toDF</code> on a Dataset or when instantiating DataFrame through DataFrameReader.</p> <p>=== [[row-object]] Row Object</p> <p><code>Row</code> companion object offers factory methods to create <code>Row</code> instances from a collection of elements (<code>apply</code>), a sequence of elements (<code>fromSeq</code>) and tuples (<code>fromTuple</code>).</p>"},{"location":"Row/#source-scala_3","title":"[source, scala]","text":"<p>scala&gt; Row(1, \"hello\") res0: org.apache.spark.sql.Row = [1,hello]</p> <p>scala&gt; Row.fromSeq(Seq(1, \"hello\")) res1: org.apache.spark.sql.Row = [1,hello]</p> <p>scala&gt; Row.fromTuple((0, \"hello\")) res2: org.apache.spark.sql.Row = [0,hello]</p> <p><code>Row</code> object can merge <code>Row</code> instances.</p>"},{"location":"Row/#source-scala_4","title":"[source, scala]","text":"<p>scala&gt; Row.merge(Row(1), Row(\"hello\")) res3: org.apache.spark.sql.Row = [1,hello]</p> <p>It can also return an empty <code>Row</code> instance.</p>"},{"location":"Row/#source-scala_5","title":"[source, scala]","text":"<p>scala&gt; Row.empty == Row() res4: Boolean = true</p> <p>=== [[pattern-matching-on-row]] Pattern Matching on Row</p> <p><code>Row</code> can be used in pattern matching (since &lt;&gt; comes with <code>unapplySeq</code>)."},{"location":"Row/#source-scala_6","title":"[source, scala]","text":"<p>scala&gt; Row.unapplySeq(Row(1, \"hello\")) res5: Some[Seq[Any]] = Some(WrappedArray(1, hello))</p> <p>Row(1, \"hello\") match { case Row(key: Int, value: String) =&gt;   key -&gt; value }</p>"},{"location":"RowEncoder/","title":"RowEncoder","text":"<p><code>RowEncoder</code> is part of the Encoder framework and is used as the encoder of DataFrames (Datasets of Rows).</p> <p>Note</p> <p><code>DataFrame</code> type is a type alias for <code>Dataset[Row]</code> that expects an <code>Encoder[Row]</code> available in scope which is <code>RowEncoder</code> itself.</p>"},{"location":"RowEncoder/#creating-expressionencoder-of-rows","title":"Creating ExpressionEncoder of Rows <pre><code>apply(\n  schema: StructType): ExpressionEncoder[Row]\n</code></pre> <p><code>apply</code> creates a BoundReference to a nullable field of ObjectType (with Row).</p> <p><code>apply</code> creates a serializer for input objects as the <code>BoundReference</code> with the given schema.</p> <p><code>apply</code> creates a deserializer for a <code>GetColumnByOrdinal</code> expression with the given schema.</p> <p>In the end, <code>apply</code> creates an ExpressionEncoder for Rows with the serializer and deserializer.</p>","text":""},{"location":"RowEncoder/#serializerfor","title":"serializerFor <pre><code>serializerFor(\n  inputObject: Expression,\n  inputType: DataType): Expression\n</code></pre> <p><code>serializerFor</code> is a recursive method that decomposes non-primitive (complex) DataTypes (e.g. <code>ArrayType</code>, <code>MapType</code> and StructType) to primitives.</p>  <p>For a StructType, creates a CreateNamedStruct with serializer expressions for the (inner) fields.</p> <p>For native types, <code>serializerFor</code> returns the given <code>inputObject</code> expression (that ends recursion).</p> <p><code>serializerFor</code> handles the following DataTypes in a custom way (and the given order):</p> <ol> <li><code>PythonUserDefinedType</code></li> <li>UserDefinedType</li> <li><code>TimestampType</code></li> <li><code>DateType</code></li> <li><code>DayTimeIntervalType</code></li> <li><code>YearMonthIntervalType</code></li> <li><code>DecimalType</code></li> <li><code>StringType</code></li> <li>ArrayType</li> <li><code>MapType</code></li> </ol>","text":""},{"location":"RowEncoder/#demo","title":"Demo <pre><code>import org.apache.spark.sql.types._\nval schema = StructType(\n  StructField(\"id\", LongType, nullable = false) ::\n  StructField(\"name\", StringType, nullable = false) :: Nil)\n\nimport org.apache.spark.sql.catalyst.encoders.RowEncoder\nval encoder = RowEncoder(schema)\n\nassert(encoder.flat == false, \"RowEncoder is never flat\")\n</code></pre>","text":""},{"location":"RuntimeConfig/","title":"RuntimeConfig -- Management Interface of Runtime Configuration","text":"<p><code>RuntimeConfig</code> is the &lt;&gt; of the &lt;&gt;. <p>[[methods]] .RuntimeConfig API [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| &lt;&gt; a|"},{"location":"RuntimeConfig/#source-scala","title":"[source, scala]","text":"<p>get(key: String): String get(key: String, default: String): String</p> <p>| &lt;&gt; a|"},{"location":"RuntimeConfig/#source-scala_1","title":"[source, scala]","text":""},{"location":"RuntimeConfig/#getall-mapstring-string","title":"getAll: Map[String, String]","text":"<p>| &lt;&gt; a|"},{"location":"RuntimeConfig/#source-scala_2","title":"[source, scala]","text":""},{"location":"RuntimeConfig/#getoptionkey-string-optionstring","title":"getOption(key: String): Option[String]","text":"<p>| <code>isModifiable</code> a| [[isModifiable]]</p>"},{"location":"RuntimeConfig/#source-scala_3","title":"[source, scala]","text":""},{"location":"RuntimeConfig/#ismodifiablekey-string-boolean","title":"isModifiable(key: String): Boolean","text":"<p>(New in 2.4.0)</p> <p>| &lt;&gt; a|"},{"location":"RuntimeConfig/#source-scala_4","title":"[source, scala]","text":"<p>set(key: String, value: Boolean): Unit set(key: String, value: Long): Unit set(key: String, value: String): Unit</p> <p>| &lt;&gt; a|"},{"location":"RuntimeConfig/#source-scala_5","title":"[source, scala]","text":""},{"location":"RuntimeConfig/#unsetkey-string-unit","title":"unset(key: String): Unit","text":"<p>|===</p> <p><code>RuntimeConfig</code> is available using the &lt;&gt; attribute of a <code>SparkSession</code>."},{"location":"RuntimeConfig/#source-scala_6","title":"[source, scala]","text":"<p>scala&gt; :type spark org.apache.spark.sql.SparkSession</p> <p>scala&gt; :type spark.conf org.apache.spark.sql.RuntimeConfig</p> <p>.RuntimeConfig, SparkSession and SQLConf image::images/spark-sql-RuntimeConfig.png[align=\"center\"]</p> <p><code>RuntimeConfig</code> is &lt;&gt; exclusively when <code>SparkSession</code> is requested for &lt;&gt;. <p>[[sqlConf]] [[creating-instance]] <code>RuntimeConfig</code> takes a SQLConf when created.</p> <p>=== [[get]] <code>get</code> Method</p>"},{"location":"RuntimeConfig/#source-scala_7","title":"[source, scala]","text":"<p>get(key: String): String get(key: String, default: String): String</p> <p><code>get</code>...FIXME</p> <p>NOTE: <code>get</code> is used when...FIXME</p> <p>=== [[getAll]] <code>getAll</code> Method</p>"},{"location":"RuntimeConfig/#source-scala_8","title":"[source, scala]","text":""},{"location":"RuntimeConfig/#getall-mapstring-string_1","title":"getAll: Map[String, String]","text":"<p><code>getAll</code>...FIXME</p> <p>NOTE: <code>getAll</code> is used when...FIXME</p> <p>=== [[getOption]] <code>getOption</code> Method</p>"},{"location":"RuntimeConfig/#source-scala_9","title":"[source, scala]","text":""},{"location":"RuntimeConfig/#getoptionkey-string-optionstring_1","title":"getOption(key: String): Option[String]","text":"<p><code>getOption</code>...FIXME</p> <p>NOTE: <code>getOption</code> is used when...FIXME</p> <p>=== [[set]] <code>set</code> Method</p>"},{"location":"RuntimeConfig/#source-scala_10","title":"[source, scala]","text":"<p>set(key: String, value: Boolean): Unit set(key: String, value: Long): Unit set(key: String, value: String): Unit</p> <p><code>set</code>...FIXME</p> <p>NOTE: <code>set</code> is used when...FIXME</p> <p>=== [[unset]] <code>unset</code> Method</p>"},{"location":"RuntimeConfig/#source-scala_11","title":"[source, scala]","text":""},{"location":"RuntimeConfig/#unsetkey-string-unit_1","title":"unset(key: String): Unit","text":"<p><code>unset</code>...FIXME</p> <p>NOTE: <code>unset</code> is used when...FIXME</p>"},{"location":"SQLConf/","title":"SQLConf","text":"<p><code>SQLConf</code> is an internal configuration store of the configuration properties and hints used in Spark SQL.</p> <p>Important</p> <p><code>SQLConf</code> is an internal part of Spark SQL and is not supposed to be used directly. Spark SQL configuration is available through the developer-facing RuntimeConfig.</p> <p><code>SQLConf</code> offers methods to <code>get</code>, <code>set</code>, <code>unset</code> or <code>clear</code> values of the configuration properties and hints as well as to read the current values.</p>"},{"location":"SQLConf/#accessing-sqlconf","title":"Accessing SQLConf","text":"<p>You can access a <code>SQLConf</code> using:</p> <ul> <li> <p><code>SQLConf.get</code> (preferred) - the <code>SQLConf</code> of the current active <code>SparkSession</code></p> </li> <li> <p>SessionState - direct access through SessionState of the <code>SparkSession</code> of your choice (that gives more flexibility on what <code>SparkSession</code> is used that can be different from the current active <code>SparkSession</code>)</p> </li> </ul> <pre><code>import org.apache.spark.sql.internal.SQLConf\n\n// Use type-safe access to configuration properties\n// using SQLConf.get.getConf\nval parallelFileListingInStatsComputation = SQLConf.get.getConf(SQLConf.PARALLEL_FILE_LISTING_IN_STATS_COMPUTATION)\n\n// or even simpler\nSQLConf.get.parallelFileListingInStatsComputation\n</code></pre> <pre><code>scala&gt; :type spark\norg.apache.spark.sql.SparkSession\n\n// Direct access to the session SQLConf\nval sqlConf = spark.sessionState.conf\nscala&gt; :type sqlConf\norg.apache.spark.sql.internal.SQLConf\n\nscala&gt; println(sqlConf.offHeapColumnVectorEnabled)\nfalse\n\n// Or simply import the conf value\nimport spark.sessionState.conf\n\n// accessing properties through accessor methods\nscala&gt; conf.numShufflePartitions\nres1: Int = 200\n\n// Prefer SQLConf.get (over direct access)\nimport org.apache.spark.sql.internal.SQLConf\nval cc = SQLConf.get\nscala&gt; cc == conf\nres4: Boolean = true\n\n// setting properties using aliases\nimport org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS\nconf.setConf(SHUFFLE_PARTITIONS, 2)\nscala&gt; conf.numShufflePartitions\nres2: Int = 2\n\n// unset aka reset properties to the default value\nconf.unsetConf(SHUFFLE_PARTITIONS)\nscala&gt; conf.numShufflePartitions\nres3: Int = 200\n</code></pre>"},{"location":"SQLConf/#adaptive_auto_broadcastjoin_threshold","title":"ADAPTIVE_AUTO_BROADCASTJOIN_THRESHOLD <p>spark.sql.adaptive.autoBroadcastJoinThreshold</p> <p>Used when:</p> <ul> <li><code>JoinSelectionHelper</code> is requested to canBroadcastBySize</li> </ul>","text":""},{"location":"SQLConf/#adaptive_execution_force_apply","title":"ADAPTIVE_EXECUTION_FORCE_APPLY <p>spark.sql.adaptive.forceApply configuration property</p> <p>Used when:</p> <ul> <li>InsertAdaptiveSparkPlan physical optimization is executed</li> </ul>","text":""},{"location":"SQLConf/#adaptiveexecutionenabled","title":"adaptiveExecutionEnabled <p>The value of spark.sql.adaptive.enabled configuration property</p> <p>Used when:</p> <ul> <li>InsertAdaptiveSparkPlan physical optimization is executed</li> <li><code>SQLConf</code> is requested for the numShufflePartitions</li> </ul>","text":""},{"location":"SQLConf/#adaptiveexecutionloglevel","title":"adaptiveExecutionLogLevel <p>The value of spark.sql.adaptive.logLevel configuration property</p> <p>Used when AdaptiveSparkPlanExec physical operator is executed</p>","text":""},{"location":"SQLConf/#adaptive_max_shuffle_hash_join_local_map_threshold","title":"ADAPTIVE_MAX_SHUFFLE_HASH_JOIN_LOCAL_MAP_THRESHOLD <p>spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold configuration property</p> <p>Used when:</p> <ul> <li><code>DynamicJoinSelection</code> is requested to preferShuffledHashJoin</li> </ul>","text":""},{"location":"SQLConf/#adaptive_optimizer_excluded_rules","title":"ADAPTIVE_OPTIMIZER_EXCLUDED_RULES <p>spark.sql.adaptive.optimizer.excludedRules</p>","text":""},{"location":"SQLConf/#advisory_partition_size_in_bytes","title":"ADVISORY_PARTITION_SIZE_IN_BYTES <p>spark.sql.adaptive.advisoryPartitionSizeInBytes configuration property</p> <p>Used when:</p> <ul> <li>CoalesceShufflePartitions and OptimizeSkewedJoin physical optimizations are executed</li> </ul>","text":""},{"location":"SQLConf/#autobroadcastjointhreshold","title":"autoBroadcastJoinThreshold <p>The value of spark.sql.autoBroadcastJoinThreshold configuration property</p> <p>Used when:</p> <ul> <li>JoinSelection execution planning strategy is executed</li> </ul>","text":""},{"location":"SQLConf/#autobucketedscanenabled","title":"autoBucketedScanEnabled <p>The value of spark.sql.sources.bucketing.autoBucketedScan.enabled configuration property</p> <p>Used when:</p> <ul> <li>DisableUnnecessaryBucketedScan physical optimization is executed</li> </ul>","text":""},{"location":"SQLConf/#allowstarwithsingletableidentifierincount","title":"allowStarWithSingleTableIdentifierInCount <p>spark.sql.legacy.allowStarWithSingleTableIdentifierInCount</p> <p>Used when:</p> <ul> <li><code>ResolveReferences</code> logical resolution rule is executed</li> </ul>","text":""},{"location":"SQLConf/#arrowpysparkselfdestructenabled","title":"arrowPySparkSelfDestructEnabled <p>spark.sql.execution.arrow.pyspark.selfDestruct.enabled</p> <p>Used when:</p> <ul> <li><code>PandasConversionMixin</code> is requested to <code>toPandas</code></li> </ul>","text":""},{"location":"SQLConf/#allowautogeneratedaliasforview","title":"allowAutoGeneratedAliasForView <p>spark.sql.legacy.allowAutoGeneratedAliasForView</p> <p>Used when:</p> <ul> <li><code>ViewHelper</code> utility is used to <code>verifyAutoGeneratedAliasesNotExists</code></li> </ul>","text":""},{"location":"SQLConf/#allownonemptylocationinctas","title":"allowNonEmptyLocationInCTAS <p>spark.sql.legacy.allowNonEmptyLocationInCTAS</p> <p>Used when:</p> <ul> <li><code>DataWritingCommand</code> utility is used to assertEmptyRootPath</li> </ul>","text":""},{"location":"SQLConf/#allownonemptylocationinctas_1","title":"allowNonEmptyLocationInCTAS <p>spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled</p> <p>Used when:</p> <ul> <li><code>OptimizeSkewInRebalancePartitions</code> physical optimization is executed</li> </ul>","text":""},{"location":"SQLConf/#adaptive_custom_cost_evaluator_class","title":"ADAPTIVE_CUSTOM_COST_EVALUATOR_CLASS <p>spark.sql.adaptive.customCostEvaluatorClass</p>","text":""},{"location":"SQLConf/#autosizeupdateenabled","title":"autoSizeUpdateEnabled <p>The value of spark.sql.statistics.size.autoUpdate.enabled configuration property</p> <p>Used when:</p> <ul> <li><code>CommandUtils</code> is requested for updating existing table statistics</li> <li><code>AlterTableAddPartitionCommand</code> logical command is executed</li> </ul>","text":""},{"location":"SQLConf/#avrocompressioncodec","title":"avroCompressionCodec <p>The value of spark.sql.avro.compression.codec configuration property</p> <p>Used when <code>AvroOptions</code> is requested for the compression configuration property (and it was not set explicitly)</p>","text":""},{"location":"SQLConf/#broadcasttimeout","title":"broadcastTimeout <p>The value of spark.sql.broadcastTimeout configuration property</p> <p>Used in BroadcastExchangeExec (for broadcasting a table to executors)</p>","text":""},{"location":"SQLConf/#bucketingenabled","title":"bucketingEnabled <p>The value of spark.sql.sources.bucketing.enabled configuration property</p> <p>Used when <code>FileSourceScanExec</code> physical operator is requested for the input RDD and to determine output partitioning and ordering</p>","text":""},{"location":"SQLConf/#cachevectorizedreaderenabled","title":"cacheVectorizedReaderEnabled <p>The value of spark.sql.inMemoryColumnarStorage.enableVectorizedReader configuration property</p> <p>Used when <code>InMemoryTableScanExec</code> physical operator is requested for supportsBatch flag.</p>","text":""},{"location":"SQLConf/#can_change_cached_plan_output_partitioning","title":"CAN_CHANGE_CACHED_PLAN_OUTPUT_PARTITIONING <p>spark.sql.optimizer.canChangeCachedPlanOutputPartitioning</p> <p>Used when:</p> <ul> <li><code>CacheManager</code> is requested to getOrCloneSessionWithConfigsOff</li> </ul>","text":""},{"location":"SQLConf/#casesensitiveanalysis","title":"caseSensitiveAnalysis <p>The value of spark.sql.caseSensitive configuration property</p>","text":""},{"location":"SQLConf/#cboenabled","title":"cboEnabled <p>The value of spark.sql.cbo.enabled configuration property</p> <p>Used in:</p> <ul> <li>ReorderJoin logical plan optimization (and indirectly in <code>StarSchemaDetection</code> for <code>reorderStarJoins</code>)</li> <li>CostBasedJoinReorder logical plan optimization</li> </ul>","text":""},{"location":"SQLConf/#cliprintheader","title":"cliPrintHeader <p>spark.sql.cli.print.header</p> <p>Used when:</p> <ul> <li><code>SparkSQLCLIDriver</code> is requested to <code>processCmd</code></li> </ul>","text":""},{"location":"SQLConf/#coalescebucketsinjoinenabled","title":"coalesceBucketsInJoinEnabled <p>The value of spark.sql.bucketing.coalesceBucketsInJoin.enabled configuration property</p> <p>Used when:</p> <ul> <li>CoalesceBucketsInJoin physical optimization is executed</li> </ul>","text":""},{"location":"SQLConf/#coalesce_partitions_min_partition_size","title":"COALESCE_PARTITIONS_MIN_PARTITION_SIZE <p>spark.sql.adaptive.coalescePartitions.minPartitionSize configuration property</p> <p>Used when:</p> <ul> <li>CoalesceShufflePartitions physical optimization is executed</li> </ul>","text":""},{"location":"SQLConf/#coalesce_partitions_parallelism_first","title":"COALESCE_PARTITIONS_PARALLELISM_FIRST <p>spark.sql.adaptive.coalescePartitions.parallelismFirst configuration property</p> <p>Used when:</p> <ul> <li>CoalesceShufflePartitions physical optimization is executed</li> </ul>","text":""},{"location":"SQLConf/#coalesceshufflepartitionsenabled","title":"coalesceShufflePartitionsEnabled <p>The value of spark.sql.adaptive.coalescePartitions.enabled configuration property</p> <p>Used when:</p> <ul> <li>CoalesceShufflePartitions and EnsureRequirements physical optimizations are executed</li> </ul>","text":""},{"location":"SQLConf/#codegencachemaxentries","title":"codegenCacheMaxEntries <p>spark.sql.codegen.cache.maxEntries</p>","text":""},{"location":"SQLConf/#columnbatchsize","title":"columnBatchSize <p>The value of spark.sql.inMemoryColumnarStorage.batchSize configuration property</p> <p>Used when:</p> <ul> <li><code>CacheManager</code> is requested to cache a structured query</li> <li><code>RowToColumnarExec</code> physical operator is requested to doExecuteColumnar</li> </ul>","text":""},{"location":"SQLConf/#constraintpropagationenabled","title":"constraintPropagationEnabled <p>The value of spark.sql.constraintPropagation.enabled configuration property</p> <p>Used when:</p> <ul> <li>InferFiltersFromConstraints logical optimization is executed</li> <li><code>QueryPlanConstraints</code> is requested for the constraints</li> </ul>","text":""},{"location":"SQLConf/#convert_metastore_orc","title":"CONVERT_METASTORE_ORC <p>The value of spark.sql.hive.convertMetastoreOrc configuration property</p> <p>Used when RelationConversions logical post-hoc evaluation rule is executed (and requested to isConvertible)</p>","text":""},{"location":"SQLConf/#convert_metastore_parquet","title":"CONVERT_METASTORE_PARQUET <p>The value of spark.sql.hive.convertMetastoreParquet configuration property</p> <p>Used when RelationConversions logical post-hoc evaluation rule is executed (and requested to isConvertible)</p>","text":""},{"location":"SQLConf/#csvexpressionoptimization","title":"csvExpressionOptimization <p>spark.sql.optimizer.enableCsvExpressionOptimization</p> <p>Used when:</p> <ul> <li><code>OptimizeCsvJsonExprs</code> logical optimization is executed</li> </ul>","text":""},{"location":"SQLConf/#dataframepivotmaxvalues","title":"dataFramePivotMaxValues <p>The value of spark.sql.pivotMaxValues configuration property</p> <p>Used in pivot operator.</p>","text":""},{"location":"SQLConf/#dataframeretaingroupcolumns","title":"dataFrameRetainGroupColumns <p>The value of spark.sql.retainGroupColumns configuration property</p> <p>Used in RelationalGroupedDataset when creating the result <code>Dataset</code> (after <code>agg</code>, <code>count</code>, <code>mean</code>, <code>max</code>, <code>avg</code>, <code>min</code>, and <code>sum</code> operators).</p>","text":""},{"location":"SQLConf/#decorrelateinnerqueryenabled","title":"decorrelateInnerQueryEnabled <p>spark.sql.optimizer.decorrelateInnerQuery.enabled</p> <p>Used when:</p> <ul> <li><code>CheckAnalysis</code> is requested to checkCorrelationsInSubquery (with a Project unary logical operator)</li> <li>PullupCorrelatedPredicates logical optimization is executed</li> </ul>","text":""},{"location":"SQLConf/#default_catalog","title":"DEFAULT_CATALOG <p>The value of spark.sql.defaultCatalog configuration property</p> <p>Used when <code>CatalogManager</code> is requested for the current CatalogPlugin</p>","text":""},{"location":"SQLConf/#defaultdatasourcename","title":"defaultDataSourceName <p>spark.sql.sources.default</p>","text":""},{"location":"SQLConf/#defaultsizeinbytes","title":"defaultSizeInBytes <p>spark.sql.defaultSizeInBytes</p> <p>Used when:</p> <ul> <li><code>DetermineTableStats</code> logical resolution rule could not compute the table size or spark.sql.statistics.fallBackToHdfs is disabled</li> <li>ExternalRDD, LogicalRDD and DataSourceV2Relation are requested to compute stats</li> <li>(Spark Structured Streaming) <code>StreamingRelation</code>, <code>StreamingExecutionRelation</code>, <code>StreamingRelationV2</code> and <code>ContinuousExecutionRelation</code> are requested for statistics (i.e. <code>computeStats</code>)</li> <li><code>DataSource</code> creates a HadoopFsRelation for FileFormat data source (and builds a CatalogFileIndex when no table statistics are available)</li> <li><code>BaseRelation</code> is requested for an estimated size of this relation (in bytes)</li> </ul>","text":""},{"location":"SQLConf/#dynamicpartitionpruningenabled","title":"dynamicPartitionPruningEnabled <p>spark.sql.optimizer.dynamicPartitionPruning.enabled</p>","text":""},{"location":"SQLConf/#dynamicpartitionpruningfallbackfilterratio","title":"dynamicPartitionPruningFallbackFilterRatio <p>The value of spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio configuration property</p> <p>Used when:</p> <ul> <li>PartitionPruning logical optimization rule is executed</li> </ul>","text":""},{"location":"SQLConf/#dynamicpartitionpruningpruningsideextrafilterratio","title":"dynamicPartitionPruningPruningSideExtraFilterRatio <p>The value of spark.sql.optimizer.dynamicPartitionPruning.pruningSideExtraFilterRatio configuration property</p> <p>Used when:</p> <ul> <li>PartitionPruning logical optimization rule is executed</li> </ul>","text":""},{"location":"SQLConf/#dynamicpartitionpruningreusebroadcastonly","title":"dynamicPartitionPruningReuseBroadcastOnly <p>spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly</p>","text":""},{"location":"SQLConf/#dynamicpartitionpruningusestats","title":"dynamicPartitionPruningUseStats <p>spark.sql.optimizer.dynamicPartitionPruning.useStats</p>","text":""},{"location":"SQLConf/#enable_full_outer_shuffled_hash_join_codegen","title":"ENABLE_FULL_OUTER_SHUFFLED_HASH_JOIN_CODEGEN <p>spark.sql.codegen.join.fullOuterShuffledHashJoin.enabled</p>","text":""},{"location":"SQLConf/#enabledefaultcolumns","title":"enableDefaultColumns <p>spark.sql.defaultColumn.enabled</p>","text":""},{"location":"SQLConf/#enableradixsort","title":"enableRadixSort <p>spark.sql.sort.enableRadixSort</p> <p>Used when:</p> <ul> <li><code>SortExec</code> physical operator is requested to create an UnsafeExternalRowSorter.</li> </ul>","text":""},{"location":"SQLConf/#enabletwolevelaggmap","title":"enableTwoLevelAggMap <p>spark.sql.codegen.aggregate.map.twolevel.enabled</p>","text":""},{"location":"SQLConf/#enablevectorizedhashmap","title":"enableVectorizedHashMap <p>spark.sql.codegen.aggregate.map.vectorized.enable</p>","text":""},{"location":"SQLConf/#exchangereuseenabled","title":"exchangeReuseEnabled <p>spark.sql.exchange.reuse</p> <p>Used when:</p> <ul> <li> <p>AdaptiveSparkPlanExec physical operator is requested to createQueryStages</p> </li> <li> <p>PartitionPruning logical optimization rule is executed.</p> </li> <li> <p><code>PlanDynamicPruningFilters</code> and ReuseExchange physical optimizations are executed</p> </li> </ul>","text":""},{"location":"SQLConf/#fallbacktohdfsforstatsenabled","title":"fallBackToHdfsForStatsEnabled <p>spark.sql.statistics.fallBackToHdfs</p> <p>Used when <code>DetermineTableStats</code> logical resolution rule is executed.</p>","text":""},{"location":"SQLConf/#fasthashaggregaterowmaxcapacitybit","title":"fastHashAggregateRowMaxCapacityBit <p>spark.sql.codegen.aggregate.fastHashMap.capacityBit</p>","text":""},{"location":"SQLConf/#fetchshuffleblocksinbatch","title":"fetchShuffleBlocksInBatch <p>The value of spark.sql.adaptive.fetchShuffleBlocksInBatch configuration property</p> <p>Used when ShuffledRowRDD is created</p>","text":""},{"location":"SQLConf/#filecommitprotocolclass","title":"fileCommitProtocolClass <p>spark.sql.sources.commitProtocolClass</p>","text":""},{"location":"SQLConf/#filecompressionfactor","title":"fileCompressionFactor <p>The value of spark.sql.sources.fileCompressionFactor configuration property</p> <p>Used when:</p> <ul> <li><code>HadoopFsRelation</code> is requested for a size</li> <li><code>FileScan</code> is requested to estimate statistics</li> </ul>","text":""},{"location":"SQLConf/#filesmaxpartitionbytes","title":"filesMaxPartitionBytes <p>spark.sql.files.maxPartitionBytes</p>","text":""},{"location":"SQLConf/#filesminpartitionnum","title":"filesMinPartitionNum <p>spark.sql.files.minPartitionNum</p>","text":""},{"location":"SQLConf/#filesopencostinbytes","title":"filesOpenCostInBytes <p>spark.sql.files.openCostInBytes</p>","text":""},{"location":"SQLConf/#filesourcepartitionfilecachesize","title":"filesourcePartitionFileCacheSize <p>spark.sql.hive.filesourcePartitionFileCacheSize</p>","text":""},{"location":"SQLConf/#histogramenabled","title":"histogramEnabled <p>The value of spark.sql.statistics.histogram.enabled configuration property</p> <p>Used when AnalyzeColumnCommand logical command is executed.</p>","text":""},{"location":"SQLConf/#histogramnumbins","title":"histogramNumBins <p>spark.sql.statistics.histogram.numBins</p> <p>Used when <code>AnalyzeColumnCommand</code> is AnalyzeColumnCommand.md#run[executed] with configuration-properties.md#spark.sql.statistics.histogram.enabled[spark.sql.statistics.histogram.enabled] turned on (and AnalyzeColumnCommand.md#computePercentiles[calculates percentiles]).</p>","text":""},{"location":"SQLConf/#hive_table_property_length_threshold","title":"HIVE_TABLE_PROPERTY_LENGTH_THRESHOLD <p>spark.sql.hive.tablePropertyLengthThreshold</p> <p>Used when:</p> <ul> <li><code>CatalogTable</code> is requested to splitLargeTableProp</li> </ul>","text":""},{"location":"SQLConf/#hugemethodlimit","title":"hugeMethodLimit <p>spark.sql.codegen.hugeMethodLimit</p>","text":""},{"location":"SQLConf/#ignorecorruptfiles","title":"ignoreCorruptFiles <p>The value of spark.sql.files.ignoreCorruptFiles configuration property</p> <p>Used when:</p> <ul> <li><code>AvroUtils</code> utility is requested to <code>inferSchema</code></li> <li><code>OrcFileFormat</code> is requested to <code>inferSchema</code> and <code>buildReader</code></li> <li><code>FileScanRDD</code> is created (and then to compute a partition)</li> <li><code>SchemaMergeUtils</code> utility is requested to <code>mergeSchemasInParallel</code></li> <li><code>OrcUtils</code> utility is requested to <code>readSchema</code></li> <li><code>FilePartitionReader</code> is requested to <code>ignoreCorruptFiles</code></li> </ul>","text":""},{"location":"SQLConf/#ignoremissingfiles","title":"ignoreMissingFiles <p>The value of spark.sql.files.ignoreMissingFiles configuration property</p> <p>Used when:</p> <ul> <li><code>FileScanRDD</code> is created (and then to compute a partition)</li> <li><code>InMemoryFileIndex</code> utility is requested to bulkListLeafFiles</li> <li><code>FilePartitionReader</code> is requested to <code>ignoreMissingFiles</code></li> </ul>","text":""},{"location":"SQLConf/#inmemorypartitionpruning","title":"inMemoryPartitionPruning <p>spark.sql.inMemoryColumnarStorage.partitionPruning</p>","text":""},{"location":"SQLConf/#isparquetbinaryasstring","title":"isParquetBinaryAsString <p>spark.sql.parquet.binaryAsString</p>","text":""},{"location":"SQLConf/#isparquetint96astimestamp","title":"isParquetINT96AsTimestamp <p>spark.sql.parquet.int96AsTimestamp</p>","text":""},{"location":"SQLConf/#isparquetint96timestampconversion","title":"isParquetINT96TimestampConversion <p>spark.sql.parquet.int96TimestampConversion</p> <p>Used when <code>ParquetFileFormat</code> is requested to build a data reader with partition column values appended.</p>","text":""},{"location":"SQLConf/#isparquetschemamergingenabled","title":"isParquetSchemaMergingEnabled <p>spark.sql.parquet.mergeSchema</p>","text":""},{"location":"SQLConf/#isparquetschemarespectsummaries","title":"isParquetSchemaRespectSummaries <p>spark.sql.parquet.respectSummaryFiles</p> <p>Used when:</p> <ul> <li><code>ParquetUtils</code> is used to inferSchema</li> </ul>","text":""},{"location":"SQLConf/#joinreorderenabled","title":"joinReorderEnabled <p>spark.sql.cbo.joinReorder.enabled</p> <p>Used in CostBasedJoinReorder logical plan optimization</p>","text":""},{"location":"SQLConf/#legacyintervalenabled","title":"legacyIntervalEnabled <p>spark.sql.legacy.interval.enabled</p> <p>Used when:</p> <ul> <li><code>SubtractTimestamps</code> expression is created</li> <li><code>SubtractDates</code> expression is created</li> <li><code>AstBuilder</code> is requested to visitTypeConstructor and visitInterval</li> </ul>","text":""},{"location":"SQLConf/#limitscaleupfactor","title":"limitScaleUpFactor <p>spark.sql.limit.scaleUpFactor</p> <p>Used when a physical operator is requested the first n rows as an array.</p>","text":""},{"location":"SQLConf/#local_shuffle_reader_enabled","title":"LOCAL_SHUFFLE_READER_ENABLED <p>spark.sql.adaptive.localShuffleReader.enabled</p> <p>Used when:</p> <ul> <li>OptimizeShuffleWithLocalRead adaptive physical optimization is executed</li> </ul>","text":""},{"location":"SQLConf/#managefilesourcepartitions","title":"manageFilesourcePartitions <p>spark.sql.hive.manageFilesourcePartitions</p>","text":""},{"location":"SQLConf/#maxconcurrentoutputfilewriters","title":"maxConcurrentOutputFileWriters <p>The value of spark.sql.maxConcurrentOutputFileWriters configuration property</p> <p>Used when:</p> <ul> <li><code>FileFormatWriter</code> is requested to write out a query result</li> </ul>","text":""},{"location":"SQLConf/#maxmetadatastringlength","title":"maxMetadataStringLength <p>spark.sql.maxMetadataStringLength</p> <p>Used when:</p> <ul> <li><code>DataSourceScanExec</code> is requested for simpleString</li> <li><code>FileScan</code> is requested for description and metadata</li> <li><code>HiveTableRelation</code> is requested for simpleString</li> </ul>","text":""},{"location":"SQLConf/#maxrecordsperfile","title":"maxRecordsPerFile <p>spark.sql.files.maxRecordsPerFile</p> <p>Used when:</p> <ul> <li><code>FileFormatWriter</code> utility is used to write out a query result</li> <li><code>FileWrite</code> is requested for a BatchWrite</li> </ul>","text":""},{"location":"SQLConf/#maxtostringfields","title":"maxToStringFields <p>The value of spark.sql.debug.maxToStringFields configuration property</p>","text":""},{"location":"SQLConf/#metastorepartitionpruning","title":"metastorePartitionPruning <p>spark.sql.hive.metastorePartitionPruning</p> <p>Used when HiveTableScanExec physical operator is executed with a partitioned table (and requested for rawPartitions)</p>","text":""},{"location":"SQLConf/#methodsplitthreshold","title":"methodSplitThreshold <p>spark.sql.codegen.methodSplitThreshold</p> <p>Used when:</p> <ul> <li><code>Expression</code> is requested to reduceCodeSize</li> <li><code>CodegenContext</code> is requested to buildCodeBlocks and subexpressionEliminationForWholeStageCodegen</li> <li><code>ExpandExec</code> physical operator is requested to <code>doConsume</code></li> <li><code>HashAggregateExec</code> physical operator is requested to generateEvalCodeForAggFuncs</li> </ul>","text":""},{"location":"SQLConf/#minnumpostshufflepartitions","title":"minNumPostShufflePartitions <p>spark.sql.adaptive.minNumPostShufflePartitions</p> <p>Used when EnsureRequirements physical optimization is executed (for Adaptive Query Execution).</p>","text":""},{"location":"SQLConf/#nestedschemapruningenabled","title":"nestedSchemaPruningEnabled <p>The value of spark.sql.optimizer.nestedSchemaPruning.enabled configuration property</p> <p>Used when SchemaPruning, ColumnPruning and V2ScanRelationPushDown logical optimizations are executed</p>","text":""},{"location":"SQLConf/#nonemptypartitionratioforbroadcastjoin","title":"nonEmptyPartitionRatioForBroadcastJoin <p>The value of spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin configuration property</p> <p>Used when:</p> <ul> <li>DynamicJoinSelection adaptive logical optimization is executed (and shouldDemoteBroadcastHashJoin)</li> </ul>","text":""},{"location":"SQLConf/#numshufflepartitions","title":"numShufflePartitions <p>spark.sql.shuffle.partitions</p>","text":""},{"location":"SQLConf/#offheapcolumnvectorenabled","title":"offHeapColumnVectorEnabled <p>spark.sql.columnVector.offheap.enabled</p>","text":""},{"location":"SQLConf/#rangeexchangesamplesizeperpartition","title":"rangeExchangeSampleSizePerPartition <p>The value of spark.sql.execution.rangeExchange.sampleSizePerPartition configuration property</p> <p>Used when:</p> <ul> <li>ShuffleExchangeExec physical operator is executed</li> </ul>","text":""},{"location":"SQLConf/#remove_redundant_sorts_enabled","title":"REMOVE_REDUNDANT_SORTS_ENABLED <p>The value of spark.sql.execution.removeRedundantSorts configuration property</p> <p>Used when:</p> <ul> <li>RemoveRedundantSorts physical optimization is executed</li> </ul>","text":""},{"location":"SQLConf/#replace_hash_with_sort_agg_enabled","title":"REPLACE_HASH_WITH_SORT_AGG_ENABLED <p>spark.sql.execution.replaceHashWithSortAgg</p>","text":""},{"location":"SQLConf/#runtimefilterbloomfilterenabled","title":"runtimeFilterBloomFilterEnabled <p>spark.sql.optimizer.runtime.bloomFilter.enabled</p>","text":""},{"location":"SQLConf/#runtime_bloom_filter_max_num_bits","title":"RUNTIME_BLOOM_FILTER_MAX_NUM_BITS <p>spark.sql.optimizer.runtime.bloomFilter.maxNumBits</p>","text":""},{"location":"SQLConf/#runtime_filter_number_threshold","title":"RUNTIME_FILTER_NUMBER_THRESHOLD <p>spark.sql.optimizer.runtimeFilter.number.threshold</p>","text":""},{"location":"SQLConf/#runtimefiltersemijoinreductionenabled","title":"runtimeFilterSemiJoinReductionEnabled <p>spark.sql.optimizer.runtimeFilter.semiJoinReduction.enabled</p>","text":""},{"location":"SQLConf/#skew_join_skewed_partition_factor","title":"SKEW_JOIN_SKEWED_PARTITION_FACTOR <p>spark.sql.adaptive.skewJoin.skewedPartitionFactor configuration property</p> <p>Used when:</p> <ul> <li>OptimizeSkewedJoin physical optimization is executed</li> </ul>","text":""},{"location":"SQLConf/#skew_join_skewed_partition_threshold","title":"SKEW_JOIN_SKEWED_PARTITION_THRESHOLD <p>spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes configuration property</p> <p>Used when:</p> <ul> <li>OptimizeSkewedJoin physical optimization is executed</li> </ul>","text":""},{"location":"SQLConf/#skew_join_enabled","title":"SKEW_JOIN_ENABLED <p>spark.sql.adaptive.skewJoin.enabled configuration property</p> <p>Used when:</p> <ul> <li>OptimizeSkewedJoin physical optimization is executed</li> </ul>","text":""},{"location":"SQLConf/#objectaggsortbasedfallbackthreshold","title":"objectAggSortBasedFallbackThreshold <p>spark.sql.objectHashAggregate.sortBased.fallbackThreshold</p>","text":""},{"location":"SQLConf/#offheapcolumnvectorenabled_1","title":"offHeapColumnVectorEnabled <p>spark.sql.columnVector.offheap.enabled</p> <p>Used when:</p> <ul> <li><code>InMemoryTableScanExec</code> is requested for the vectorTypes and the input RDD</li> <li><code>OrcFileFormat</code> is requested to <code>buildReaderWithPartitionValues</code></li> <li><code>ParquetFileFormat</code> is requested for vectorTypes and build a data reader with partition column values appended</li> </ul>","text":""},{"location":"SQLConf/#optimize_one_row_relation_subquery","title":"OPTIMIZE_ONE_ROW_RELATION_SUBQUERY <p>spark.sql.optimizer.optimizeOneRowRelationSubquery</p> <p>Used when:</p> <ul> <li><code>OptimizeOneRowRelationSubquery</code> logical optimization is executed</li> </ul>","text":""},{"location":"SQLConf/#optimizenullawareantijoin","title":"optimizeNullAwareAntiJoin <p>spark.sql.optimizeNullAwareAntiJoin configuration property</p> <p>Used when:</p> <ul> <li>ExtractSingleColumnNullAwareAntiJoin Scala extractor is executed</li> </ul>","text":""},{"location":"SQLConf/#optimizerexcludedrules","title":"optimizerExcludedRules <p>The value of spark.sql.optimizer.excludedRules configuration property</p> <p>Used when <code>Optimizer</code> is requested for the batches</p>","text":""},{"location":"SQLConf/#optimizerinsetconversionthreshold","title":"optimizerInSetConversionThreshold <p>spark.sql.optimizer.inSetConversionThreshold</p> <p>Used when OptimizeIn logical query optimization is executed</p>","text":""},{"location":"SQLConf/#orcvectorizedreadernestedcolumnenabled","title":"orcVectorizedReaderNestedColumnEnabled <p>spark.sql.orc.enableNestedColumnVectorizedReader</p> <p>Used when:</p> <ul> <li><code>OrcFileFormat</code> is requested to <code>supportBatchForNestedColumn</code></li> </ul>","text":""},{"location":"SQLConf/#output_committer_class","title":"OUTPUT_COMMITTER_CLASS <p>spark.sql.sources.outputCommitterClass</p> <p>Used when:</p> <ul> <li><code>SQLHadoopMapReduceCommitProtocol</code> is requested to setupCommitter</li> <li><code>ParquetFileFormat</code> is requested to prepareWrite</li> <li><code>ParquetWrite</code> is requested to prepareWrite</li> </ul>","text":""},{"location":"SQLConf/#parallelfilelistinginstatscomputation","title":"parallelFileListingInStatsComputation <p>spark.sql.statistics.parallelFileListingInStatsComputation.enabled</p> <p>Used when <code>CommandUtils</code> helper object is requested to calculate the total size of a table (with partitions) (for AnalyzeColumnCommand and AnalyzeTableCommand commands)</p>","text":""},{"location":"SQLConf/#parquetaggregatepushdown","title":"parquetAggregatePushDown <p>spark.sql.parquet.aggregatePushdown</p>","text":""},{"location":"SQLConf/#parquetcompressioncodec","title":"parquetCompressionCodec <p>spark.sql.parquet.compression.codec</p> <p>Used when:</p> <ul> <li><code>ParquetOptions</code> is requested for compressionCodecClassName</li> </ul>","text":""},{"location":"SQLConf/#parquetfilterpushdown","title":"parquetFilterPushDown <p>spark.sql.parquet.filterPushdown</p>","text":""},{"location":"SQLConf/#parquetfilterpushdowndate","title":"parquetFilterPushDownDate <p>spark.sql.parquet.filterPushdown.date</p> <p>Used when:</p> <ul> <li><code>ParquetFileFormat</code> is requested to build a data reader (with partition column values appended)</li> </ul>","text":""},{"location":"SQLConf/#parquetfilterpushdowndecimal","title":"parquetFilterPushDownDecimal <p>spark.sql.parquet.filterPushdown.decimal</p> <p>Used when:</p> <ul> <li><code>ParquetFileFormat</code> is requested to build a data reader (with partition column values appended)</li> <li><code>ParquetPartitionReaderFactory</code> is requested to buildReaderBase</li> <li><code>ParquetScanBuilder</code> is requested for pushedParquetFilters</li> </ul>","text":""},{"location":"SQLConf/#parquetfilterpushdowninfilterthreshold","title":"parquetFilterPushDownInFilterThreshold <p>spark.sql.parquet.pushdown.inFilterThreshold</p> <p>Used when:</p> <ul> <li><code>ParquetFileFormat</code> is requested to build a data reader (with partition column values appended)</li> <li><code>ParquetPartitionReaderFactory</code> is requested to buildReaderBase</li> <li><code>ParquetScanBuilder</code> is requested for pushedParquetFilters</li> </ul>","text":""},{"location":"SQLConf/#parquetfilterpushdownstringpredicate","title":"parquetFilterPushDownStringPredicate <p>spark.sql.parquet.filterPushdown.stringPredicate</p>","text":""},{"location":"SQLConf/#parquetfilterpushdownstringstartwith","title":"parquetFilterPushDownStringStartWith <p>spark.sql.parquet.filterPushdown.string.startsWith</p>","text":""},{"location":"SQLConf/#parquetfilterpushdowntimestamp","title":"parquetFilterPushDownTimestamp <p>spark.sql.parquet.filterPushdown.timestamp</p> <p>Used when:</p> <ul> <li><code>ParquetFileFormat</code> is requested to build a data reader (with partition column values appended)</li> <li><code>ParquetPartitionReaderFactory</code> is requested to buildReaderBase</li> <li><code>ParquetScanBuilder</code> is requested for pushedParquetFilters</li> </ul>","text":""},{"location":"SQLConf/#parquetoutputcommitterclass","title":"parquetOutputCommitterClass <p>spark.sql.parquet.output.committer.class</p> <p>Used when:</p> <ul> <li><code>ParquetFileFormat</code> is requested to prepareWrite</li> <li><code>ParquetWrite</code> is requested to prepareWrite</li> </ul>","text":""},{"location":"SQLConf/#parquetoutputtimestamptype","title":"parquetOutputTimestampType <p>spark.sql.parquet.outputTimestampType</p> <p>Used when:</p> <ul> <li><code>ParquetFileFormat</code> is requested to prepareWrite</li> <li><code>SparkToParquetSchemaConverter</code> is created</li> <li><code>ParquetWriteSupport</code> is requested to init</li> <li><code>ParquetWrite</code> is requested to prepareWrite</li> </ul>","text":""},{"location":"SQLConf/#parquetrecordfilterenabled","title":"parquetRecordFilterEnabled <p>spark.sql.parquet.recordLevelFilter.enabled</p> <p>Used when <code>ParquetFileFormat</code> is requested to build a data reader (with partition column values appended).</p>","text":""},{"location":"SQLConf/#parquetvectorizedreaderbatchsize","title":"parquetVectorizedReaderBatchSize <p>spark.sql.parquet.columnarReaderBatchSize</p>","text":""},{"location":"SQLConf/#parquetvectorizedreaderenabled","title":"parquetVectorizedReaderEnabled <p>spark.sql.parquet.enableVectorizedReader</p> <p>Used when:</p> <ul> <li><code>FileSourceScanExec</code> is requested for needsUnsafeRowConversion flag</li> <li><code>ParquetFileFormat</code> is requested for supportBatch flag and build a data reader with partition column values appended</li> </ul>","text":""},{"location":"SQLConf/#parquetvectorizedreadernestedcolumnenabled","title":"parquetVectorizedReaderNestedColumnEnabled <p>spark.sql.parquet.enableNestedColumnVectorizedReader</p>","text":""},{"location":"SQLConf/#partitionoverwritemode","title":"partitionOverwriteMode <p>The value of spark.sql.sources.partitionOverwriteMode configuration property</p> <p>Used when InsertIntoHadoopFsRelationCommand logical command is executed</p>","text":""},{"location":"SQLConf/#planchangeloglevel","title":"planChangeLogLevel <p>The value of spark.sql.planChangeLog.level configuration property</p> <p>Used when:</p> <ul> <li>PlanChangeLogger is created</li> </ul>","text":""},{"location":"SQLConf/#planchangebatches","title":"planChangeBatches <p>The value of spark.sql.planChangeLog.batches configuration property</p> <p>Used when:</p> <ul> <li><code>PlanChangeLogger</code> is requested to logBatch</li> </ul>","text":""},{"location":"SQLConf/#planchangerules","title":"planChangeRules <p>The value of spark.sql.planChangeLog.rules configuration property</p> <p>Used when:</p> <ul> <li><code>PlanChangeLogger</code> is requested to logRule</li> </ul>","text":""},{"location":"SQLConf/#prefersortmergejoin","title":"preferSortMergeJoin <p>spark.sql.join.preferSortMergeJoin</p> <p>Used in JoinSelection execution planning strategy to prefer sort merge join over shuffle hash join.</p>","text":""},{"location":"SQLConf/#leaf_node_default_parallelism","title":"LEAF_NODE_DEFAULT_PARALLELISM <p>spark.sql.leafNodeDefaultParallelism</p> <p>Used when:</p> <ul> <li><code>SparkSession</code> is requested for the leafNodeDefaultParallelism</li> </ul>","text":""},{"location":"SQLConf/#legacy_cte_precedence_policy","title":"LEGACY_CTE_PRECEDENCE_POLICY <p>spark.sql.legacy.ctePrecedencePolicy</p>","text":""},{"location":"SQLConf/#replacedatabrickssparkavroenabled","title":"replaceDatabricksSparkAvroEnabled <p>spark.sql.legacy.replaceDatabricksSparkAvro.enabled</p>","text":""},{"location":"SQLConf/#replaceexceptwithfilter","title":"replaceExceptWithFilter <p>spark.sql.optimizer.replaceExceptWithFilter</p> <p>Used when ReplaceExceptWithFilter logical optimization is executed</p>","text":""},{"location":"SQLConf/#runsqlonfile","title":"runSQLonFile <p>spark.sql.runSQLOnFiles</p> <p>Used when:</p> <ul> <li><code>ResolveSQLOnFile</code> is requested to maybeSQLFile</li> </ul>","text":""},{"location":"SQLConf/#runtime_bloom_filter_expected_num_items","title":"RUNTIME_BLOOM_FILTER_EXPECTED_NUM_ITEMS <p>spark.sql.optimizer.runtime.bloomFilter.expectedNumItems</p>","text":""},{"location":"SQLConf/#sessionlocaltimezone","title":"sessionLocalTimeZone <p>spark.sql.session.timeZone</p>","text":""},{"location":"SQLConf/#sessionwindowbufferinmemorythreshold","title":"sessionWindowBufferInMemoryThreshold <p>spark.sql.sessionWindow.buffer.in.memory.threshold</p> <p>Used when:</p> <ul> <li><code>UpdatingSessionsExec</code> unary physical operator is executed</li> </ul>","text":""},{"location":"SQLConf/#sessionwindowbufferspillthreshold","title":"sessionWindowBufferSpillThreshold <p>spark.sql.sessionWindow.buffer.spill.threshold</p> <p>Used when:</p> <ul> <li><code>UpdatingSessionsExec</code> unary physical operator is executed</li> </ul>","text":""},{"location":"SQLConf/#sortbeforerepartition","title":"sortBeforeRepartition <p>The value of spark.sql.execution.sortBeforeRepartition configuration property</p> <p>Used when ShuffleExchangeExec physical operator is executed</p>","text":""},{"location":"SQLConf/#starschemadetection","title":"starSchemaDetection <p>spark.sql.cbo.starSchemaDetection</p> <p>Used in ReorderJoin logical optimization (and indirectly in <code>StarSchemaDetection</code>)</p>","text":""},{"location":"SQLConf/#stringredactionpattern","title":"stringRedactionPattern <p>spark.sql.redaction.string.regex</p> <p>Used when:</p> <ul> <li><code>DataSourceScanExec</code> is requested to redact sensitive information (in text representations)</li> <li><code>QueryExecution</code> is requested to redact sensitive information (in text representations)</li> </ul>","text":""},{"location":"SQLConf/#subexpressioneliminationenabled","title":"subexpressionEliminationEnabled <p>spark.sql.subexpressionElimination.enabled</p> <p>Used when <code>SparkPlan</code> is requested for subexpressionEliminationEnabled flag.</p>","text":""},{"location":"SQLConf/#subqueryreuseenabled","title":"subqueryReuseEnabled <p>spark.sql.execution.reuseSubquery</p> <p>Used when:</p> <ul> <li>ReuseAdaptiveSubquery adaptive physical optimization is executed</li> <li>ReuseExchangeAndSubquery physical optimization is executed</li> </ul>","text":""},{"location":"SQLConf/#supportquotedregexcolumnname","title":"supportQuotedRegexColumnName <p>spark.sql.parser.quotedRegexColumnNames</p> <p>Used when:</p> <ul> <li>Dataset.col operator is used</li> <li><code>AstBuilder</code> is requested to parse a dereference and column reference in a SQL statement</li> </ul>","text":""},{"location":"SQLConf/#targetpostshuffleinputsize","title":"targetPostShuffleInputSize <p>spark.sql.adaptive.shuffle.targetPostShuffleInputSize</p> <p>Used when EnsureRequirements physical optimization is executed (for Adaptive Query Execution)</p>","text":""},{"location":"SQLConf/#thriftserver_force_cancel","title":"THRIFTSERVER_FORCE_CANCEL <p>spark.sql.thriftServer.interruptOnCancel</p> <p>Used when:</p> <ul> <li><code>SparkExecuteStatementOperation</code> is created (<code>forceCancel</code>)</li> </ul>","text":""},{"location":"SQLConf/#truncatetableignorepermissionacl","title":"truncateTableIgnorePermissionAcl <p>spark.sql.truncateTable.ignorePermissionAcl.enabled</p> <p>Used when TruncateTableCommand logical command is executed</p>","text":""},{"location":"SQLConf/#usecompression","title":"useCompression <p>The value of spark.sql.inMemoryColumnarStorage.compressed configuration property</p> <p>Used when <code>CacheManager</code> is requested to cache a structured query</p>","text":""},{"location":"SQLConf/#useobjecthashaggregation","title":"useObjectHashAggregation <p>spark.sql.execution.useObjectHashAggregateExec</p> <p>Used when Aggregation execution planning strategy is executed (and uses <code>AggUtils</code> to create an aggregation physical operator).</p>","text":""},{"location":"SQLConf/#variablesubstituteenabled","title":"variableSubstituteEnabled <p>spark.sql.variable.substitute</p> <p>Used when:</p> <ul> <li><code>VariableSubstitution</code> is requested to substitute variables in a SQL command</li> </ul>","text":""},{"location":"SQLConf/#wholestageenabled","title":"wholeStageEnabled <p>spark.sql.codegen.wholeStage</p> <p>Used in:</p> <ul> <li>CollapseCodegenStages to control codegen</li> <li>ParquetFileFormat to control row batch reading</li> </ul>","text":""},{"location":"SQLConf/#wholestagefallback","title":"wholeStageFallback <p>spark.sql.codegen.fallback</p>","text":""},{"location":"SQLConf/#wholestagemaxnumfields","title":"wholeStageMaxNumFields <p>spark.sql.codegen.maxFields</p> <p>Used in:</p> <ul> <li>CollapseCodegenStages to control codegen</li> <li>ParquetFileFormat to control row batch reading</li> </ul>","text":""},{"location":"SQLConf/#wholestagesplitconsumefuncbyoperator","title":"wholeStageSplitConsumeFuncByOperator <p>spark.sql.codegen.splitConsumeFuncByOperator</p> <p>Used when <code>CodegenSupport</code> is requested to consume</p>","text":""},{"location":"SQLConf/#wholestageuseidinclassname","title":"wholeStageUseIdInClassName <p>spark.sql.codegen.useIdInClassName</p> <p>Used when <code>WholeStageCodegenExec</code> is requested to generate the Java source code for the child physical plan subtree (when created)</p>","text":""},{"location":"SQLConf/#windowexecbufferinmemorythreshold","title":"windowExecBufferInMemoryThreshold <p>spark.sql.windowExec.buffer.in.memory.threshold</p> <p>Used when:</p> <ul> <li>WindowExec unary physical operator is executed</li> </ul>","text":""},{"location":"SQLConf/#windowexecbufferspillthreshold","title":"windowExecBufferSpillThreshold <p>spark.sql.windowExec.buffer.spill.threshold</p> <p>Used when:</p> <ul> <li>WindowExec unary physical operator is executed</li> </ul>","text":""},{"location":"SQLConfHelper/","title":"SQLConfHelper","text":"<p><code>SQLConfHelper</code> is...FIXME</p>"},{"location":"SQLExecution/","title":"SQLExecution","text":""},{"location":"SQLExecution/#sparksqlexecutionid","title":"spark.sql.execution.id <p><code>SQLExecution</code> defines <code>spark.sql.execution.id</code> that is used as the key of the Spark local property in the following:</p> <ul> <li>withNewExecutionId</li> <li>withExecutionId</li> </ul> <p><code>spark.sql.execution.id</code> is used to track multiple Spark jobs that should all together be considered part of a single execution of a structured query.</p> <pre><code>import org.apache.spark.sql.execution.SQLExecution\nscala&gt; println(SQLExecution.EXECUTION_ID_KEY)\nspark.sql.execution.id\n</code></pre> <p><code>spark.sql.execution.id</code> allows \"stitching\" different Spark jobs (esp. executed on separate threads) as part of one structured query (that you can then see in web UI's SQL tab).</p>  <p>Tip</p> <p>Use <code>SparkListener</code> (Spark Core) to listen to <code>SparkListenerSQLExecutionStart</code> events and know the execution IDs of structured queries that have been executed in a Spark SQL application.</p> <pre><code>// \"SQLAppStatusListener\" idea is borrowed from\n// Spark SQL's org.apache.spark.sql.execution.ui.SQLAppStatusListener\nimport org.apache.spark.scheduler.{SparkListener, SparkListenerEvent}\nimport org.apache.spark.sql.execution.ui.{SparkListenerDriverAccumUpdates, SparkListenerSQLExecutionEnd, SparkListenerSQLExecutionStart}\npublic class SQLAppStatusListener extends SparkListener {\n  override def onOtherEvent(event: SparkListenerEvent): Unit = event match {\n    case e: SparkListenerSQLExecutionStart =&gt; onExecutionStart(e)\n    case e: SparkListenerSQLExecutionEnd =&gt; onExecutionEnd(e)\n    case e: SparkListenerDriverAccumUpdates =&gt; onDriverAccumUpdates(e)\n    case _ =&gt; // Ignore\n  }\n  def onExecutionStart(event: SparkListenerSQLExecutionStart): Unit = {\n    // Find the QueryExecution for the Dataset action that triggered the event\n    // This is the SQL-specific way\n    import org.apache.spark.sql.execution.SQLExecution\n    queryExecution = SQLExecution.getQueryExecution(event.executionId)\n  }\n  def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n    // Find the QueryExecution for the Dataset action that triggered the event\n    // This is a general Spark Core way using local properties\n    import org.apache.spark.sql.execution.SQLExecution\n    val executionIdStr = jobStart.properties.getProperty(SQLExecution.EXECUTION_ID_KEY)\n    // Note that the Spark job may or may not be a part of a structured query\n    if (executionIdStr != null) {\n      queryExecution = SQLExecution.getQueryExecution(executionIdStr.toLong)\n    }\n  }\n  def onExecutionEnd(event: SparkListenerSQLExecutionEnd): Unit = {}\n  def onDriverAccumUpdates(event: SparkListenerDriverAccumUpdates): Unit = {}\n}\n\nval sqlListener = new SQLAppStatusListener()\nspark.sparkContext.addSparkListener(sqlListener)\n</code></pre>  <p>Spark jobs without <code>spark.sql.execution.id</code> local property can then be considered to belong to a SQL query execution.</p>","text":""},{"location":"SQLExecution/#withnewexecutionid","title":"withNewExecutionId <pre><code>withNewExecutionId[T](\n  queryExecution: QueryExecution,\n  name: Option[String] = None)(\n  body: =&gt; T): T\n</code></pre> <p><code>withNewExecutionId</code> executes <code>body</code> query action with a next available execution ID.</p> <p><code>withNewExecutionId</code> replaces an existing execution ID, if defined, until the entire <code>body</code> finishes.</p> <p><code>withNewExecutionId</code> posts <code>SparkListenerSQLExecutionStart</code> and SparkListenerSQLExecutionEnd events right before and right after executing the <code>body</code> action, respectively.</p>  <p><code>withNewExecutionId</code> is used when:</p> <ul> <li><code>Dataset</code> is requested to withNewExecutionId, withNewRDDExecutionId, Dataset.withAction</li> <li><code>QueryExecution</code> is requested to eagerlyExecuteCommands</li> <li>others (in Spark Thrift Server and Spark Structured Streaming)</li> </ul>","text":""},{"location":"SQLExecution/#withexecutionid","title":"withExecutionId <pre><code>withExecutionId[T](\n  sparkSession: SparkSession,\n  executionId: String)(\n  body: =&gt; T): T\n</code></pre> <p><code>withExecutionId</code> executes the <code>body</code> under the given <code>executionId</code> execution identifier.</p> <pre><code>val rdd = sc.parallelize(0 to 5, numSlices = 2)\n\nimport org.apache.spark.TaskContext\ndef func(ctx: TaskContext, ns: Iterator[Int]): Int = {\n  ctx.partitionId\n}\n\ndef runSparkJobs = {\n  sc.runJob(rdd, func _)\n}\n\nimport org.apache.spark.sql.execution.SQLExecution\nSQLExecution.withExecutionId(spark, executionId = \"100\")(body = runSparkJobs)\n</code></pre>  <p><code>withExecutionId</code> is used when:</p> <ul> <li><code>BroadcastExchangeExec</code> physical operator is requested to prepare for execution (and initializes relationFuture)</li> <li><code>SubqueryExec</code> physical operator is requested to prepare for execution (and initializes relationFuture)</li> </ul>","text":""},{"location":"SQLExecutionRDD/","title":"SQLExecutionRDD","text":"<p><code>SQLExecutionRDD</code> is an <code>RDD[InternalRow]</code> to wrap the parent RDD and make sure that the SQL configuration properties are always propagated to executors (even when <code>rdd</code> or QueryExecution.toRdd are used).</p> <p>Tip</p> <p>Review SPARK-28939 to learn when and why <code>SQLExecutionRDD</code> would be used outside a tracked SQL operation (and with no <code>spark.sql.execution.id</code> defined).</p>"},{"location":"SQLExecutionRDD/#creating-instance","title":"Creating Instance","text":"<p><code>SQLExecutionRDD</code> takes the following to be created:</p> <ul> <li>RDD[InternalRow]</li> <li> SQLConf <p>While being created, <code>SQLExecutionRDD</code> initializes a sqlConfigs internal registry.</p> <p><code>SQLExecutionRDD</code> is created\u00a0when:</p> <ul> <li><code>QueryExecution</code> is requested to toRdd</li> </ul>"},{"location":"SQLExecutionRDD/#sql-rdd","title":"SQL RDD <p><code>SQLExecutionRDD</code> is given an <code>RDD[InternalRow]</code> when created.</p> <p>The <code>RDD[InternalRow]</code> is the executedPlan requested to execute.</p>","text":""},{"location":"SQLExecutionRDD/#sqlconfigs","title":"sqlConfigs <p><code>SQLExecutionRDD</code> requests the given SQLConf for all the configuration properties that have been set when created.</p>  Lazy Value <p><code>sqlConfigs</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>","text":""},{"location":"SQLExecutionRDD/#computing-partition","title":"Computing Partition <pre><code>compute(\n  split: Partition,\n  context: TaskContext): Iterator[InternalRow]\n</code></pre> <p><code>compute</code> looks up the spark.sql.execution.id local property in the given <code>TaskContext</code> (Apache Spark).</p> <p>If not defined (<code>null</code>), <code>compute</code> sets the sqlConfigs as thread-local properties to requests the sqlRDD for <code>iterator</code> (execute the <code>sqlRDD</code>). Otherwise, if in the context of a tracked SQL operation (and the <code>spark.sql.execution.id</code> is defined), <code>compute</code> simply requests the parent sqlRDD for <code>iterator</code>.</p> <p><code>compute</code>\u00a0is part of the <code>RDD</code> (Apache Spark) abstraction.</p>","text":""},{"location":"SQLMetric/","title":"SQLMetric","text":"<p><code>SQLMetric</code> is an <code>AccumulatorV2</code> (Spark Core) for performance metrics of physical operators.</p> <p>Note</p> <p>Use Details for Query page in SQL tab in web UI to see the SQL execution metrics of a structured query.</p>"},{"location":"SQLMetric/#creating-instance","title":"Creating Instance","text":"<p><code>SQLMetric</code> takes the following to be created:</p> <ul> <li>Metric Type</li> <li> Initial Value <p><code>SQLMetric</code> is created\u00a0using the specialized utilities:</p> <ul> <li>createMetric</li> <li>createSizeMetric</li> <li>createTimingMetric</li> <li>createNanoTimingMetric</li> <li>createAverageMetric</li> </ul>"},{"location":"SQLMetric/#createaveragemetric","title":"createAverageMetric <pre><code>createAverageMetric(\n  sc: SparkContext,\n  name: String): SQLMetric\n</code></pre> <p><code>createAverageMetric</code> creates a <code>SQLMetric</code> with average type and registers it with the given <code>name</code>.</p>","text":""},{"location":"SQLMetric/#createmetric","title":"createMetric <pre><code>createMetric(\n  sc: SparkContext,\n  name: String): SQLMetric\n</code></pre> <p><code>createMetric</code> creates a <code>SQLMetric</code> with sum type and registers it with the given <code>name</code>.</p>","text":""},{"location":"SQLMetric/#createnanotimingmetric","title":"createNanoTimingMetric <pre><code>createNanoTimingMetric(\n  sc: SparkContext,\n  name: String): SQLMetric\n</code></pre> <p><code>createNanoTimingMetric</code> creates a <code>SQLMetric</code> with nsTiming type and registers it with the given <code>name</code>.</p>","text":""},{"location":"SQLMetric/#createsizemetric","title":"createSizeMetric <pre><code>createSizeMetric(\n  sc: SparkContext,\n  name: String): SQLMetric\n</code></pre> <p><code>createSizeMetric</code> creates a <code>SQLMetric</code> with size type and registers it with the given <code>name</code>.</p>","text":""},{"location":"SQLMetric/#createtimingmetric","title":"createTimingMetric <pre><code>createTimingMetric(\n  sc: SparkContext,\n  name: String): SQLMetric\n</code></pre> <p><code>createTimingMetric</code> creates a <code>SQLMetric</code> with timing type and registers it with the given <code>name</code>.</p>","text":""},{"location":"SQLMetric/#metric-types","title":"Metric Types <p><code>SQLMetric</code> is given a metric type to be created:</p> <ul> <li> <code>average</code> <li> <code>nsTiming</code> <li> <code>size</code> <li> <code>sum</code> <li> <code>timing</code>","text":""},{"location":"SQLMetric/#posting-driver-side-metric-updates","title":"Posting Driver-Side Metric Updates <pre><code>postDriverMetricUpdates(\n  sc: SparkContext,\n  executionId: String,\n  metrics: Seq[SQLMetric]): Unit\n</code></pre> <p><code>postDriverMetricUpdates</code> posts a <code>SparkListenerDriverAccumUpdates</code> event to <code>LiveListenerBus</code> (only if <code>executionId</code> is specified).</p>  <p><code>postDriverMetricUpdates</code> is used when:</p> <ul> <li><code>BasicWriteJobStatsTracker</code> is requested for processStats</li> <li><code>BroadcastExchangeExec</code> is requested for relationFuture</li> <li><code>FileSourceScanExec</code> physical operator is requested for sendDriverMetrics</li> <li><code>SubqueryBroadcastExec</code> physical operator is requested for <code>relationFuture</code></li> <li><code>SubqueryExec</code> physical operator is requested for relationFuture</li> </ul>","text":""},{"location":"ScalaReflection/","title":"ScalaReflection","text":"<p><code>ScalaReflection</code> is the contract and the only implementation of the contract with...FIXME</p> <p>=== [[serializerFor]] <code>serializerFor</code> Object Method</p>"},{"location":"ScalaReflection/#source-scala","title":"[source, scala]","text":""},{"location":"ScalaReflection/#serializerfort-typetag-createnamedstruct","title":"serializerForT : TypeTag: CreateNamedStruct","text":"<p><code>serializerFor</code> firstly finds the &lt;&gt; the input type <code>T</code> and then the &lt;&gt;. <p><code>serializerFor</code> uses the &lt;&gt; with the input <code>inputObject</code> expression, the <code>tpe</code> type and the <code>walkedTypePath</code> with the class name found earlier (of the input type <code>T</code>). <pre><code>- root class: \"[clsName]\"\n</code></pre> <p>In the end, <code>serializerFor</code> returns one of the following:</p> <ul> <li> <p>The &lt;&gt; expression from the false value of the <code>If</code> expression returned only if the type <code>T</code> is &lt;&gt; <li> <p>Creates a &lt;&gt; expression with the &lt;&gt; with the &lt;&gt; as <code>\"value\"</code> and the expression returned"},{"location":"ScalaReflection/#source-scala_1","title":"[source, scala]","text":"<p>import org.apache.spark.sql.functions.lit val inputObject = lit(1).expr</p> <p>import org.apache.spark.sql.catalyst.ScalaReflection val serializer = ScalaReflection.serializerFor(inputObject) scala&gt; println(serializer) named_struct(value, 1)</p> <p>NOTE: <code>serializerFor</code> is used when...FIXME</p> <p>==== [[serializerFor-internal]] <code>serializerFor</code> Internal Method</p>"},{"location":"ScalaReflection/#source-scala_2","title":"[source, scala]","text":"<p>serializerFor(   inputObject: Expression,   tpe: <code>Type</code>,   walkedTypePath: Seq[String],   seenTypeSet: Set[<code>Type</code>] = Set.empty): Expression</p> <p><code>serializerFor</code>...FIXME</p> <p>NOTE: <code>serializerFor</code> is used exclusively when <code>ScalaReflection</code> is requested to &lt;&gt;. <p>=== [[serializerFor]][[ScalaReflection-serializerFor]] Creating Serialize Expression -- <code>ScalaReflection.serializerFor</code> Method</p>"},{"location":"ScalaReflection/#source-scala_3","title":"[source, scala]","text":""},{"location":"ScalaReflection/#serializerfort-typetag-createnamedstruct_1","title":"serializerForT: TypeTag: CreateNamedStruct","text":"<p><code>serializerFor</code> creates a &lt;&gt; expression to serialize a Scala object of type <code>T</code> to InternalRow. <pre><code>import org.apache.spark.sql.catalyst.ScalaReflection.serializerFor\n\nimport org.apache.spark.sql.catalyst.expressions.BoundReference\nimport org.apache.spark.sql.types.TimestampType\nval boundRef = BoundReference(ordinal = 0, dataType = TimestampType, nullable = true)\n\nval timestampSerExpr = serializerFor[java.sql.Timestamp](boundRef)\nscala&gt; println(timestampSerExpr.numberedTreeString)\n00 named_struct(value, input[0, timestamp, true])\n01 :- value\n02 +- input[0, timestamp, true]\n</code></pre> <p>Internally, <code>serializerFor</code> calls the recursive internal variant of &lt;&gt; with a single-element walked type path with <code>- root class: \"[clsName]\"</code> and pattern match on the result expressions/Expression.md[expression]. <p>CAUTION: FIXME the pattern match part</p> <p>TIP: Read up on Scala's <code>TypeTags</code> in http://docs.scala-lang.org/overviews/reflection/typetags-manifests.html[TypeTags and Manifests].</p> <p>NOTE: <code>serializerFor</code> is used exclusively when <code>ExpressionEncoder</code> &lt;&gt; for a Scala type <code>T</code>. <p>==== [[serializerFor-recursive]] Recursive Internal <code>serializerFor</code> Method</p>"},{"location":"ScalaReflection/#source-scala_4","title":"[source, scala]","text":"<p>serializerFor(   inputObject: Expression,   tpe: <code>Type</code>,   walkedTypePath: Seq[String],   seenTypeSet: Set[<code>Type</code>] = Set.empty): Expression</p> <p><code>serializerFor</code> creates an expressions/Expression.md[expression] for serializing an object of type <code>T</code> to an internal row.</p> <p>CAUTION: FIXME</p> <p>=== [[deserializerFor]][[ScalaReflection-deserializerFor]] Creating Deserialize Expression -- <code>ScalaReflection.deserializerFor</code> Method</p>"},{"location":"ScalaReflection/#source-scala_5","title":"[source, scala]","text":""},{"location":"ScalaReflection/#deserializerfort-typetag-expression","title":"deserializerFor[T: TypeTag]: Expression","text":"<p><code>deserializerFor</code> creates an expressions/Expression.md[expression] to deserialize from InternalRow to a Scala object of type <code>T</code>.</p> <pre><code>import org.apache.spark.sql.catalyst.ScalaReflection.deserializerFor\nval timestampDeExpr = deserializerFor[java.sql.Timestamp]\nscala&gt; println(timestampDeExpr.numberedTreeString)\n00 staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, ObjectType(class java.sql.Timestamp), toJavaTimestamp, upcast(getcolumnbyordinal(0, TimestampType), TimestampType, - root class: \"java.sql.Timestamp\"), true)\n01 +- upcast(getcolumnbyordinal(0, TimestampType), TimestampType, - root class: \"java.sql.Timestamp\")\n02    +- getcolumnbyordinal(0, TimestampType)\n\nval tuple2DeExpr = deserializerFor[(java.sql.Timestamp, Double)]\nscala&gt; println(tuple2DeExpr.numberedTreeString)\n00 newInstance(class scala.Tuple2)\n01 :- staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, ObjectType(class java.sql.Timestamp), toJavaTimestamp, upcast(getcolumnbyordinal(0, TimestampType), TimestampType, - field (class: \"java.sql.Timestamp\", name: \"_1\"), - root class: \"scala.Tuple2\"), true)\n02 :  +- upcast(getcolumnbyordinal(0, TimestampType), TimestampType, - field (class: \"java.sql.Timestamp\", name: \"_1\"), - root class: \"scala.Tuple2\")\n03 :     +- getcolumnbyordinal(0, TimestampType)\n04 +- upcast(getcolumnbyordinal(1, DoubleType), DoubleType, - field (class: \"scala.Double\", name: \"_2\"), - root class: \"scala.Tuple2\")\n05    +- getcolumnbyordinal(1, DoubleType)\n</code></pre> <p>Internally, <code>deserializerFor</code> calls the recursive internal variant of &lt;&gt; with a single-element walked type path with <code>- root class: \"[clsName]\"</code> <p>Tip</p> <p>Read up on Scala's <code>TypeTags</code> in TypeTags and Manifests.</p> <p>NOTE: <code>deserializerFor</code> is used exclusively when <code>ExpressionEncoder</code> &lt;&gt; for a Scala type <code>T</code>. <p>=== [[localTypeOf]] <code>localTypeOf</code> Object Method</p>"},{"location":"ScalaReflection/#source-scala_6","title":"[source, scala]","text":""},{"location":"ScalaReflection/#localtypeoft-typetag-type","title":"localTypeOf[T: TypeTag]: <code>Type</code>","text":"<p><code>localTypeOf</code>...FIXME</p>"},{"location":"ScalaReflection/#source-scala_7","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.ScalaReflection val tpe = ScalaReflection.localTypeOf[Int] scala&gt; :type tpe org.apache.spark.sql.catalyst.ScalaReflection.universe.Type</p> <p>scala&gt; println(tpe) Int</p> <p>NOTE: <code>localTypeOf</code> is used when...FIXME</p> <p>=== [[getClassNameFromType]] <code>getClassNameFromType</code> Object Method</p>"},{"location":"ScalaReflection/#source-scala_8","title":"[source, scala]","text":""},{"location":"ScalaReflection/#getclassnamefromtypetpe-type-string","title":"getClassNameFromType(tpe: <code>Type</code>): String","text":"<p><code>getClassNameFromType</code>...FIXME</p>"},{"location":"ScalaReflection/#source-scala_9","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.ScalaReflection val tpe = ScalaReflection.localTypeOf[java.time.LocalDateTime] val className = ScalaReflection.getClassNameFromType(tpe) scala&gt; println(className) java.time.LocalDateTime</p> <p>NOTE: <code>getClassNameFromType</code> is used when...FIXME</p> <p>=== [[definedByConstructorParams]] <code>definedByConstructorParams</code> Object Method</p>"},{"location":"ScalaReflection/#source-scala_10","title":"[source, scala]","text":""},{"location":"ScalaReflection/#definedbyconstructorparamstpe-type-boolean","title":"definedByConstructorParams(tpe: Type): Boolean","text":"<p><code>definedByConstructorParams</code>...FIXME</p> <p>NOTE: <code>definedByConstructorParams</code> is used when...FIXME</p>"},{"location":"SchemaRelationProvider/","title":"SchemaRelationProvider \u2014 Relation Providers With Mandatory User-Defined Schema","text":"<p><code>SchemaRelationProvider</code> is the &lt;&gt; of &lt;&gt; that &lt;&gt;. <p>The requirement of specifying a user-defined schema is enforced when <code>DataSource</code> is requested for a BaseRelation for a given data source format. If not specified, <code>DataSource</code> throws a <code>AnalysisException</code>:</p> <pre><code>A schema needs to be specified when using [className].\n</code></pre> <p>[[contract]] [source, scala]</p> <p>package org.apache.spark.sql.sources</p> <p>trait SchemaRelationProvider {   def createRelation(     sqlContext: SQLContext,     parameters: Map[String, String],     schema: StructType): BaseRelation }</p> <p>.SchemaRelationProvider Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| <code>createRelation</code> | [[createRelation]] Creates a BaseRelation for the user-defined schema</p> <p>Used exclusively when <code>DataSource</code> is requested for a BaseRelation for a given data source format |===</p>"},{"location":"SchemaRelationProvider/#implementations","title":"Implementations","text":"<p>Note</p> <p>No known native Spark SQL implementations.</p> <p>Tip</p> <p>RelationProvider is used for data source providers with schema inference.</p> <p>Tip</p> <p>Use both <code>SchemaRelationProvider</code> and RelationProvider if a data source should support both schema inference and user-defined schemas.</p>"},{"location":"SerializerBuildHelper/","title":"SerializerBuildHelper","text":""},{"location":"SerializerBuildHelper/#creating-serializer-expression-for-object","title":"Creating Serializer (Expression) for Object <pre><code>createSerializerForObject(\n  inputObject: Expression,\n  fields: Seq[(String, Expression)]): Expression\n</code></pre> <p><code>createSerializerForObject</code> creates a CreateNamedStruct expression with argumentsForFieldSerializer for the given <code>fields</code>.</p> <p>Only when the given Expression is nullable, <code>createSerializerForObject</code> wraps the <code>CreateNamedStruct</code> expression with an <code>If</code> expression with <code>IsNull</code> child expression, a <code>null</code> Literal and the <code>CreateNamedStruct</code> expressions (for the positive and negative branches of the <code>If</code> expression, respectively).</p> <p><code>createSerializerForObject</code> is used when:</p> <ul> <li><code>JavaTypeInference</code> utility is used to <code>serializerFor</code></li> <li><code>ScalaReflection</code> utility is used to create a serializer (for Scala <code>Product</code>s or <code>DefinedByConstructorParams</code>s)</li> </ul>","text":""},{"location":"SerializerBuildHelper/#argumentsforfieldserializer","title":"argumentsForFieldSerializer <pre><code>argumentsForFieldSerializer(\n  fieldName: String,\n  serializerForFieldValue: Expression): Seq[Expression]\n</code></pre> <p><code>argumentsForFieldSerializer</code> creates a two-element collection of the following:</p> <ol> <li>Literal for the given <code>fieldName</code></li> <li><code>serializerForFieldValue</code> Expression</li> </ol>","text":""},{"location":"SessionCatalog/","title":"SessionCatalog \u2014 Session-Scoped Registry of Relational Entities","text":"<p><code>SessionCatalog</code> is a catalog of relational entities in SparkSession (e.g. databases, tables, views, partitions, and functions).</p> <p><code>SessionCatalog</code> is a SQLConfHelper.</p>"},{"location":"SessionCatalog/#creating-instance","title":"Creating Instance","text":"<p><code>SessionCatalog</code> takes the following to be created:</p> <ul> <li> ExternalCatalog <li> GlobalTempViewManager <li> FunctionRegistry <li>TableFunctionRegistry</li> <li> <code>Configuration</code> (Apache Hadoop) <li> ParserInterface <li> FunctionResourceLoader <li> <code>FunctionExpressionBuilder</code> <li> Cache Size (default: spark.sql.filesourceTableRelationCacheSize) <li> Cache TTL (default: spark.sql.metadataCacheTTLSeconds) <li> Default database name (default: spark.sql.catalog.spark_catalog.defaultDatabase) <p></p> <p><code>SessionCatalog</code> is created (and cached for later usage) when <code>BaseSessionStateBuilder</code> is requested for one.</p>"},{"location":"SessionCatalog/#tablefunctionregistry","title":"TableFunctionRegistry <p><code>SessionCatalog</code> is given a TableFunctionRegistry when created.</p> <p>The <code>TableFunctionRegistry</code> is used in the following:</p> <ul> <li>dropTempFunction</li> <li>isRegisteredFunction</li> <li>listBuiltinAndTempFunctions</li> <li>listTemporaryFunctions</li> <li>lookupBuiltinOrTempTableFunction</li> <li>lookupPersistentFunction</li> <li>resolveBuiltinOrTempTableFunction</li> <li>resolvePersistentTableFunction</li> <li>reset</li> <li>unregisterFunction</li> </ul>","text":""},{"location":"SessionCatalog/#accessing-sessioncatalog","title":"Accessing SessionCatalog","text":"<p><code>SessionCatalog</code> is available through SessionState (of the owning SparkSession).</p> <pre><code>val catalog = spark.sessionState.catalog\nassert(catalog.isInstanceOf[org.apache.spark.sql.catalyst.catalog.SessionCatalog])\n</code></pre>"},{"location":"SessionCatalog/#default-database-name","title":"Default Database Name <p><code>SessionCatalog</code> defines <code>default</code> as the name of the default database.</p>","text":""},{"location":"SessionCatalog/#externalcatalog","title":"ExternalCatalog <p><code>SessionCatalog</code> creates an ExternalCatalog for the metadata of permanent entities (when first requested).</p> <p><code>SessionCatalog</code> is in fact a layer over <code>ExternalCatalog</code> (in a SparkSession) which allows for different metastores (i.e. <code>in-memory</code> or hive).</p>","text":""},{"location":"SessionCatalog/#looking-up-function","title":"Looking Up Function <pre><code>lookupFunction(\n  name: FunctionIdentifier,\n  children: Seq[Expression]): Expression\n</code></pre> <p><code>lookupFunction</code> looks up a function by <code>name</code>.</p> <p>For a function with no database defined that exists in FunctionRegistry, <code>lookupFunction</code> requests <code>FunctionRegistry</code> to find the function (by its unqualified name, i.e. with no database).</p> <p>If the <code>name</code> function has the database defined or does not exist in <code>FunctionRegistry</code>, <code>lookupFunction</code> uses the fully-qualified function <code>name</code> to check if the function exists in FunctionRegistry (by its fully-qualified name, i.e. with a database).</p> <p>For other cases, <code>lookupFunction</code> requests ExternalCatalog to find the function and loads its resources. <code>lookupFunction</code> then creates a corresponding temporary function and looks up the function again.</p> <p><code>lookupFunction</code> is used when:</p> <ul> <li>ResolveFunctions logical resolution rule executed (and resolves UnresolvedGenerator or UnresolvedFunction expressions)</li> <li><code>HiveSessionCatalog</code> is requested to lookupFunction0</li> </ul>","text":""},{"location":"SessionCatalog/#looking-up-relation","title":"Looking Up Relation <pre><code>lookupRelation(\n  name: TableIdentifier): LogicalPlan\n</code></pre> <p><code>lookupRelation</code> finds the <code>name</code> table in the catalogs (i.e. GlobalTempViewManager, ExternalCatalog or registry of temporary views) and gives a <code>SubqueryAlias</code> per table type.</p> <p>Internally, <code>lookupRelation</code> looks up the <code>name</code> table using:</p> <ol> <li> <p>GlobalTempViewManager when the database name of the table matches the name of <code>GlobalTempViewManager</code></p> <ul> <li>Gives <code>SubqueryAlias</code> or reports a <code>NoSuchTableException</code></li> </ul> </li> <li> <p>ExternalCatalog when the database name of the table is specified explicitly or the registry of temporary views does not contain the table</p> <ul> <li>Gives <code>SubqueryAlias</code> with <code>View</code> when the table is a view (aka temporary table)</li> <li>Gives <code>SubqueryAlias</code> with <code>UnresolvedCatalogRelation</code> otherwise</li> </ul> </li> <li> <p>The registry of temporary views</p> <ul> <li>Gives <code>SubqueryAlias</code> with the logical plan per the table as registered in the registry of temporary views</li> </ul> </li> </ol>  <p>Note</p> <p><code>lookupRelation</code> considers default to be the name of the database if the <code>name</code> table does not specify the database explicitly.</p>","text":""},{"location":"SessionCatalog/#demo","title":"Demo <pre><code>scala&gt; :type spark.sessionState.catalog\norg.apache.spark.sql.catalyst.catalog.SessionCatalog\n\nimport spark.sessionState.{catalog =&gt; c}\nimport org.apache.spark.sql.catalyst.TableIdentifier\n\n// Global temp view\nval db = spark.sharedState.globalTempViewManager.database\n// Make the example reproducible (and so \"replace\")\nspark.range(1).createOrReplaceGlobalTempView(\"gv1\")\nval gv1 = TableIdentifier(table = \"gv1\", database = Some(db))\nval plan = c.lookupRelation(gv1)\nscala&gt; println(plan.numberedTreeString)\n00 SubqueryAlias gv1\n01 +- Range (0, 1, step=1, splits=Some(8))\n\nval metastore = spark.sharedState.externalCatalog\n\n// Regular table\nval db = spark.catalog.currentDatabase\nmetastore.dropTable(db, table = \"t1\", ignoreIfNotExists = true, purge = true)\nsql(\"CREATE TABLE t1 (id LONG) USING parquet\")\nval t1 = TableIdentifier(table = \"t1\", database = Some(db))\nval plan = c.lookupRelation(t1)\nscala&gt; println(plan.numberedTreeString)\n00 'SubqueryAlias t1\n01 +- 'UnresolvedCatalogRelation `default`.`t1`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n\n// Regular view (not temporary view!)\n// Make the example reproducible\nmetastore.dropTable(db, table = \"v1\", ignoreIfNotExists = true, purge = true)\nimport org.apache.spark.sql.catalyst.catalog.{CatalogStorageFormat, CatalogTable, CatalogTableType}\nval v1 = TableIdentifier(table = \"v1\", database = Some(db))\nimport org.apache.spark.sql.types.StructType\nval schema = new StructType().add($\"id\".long)\nval storage = CatalogStorageFormat(locationUri = None, inputFormat = None, outputFormat = None, serde = None, compressed = false, properties = Map())\nval tableDef = CatalogTable(\n  identifier = v1,\n  tableType = CatalogTableType.VIEW,\n  storage,\n  schema,\n  viewText = Some(\"SELECT 1\") /** Required or RuntimeException reported */)\nmetastore.createTable(tableDef, ignoreIfExists = false)\nval plan = c.lookupRelation(v1)\nscala&gt; println(plan.numberedTreeString)\n00 'SubqueryAlias v1\n01 +- View (`default`.`v1`, [id#77L])\n02    +- 'Project [unresolvedalias(1, None)]\n03       +- OneRowRelation\n\n// Temporary view\nspark.range(1).createOrReplaceTempView(\"v2\")\nval v2 = TableIdentifier(table = \"v2\", database = None)\nval plan = c.lookupRelation(v2)\nscala&gt; println(plan.numberedTreeString)\n00 SubqueryAlias v2\n01 +- Range (0, 1, step=1, splits=Some(8))\n</code></pre>","text":""},{"location":"SessionCatalog/#retrieving-table-metadata","title":"Retrieving Table Metadata <pre><code>getTempViewOrPermanentTableMetadata(\n  name: TableIdentifier): CatalogTable\n</code></pre> <p><code>getTempViewOrPermanentTableMetadata</code> branches off based on the database (in the given <code>TableIdentifier</code>).</p> <p>When a database name is not specified, <code>getTempViewOrPermanentTableMetadata</code> finds a local temporary view and creates a CatalogTable (with <code>VIEW</code> table type and an undefined storage) or retrieves the table metadata from an external catalog.</p> <p>With the database name of the GlobalTempViewManager, <code>getTempViewOrPermanentTableMetadata</code> requests GlobalTempViewManager for the global view definition and creates a CatalogTable (with the name of <code>GlobalTempViewManager</code> in table identifier, <code>VIEW</code> table type and an undefined storage) or reports a <code>NoSuchTableException</code>.</p> <p>With the database name not of <code>GlobalTempViewManager</code>, <code>getTempViewOrPermanentTableMetadata</code> simply retrieves the table metadata from an external catalog.</p>","text":""},{"location":"SessionCatalog/#lookupfunctioninfo","title":"lookupFunctionInfo <pre><code>lookupFunctionInfo(\n  name: FunctionIdentifier): ExpressionInfo\n</code></pre> <p><code>lookupFunctionInfo</code>...FIXME</p>  <p><code>lookupFunctionInfo</code> is used when:</p> <ul> <li><code>SparkGetFunctionsOperation</code> (Spark Thrift Server) is requested to <code>runInternal</code></li> <li><code>CatalogImpl</code> is requested to makeFunction</li> </ul>","text":""},{"location":"SessionCatalog/#lookupbuiltinortempfunction","title":"lookupBuiltinOrTempFunction <pre><code>lookupBuiltinOrTempFunction(\n  name: String): Option[ExpressionInfo]\n</code></pre> <p><code>lookupBuiltinOrTempFunction</code>...FIXME</p>  <p><code>lookupBuiltinOrTempFunction</code> is used when:</p> <ul> <li><code>ResolveFunctions</code> logical analysis is requested to lookupBuiltinOrTempFunction</li> <li><code>SessionCatalog</code> is requested to lookupFunctionInfo</li> </ul>","text":""},{"location":"SessionCatalog/#lookupbuiltinortemptablefunction","title":"lookupBuiltinOrTempTableFunction <pre><code>lookupBuiltinOrTempTableFunction(\n  name: String): Option[ExpressionInfo]\n</code></pre> <p><code>lookupBuiltinOrTempTableFunction</code>...FIXME</p>  <p><code>lookupBuiltinOrTempTableFunction</code> is used when:</p> <ul> <li><code>ResolveFunctions</code> logical analysis is requested to lookupBuiltinOrTempTableFunction</li> <li><code>SessionCatalog</code> is requested to lookupFunctionInfo</li> </ul>","text":""},{"location":"SessionCatalog/#lookuppersistentfunction","title":"lookupPersistentFunction <pre><code>lookupPersistentFunction(\n  name: FunctionIdentifier): ExpressionInfo\n</code></pre> <p><code>lookupPersistentFunction</code>...FIXME</p>  <p><code>lookupPersistentFunction</code> is used when:</p> <ul> <li><code>SessionCatalog</code> is requested to lookupFunctionInfo</li> <li><code>V2SessionCatalog</code> is requested to load a function</li> </ul>","text":""},{"location":"SessionCatalog/#resolvebuiltinortemptablefunction","title":"resolveBuiltinOrTempTableFunction <pre><code>resolveBuiltinOrTempTableFunction(\n  name: String,\n  arguments: Seq[Expression]): Option[LogicalPlan]\n</code></pre> <p><code>resolveBuiltinOrTempTableFunction</code> resolveBuiltinOrTempFunctionInternal with the TableFunctionRegistry.</p>  <p><code>resolveBuiltinOrTempTableFunction</code> is used when:</p> <ul> <li>ResolveFunctions logical analysis rule is executed (to resolve a UnresolvedTableValuedFunction logical operator)</li> <li><code>SessionCatalog</code> is requested to lookupTableFunction</li> </ul>","text":""},{"location":"SessionCatalog/#resolvebuiltinortempfunctioninternal","title":"resolveBuiltinOrTempFunctionInternal <pre><code>resolveBuiltinOrTempFunctionInternal[T](\n  name: String,\n  arguments: Seq[Expression],\n  isBuiltin: FunctionIdentifier =&gt; Boolean,\n  registry: FunctionRegistryBase[T]): Option[T]\n</code></pre> <p><code>resolveBuiltinOrTempFunctionInternal</code> creates a <code>FunctionIdentifier</code> (for the given <code>name</code>).</p> <p><code>resolveBuiltinOrTempFunctionInternal</code>...FIXME</p>  <p>Note</p> <p><code>resolveBuiltinOrTempFunctionInternal</code> is fairly simple yet I got confused what it does actually so I marked it <code>FIXME</code>.</p>   <p><code>resolveBuiltinOrTempFunctionInternal</code> is used when:</p> <ul> <li><code>SessionCatalog</code> is requested to resolveBuiltinOrTempFunction, resolveBuiltinOrTempTableFunction</li> </ul>","text":""},{"location":"SessionCatalog/#registerfunction","title":"registerFunction <pre><code>registerFunction(\n  funcDefinition: CatalogFunction,\n  overrideIfExists: Boolean,\n  functionBuilder: Option[Seq[Expression] =&gt; Expression] = None): Unit\nregisterFunction[T](\n  funcDefinition: CatalogFunction,\n  overrideIfExists: Boolean,\n  registry: FunctionRegistryBase[T],\n  functionBuilder: Seq[Expression] =&gt; T): Unit\n</code></pre> <p><code>registerFunction</code>...FIXME</p>  <p><code>registerFunction</code> is used when:</p> <ul> <li><code>CreateFunctionCommand</code> is executed</li> <li><code>RefreshFunctionCommand</code> is executed</li> <li><code>SessionCatalog</code> is requested to resolvePersistentFunctionInternal</li> </ul>","text":""},{"location":"SessionState/","title":"SessionState \u2014 State Separation Layer Between SparkSessions","text":"<p><code>SessionState</code> is a state separation layer between Spark SQL sessions, including SQL configuration, tables, functions, UDFs, SQL parser, and everything else that depends on a SQLConf.</p>"},{"location":"SessionState/#attributes","title":"Attributes","text":""},{"location":"SessionState/#columnarrules","title":"ColumnarRules <pre><code>columnarRules: Seq[ColumnarRule]\n</code></pre> <p>ColumnarRules</p>","text":""},{"location":"SessionState/#executionlistenermanager","title":"ExecutionListenerManager <pre><code>listenerManager: ExecutionListenerManager\n</code></pre> <p>ExecutionListenerManager</p>","text":""},{"location":"SessionState/#experimentalmethods","title":"ExperimentalMethods <pre><code>experimentalMethods: ExperimentalMethods\n</code></pre> <p>ExperimentalMethods</p>","text":""},{"location":"SessionState/#functionregistry","title":"FunctionRegistry <pre><code>functionRegistry: FunctionRegistry\n</code></pre> <p>FunctionRegistry</p>","text":""},{"location":"SessionState/#logical-analyzer","title":"Logical Analyzer <pre><code>analyzer: Analyzer\n</code></pre> <p>Analyzer</p> <p>Initialized lazily (only when requested the first time) using the analyzerBuilder factory function.</p>","text":""},{"location":"SessionState/#logical-optimizer","title":"Logical Optimizer <pre><code>optimizer: Optimizer\n</code></pre> <p>Logical Optimizer that is created using the optimizerBuilder function (and cached for later usage)</p> <p>Used when:</p> <ul> <li><code>QueryExecution</code> is requested to create an optimized logical plan</li> <li>(Structured Streaming) <code>IncrementalExecution</code> is requested to create an optimized logical plan</li> </ul>","text":""},{"location":"SessionState/#parserinterface","title":"ParserInterface <pre><code>sqlParser: ParserInterface\n</code></pre> <p>ParserInterface</p>","text":""},{"location":"SessionState/#sessioncatalog","title":"SessionCatalog <pre><code>catalog: SessionCatalog\n</code></pre> <p>SessionCatalog that is created using the catalogBuilder function (and cached for later usage).</p>","text":""},{"location":"SessionState/#sessionresourceloader","title":"SessionResourceLoader <pre><code>resourceLoader: SessionResourceLoader\n</code></pre>","text":""},{"location":"SessionState/#spark-query-planner","title":"Spark Query Planner <pre><code>planner: SparkPlanner\n</code></pre> <p>SparkPlanner</p>","text":""},{"location":"SessionState/#sqlconf","title":"SQLConf <pre><code>conf: SQLConf\n</code></pre> <p>SQLConf</p>","text":""},{"location":"SessionState/#streamingquerymanager","title":"StreamingQueryManager <pre><code>streamingQueryManager: StreamingQueryManager\n</code></pre>","text":""},{"location":"SessionState/#udfregistration","title":"UDFRegistration <pre><code>udfRegistration: UDFRegistration\n</code></pre> <p><code>SessionState</code> is given an UDFRegistration when created.</p>","text":""},{"location":"SessionState/#aqe-querystage-physical-preparation-rules","title":"AQE QueryStage Physical Preparation Rules <pre><code>queryStagePrepRules: Seq[Rule[SparkPlan]]\n</code></pre> <p><code>SessionState</code> can be given a collection of physical optimizations (<code>Rule[SparkPlan]</code>s) when created.</p> <p><code>queryStagePrepRules</code> is given when <code>BaseSessionStateBuilder</code> is requested to build a SessionState based on queryStagePrepRules (from a SparkSessionExtensions).</p> <p><code>queryStagePrepRules</code> is used to extend the built-in QueryStage Physical Preparation Rules in Adaptive Query Execution.</p>","text":""},{"location":"SessionState/#creating-instance","title":"Creating Instance","text":"<p><code>SessionState</code> takes the following to be created:</p> <ul> <li> SharedState <li>SQLConf</li> <li>ExperimentalMethods</li> <li>FunctionRegistry</li> <li>UDFRegistration</li> <li> Function to build a SessionCatalog (<code>() =&gt; SessionCatalog</code>) <li>ParserInterface</li> <li> Function to build a Analyzer (<code>() =&gt; Analyzer</code>) <li> Function to build a Logical Optimizer (<code>() =&gt; Optimizer</code>) <li>SparkPlanner</li> <li> Function to build a <code>StreamingQueryManager</code> (<code>() =&gt; StreamingQueryManager</code>) <li>ExecutionListenerManager</li> <li> Function to build a <code>SessionResourceLoader</code> (<code>() =&gt; SessionResourceLoader</code>) <li> Function to build a QueryExecution (<code>LogicalPlan =&gt; QueryExecution</code>) <li> <code>SessionState</code> Clone Function (<code>(SparkSession, SessionState) =&gt; SessionState</code>) <li>ColumnarRules</li> <li>AQE QueryStage Preparation Rules</li> <p><code>SessionState</code> is created when:</p> <ul> <li><code>SparkSession</code> is requested to instantiateSessionState (when requested for the SessionState per spark.sql.catalogImplementation configuration property)</li> </ul> <p></p> <p>When requested for the SessionState, <code>SparkSession</code> uses spark.sql.catalogImplementation configuration property to load and create a BaseSessionStateBuilder that is then requested to create a SessionState instance.</p> <p>There are two <code>BaseSessionStateBuilders</code> available:</p> <ul> <li>(default) SessionStateBuilder for <code>in-memory</code> catalog</li> <li>HiveSessionStateBuilder for <code>hive</code> catalog</li> </ul> <p><code>hive</code> catalog is set when the <code>SparkSession</code> was created with the Hive support enabled (using Builder.enableHiveSupport).</p>"},{"location":"SessionState/#creating-queryexecution-for-logicalplan","title":"Creating QueryExecution For LogicalPlan <pre><code>executePlan(\n  plan: LogicalPlan): QueryExecution\n</code></pre> <p><code>executePlan</code> uses the createQueryExecution function to create a QueryExecution for the given LogicalPlan.</p>","text":""},{"location":"SessionState/#creating-new-hadoop-configuration","title":"Creating New Hadoop Configuration <pre><code>newHadoopConf(): Configuration\n</code></pre> <p><code>newHadoopConf</code> returns a new Hadoop Configuration (with the <code>SparkContext.hadoopConfiguration</code> and all the configuration properties of the SQLConf).</p>","text":""},{"location":"SessionState/#creating-new-hadoop-configuration-with-extra-options","title":"Creating New Hadoop Configuration With Extra Options <pre><code>newHadoopConfWithOptions(\n  options: Map[String, String]): Configuration\n</code></pre> <p><code>newHadoopConfWithOptions</code> creates a new Hadoop Configuration with the input <code>options</code> set (except <code>path</code> and <code>paths</code> options that are skipped).</p> <p><code>newHadoopConfWithOptions</code> is used when:</p> <ul> <li><code>TextBasedFileFormat</code> is requested to <code>isSplitable</code></li> <li><code>FileSourceScanExec</code> physical operator is requested for the input RDD</li> <li>InsertIntoHadoopFsRelationCommand logical command is executed</li> <li><code>PartitioningAwareFileIndex</code> is requested for the Hadoop Configuration</li> </ul>","text":""},{"location":"SessionState/#accessing-sessionstate","title":"Accessing SessionState <p><code>SessionState</code> is available using SparkSession.sessionState.</p> <pre><code>import org.apache.spark.sql.SparkSession\nassert(spark.isInstanceOf[SparkSession])\n</code></pre> <pre><code>// object SessionState in package org.apache.spark.sql.internal cannot be accessed directly\nscala&gt; :type spark.sessionState\norg.apache.spark.sql.internal.SessionState\n</code></pre>","text":""},{"location":"SessionStateBuilder/","title":"SessionStateBuilder","text":"<p><code>SessionStateBuilder</code> is...FIXME</p>"},{"location":"SharedState/","title":"SharedState \u2014 State Shared Across SparkSessions","text":"<p><code>SharedState</code> holds the state that can be shared across SparkSessions:</p> <ul> <li> CacheManager <li>ExternalCatalogWithListener</li> <li>GlobalTempViewManager</li> <li>Hadoop Configuration</li> <li> <code>NonClosableMutableURLClassLoader</code> <li>SparkConf</li> <li>SparkContext</li> <li>SQLAppStatusStore</li> <li><code>StreamingQueryStatusListener</code></li> <p><code>SharedState</code> is shared when <code>SparkSession</code> is created using SparkSession.newSession:</p> <pre><code>assert(spark.sharedState == spark.newSession.sharedState)\n</code></pre>"},{"location":"SharedState/#creating-instance","title":"Creating Instance","text":"<p><code>SharedState</code> takes the following to be created:</p> <ul> <li> <code>SparkContext</code> <li> Initial configuration properties <p><code>SharedState</code> is created for SparkSession (and cached for later reuse).</p>"},{"location":"SharedState/#accessing-sharedstate","title":"Accessing SharedState","text":"<p><code>SharedState</code> is available using SparkSession.sharedState.</p> <pre><code>scala&gt; :type spark\norg.apache.spark.sql.SparkSession\n\nscala&gt; :type spark.sharedState\norg.apache.spark.sql.internal.SharedState\n</code></pre>"},{"location":"SharedState/#shared-sql-services","title":"Shared SQL Services","text":""},{"location":"SharedState/#externalcatalog","title":"ExternalCatalog <pre><code>externalCatalog: ExternalCatalog\n</code></pre> <p>ExternalCatalog that is created reflectively based on spark.sql.catalogImplementation internal configuration property:</p> <ul> <li>HiveExternalCatalog for <code>hive</code></li> <li>InMemoryCatalog for <code>in-memory</code></li> </ul> <p>While initialized:</p> <ol> <li> <p>Creates the default database (with <code>default database</code> description and warehousePath location) unless available already.</p> </li> <li> <p>Registers a <code>ExternalCatalogEventListener</code> that propagates external catalog events to the Spark listener bus.</p> </li> </ol>","text":""},{"location":"SharedState/#globaltempviewmanager","title":"GlobalTempViewManager <pre><code>globalTempViewManager: GlobalTempViewManager\n</code></pre> <p>GlobalTempViewManager</p> <p>When accessed for the very first time, <code>globalTempViewManager</code> gets the name of the global temporary view database based on spark.sql.globalTempDatabase internal static configuration property.</p> <p>In the end, <code>globalTempViewManager</code> creates a new GlobalTempViewManager (with the configured database name).</p> <p><code>globalTempViewManager</code> throws a <code>SparkException</code> when the global temporary view database exist in the ExternalCatalog:</p> <pre><code>[globalTempDB] is a system preserved database, please rename your existing database to resolve the name conflict, or set a different value for spark.sql.globalTempDatabase, and launch your Spark application again.\n</code></pre> <p><code>globalTempViewManager</code> is used when BaseSessionStateBuilder and HiveSessionStateBuilder are requested for a SessionCatalog.</p>","text":""},{"location":"SharedState/#sqlappstatusstore","title":"SQLAppStatusStore <pre><code>statusStore: SQLAppStatusStore\n</code></pre> <p><code>SharedState</code> creates a SQLAppStatusStore when created.</p> <p>When initialized, <code>statusStore</code> requests the SparkContext for <code>AppStatusStore</code> that is then requested for the <code>KVStore</code> (which is assumed a <code>ElementTrackingStore</code>).</p> <p><code>statusStore</code> creates a SQLAppStatusListener (with the <code>live</code> flag on) and registers it with the <code>LiveListenerBus</code> to application status queue.</p> <p><code>statusStore</code> creates a SQLAppStatusStore (with the <code>KVStore</code> and the <code>SQLAppStatusListener</code>).</p> <p>In the end, <code>statusStore</code> creates a SQLTab (with the <code>SQLAppStatusStore</code> and the <code>SparkUI</code> if available).</p>","text":""},{"location":"SharedState/#externalcatalogclassname-internal-method","title":"externalCatalogClassName Internal Method <pre><code>externalCatalogClassName(\n  conf: SparkConf): String\n</code></pre> <p><code>externalCatalogClassName</code> gives the name of the class of the ExternalCatalog implementation based on spark.sql.catalogImplementation configuration property:</p> <ul> <li>org.apache.spark.sql.hive.HiveExternalCatalog for <code>hive</code></li> <li>org.apache.spark.sql.catalyst.catalog.InMemoryCatalog for <code>in-memory</code></li> </ul> <p><code>externalCatalogClassName</code> is used when <code>SharedState</code> is requested for the ExternalCatalog.</p>","text":""},{"location":"SharedState/#warehouse-location","title":"Warehouse Location <pre><code>warehousePath: String\n</code></pre>  <p>Warning</p> <p>This is no longer part of SharedState and will go away once I find out where. Your help is appreciated.</p>  <p><code>warehousePath</code> is the location of the warehouse.</p> <p><code>warehousePath</code> is hive.metastore.warehouse.dir (if defined) or spark.sql.warehouse.dir.</p> <p><code>warehousePath</code> prints out the following INFO message to the logs when <code>SharedState</code> is created:</p> <pre><code>Warehouse path is '[warehousePath]'.\n</code></pre> <p><code>warehousePath</code> is used when <code>SharedState</code> initializes ExternalCatalog (and creates the default database in the metastore).</p> <p>While initialized, <code>warehousePath</code> does the following:</p> <ol> <li> <p>Loads <code>hive-site.xml</code> when found on CLASSPATH, i.e. adds it as a configuration resource to Hadoop's http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration] (of <code>SparkContext</code>).</p> </li> <li> <p>Removes <code>hive.metastore.warehouse.dir</code> from <code>SparkConf</code> (of <code>SparkContext</code>) and leaves it off if defined using any of the Hadoop configuration resources.</p> </li> <li> <p>Sets spark.sql.warehouse.dir or hive.metastore.warehouse.dir in the Hadoop configuration (of <code>SparkContext</code>)</p> <ul> <li>If <code>hive.metastore.warehouse.dir</code> has been defined in any of the Hadoop configuration resources but spark.sql.warehouse.dir has not, <code>spark.sql.warehouse.dir</code> becomes the value of <code>hive.metastore.warehouse.dir</code>.</li> </ul> <p><code>warehousePath</code> prints out the following INFO message to the logs:</p> <pre><code>spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('[hiveWarehouseDir]').\n</code></pre> <ul> <li>Otherwise, the Hadoop configuration's <code>hive.metastore.warehouse.dir</code> is set to <code>spark.sql.warehouse.dir</code></li> </ul> <p><code>warehousePath</code> prints out the following INFO message to the logs:</p> <pre><code>Setting hive.metastore.warehouse.dir ('[hiveWarehouseDir]') to the value of spark.sql.warehouse.dir ('[sparkWarehouseDir]').\n</code></pre> </li> </ol>","text":""},{"location":"SharedState/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.internal.SharedState</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.internal.SharedState=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"ShuffledRowRDD/","title":"ShuffledRowRDD","text":"<p><code>ShuffledRowRDD</code> is an <code>RDD</code> (Spark Core) of InternalRows (<code>RDD[InternalRow]</code>) used for execution of the following physical operators:</p> <ul> <li>AQEShuffleReadExec (Adaptive Query Execution)</li> <li>CollectLimitExec</li> <li>ShuffleExchangeExec</li> <li><code>TakeOrderedAndProjectExec</code></li> </ul>"},{"location":"ShuffledRowRDD/#shuffledrdd","title":"ShuffledRDD <p><code>ShuffledRowRDD</code> is similar to <code>ShuffledRDD</code> (Spark Core), with the difference of the type of the values to process, i.e. InternalRow and <code>(K, C)</code> key-value pairs, respectively.</p>","text":""},{"location":"ShuffledRowRDD/#creating-instance","title":"Creating Instance <p><code>ShuffledRowRDD</code> takes the following to be created:</p> <ul> <li> <code>ShuffleDependency[Int, InternalRow, InternalRow]</code> (Spark Core) <li>Write Metrics</li> <li>ShufflePartitionSpecs (default: <code>CoalescedPartitionSpec</code>s)</li>  <p>When created, <code>ShuffledRowRDD</code> uses the spark.sql.adaptive.fetchShuffleBlocksInBatch configuration property to set the __fetch_continuous_blocks_in_batch_enabled local property to <code>true</code>.</p> <p><code>ShuffledRowRDD</code> is created when:</p> <ul> <li>CollectLimitExec, ShuffleExchangeExec and <code>TakeOrderedAndProjectExec</code> physical operators are executed</li> <li><code>ShuffleExchangeExec</code> is requested for a shuffle RDD (for AQEShuffleReadExec)</li> </ul>","text":""},{"location":"ShuffledRowRDD/#write-metrics","title":"Write Metrics <pre><code>metrics: Map[String, SQLMetric]\n</code></pre> <p><code>ShuffledRowRDD</code> is given a collection of SQLMetrics by name.</p> <p><code>metrics</code> is used to create a <code>SQLShuffleReadMetricsReporter</code> while computing a partition (to create a <code>ShuffleReader</code> (Spark Core)).</p>  <p>Note</p> <p><code>SQLShuffleReadMetricsReporter</code> is a <code>ShuffleReadMetricsReporter</code> (Spark Core).</p>","text":""},{"location":"ShuffledRowRDD/#computing-partition","title":"Computing Partition <pre><code>compute(\n  split: Partition,\n  context: TaskContext): Iterator[InternalRow]\n</code></pre> <p><code>compute</code> requests the given <code>TaskContext</code> (Spark Core) for the <code>TaskMetrics</code> (Spark Core) that is in turn requested for a <code>TempShuffleReadMetrics</code>.</p> <p><code>compute</code> creates a <code>SQLShuffleReadMetricsReporter</code> (with the <code>TempShuffleReadMetrics</code> and the SQL Metrics).</p> <p><code>compute</code> assumes that the given <code>Partition</code> (Spark Core) is a <code>ShuffledRowRDDPartition</code> and requests it for the <code>ShufflePartitionSpec</code>.</p> <p><code>compute</code> requests the <code>ShuffleManager</code> (Spark Core) for a <code>ShuffleReader</code> (Spark Core) based on the type of <code>ShufflePartitionSpec</code>.</p>    ShufflePartitionSpec startPartition endPartition     CoalescedPartitionSpec startReducerIndex endReducerIndex       ShufflePartitionSpec startMapIndex endMapIndex startPartition endPartition     PartialReducerPartitionSpec startMapIndex endMapIndex reducerIndex reducerIndex + 1   PartialMapperPartitionSpec mapIndex mapIndex + 1 startReducerIndex endReducerIndex   CoalescedMapperPartitionSpec startMapIndex endMapIndex 0 numReducers    <p>In the end, <code>compute</code> requests the <code>ShuffleReader</code> to read combined records (<code>Iterator[Product2[Int, InternalRow]]</code>) and takes out <code>InternalRow</code> values only (and ignoring keys).</p> <p><code>compute</code> is part of <code>RDD</code> (Spark Core) abstraction.</p>","text":""},{"location":"ShuffledRowRDD/#partition-specs","title":"Partition Specs <p><code>ShuffledRowRDD</code> can be given a Partition Specs when created.</p> <p>When not given, it is assumed to use as many <code>CoalescedPartitionSpec</code>s as the number of partitions of ShuffleDependency (based on the <code>Partitioner</code>).</p>","text":""},{"location":"ShuffledRowRDD/#rdd-dependencies","title":"RDD Dependencies <pre><code>getDependencies: Seq[Dependency[_]]\n</code></pre> <p><code>getDependencies</code> is part of <code>RDD</code> (Spark Core) abstraction.</p> <p>A single-element collection with <code>ShuffleDependency[Int, InternalRow, InternalRow]</code>.</p>","text":""},{"location":"ShuffledRowRDD/#partitioner","title":"Partitioner <pre><code>partitioner: Option[Partitioner]\n</code></pre> <p><code>partitioner</code> is part of <code>RDD</code> (Spark Core) abstraction.</p> <p><code>partitioner</code> is <code>CoalescedPartitioner</code> when the following all hold:</p> <ol> <li>Partition Specs are all <code>CoalescedPartitionSpec</code></li> <li>The <code>startReducerIndex</code>s of the <code>CoalescedPartitionSpec</code>s are all unique</li> </ol> <p>Otherwise, <code>partitioner</code> is undefined (<code>None</code>).</p>","text":""},{"location":"ShuffledRowRDD/#partitions","title":"Partitions <pre><code>getPartitions: Array[Partition]\n</code></pre> <p><code>getPartitions</code> is part of <code>RDD</code> (Spark Core) abstraction.</p> <p><code>getPartitions</code>...FIXME</p>","text":""},{"location":"ShuffledRowRDD/#preferred-locations-of-partition","title":"Preferred Locations of Partition <pre><code>getPreferredLocations(\n  partition: Partition): Seq[String]\n</code></pre> <p><code>getPreferredLocations</code> is part of <code>RDD</code> (Spark Core) abstraction.</p> <p><code>getPreferredLocations</code>...FIXME</p>","text":""},{"location":"ShuffledRowRDD/#clearing-dependencies","title":"Clearing Dependencies <pre><code>clearDependencies(): Unit\n</code></pre> <p><code>clearDependencies</code> is part of <code>RDD</code> (Spark Core) abstraction.</p> <p><code>clearDependencies</code> simply requests the parent RDD to <code>clearDependencies</code> followed by clear the given dependency (i.e. set to <code>null</code>).</p>","text":""},{"location":"SimpleFunctionRegistry/","title":"SimpleFunctionRegistry","text":"<p><code>SimpleFunctionRegistry</code> is...FIXME</p>"},{"location":"SimpleFunctionRegistryBase/","title":"SimpleFunctionRegistryBase","text":"<p><code>SimpleFunctionRegistryBase[T]</code> is an extension of the FunctionRegistryBase abstraction for function registries (of type <code>T</code>).</p>"},{"location":"SimpleFunctionRegistryBase/#implementations","title":"Implementations","text":"<ul> <li>SimpleFunctionRegistry (for scalar functions using Expressions)</li> <li>SimpleTableFunctionRegistry (for table-valued functions using LogicalPlans)</li> </ul>"},{"location":"SimpleFunctionRegistryBase/#functionbuilders","title":"functionBuilders <pre><code>functionBuilders: HashMap[FunctionIdentifier, (ExpressionInfo, FunctionBuilder)]\n</code></pre> <p><code>SimpleFunctionRegistryBase</code> defines <code>functionBuilders</code> registry of named user-defined functions.</p> <p>A new user-defined function is registered in internalRegisterFunction.</p> <p><code>functionBuilders</code> is used when:</p> <ul> <li>lookupFunction</li> <li>listFunction</li> <li>lookupFunctionBuilder</li> <li>dropFunction</li> <li>clear</li> <li>clone</li> </ul>","text":""},{"location":"SimpleFunctionRegistryBase/#lookupfunction","title":"lookupFunction <pre><code>lookupFunction(\n  name: FunctionIdentifier): Option[ExpressionInfo]\n</code></pre> <p><code>lookupFunction</code> is part of the FunctionRegistryBase abstraction.</p>  <p><code>lookupFunction</code> finds the given <code>name</code> in the functionBuilders registry.</p>","text":""},{"location":"SimpleFunctionRegistryBase/#registering-named-user-defined-function","title":"Registering Named User-Defined Function <pre><code>registerFunction(\n  name: FunctionIdentifier,\n  info: ExpressionInfo,\n  builder: FunctionBuilder): Unit\n</code></pre> <p><code>registerFunction</code> is part of the FunctionRegistryBase abstraction.</p>  <p><code>registerFunction</code> internalRegisterFunction with a normalized (lower-case) name.</p>","text":""},{"location":"SimpleFunctionRegistryBase/#internalregisterfunction","title":"internalRegisterFunction <pre><code>internalRegisterFunction(\n  name: FunctionIdentifier,\n  info: ExpressionInfo,\n  builder: FunctionBuilder): Unit\n</code></pre> <p><code>internalRegisterFunction</code> adds a new function to the functionBuilders registry.</p>  <p><code>internalRegisterFunction</code> is used when;</p> <ul> <li><code>SimpleFunctionRegistryBase</code> is requested to registerFunction</li> <li><code>FunctionRegistry</code> utility is used to create a SimpleFunctionRegistry with built-in functions</li> <li><code>TableFunctionRegistry</code> utility is used to create a SimpleTableFunctionRegistry with built-in functions</li> </ul>","text":""},{"location":"SimpleTableFunctionRegistry/","title":"SimpleTableFunctionRegistry","text":"<p><code>SimpleTableFunctionRegistry</code> is a TableFunctionRegistry and SimpleFunctionRegistryBase (of LogicalPlans).</p>"},{"location":"SimpleTableFunctionRegistry/#creating-instance","title":"Creating Instance","text":"<p><code>SimpleTableFunctionRegistry</code> takes no arguments to be created.</p> <p><code>SimpleTableFunctionRegistry</code> is created when:</p> <ul> <li><code>TableFunctionRegistry</code> is requested for the builtin TableFunctionRegistry</li> </ul>"},{"location":"SparkOptimizer/","title":"SparkOptimizer \u2014 Logical Query Plan Optimizer","text":"<p><code>SparkOptimizer</code> is a concrete logical query plan optimizer.</p> <p><code>SparkOptimizer</code> offers the following extension points for additional user-defined optimization rules:</p> <ul> <li> <p>Pre-Optimization Batches</p> </li> <li> <p>Post-Hoc Optimization Batches</p> </li> <li> <p>User Provided Optimizers (as extraOptimizations of the ExperimentalMethods)</p> </li> </ul>"},{"location":"SparkOptimizer/#creating-instance","title":"Creating Instance","text":"<p><code>SparkOptimizer</code> takes the following to be created:</p> <ul> <li> CatalogManager <li> SessionCatalog <li> ExperimentalMethods <p><code>SparkOptimizer</code> is created when <code>SessionState</code> is requested for a logical query plan optimizer (indirectly using <code>BaseSessionStateBuilder</code> is requested for an Optimizer).</p> <p></p>"},{"location":"SparkOptimizer/#earlyscanpushdownrules","title":"earlyScanPushDownRules  Signature <pre><code>earlyScanPushDownRules: Seq[Rule[LogicalPlan]]\n</code></pre> <p><code>earlyScanPushDownRules</code> is part of the Optimizer abstraction.</p>  <p><code>earlyScanPushDownRules</code> adds the following rules to the default nonExcludableRules:</p> <ul> <li><code>ExtractPythonUDFFromJoinCondition</code></li> <li><code>ExtractPythonUDFFromAggregate</code></li> <li><code>ExtractGroupingPythonUDFFromAggregate</code></li> <li><code>ExtractPythonUDFs</code></li> <li>GroupBasedRowLevelOperationScanPlanning</li> <li>V2ScanRelationPushDown</li> <li><code>V2ScanPartitioningAndOrdering</code></li> <li>V2Writes</li> <li><code>ReplaceCTERefWithRepartition</code></li> </ul>","text":""},{"location":"SparkOptimizer/#default-rule-batches","title":"Default Rule Batches <p><code>SparkOptimizer</code> overrides the optimization rules.</p>","text":""},{"location":"SparkOptimizer/#pre-optimization-batches-extension-point","title":"Pre-Optimization Batches (Extension Point) <pre><code>preOptimizationBatches: Seq[Batch]\n</code></pre> <p>Extension point for Pre-Optimization Batches that are executed first (before the regular optimization batches and the defaultBatches).</p>","text":""},{"location":"SparkOptimizer/#base-logical-optimization-batches","title":"Base Logical Optimization Batches <p>Optimization rules of the base Logical Optimizer</p>","text":""},{"location":"SparkOptimizer/#optimize-metadata-only-query","title":"Optimize Metadata Only Query <p>Rules:</p> <ul> <li>OptimizeMetadataOnlyQuery</li> </ul> <p>Strategy: <code>Once</code></p>","text":""},{"location":"SparkOptimizer/#partitionpruning","title":"PartitionPruning <p>Rules:</p> <ul> <li>PartitionPruning</li> <li>OptimizeSubqueries</li> </ul> <p>Strategy: <code>Once</code></p>","text":""},{"location":"SparkOptimizer/#pushdown-filters-from-partitionpruning","title":"Pushdown Filters from PartitionPruning <p>Rules:</p> <ul> <li>PushDownPredicates</li> </ul> <p>Strategy: fixedPoint</p>","text":""},{"location":"SparkOptimizer/#cleanup-filters-that-cannot-be-pushed-down","title":"Cleanup filters that cannot be pushed down <p>Rules:</p> <ul> <li>CleanupDynamicPruningFilters</li> <li><code>BooleanSimplification</code></li> <li>PruneFilters</li> </ul> <p>Strategy: <code>Once</code></p>","text":""},{"location":"SparkOptimizer/#post-hoc-optimization-batches-extension-point","title":"Post-Hoc Optimization Batches (Extension Point) <pre><code>postHocOptimizationBatches: Seq[Batch] = Nil\n</code></pre> <p>Extension point for Post-Hoc Optimization Batches</p>","text":""},{"location":"SparkOptimizer/#extract-python-udfs","title":"Extract Python UDFs <p>Rules:</p> <ul> <li><code>ExtractPythonUDFFromJoinCondition</code></li> <li><code>CheckCartesianProducts</code></li> <li>ExtractPythonUDFFromAggregate</li> <li><code>ExtractGroupingPythonUDFFromAggregate</code></li> <li><code>ExtractPythonUDFs</code></li> <li><code>ColumnPruning</code></li> <li><code>PushPredicateThroughNonJoin</code></li> <li><code>RemoveNoopOperators</code></li> </ul> <p>Strategy: <code>Once</code></p>","text":""},{"location":"SparkOptimizer/#user-provided-optimizers-extension-point","title":"User Provided Optimizers (Extension Point) <p>Extension point for Extra Optimization Rules using the given ExperimentalMethods</p> <p>Strategy: fixedPoint</p>","text":""},{"location":"SparkOptimizer/#non-excludable-rules","title":"Non-Excludable Rules <pre><code>nonExcludableRules: Seq[String]\n</code></pre> <p><code>nonExcludableRules</code> is part of the Optimizer abstraction.</p>  <p><code>nonExcludableRules</code> adds the following optimization rules to the existing nonExcludableRules:</p> <ul> <li><code>ExtractGroupingPythonUDFFromAggregate</code></li> <li><code>ExtractPythonUDFFromAggregate</code></li> <li><code>ExtractPythonUDFFromJoinCondition</code></li> <li><code>ExtractPythonUDFs</code></li> <li><code>GroupBasedRowLevelOperationScanPlanning</code></li> <li><code>ReplaceCTERefWithRepartition</code></li> <li><code>V2ScanPartitioning</code></li> <li>V2ScanRelationPushDown</li> <li>V2Writes</li> </ul>","text":""},{"location":"SparkOptimizer/#accessing-sparkoptimizer","title":"Accessing SparkOptimizer <p><code>SparkOptimizer</code> is available as the optimizer property of a session-specific <code>SessionState</code>.</p> <pre><code>scala&gt; :type spark\norg.apache.spark.sql.SparkSession\n\nscala&gt; :type spark.sessionState.optimizer\norg.apache.spark.sql.catalyst.optimizer.Optimizer\n\n// It is a SparkOptimizer really.\n// Let's check that out with a type cast\n\nimport org.apache.spark.sql.execution.SparkOptimizer\nscala&gt; spark.sessionState.optimizer.isInstanceOf[SparkOptimizer]\nres1: Boolean = true\n</code></pre> <p>The optimized logical plan of a structured query is available as QueryExecution.optimizedPlan.</p> <pre><code>// Applying two filter in sequence on purpose\n// We want to kick CombineTypedFilters optimizer in\nval dataset = spark.range(10).filter(_ % 2 == 0).filter(_ == 0)\n\n// optimizedPlan is a lazy value\n// Only at the first time you call it you will trigger optimizations\n// Next calls end up with the cached already-optimized result\n// Use explain to trigger optimizations again\nscala&gt; dataset.queryExecution.optimizedPlan\nres0: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =\nTypedFilter &lt;function1&gt;, class java.lang.Long, [StructField(value,LongType,true)], newInstance(class java.lang.Long)\n+- Range (0, 10, step=1, splits=Some(8))\n</code></pre>","text":""},{"location":"SparkOptimizer/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.SparkOptimizer</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.SparkOptimizer=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"SparkOptimizer/#further-reading-and-watching","title":"Further Reading and Watching <ol> <li> <p>Deep Dive into Spark SQL\u2019s Catalyst Optimizer</p> </li> <li> <p>(video) Modern Spark DataFrame and Dataset (Intermediate Tutorial) by Adam Breindel</p> </li> </ol>","text":""},{"location":"SparkPlanner/","title":"SparkPlanner \u2014 Spark Query Planner","text":"<p><code>SparkPlanner</code> is a concrete Catalyst Query Planner that converts a logical plan to one or more physical plans using execution planning strategies with support for extension points:</p> <ul> <li>extra strategies (by means of ExperimentalMethods)</li> <li>extraPlanningStrategies</li> </ul> <p><code>SparkPlanner</code> is expected to plan (create) at least one physical plan for a given logical plan.</p> <p><code>SparkPlanner</code> extends SparkStrategies abstract class.</p>"},{"location":"SparkPlanner/#execution-planning-strategies","title":"Execution Planning Strategies <ol> <li>extraStrategies of the ExperimentalMethods</li> <li>extraPlanningStrategies</li> <li>LogicalQueryStageStrategy</li> <li>PythonEvals</li> <li>DataSourceV2Strategy</li> <li>FileSourceStrategy</li> <li>DataSourceStrategy</li> <li>SpecialLimits</li> <li>Aggregation</li> <li>Window</li> <li>JoinSelection</li> <li>InMemoryScans</li> <li>SparkScripts</li> <li>WithCTEStrategy</li> <li>BasicOperators</li> </ol>","text":""},{"location":"SparkPlanner/#creating-instance","title":"Creating Instance <p><code>SparkPlanner</code> takes the following to be created:</p> <ul> <li> SparkSession <li> SQLConf <li> ExperimentalMethods  <p><code>SparkPlanner</code> is created when the following are requested for one:</p> <ul> <li>BaseSessionStateBuilder</li> <li>HiveSessionStateBuilder</li> <li>Structured Streaming's <code>IncrementalExecution</code></li> </ul>","text":""},{"location":"SparkPlanner/#accessing-sparkplanner","title":"Accessing SparkPlanner <p><code>SparkPlanner</code> is available as planner of a <code>SessionState</code>.</p> <pre><code>val spark: SparkSession = ...\nscala&gt; :type spark.sessionState.planner\norg.apache.spark.sql.execution.SparkPlanner\n</code></pre>","text":""},{"location":"SparkPlanner/#extra-planning-strategies","title":"Extra Planning Strategies <pre><code>extraPlanningStrategies: Seq[Strategy] = Nil\n</code></pre> <p><code>extraPlanningStrategies</code> is an extension point to register extra planning strategies.</p> <p><code>extraPlanningStrategies</code> is used when <code>SparkPlanner</code> is requested for planning strategies.</p> <p><code>extraPlanningStrategies</code> is overriden in the <code>SessionState</code> builders (BaseSessionStateBuilder and HiveSessionStateBuilder).</p>","text":""},{"location":"SparkPlanner/#collecting-planlater-physical-operators","title":"Collecting PlanLater Physical Operators <pre><code>collectPlaceholders(\n  plan: SparkPlan): Seq[(SparkPlan, LogicalPlan)]\n</code></pre> <p><code>collectPlaceholders</code> collects all PlanLater physical operators in the given physical plan.</p> <p><code>collectPlaceholders</code> is part of QueryPlanner abstraction.</p>","text":""},{"location":"SparkPlanner/#filter-and-project-pruning","title":"Filter and Project Pruning <pre><code>pruneFilterProject(\n  projectList: Seq[NamedExpression],\n  filterPredicates: Seq[Expression],\n  prunePushedDownFilters: Seq[Expression] =&gt; Seq[Expression],\n  scanBuilder: Seq[Attribute] =&gt; SparkPlan): SparkPlan\n</code></pre>  <p>Note</p> <p><code>pruneFilterProject</code> is almost like DataSourceStrategy.pruneFilterProjectRaw.</p>  <p><code>pruneFilterProject</code> branches off per whether it is possible to use a column pruning only (to get the right projection) and the input <code>projectList</code> columns of this projection are enough to evaluate all input <code>filterPredicates</code> filter conditions.</p> <p>If so, <code>pruneFilterProject</code> does the following:</p> <ol> <li> <p>Applies the input <code>scanBuilder</code> function to the input <code>projectList</code> columns that creates a new physical operator</p> </li> <li> <p>If there are Catalyst predicate expressions in the input <code>prunePushedDownFilters</code> that cannot be pushed down, <code>pruneFilterProject</code> creates a FilterExec unary physical operator (with the unhandled predicate expressions)</p> </li> <li> <p>Otherwise, <code>pruneFilterProject</code> simply returns the physical operator</p> </li> </ol> <p>In this case no extra ProjectExec unary physical operator is created.</p> <p>If not (i.e. it is neither possible to use a column pruning only nor evaluate filter conditions), <code>pruneFilterProject</code> does the following:</p> <ol> <li> <p>Applies the input <code>scanBuilder</code> function to the projection and filtering columns that creates a new physical operator</p> </li> <li> <p>Creates a FilterExec unary physical operator (with the unhandled predicate expressions if available)</p> </li> <li> <p>Creates a ProjectExec unary physical operator with the optional <code>FilterExec</code> operator (with the scan physical operator) or simply the scan physical operator alone</p> </li> </ol> <p><code>pruneFilterProject</code> is used when HiveTableScans and InMemoryScans execution planning strategies are executed.</p>","text":""},{"location":"SparkSession-Builder/","title":"SparkSession.Builder","text":"<p><code>SparkSession.Builder</code> is a builder interface to create SparkSessions.</p>"},{"location":"SparkSession-Builder/#accessing-builder","title":"Accessing Builder","text":"<p><code>Builder</code> is available using the SparkSession.builder factory method.</p> <pre><code>import org.apache.spark.sql.SparkSession\nval spark = SparkSession.builder\n.appName(\"My Spark Application\")  // optional and will be autogenerated if not specified\n.master(\"local[*]\")               // only for demo and testing purposes, use spark-submit instead\n.enableHiveSupport()              // self-explanatory, isn't it?\n.config(\"spark.sql.warehouse.dir\", \"target/spark-warehouse\")\n.withExtensions { extensions =&gt;\nextensions.injectResolutionRule { session =&gt;\n...\n}\nextensions.injectOptimizerRule { session =&gt;\n...\n}\n}\n.getOrCreate\n</code></pre>"},{"location":"SparkSession-Builder/#enabling-hive-support","title":"Enabling Hive Support <pre><code>enableHiveSupport(): Builder\n</code></pre> <p><code>enableHiveSupport</code> enables Hive support.</p>  <p>Note</p> <p>You do not need any existing Hive installation to use Spark's Hive support. <code>SparkSession</code> context will automatically create <code>metastore_db</code> in the current directory of a Spark application and the directory configured by spark.sql.warehouse.dir configuration property.</p> <p>Consult SharedState.</p>  <p>Internally, <code>enableHiveSupport</code> checks whether Hive classes are available or not. If so, <code>enableHiveSupport</code> sets spark.sql.catalogImplementation internal configuration property to <code>hive</code>. Otherwise, <code>enableHiveSupport</code> throws an <code>IllegalArgumentException</code>:</p> <pre><code>Unable to instantiate SparkSession with Hive support because Hive classes are not found.\n</code></pre>","text":""},{"location":"SparkSession-Builder/#getting-or-creating-sparksession-instance","title":"Getting Or Creating SparkSession Instance <pre><code>getOrCreate(): SparkSession\n</code></pre> <p><code>getOrCreate</code> gives the active SparkSession or creates a new one.</p> <p>While creating a new one, <code>getOrCreate</code> finds the SparkSession extensions (based on spark.sql.extensions configuration property) and applies them to the SparkSessionExtensions.</p>","text":""},{"location":"SparkSession-Builder/#sparksessionextensions","title":"SparkSessionExtensions <pre><code>extensions: SparkSessionExtensions\n</code></pre> <p><code>Builder</code> creates a new SparkSessionExtensions when created.</p> <p>The <code>SparkSessionExtensions</code> is used to apply SparkSession extensions registered using spark.sql.extensions configuration property or Builder.withExtensions method.</p> <p>In the end, <code>Builder</code> uses the <code>SparkSessionExtensions</code> to create a new SparkSession.</p>","text":""},{"location":"SparkSession-Builder/#registering-sparksessionextensions","title":"Registering SparkSessionExtensions <pre><code>withExtensions(\n  f: SparkSessionExtensions =&gt; Unit): Builder\n</code></pre> <p>Allows registering SparkSession extensions using SparkSessionExtensions.</p> <p><code>withExtensions</code> simply executes the input <code>f</code> function with a <code>SparkSessionExtensions</code>.</p>","text":""},{"location":"SparkSession-Builder/#hiveclassesarepresent","title":"hiveClassesArePresent <pre><code>hiveClassesArePresent: Boolean\n</code></pre> <p><code>hiveClassesArePresent</code> loads and initializes org.apache.spark.sql.hive.HiveSessionStateBuilder and <code>org.apache.hadoop.hive.conf.HiveConf</code> classes from the current classloader.</p> <p><code>hiveClassesArePresent</code> returns <code>true</code> when the initialization succeeded, and <code>false</code> otherwise (due to <code>ClassNotFoundException</code> or <code>NoClassDefFoundError</code> errors).</p> <p><code>hiveClassesArePresent</code> is used when:</p> <ul> <li> <p><code>Builder</code> is requested to enableHiveSupport</p> </li> <li> <p><code>spark-shell</code> is executed</p> </li> </ul>","text":""},{"location":"SparkSession/","title":"SparkSession \u2014 The Entry Point to Spark SQL","text":"<p><code>SparkSession</code> is the entry point to Spark SQL. It is one of the first objects created in a Spark SQL application.</p> <p><code>SparkSession</code> is created using the SparkSession.builder method.</p> <pre><code>import org.apache.spark.sql.SparkSession\nval spark = SparkSession.builder\n.appName(\"My Spark Application\")  // optional and will be autogenerated if not specified\n.master(\"local[*]\")               // only for demo and testing purposes, use spark-submit instead\n.enableHiveSupport()              // self-explanatory, isn't it?\n.config(\"spark.sql.warehouse.dir\", \"target/spark-warehouse\")\n.withExtensions { extensions =&gt;\nextensions.injectResolutionRule { session =&gt;\n...\n}\nextensions.injectOptimizerRule { session =&gt;\n...\n}\n}\n.getOrCreate\n</code></pre> <p><code>SparkSession</code> is a namespace of relational entities (e.g. databases, tables). A Spark SQL application could use many <code>SparkSessions</code> to keep the relational entities separate logically in metadata catalogs.</p> <p>SparkSession in spark-shell</p> <p><code>spark</code> object in <code>spark-shell</code> (the instance of <code>SparkSession</code> that is auto-created) has Hive support enabled.</p> <p>In order to disable the pre-configured Hive support in the <code>spark</code> object, use spark.sql.catalogImplementation internal configuration property with <code>in-memory</code> value (that uses InMemoryCatalog external catalog instead).</p> <pre><code>$ spark-shell --conf spark.sql.catalogImplementation=in-memory\n</code></pre>"},{"location":"SparkSession/#creating-instance","title":"Creating Instance","text":"<p><code>SparkSession</code> takes the following to be created:</p> <ul> <li> <code>SparkContext</code> <li> Existing SharedState (if given) <li> Parent SessionState (if given) <li> SparkSessionExtensions <p><code>SparkSession</code> is created when:</p> <ul> <li><code>SparkSession.Builder</code> is requested to getOrCreate</li> <li>Indirectly using newSession or cloneSession</li> </ul>"},{"location":"SparkSession/#sessionstate","title":"SessionState <pre><code>sessionState: SessionState\n</code></pre> <p><code>sessionState</code> is the current SessionState.</p> <p>Internally, <code>sessionState</code> &lt;&gt; the optional &lt;&gt; (if given when &lt;&gt;) or &lt;&gt; using &lt;&gt; as defined by &lt;&gt; configuration property: <ul> <li>in-memory (default) for SessionStateBuilder.md[org.apache.spark.sql.internal.SessionStateBuilder]</li> <li>hive for hive/HiveSessionStateBuilder.md[org.apache.spark.sql.hive.HiveSessionStateBuilder]</li> </ul>","text":""},{"location":"SparkSession/#creating-new-sparksession","title":"Creating New SparkSession <pre><code>newSession(): SparkSession\n</code></pre> <p><code>newSession</code> creates a new <code>SparkSession</code> with an undefined parent SessionState and (re)using the following:</p> <ul> <li>SparkContext</li> <li>SharedState</li> <li>SparkSessionExtensions</li> </ul>  <p>SparkSession.newSession and SparkSession.cloneSession</p> <p><code>SparkSession.newSession</code> uses no parent SessionState while SparkSession.cloneSession (re)uses SessionState.</p>","text":""},{"location":"SparkSession/#cloning-sparksession","title":"Cloning SparkSession <pre><code>cloneSession(): SparkSession\n</code></pre> <p><code>cloneSession</code>...FIXME</p> <p><code>cloneSession</code> is used when:</p> <ul> <li><code>AdaptiveSparkPlanHelper</code> is requested to <code>getOrCloneSessionWithAqeOff</code></li> <li><code>StreamExecution</code> (Spark Structured Streaming) is created</li> </ul>","text":""},{"location":"SparkSession/#creating-sparksession-using-builder-pattern","title":"Creating SparkSession Using Builder Pattern <pre><code>builder(): Builder\n</code></pre> <p><code>builder</code> is an object method that creates a new Builder to build a <code>SparkSession</code> using a fluent API.</p> <pre><code>import org.apache.spark.sql.SparkSession\nval builder = SparkSession.builder\n</code></pre> <p>TIP: Read about https://en.wikipedia.org/wiki/Fluent_interface[Fluent interface] design pattern in Wikipedia, the free encyclopedia.</p>","text":""},{"location":"SparkSession/#spark-version","title":"Spark Version <pre><code>version: String\n</code></pre> <p><code>version</code> returns the version of Apache Spark in use.</p> <p>Internally, <code>version</code> uses <code>spark.SPARK_VERSION</code> value that is the <code>version</code> property in <code>spark-version-info.properties</code> properties file on CLASSPATH.</p>","text":""},{"location":"SparkSession/#creating-empty-dataset-given-encoder","title":"Creating Empty Dataset (Given Encoder) <pre><code>emptyDataset[T: Encoder]: Dataset[T]\n</code></pre> <p><code>emptyDataset</code> creates an empty Dataset (assuming that future records being of type <code>T</code>).</p> <pre><code>scala&gt; val strings = spark.emptyDataset[String]\nstrings: org.apache.spark.sql.Dataset[String] = [value: string]\n\nscala&gt; strings.printSchema\nroot\n |-- value: string (nullable = true)\n</code></pre> <p><code>emptyDataset</code> creates a LocalRelation logical operator.</p>","text":""},{"location":"SparkSession/#creating-dataset-from-local-collections-or-rdds","title":"Creating Dataset from Local Collections or RDDs <pre><code>createDataset[T : Encoder](\n  data: RDD[T]): Dataset[T]\ncreateDataset[T : Encoder](\n  data: Seq[T]): Dataset[T]\n</code></pre> <p><code>createDataset</code> creates a Dataset from a local Scala collection, i.e. <code>Seq[T]</code>, Java's <code>List[T]</code>, or a distributed <code>RDD[T]</code>.</p> <pre><code>scala&gt; val one = spark.createDataset(Seq(1))\none: org.apache.spark.sql.Dataset[Int] = [value: int]\n\nscala&gt; one.show\n+-----+\n|value|\n+-----+\n|    1|\n+-----+\n</code></pre> <p><code>createDataset</code> creates logical operators:</p> <ul> <li>LocalRelation for the input <code>data</code> collection</li> <li>LogicalRDD for the input <code>RDD[T]</code></li> </ul>  <p>implicits object</p> <p>You may want to consider implicits object and <code>toDS</code> method instead.</p> <pre><code>val spark: SparkSession = ...\nimport spark.implicits._\n\nscala&gt; val one = Seq(1).toDS\none: org.apache.spark.sql.Dataset[Int] = [value: int]\n</code></pre>  <p>Internally, <code>createDataset</code> first looks up the implicit ExpressionEncoder in scope to access the <code>AttributeReference</code>s (of the schema).</p> <p>The expression encoder is then used to map elements (of the input <code>Seq[T]</code>) into a collection of InternalRows. With the references and rows, <code>createDataset</code> returns a Dataset.md[Dataset] with a LocalRelation.md[<code>LocalRelation</code> logical query plan].</p>","text":""},{"location":"SparkSession/#executing-sql-queries-sql-mode","title":"Executing SQL Queries (SQL Mode) <pre><code>sql(\n  sqlText: String): DataFrame\n</code></pre> <p><code>sql</code> creates a QueryPlanningTracker to measure executing the following in parsing phase:</p> <ul> <li><code>sql</code> requests the SessionState for the ParserInterface to parse the given <code>sqlText</code> SQL statement (that gives a LogicalPlan)</li> </ul> <p>In the end, <code>sql</code> creates a DataFrame with the following:</p> <ul> <li>This <code>SparkSession</code></li> <li>The <code>LogicalPlan</code></li> <li>The <code>QueryPlanningTracker</code></li> </ul>","text":""},{"location":"SparkSession/#accessing-udfregistration","title":"Accessing UDFRegistration <pre><code>udf: UDFRegistration\n</code></pre> <p><code>udf</code> attribute is UDFRegistration (for registering user-defined functions for SQL-based queries).</p> <pre><code>val spark: SparkSession = ...\nspark.udf.register(\"myUpper\", (s: String) =&gt; s.toUpperCase)\n\nval strs = ('a' to 'c').map(_.toString).toDS\nstrs.registerTempTable(\"strs\")\n\nscala&gt; sql(\"SELECT *, myUpper(value) UPPER FROM strs\").show\n+-----+-----+\n|value|UPPER|\n+-----+-----+\n|    a|    A|\n|    b|    B|\n|    c|    C|\n+-----+-----+\n</code></pre> <p>Internally, it is simply an alias for SessionState.udfRegistration.</p>","text":""},{"location":"SparkSession/#loading-data-from-table","title":"Loading Data From Table <pre><code>table(\n  multipartIdentifier: Seq[String]): DataFrame\ntable(\n  tableName: String): DataFrame\ntable(\n  tableIdent: TableIdentifier): DataFrame\n</code></pre> <p><code>table</code> creates a DataFrame for the input <code>tableName</code> table.</p>  <p>Note</p> <p>baseRelationToDataFrame acts as a mechanism to plug <code>BaseRelation</code> object hierarchy in into adocLogicalPlan object hierarchy that <code>SparkSession</code> uses to bridge them.</p>  <pre><code>scala&gt; spark.catalog.tableExists(\"t1\")\nres1: Boolean = true\n\n// t1 exists in the catalog\n// let's load it\nval t1 = spark.table(\"t1\")\n</code></pre>","text":""},{"location":"SparkSession/#catalog","title":"Catalog <pre><code>catalog: Catalog\n</code></pre> <p><code>catalog</code> creates a CatalogImpl when first accessed.</p>  lazy value <p><code>catalog</code> is a Scala lazy value which is computed once when accessed and cached afterwards.</p>","text":""},{"location":"SparkSession/#dataframereader","title":"DataFrameReader <pre><code>read: DataFrameReader\n</code></pre> <p><code>read</code> gives DataFrameReader to load data from external data sources and load it into a <code>DataFrame</code>.</p> <pre><code>val spark: SparkSession = ... // create instance\nval dfReader: DataFrameReader = spark.read\n</code></pre>","text":""},{"location":"SparkSession/#runtime-configuration","title":"Runtime Configuration <pre><code>conf: RuntimeConfig\n</code></pre> <p><code>conf</code> returns the current RuntimeConfig.</p> <p>Internally, <code>conf</code> creates a RuntimeConfig (when requested the very first time and cached afterwards) with the SQLConf (of the SessionState).</p>","text":""},{"location":"SparkSession/#experimentalmethods","title":"ExperimentalMethods <pre><code>experimental: ExperimentalMethods\n</code></pre> <p><code>experimentalMethods</code> is an extension point with ExperimentalMethods that is a per-session collection of extra strategies and <code>Rule[LogicalPlan]</code>s.</p> <p><code>experimental</code> is used in SparkPlanner and SparkOptimizer.</p>","text":""},{"location":"SparkSession/#create-dataframe-for-baserelation","title":"Create DataFrame for BaseRelation <pre><code>baseRelationToDataFrame(\n  baseRelation: BaseRelation): DataFrame\n</code></pre> <p>Internally, <code>baseRelationToDataFrame</code> creates a DataFrame from the input BaseRelation wrapped inside LogicalRelation.</p> <p><code>baseRelationToDataFrame</code> is used when:</p> <ul> <li><code>DataFrameReader</code> is requested to load data from data source or JDBC table</li> <li><code>TextInputCSVDataSource</code> creates a base <code>Dataset</code> (of Strings)</li> <li><code>TextInputJsonDataSource</code> creates a base <code>Dataset</code> (of Strings)</li> </ul>","text":""},{"location":"SparkSession/#creating-sessionstate","title":"Creating SessionState <pre><code>instantiateSessionState(\n  className: String,\n  sparkSession: SparkSession): SessionState\n</code></pre> <p><code>instantiateSessionState</code> finds the <code>className</code> that is then used to create and build a <code>BaseSessionStateBuilder</code>.</p> <p><code>instantiateSessionState</code> may report an <code>IllegalArgumentException</code> while instantiating the class of a <code>SessionState</code>:</p> <pre><code>Error while instantiating '[className]'\n</code></pre> <p><code>instantiateSessionState</code> is used when <code>SparkSession</code> is requested for SessionState (based on spark.sql.catalogImplementation configuration property).</p>","text":""},{"location":"SparkSession/#sessionstateclassname","title":"sessionStateClassName <pre><code>sessionStateClassName(\n  conf: SparkConf): String\n</code></pre> <p><code>sessionStateClassName</code> gives the name of the class of the SessionState per spark.sql.catalogImplementation, i.e.</p> <ul> <li>org.apache.spark.sql.hive.HiveSessionStateBuilder for <code>hive</code></li> <li>org.apache.spark.sql.internal.SessionStateBuilder for <code>in-memory</code></li> </ul> <p><code>sessionStateClassName</code> is used when <code>SparkSession</code> is requested for the SessionState (and one is not available yet).</p>","text":""},{"location":"SparkSession/#creating-dataframe-from-rdd-of-internal-binary-rows-and-schema","title":"Creating DataFrame From RDD Of Internal Binary Rows and Schema <pre><code>internalCreateDataFrame(\n  catalystRows: RDD[InternalRow],\n  schema: StructType,\n  isStreaming: Boolean = false): DataFrame\n</code></pre> <p><code>internalCreateDataFrame</code> creates a DataFrame with LogicalRDD.</p> <p><code>internalCreateDataFrame</code> is used when:</p> <ul> <li> <p><code>DataFrameReader</code> is requested to create a DataFrame from Dataset of JSONs or CSVs</p> </li> <li> <p><code>SparkSession</code> is requested to create a DataFrame from RDD of rows</p> </li> <li> <p>InsertIntoDataSourceCommand logical command is executed</p> </li> </ul>","text":""},{"location":"SparkSession/#executionlistenermanager","title":"ExecutionListenerManager <pre><code>listenerManager: ExecutionListenerManager\n</code></pre> <p>ExecutionListenerManager</p>","text":""},{"location":"SparkSession/#sharedstate","title":"SharedState <pre><code>sharedState: SharedState\n</code></pre> <p>SharedState</p>","text":""},{"location":"SparkSession/#measuring-duration-of-executing-code-block","title":"Measuring Duration of Executing Code Block <pre><code>time[T](f: =&gt; T): T\n</code></pre> <p><code>time</code> executes a code block and prints out (to standard output) the time taken to execute it</p>","text":""},{"location":"SparkSession/#applying-sparksessionextensions","title":"Applying SparkSessionExtensions <pre><code>applyExtensions(\n  extensionConfClassNames: Seq[String],\n  extensions: SparkSessionExtensions): SparkSessionExtensions\n</code></pre> <p><code>applyExtensions</code> uses the given <code>extensionConfClassNames</code> as the names of the extension classes.</p> <p>For every extension class, <code>applyExtensions</code> instantiates one by one passing in the given SparkSessionExtensions.</p>  <p>Note</p> <p>The given <code>SparkSessionExtensions</code> is mutated in-place.</p>  <p>In the end, <code>applyExtensions</code> returns the given <code>SparkSessionExtensions</code>.</p>  <p>In case of <code>ClassCastException</code>, <code>ClassNotFoundException</code> or <code>NoClassDefFoundError</code>, <code>applyExtensions</code> prints out the following WARN message to the logs:</p> <pre><code>Cannot use [extensionConfClassName] to configure session extensions.\n</code></pre>  <p><code>applyExtensions</code> is used when:</p> <ul> <li><code>SparkSession.Builder</code> is requested to get active or create a new SparkSession instance</li> <li><code>SparkSession</code> is created (from a <code>SparkContext</code>)</li> </ul>","text":""},{"location":"SparkSession/#default-parallelism-of-leaf-nodes","title":"Default Parallelism of Leaf Nodes <pre><code>leafNodeDefaultParallelism: Int\n</code></pre> <p><code>leafNodeDefaultParallelism</code> is the value of spark.sql.leafNodeDefaultParallelism if defined or <code>SparkContext.defaultParallelism</code> (Spark Core).</p>  <p><code>leafNodeDefaultParallelism</code>\u00a0is used when:</p> <ul> <li>SparkSession.range operator is used</li> <li><code>RangeExec</code> leaf physical operator is created</li> <li><code>CommandResultExec</code> physical operator is requested for the <code>RDD[InternalRow]</code></li> <li><code>LocalTableScanExec</code> physical operator is requested for the RDD</li> <li><code>FilePartition</code> is requested for maxSplitBytes</li> </ul>","text":""},{"location":"SparkSessionExtensions/","title":"SparkSessionExtensions","text":"<p><code>SparkSessionExtensions</code> is an Injection API for Spark SQL developers to extend the capabilities of a SparkSession.</p> <p>Spark SQL developers use Builder.withExtensions method or register custom extensions using spark.sql.extensions configuration property.</p> <p><code>SparkSessionExtensions</code> is an integral part of SparkSession.</p>","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#injectedfunctions","title":"injectedFunctions <p><code>SparkSessionExtensions</code> uses a collection of 3-element tuples with the following:</p> <ol> <li><code>FunctionIdentifier</code></li> <li><code>ExpressionInfo</code></li> <li><code>Seq[Expression] =&gt; Expression</code></li> </ol>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#injection-api","title":"Injection API","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#injectcheckrule","title":"injectCheckRule <pre><code>type CheckRuleBuilder = SparkSession =&gt; LogicalPlan =&gt; Unit\ninjectCheckRule(\n  builder: CheckRuleBuilder): Unit\n</code></pre> <p><code>injectCheckRule</code> injects an check analysis <code>Rule</code> builder into a SparkSession.</p> <p>The injected rules will be executed after the analysis phase. A check analysis rule is used to detect problems with a LogicalPlan and should throw an exception when a problem is found.</p>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#injectcolumnar","title":"injectColumnar <pre><code>type ColumnarRuleBuilder = SparkSession =&gt; ColumnarRule\ninjectColumnar(\n  builder: ColumnarRuleBuilder): Unit\n</code></pre> <p>Injects a ColumnarRule to a SparkSession</p>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#injectfunction","title":"injectFunction <pre><code>type FunctionDescription = (FunctionIdentifier, ExpressionInfo, FunctionBuilder)\ninjectFunction(\n  functionDescription: FunctionDescription): Unit\n</code></pre> <p><code>injectFunction</code>...FIXME</p>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#injectoptimizerrule","title":"injectOptimizerRule <pre><code>type RuleBuilder = SparkSession =&gt; Rule[LogicalPlan]\ninjectOptimizerRule(\n  builder: RuleBuilder): Unit\n</code></pre> <p><code>injectOptimizerRule</code> registers a custom logical optimization rules builder.</p>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#injectparser","title":"injectParser <pre><code>type ParserBuilder = (SparkSession, ParserInterface) =&gt; ParserInterface\ninjectParser(\n  builder: ParserBuilder): Unit\n</code></pre> <p><code>injectParser</code>...FIXME</p>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#injectplannerstrategy","title":"injectPlannerStrategy <pre><code>type StrategyBuilder = SparkSession =&gt; Strategy\ninjectPlannerStrategy(\n  builder: StrategyBuilder): Unit\n</code></pre> <p><code>injectPlannerStrategy</code>...FIXME</p>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#injectposthocresolutionrule","title":"injectPostHocResolutionRule <pre><code>type RuleBuilder = SparkSession =&gt; Rule[LogicalPlan]\ninjectPostHocResolutionRule(\n  builder: RuleBuilder): Unit\n</code></pre> <p><code>injectPostHocResolutionRule</code>...FIXME</p>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#injectquerystagepreprule","title":"injectQueryStagePrepRule <pre><code>type QueryStagePrepRuleBuilder = SparkSession =&gt; Rule[SparkPlan]\ninjectQueryStagePrepRule(\n  builder: QueryStagePrepRuleBuilder): Unit\n</code></pre> <p><code>injectQueryStagePrepRule</code> registers a <code>QueryStagePrepRuleBuilder</code> (that can build a query stage preparation rule).</p>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#injectresolutionrule","title":"injectResolutionRule <pre><code>type RuleBuilder = SparkSession =&gt; Rule[LogicalPlan]\ninjectResolutionRule(\n  builder: RuleBuilder): Unit\n</code></pre> <p><code>injectResolutionRule</code>...FIXME</p>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#injecttablefunction","title":"injectTableFunction <pre><code>type TableFunctionBuilder = Seq[Expression] =&gt; LogicalPlan\ntype TableFunctionDescription = (FunctionIdentifier, ExpressionInfo, TableFunctionBuilder)\ninjectTableFunction(\n  functionDescription: TableFunctionDescription): Unit\n</code></pre> <p><code>injectTableFunction</code> registers a new Table-Valued Functions.</p>  <p><code>injectTableFunction</code> adds the given <code>TableFunctionDescription</code> to the injectedTableFunctions internal registry.</p>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#registering-custom-logical-optimization-rules","title":"Registering Custom Logical Optimization Rules <pre><code>buildOptimizerRules(\n  session: SparkSession): Seq[Rule[LogicalPlan]]\n</code></pre> <p><code>buildOptimizerRules</code> gives the optimizerRules logical rules given the input SparkSession.</p> <p><code>buildOptimizerRules</code> is used when <code>BaseSessionStateBuilder</code> is requested for the custom operator optimization rules (to add to the base Operator Optimization batch).</p>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#logical-optimizer-rules-builder","title":"Logical Optimizer Rules (Builder) <pre><code>optimizerRules: Buffer[SparkSession =&gt; Rule[LogicalPlan]]\n</code></pre> <p><code>optimizerRules</code> are functions (builders) that take a SparkSession and return logical optimizer rules (<code>Rule[LogicalPlan]</code>).</p> <p><code>optimizerRules</code> is added a new rule when <code>SparkSessionExtensions</code> is requested to injectOptimizerRule.</p>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#buildcolumnarrules-internal-method","title":"buildColumnarRules Internal Method <pre><code>buildColumnarRules(\n  session: SparkSession): Seq[ColumnarRule]\n</code></pre> <p><code>buildColumnarRules</code>...FIXME</p> <p><code>buildColumnarRules</code> is used when <code>BaseSessionStateBuilder</code> is requested for columnarRules.</p>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#buildcheckrules","title":"buildCheckRules <pre><code>buildCheckRules(\n  session: SparkSession): Seq[LogicalPlan =&gt; Unit]\n</code></pre> <p><code>buildCheckRules</code>...FIXME</p> <p><code>buildCheckRules</code> is used when <code>BaseSessionStateBuilder</code> is requested to customCheckRules.</p>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#building-query-stage-preparation-rules","title":"Building Query Stage Preparation Rules <pre><code>buildQueryStagePrepRules(\n  session: SparkSession): Seq[Rule[SparkPlan]]\n</code></pre> <p><code>buildQueryStagePrepRules</code> executes the queryStagePrepRuleBuilders (to build query stage preparation rules).</p>  <p><code>buildQueryStagePrepRules</code> is used when:</p> <ul> <li><code>BaseSessionStateBuilder</code> is requested for the query stage preparation rules</li> </ul>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#registertablefunctions","title":"registerTableFunctions <pre><code>registerTableFunctions(\n  tableFunctionRegistry: TableFunctionRegistry): TableFunctionRegistry\n</code></pre> <p><code>registerTableFunctions</code> requests the given TableFunctionRegistry to register all the injected table functions.</p>  <p><code>registerTableFunctions</code> is used when:</p> <ul> <li><code>BaseSessionStateBuilder</code> is requested for the TableFunctionRegistry</li> </ul>","text":"","tags":["DeveloperApi"]},{"location":"SparkSessionExtensions/#injectedtablefunctions","title":"injectedTableFunctions <pre><code>injectedTableFunctions: Buffer[TableFunctionDescription]\n</code></pre> <p><code>SparkSessionExtensions</code> creates an empty <code>injectedTableFunctions</code> mutable collection of <code>TableFunctionDescription</code>s:</p> <pre><code>type TableFunctionBuilder = Seq[Expression] =&gt; LogicalPlan\ntype TableFunctionDescription = (FunctionIdentifier, ExpressionInfo, TableFunctionBuilder)\n</code></pre> <p>A new <code>TableFunctionDescription</code> tuple is added using injectTableFunction injector.</p> <p><code>TableFunctionDescription</code>s are registered when <code>SparkSessionExtensions</code> is requested to registerTableFunctions.</p>","text":"","tags":["DeveloperApi"]},{"location":"StaticSQLConf/","title":"StaticSQLConf \u2014 Static Configuration Properties","text":"<p><code>StaticSQLConf</code> holds cross-session, immutable and static SQL configuration properties.</p> <pre><code>assert(sc.isInstanceOf[org.apache.spark.SparkContext])\n\nimport org.apache.spark.sql.internal.StaticSQLConf\nsc.getConf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS.key)\n</code></pre> <p><code>StaticSQLConf</code> configuration properties can only be queried and can never be changed once the first <code>SparkSession</code> is created (unlike the regular configuration properties).</p> <pre><code>import org.apache.spark.sql.internal.StaticSQLConf\nscala&gt; val metastoreName = spark.conf.get(StaticSQLConf.CATALOG_IMPLEMENTATION.key)\nmetastoreName: String = hive\n\nscala&gt; spark.conf.set(StaticSQLConf.CATALOG_IMPLEMENTATION.key, \"hive\")\norg.apache.spark.sql.AnalysisException: Cannot modify the value of a static config: spark.sql.catalogImplementation;\n  at org.apache.spark.sql.RuntimeConfig.requireNonStaticConf(RuntimeConfig.scala:144)\n  at org.apache.spark.sql.RuntimeConfig.set(RuntimeConfig.scala:41)\n  ... 50 elided\n</code></pre>"},{"location":"StaticSQLConf/#cacheserializer","title":"cache.serializer <p>spark.sql.cache.serializer</p>","text":""},{"location":"StaticSQLConf/#codegencachemaxentries","title":"codegen.cache.maxEntries <p>spark.sql.codegen.cache.maxEntries</p> <p>(internal) When non-zero, enable caching of generated classes for operators and expressions. All jobs share the cache that can use up to the specified number for generated classes.</p> <p>Default: <code>100</code></p> <p>Use SQLConf.codegenCacheMaxEntries to access the current value</p> <p>Used when:</p> <ul> <li><code>CodeGenerator</code> is loaded (and creates the cache)</li> </ul>","text":""},{"location":"StaticSQLConf/#sparksqlbroadcastexchangemaxthreadthreshold","title":"spark.sql.broadcastExchange.maxThreadThreshold <p>(internal) The maximum degree of parallelism to fetch and broadcast the table. If we encounter memory issue like frequently full GC or OOM when broadcast table we can decrease this number in order to reduce memory usage. Notice the number should be carefully chosen since decreasing parallelism might cause longer waiting for other broadcasting. Also, increasing parallelism may cause memory problem.</p> <p>The threshold must be in (0,128]</p> <p>Default: <code>128</code></p>","text":""},{"location":"StaticSQLConf/#sparksqlcatalogimplementation","title":"spark.sql.catalogImplementation <p>(internal) Configures <code>in-memory</code> (default) or <code>hive</code>-related BaseSessionStateBuilder and ExternalCatalog</p> <p>Builder.enableHiveSupport is used to enable Hive support for a SparkSession.</p> <p>Used when:</p> <ul> <li> <p><code>SparkSession</code> utility is requested for the name of the BaseSessionStateBuilder implementation (when <code>SparkSession</code> is requested for a SessionState)</p> </li> <li> <p><code>SharedState</code> utility is requested for the name of the ExternalCatalog implementation (when <code>SharedState</code> is requested for an ExternalCatalog)</p> </li> <li> <p><code>SparkSession.Builder</code> is requested to enable Hive support</p> </li> <li> <p><code>spark-shell</code> is executed</p> </li> <li> <p><code>SetCommand</code> is executed (with <code>hive.</code> keys)</p> </li> </ul>","text":""},{"location":"StaticSQLConf/#sparksqldebug","title":"spark.sql.debug <p>(internal) Only used for internal debugging when <code>HiveExternalCatalog</code> is requested to restoreTableMetadata.</p> <p>Default: <code>false</code></p> <p>Not all functions are supported when enabled.</p>","text":""},{"location":"StaticSQLConf/#sparksqldefaulturlstreamhandlerfactoryenabled","title":"spark.sql.defaultUrlStreamHandlerFactory.enabled <p>(internal) When true, register Hadoop's FsUrlStreamHandlerFactory to support ADD JAR against HDFS locations. It should be disabled when a different stream protocol handler should be registered to support a particular protocol type, or if Hadoop's FsUrlStreamHandlerFactory conflicts with other protocol types such as <code>http</code> or <code>https</code>. See also SPARK-25694 and HADOOP-14598.</p> <p>Default: <code>true</code></p>","text":""},{"location":"StaticSQLConf/#sparksqleventtruncatelength","title":"spark.sql.event.truncate.length <p>Threshold of SQL length beyond which it will be truncated before adding to event. Defaults to no truncation. If set to 0, callsite will be logged instead.</p> <p>Must be set greater or equal to zero.</p> <p>Default: <code>Int.MaxValue</code></p>","text":""},{"location":"StaticSQLConf/#sparksqlextensions","title":"spark.sql.extensions <p>A comma-separated list of SQL extension configuration classes to configure SparkSessionExtensions:</p> <ol> <li>The classes must implement <code>SparkSessionExtensions =&gt; Unit</code></li> <li>The classes must have a no-args constructor</li> <li>If multiple extensions are specified, they are applied in the specified order.</li> <li>For the case of rules and planner strategies, they are applied in the specified order.</li> <li>For the case of parsers, the last parser is used and each parser can delegate to its predecessor</li> <li>For the case of function name conflicts, the last registered function name is used</li> </ol> <p>Default: <code>(empty)</code></p> <p>Used when:</p> <ul> <li><code>SparkSession</code> utility is used to apply SparkSessionExtensions</li> </ul>","text":""},{"location":"StaticSQLConf/#sparksqlfilesourcetablerelationcachesize","title":"spark.sql.filesourceTableRelationCacheSize <p>(internal) The maximum size of the cache that maps qualified table names to table relation plans. Must not be negative.</p> <p>Default: <code>1000</code></p>","text":""},{"location":"StaticSQLConf/#sparksqlglobaltempdatabase","title":"spark.sql.globalTempDatabase <p>(internal) Name of the Spark-owned internal database of global temporary views</p> <p>Default: <code>global_temp</code></p> <p>The name of the internal database cannot conflict with the names of any database that is already available in ExternalCatalog.</p> <p>Used to create a GlobalTempViewManager when <code>SharedState</code> is first requested for one.</p>","text":""},{"location":"StaticSQLConf/#sparksqlhivethriftserversinglesession","title":"spark.sql.hive.thriftServer.singleSession <p>When enabled (<code>true</code>), Hive Thrift server is running in a single session mode. All the JDBC/ODBC connections share the temporary views, function registries, SQL configuration and the current database.</p> <p>Default: <code>false</code></p>","text":""},{"location":"StaticSQLConf/#sparksqllegacysessioninitwithconfigdefaults","title":"spark.sql.legacy.sessionInitWithConfigDefaults <p>Flag to revert to legacy behavior where a cloned SparkSession receives SparkConf defaults, dropping any overrides in its parent SparkSession.</p> <p>Default: <code>false</code></p>","text":""},{"location":"StaticSQLConf/#sparksqlqueryexecutionlisteners","title":"spark.sql.queryExecutionListeners <p>Class names of QueryExecutionListeners that will be automatically registered (with new SparkSessions)</p> <p>Default: (empty)</p> <p>The classes should have either a no-arg constructor, or a constructor that expects a <code>SparkConf</code> argument.</p>","text":""},{"location":"StaticSQLConf/#sparksqlsourcesschemastringlengththreshold","title":"spark.sql.sources.schemaStringLengthThreshold <p>(internal) The maximum length allowed in a single cell when storing additional schema information in Hive's metastore</p> <p>Default: <code>4000</code></p>","text":""},{"location":"StaticSQLConf/#sparksqlstreaminguienabled","title":"spark.sql.streaming.ui.enabled <p>Whether to run the Structured Streaming Web UI for the Spark application when the Spark Web UI is enabled.</p> <p>Default: <code>true</code></p>","text":""},{"location":"StaticSQLConf/#sparksqlstreaminguiretainedprogressupdates","title":"spark.sql.streaming.ui.retainedProgressUpdates <p>The number of progress updates to retain for a streaming query for Structured Streaming UI.</p> <p>Default: <code>100</code></p>","text":""},{"location":"StaticSQLConf/#sparksqlstreaminguiretainedqueries","title":"spark.sql.streaming.ui.retainedQueries <p>The number of inactive queries to retain for Structured Streaming UI.</p> <p>Default: <code>100</code></p>","text":""},{"location":"StaticSQLConf/#sparksqluiretainedexecutions","title":"spark.sql.ui.retainedExecutions <p>Number of executions to retain in the Spark UI.</p> <p>Default: <code>1000</code></p>","text":""},{"location":"StaticSQLConf/#sparksqlwarehousedir","title":"spark.sql.warehouse.dir <p>Directory of a Spark warehouse</p> <p>Default: <code>spark-warehouse</code></p>","text":""},{"location":"SubExprUtils/","title":"SubExprUtils Utility","text":"<p><code>SubExprUtils</code> uses PredicateHelper.</p> <p><code>SubExprUtils</code> is used to &lt;&gt;. <p>=== [[hasNullAwarePredicateWithinNot]] Checking If Condition Expression Has Any Null-Aware Predicate Subqueries Inside Not -- <code>hasNullAwarePredicateWithinNot</code> Method</p>"},{"location":"SubExprUtils/#source-scala","title":"[source, scala]","text":""},{"location":"SubExprUtils/#hasnullawarepredicatewithinnotcondition-expression-boolean","title":"hasNullAwarePredicateWithinNot(condition: Expression): Boolean","text":"<p><code>hasNullAwarePredicateWithinNot</code> splits conjunctive predicates (i.e. expressions separated by <code>And</code> expression).</p> <p><code>hasNullAwarePredicateWithinNot</code> is positive (i.e. <code>true</code>) and is considered to have a null-aware predicate subquery inside a Not expression when conjuctive predicate expressions include a <code>Not</code> expression with an spark-sql-Expression-In.md[In] predicate expression with a spark-sql-Expression-ListQuery.md[ListQuery] subquery expression.</p>"},{"location":"SubExprUtils/#source-scala_1","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.plans.logical._ import org.apache.spark.sql.catalyst.dsl.plans._ val plan = LocalRelation('key.int, 'value.string).analyze</p> <p>import org.apache.spark.sql.catalyst.expressions._ val in = In(value = Literal.create(1), Seq(ListQuery(plan))) val condition = Not(child = Or(left = Literal.create(false), right = in))</p> <p>import org.apache.spark.sql.catalyst.expressions.SubExprUtils val positive = SubExprUtils.hasNullAwarePredicateWithinNot(condition) assert(positive)</p> <p><code>hasNullAwarePredicateWithinNot</code> is negative (i.e. <code>false</code>) for all the other expressions and in particular the following expressions:</p> <p>. spark-sql-Expression-Exists.md[Exists] predicate subquery expressions</p> <p>. <code>Not</code> expressions with a spark-sql-Expression-Exists.md[Exists] predicate subquery expression as the child expression</p> <p>. spark-sql-Expression-In.md[In] expressions with a spark-sql-Expression-ListQuery.md[ListQuery] subquery expression as the spark-sql-Expression-In.md#list[list] expression</p> <p>. <code>Not</code> expressions with a spark-sql-Expression-In.md[In] expression (with a spark-sql-Expression-ListQuery.md[ListQuery] subquery expression as the spark-sql-Expression-In.md#list[list] expression)</p>"},{"location":"SubExprUtils/#source-scala_2","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.plans.logical._ import org.apache.spark.sql.catalyst.dsl.plans._ val plan = LocalRelation('key.int, 'value.string).analyze</p> <p>import org.apache.spark.sql.catalyst.expressions._ import org.apache.spark.sql.catalyst.expressions.SubExprUtils</p> <p>// Exists val condition = Exists(plan) val negative = SubExprUtils.hasNullAwarePredicateWithinNot(condition) assert(!negative)</p> <p>// Not Exists val condition = Not(child = Exists(plan)) val negative = SubExprUtils.hasNullAwarePredicateWithinNot(condition) assert(!negative)</p> <p>// In with ListQuery val condition = In(value = Literal.create(1), Seq(ListQuery(plan))) val negative = SubExprUtils.hasNullAwarePredicateWithinNot(condition) assert(!negative)</p> <p>// Not In with ListQuery val in = In(value = Literal.create(1), Seq(ListQuery(plan))) val condition = Not(child = in) val negative = SubExprUtils.hasNullAwarePredicateWithinNot(condition) assert(!negative)</p> <p>NOTE: <code>hasNullAwarePredicateWithinNot</code> is used exclusively when <code>CheckAnalysis</code> analysis validation is requested to CheckAnalysis.md#checkAnalysis[validate analysis of a logical plan] (with <code>Filter</code> logical operators).</p>"},{"location":"TableFunctionRegistry/","title":"TableFunctionRegistry","text":"<p><code>TableFunctionRegistry</code> is an extension of the FunctionRegistryBase abstraction for function registries that manage table-valued functions (table functions) that produce a LogicalPlan.</p>"},{"location":"TableFunctionRegistry/#implementations","title":"Implementations","text":"<ul> <li><code>EmptyTableFunctionRegistry</code></li> <li>SimpleTableFunctionRegistry</li> </ul>"},{"location":"TableFunctionRegistry/#built-in-table-valued-functions","title":"Built-In Table-Valued Functions    Name Logical Operator Generator Expression     <code>range</code> <code>Range</code>    <code>explode</code>  <code>Explode</code>   <code>explode_outer</code>  <code>Explode</code>   <code>inline</code>  Inline   <code>inline_outer</code>  Inline   <code>json_tuple</code>  <code>JsonTuple</code>   <code>posexplode</code>  <code>PosExplode</code>   <code>posexplode_outer</code>  <code>PosExplode</code>   <code>stack</code>  <code>Stack</code>","text":""},{"location":"TableFunctionRegistry/#simpletablefunctionregistry","title":"SimpleTableFunctionRegistry <pre><code>builtin: SimpleTableFunctionRegistry\n</code></pre> <p><code>TableFunctionRegistry</code> creates a system-wide SimpleTableFunctionRegistry and registers all the built-in table-valued functions.</p> <pre><code>import org.apache.spark.sql.catalyst.analysis.TableFunctionRegistry\nimport org.apache.spark.sql.catalyst.analysis.SimpleTableFunctionRegistry\nassert(TableFunctionRegistry.builtin.isInstanceOf[SimpleTableFunctionRegistry])\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.analysis.TableFunctionRegistry\nTableFunctionRegistry.builtin.listFunction\n</code></pre> <p><code>builtin</code> is used when:</p> <ul> <li><code>TableFunctionRegistry</code> utility is used for the functionSet</li> <li><code>SessionCatalog</code> is requested to isBuiltinFunction, lookupBuiltinOrTempTableFunction, resolveBuiltinOrTempTableFunction, reset</li> <li><code>BaseSessionStateBuilder</code> is requested for the TableFunctionRegistry</li> </ul>","text":""},{"location":"TableFunctionRegistry/#accessing-tablefunctionregistry","title":"Accessing TableFunctionRegistry <p><code>TableFunctionRegistry</code> is available using BaseSessionStateBuilder.tableFunctionRegistry.</p>","text":""},{"location":"TableFunctionRegistry/#sessioncatalog","title":"SessionCatalog <p><code>TableFunctionRegistry</code> is used to create the following:</p> <ul> <li>SessionCatalog</li> <li>HiveSessionCatalog</li> <li>SessionState</li> </ul>","text":""},{"location":"TableScan/","title":"TableScan \u2014 Relations with Column Pruning","text":"<p><code>TableScan</code> is the &lt;&gt; of &lt;&gt; with support for &lt;&gt;, i.e. can eliminate unneeded columns before producing an RDD containing all of its tuples as <code>Row</code> objects. <p>[[contract]] [source, scala]</p> <p>package org.apache.spark.sql.sources</p> <p>trait PrunedScan {   def buildScan(): RDD[Row] }</p> <p>.TableScan Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Property | Description</p> <p>| buildScan | [[buildScan]] Building distributed data scan with column pruning</p> <p>In other words, <code>buildScan</code> creates a <code>RDD[Row]</code> to represent a distributed data scan (i.e. scanning over data in a relation).</p> <p>Used exclusively when <code>DataSourceStrategy</code> execution planning strategy is requested to linkplan a LogicalRelation with a TableScan. |===</p> <p>[[implementations]] NOTE: KafkaRelation is the one and only known implementation of the &lt;&gt; in Spark SQL."},{"location":"TypedColumn/","title":"TypedColumn","text":"<p><code>TypedColumn</code> is a Column with the ExpressionEncoder for the types of the input and the output.</p> <p><code>TypedColumn</code> is created using as operator on a <code>Column</code>.</p> <pre><code>scala&gt; val id = $\"id\".as[Int]\nid: org.apache.spark.sql.TypedColumn[Any,Int] = id\n\nscala&gt; id.expr\nres1: org.apache.spark.sql.catalyst.expressions.Expression = 'id\n</code></pre> <p>=== [[name]] <code>name</code> Operator</p> <pre><code>name(\nalias: String): TypedColumn[T, U]\n</code></pre> <p><code>name</code> is part of the Column abstraction.</p> <p><code>name</code>...FIXME</p> <p>=== [[withInputType]] Creating TypedColumn -- <code>withInputType</code> Internal Method</p>"},{"location":"TypedColumn/#source-scala","title":"[source, scala]","text":"<p>withInputType(   inputEncoder: ExpressionEncoder[_],   inputAttributes: Seq[Attribute]): TypedColumn[T, U]</p> <p><code>withInputType</code>...FIXME</p> <p><code>withInputType</code> is used when the following typed operators are used:</p> <ul> <li> <p>Dataset.select</p> </li> <li> <p>KeyValueGroupedDataset.agg</p> </li> <li> <p>RelationalGroupedDataset.agg</p> </li> </ul>"},{"location":"TypedColumn/#creating-instance","title":"Creating Instance","text":"<p><code>TypedColumn</code> takes the following when created:</p> <ul> <li>[[expr]] Catalyst expressions/Expression.md[expression]</li> <li>[[encoder]] ExpressionEncoder of the column results</li> </ul> <p><code>TypedColumn</code> initializes the &lt;&gt;."},{"location":"UDFRegistration/","title":"UDFRegistration","text":"<p><code>UDFRegistration</code> is a facade to a session-scoped FunctionRegistry to register user-defined functions (UDFs) and user-defined aggregate functions (UDAFs).</p>"},{"location":"UDFRegistration/#creating-instance","title":"Creating Instance","text":"<p><code>UDFRegistration</code> takes the following to be created:</p> <ul> <li> FunctionRegistry <p><code>UDFRegistration</code> is created when:</p> <ul> <li><code>BaseSessionStateBuilder</code> is requested for the UDFRegistration</li> </ul>"},{"location":"UDFRegistration/#accessing-udfregistration","title":"Accessing UDFRegistration","text":"<p><code>UDFRegistration</code> is available using SparkSession.udf.</p> <pre><code>import org.apache.spark.sql.SparkSession\nassert(spark.isInstanceOf[SparkSession])\n</code></pre> <pre><code>import org.apache.spark.sql.UDFRegistration\nassert(spark.udf.isInstanceOf[UDFRegistration])\n</code></pre>"},{"location":"UDFRegistration/#sessionstate","title":"SessionState <p><code>UDFRegistration</code> is used to create a SessionState.</p>","text":""},{"location":"UDFRegistration/#registering-userdefinedfunction","title":"Registering UserDefinedFunction <pre><code>register(\n  name: String,\n  udf: UserDefinedFunction): UserDefinedFunction\n</code></pre> <p><code>register</code> associates the given name with the given UserDefinedFunction.</p> <p><code>register</code> requests the FunctionRegistry to createOrReplaceTempFunction under the given name and with <code>scala_udf</code> source name and a function builder based on the type of the <code>UserDefinedFunction</code>:</p> <ul> <li>For UserDefinedAggregators, the function builder requests the <code>UserDefinedAggregator</code> for a ScalaAggregator</li> <li>For all other types, the function builder requests the <code>UserDefinedFunction</code> for a Column and takes the Expression</li> </ul>","text":""},{"location":"UnsafeExternalRowSorter/","title":"UnsafeExternalRowSorter","text":"<p><code>UnsafeExternalRowSorter</code> is a facade of <code>UnsafeExternalSorter</code> (Spark Core) to allow sorting InternalRows.</p>"},{"location":"UnsafeExternalRowSorter/#creating-instance","title":"Creating Instance","text":"<p><code>UnsafeExternalRowSorter</code> takes the following to be created:</p> <ul> <li> Output Schema <li> <code>RecordComparator</code> Supplier <li> <code>PrefixComparator</code> <li> <code>UnsafeExternalRowSorter.PrefixComputer</code> <li> Page Size (bytes) <li> <code>canUseRadixSort</code> flag <p><code>UnsafeExternalRowSorter</code> is created using createWithRecordComparator and create utilities.</p>"},{"location":"UnsafeExternalRowSorter/#spilling","title":"Spilling","text":"<p><code>UnsafeExternalRowSorter</code> (UnsafeExternalSorter actually) may be forced to spill in-memory data when the number of elements reaches <code>spark.shuffle.spill.numElementsForceSpillThreshold</code> (Spark Core).</p>"},{"location":"UnsafeExternalRowSorter/#unsafeexternalsorter","title":"UnsafeExternalSorter <p><code>UnsafeExternalRowSorter</code> creates an <code>UnsafeExternalSorter</code> (Spark Core) when created.</p>","text":""},{"location":"UnsafeExternalRowSorter/#creating-unsafeexternalrowsorter","title":"Creating UnsafeExternalRowSorter <pre><code>UnsafeExternalRowSorter create(\n  StructType schema,\n  Ordering&lt;InternalRow&gt; ordering,\n  PrefixComparator prefixComparator,\n  UnsafeExternalRowSorter.PrefixComputer prefixComputer,\n  long pageSizeBytes,\n  boolean canUseRadixSort)\n</code></pre> <p><code>create</code> creates an UnsafeExternalRowSorter (with a new <code>RowComparator</code> when requested).</p>  <p><code>create</code> is used when:</p> <ul> <li><code>SortExec</code> physical operator is requested to create one</li> </ul>","text":""},{"location":"UnsafeExternalRowSorter/#sorting","title":"Sorting <pre><code>Iterator&lt;InternalRow&gt; sort()\nIterator&lt;InternalRow&gt; sort(\n  Iterator&lt;UnsafeRow&gt; inputIterator) // (1)!\n</code></pre> <ol> <li>Inserts all the input rows and calls no-argument <code>sort</code></li> </ol> <p><code>sort</code>...FIXME</p>  <p><code>sort</code> is used when:</p> <ul> <li><code>DynamicPartitionDataConcurrentWriter</code> is requested to writeWithIterator</li> <li><code>ShuffleExchangeExec</code> physical operator is requested to prepareShuffleDependency</li> <li><code>SortExec</code> physical operator is executed</li> </ul>","text":""},{"location":"UnsafeFixedWidthAggregationMap/","title":"UnsafeFixedWidthAggregationMap","text":"<p><code>UnsafeFixedWidthAggregationMap</code> is a tiny layer (extension) over Spark Core's BytesToBytesMap with UnsafeRow keys and values.</p> <p><code>UnsafeFixedWidthAggregationMap</code> is used when HashAggregateExec physical operator is executed:</p> <ul> <li>Directly in createHashMap</li> <li>Indirectly using TungstenAggregationIterator in doExecute</li> </ul>"},{"location":"UnsafeFixedWidthAggregationMap/#creating-instance","title":"Creating Instance","text":"<p><code>UnsafeFixedWidthAggregationMap</code> takes the following to be created:</p> <ul> <li> Empty aggregation buffer (InternalRow) <li> Aggregation Buffer Schema (StructType) <li> Grouping Key Schema (StructType) <li> <code>TaskContext</code> (Spark Core) <li>Initial Capacity</li> <li> Page Size (in bytes) <p><code>UnsafeFixedWidthAggregationMap</code> is created when:</p> <ul> <li><code>HashAggregateExec</code> physical operator is requested to create a HashMap</li> <li><code>TungstenAggregationIterator</code> is created</li> </ul>"},{"location":"UnsafeFixedWidthAggregationMap/#initial-capacity","title":"Initial Capacity <p><code>UnsafeFixedWidthAggregationMap</code> is given the initial capacity of the BytesToBytesMap when created.</p> <p>The initial capacity is hard-coded to <code>1024 * 16</code> (when created for HashAggregateExec and TungstenAggregationIterator).</p>","text":""},{"location":"UnsafeFixedWidthAggregationMap/#bytestobytesmap","title":"BytesToBytesMap <p>When created, <code>UnsafeFixedWidthAggregationMap</code> creates a <code>BytesToBytesMap</code> (Spark Core) with the following:</p> <ul> <li><code>TaskMemoryManager</code> (Spark Core) of the TaskContext</li> <li>Initial Capacity</li> <li>Page Size</li> </ul> <p>The <code>BytesToBytesMap</code> is used when:</p> <ul> <li>getAggregationBufferFromUnsafeRow to look up</li> <li>iterator</li> <li>getPeakMemoryUsedBytes</li> <li>free</li> <li>getAvgHashProbeBucketListIterations</li> <li>destructAndCreateExternalSorter</li> </ul>","text":""},{"location":"UnsafeFixedWidthAggregationMap/#supportsaggregationbufferschema","title":"supportsAggregationBufferSchema <pre><code>boolean supportsAggregationBufferSchema(\n  StructType schema)\n</code></pre> <p><code>supportsAggregationBufferSchema</code> is enabled (<code>true</code>) if all of the top-level fields (of the given schema) are mutable.</p> <pre><code>import org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap\nimport org.apache.spark.sql.types._\n</code></pre> <pre><code>val schemaWithImmutableField = StructType(\n  StructField(\"name\", StringType) :: Nil)\nassert(UnsafeFixedWidthAggregationMap.supportsAggregationBufferSchema(schemaWithImmutableField) == false)\n</code></pre> <pre><code>val schemaWithMutableFields = StructType(\n  StructField(\"id\", IntegerType) :: StructField(\"bool\", BooleanType) :: Nil)\nassert(UnsafeFixedWidthAggregationMap.supportsAggregationBufferSchema(schemaWithMutableFields))\n</code></pre> <p><code>supportsAggregationBufferSchema</code>\u00a0is used when:</p> <ul> <li><code>HashAggregateExec</code> utility is used for the selection requirements</li> </ul>","text":""},{"location":"UnsafeFixedWidthAggregationMap/#getaggregationbufferfromunsaferow","title":"getAggregationBufferFromUnsafeRow <pre><code>UnsafeRow getAggregationBufferFromUnsafeRow(\n  UnsafeRow key) // (1)!\nUnsafeRow getAggregationBufferFromUnsafeRow(\n  UnsafeRow key,\n  int hash)\n</code></pre> <ol> <li>Uses the hash code of the given key</li> </ol> <p><code>getAggregationBufferFromUnsafeRow</code> returns the following:</p> <ul> <li><code>null</code> when the given <code>key</code> was not found and had to be inserted but failed</li> <li>currentAggregationBuffer pointed to the value (object)</li> </ul>  <p><code>getAggregationBufferFromUnsafeRow</code> uses the BytesToBytesMap to look up <code>BytesToBytesMap.Location</code> of the given (grouping) <code>key</code>.</p> <p>If the key has not been found (is not defined at the key's position), <code>getAggregationBufferFromUnsafeRow</code> inserts a copy of the emptyAggregationBuffer into the map. <code>getAggregationBufferFromUnsafeRow</code> returns <code>null</code> if insertion failed.</p> <p><code>getAggregationBufferFromUnsafeRow</code> requests the currentAggregationBuffer to pointTo to an object at <code>BytesToBytesMap.Location</code>.</p>  <p><code>getAggregationBufferFromUnsafeRow</code> is used when:</p> <ul> <li><code>TungstenAggregationIterator</code> is requested to process input rows</li> </ul>","text":""},{"location":"UnsafeFixedWidthAggregationMap/#currentaggregationbuffer","title":"currentAggregationBuffer <p><code>UnsafeFixedWidthAggregationMap</code> creates an UnsafeRow when created.</p> <p>The number of fields of this <code>UnsafeRow</code> is the length of the aggregationBufferSchema.</p> <p>The <code>UnsafeRow</code> is (re)used to point to the value (that was stored or looked up) in getAggregationBufferFromUnsafeRow.</p>","text":""},{"location":"UnsafeHashedRelation/","title":"UnsafeHashedRelation","text":"<p><code>UnsafeHashedRelation</code> is...FIXME</p>"},{"location":"UnsafeRow/","title":"UnsafeRow","text":"<p><code>UnsafeRow</code> is an InternalRow for mutable binary rows that are backed by raw memory outside the Java Vritual Machine (instead of Java objects that are in JVM memory space and may lead to more frequent GCs if created in excess).</p> <p><code>UnsafeRow</code> supports Java's Externalizable and Kryo's KryoSerializable serialization and deserialization protocols.</p>"},{"location":"UnsafeRow/#creating-instance","title":"Creating Instance","text":"<p><code>UnsafeRow</code> takes the following to be created:</p> <ul> <li> Number of fields <p>Whed created, <code>UnsafeRow</code> calculates the bitset width based on the number of fields.</p> <p><code>UnsafeRow</code> is created when:</p> <ul> <li><code>RowBasedKeyValueBatch</code> is created</li> <li><code>UnsafeArrayData</code> is requested for <code>getStruct</code></li> <li><code>UnsafeRow</code> is requested to getStruct, copy, createFromByteArray</li> <li><code>Collect</code> expression is requested for <code>row</code></li> <li><code>Percentile</code> expression is requested to <code>deserialize</code></li> <li><code>UnsafeRowWriter</code> is created</li> <li>many others</li> </ul>"},{"location":"UnsafeRow/#mutable-data-types","title":"Mutable Data Types <p>The following DataTypes are considered mutable data types:</p> <ul> <li><code>BooleanType</code></li> <li><code>ByteType</code></li> <li><code>CalendarIntervalType</code></li> <li><code>DateType</code></li> <li><code>DayTimeIntervalType</code></li> <li><code>DecimalType</code></li> <li><code>DoubleType</code></li> <li><code>FloatType</code></li> <li><code>IntegerType</code></li> <li><code>LongType</code></li> <li><code>NullType</code></li> <li><code>ShortType</code></li> <li><code>TimestampNTZType</code></li> <li><code>TimestampType</code></li> <li>UserDefinedType (over a mutable data type)</li> <li><code>YearMonthIntervalType</code></li> </ul> <p>Mutable data types have fixed length and can be mutated in place in <code>UnsafeRow</code>s (using <code>set</code> methods).</p> <p>Examples (possibly all) of the data types that are not mutable:</p> <ul> <li>ArrayType</li> <li><code>BinaryType</code></li> <li><code>StringType</code></li> <li><code>MapType</code></li> <li><code>ObjectType</code></li> <li>StructType</li> </ul>","text":""},{"location":"UnsafeRow/#kryoserializable-serde-protocol","title":"KryoSerializable SerDe Protocol <p>Learn more in KryoSerializable.</p>","text":""},{"location":"UnsafeRow/#javas-externalizable-serde-protocol","title":"Java's Externalizable SerDe Protocol <p>Learn more in java.io.Externalizable.</p>","text":""},{"location":"UnsafeRow/#sizeinbytes","title":"sizeInBytes <p><code>UnsafeRow</code> knows its size (in bytes).</p> <pre><code>scala&gt; println(unsafeRow.getSizeInBytes)\n32\n</code></pre>","text":""},{"location":"UnsafeRow/#field-offsets","title":"Field Offsets <p>The fields of a data row are placed using field offsets.</p>","text":""},{"location":"UnsafeRow/#mutable-types","title":"Mutable Types <p><code>UnsafeRow</code> considers a data type mutable if it is one of the following:</p> <ul> <li>BooleanType</li> <li>ByteType</li> <li>DateType</li> <li>DecimalType (see isMutable)</li> <li>DoubleType</li> <li>FloatType</li> <li>IntegerType</li> <li>LongType</li> <li>NullType</li> <li>ShortType</li> <li>TimestampType</li> </ul>","text":""},{"location":"UnsafeRow/#8-byte-word-alignment-and-three-regions","title":"8-Byte Word Alignment and Three Regions <p><code>UnsafeRow</code> is composed of three regions:</p> <ol> <li>Null Bit Set Bitmap Region (1 bit/field) for tracking <code>null</code> values</li> <li>Fixed-Length 8-Byte Values Region</li> <li>Variable-Length Data Region</li> </ol> <p><code>UnsafeRow</code> is always 8-byte word aligned and so their size is always a multiple of 8 bytes.</p>","text":""},{"location":"UnsafeRow/#equality-and-hashing","title":"Equality and Hashing <p>Equality comparision and hashing of rows can be performed on raw bytes since if two rows are identical so should be their bit-wise representation. No type-specific interpretation is required.</p>","text":""},{"location":"UnsafeRow/#baseobject","title":"baseObject <pre><code>Object baseObject\n</code></pre> <p><code>baseObject</code> is assigned in pointTo, copyFrom, readExternal and read. In most cases, <code>baseObject</code> is <code>byte[]</code> (except a variant of pointTo that allows for <code>Object</code>s).</p>","text":""},{"location":"UnsafeRow/#getbaseobject","title":"getBaseObject <pre><code>Object getBaseObject()\n</code></pre> <p><code>getBaseObject</code> returns the baseObject.</p> <p><code>getBaseObject</code> is used when:</p> <ul> <li><code>UnsafeWriter</code> is requested to <code>write</code> an <code>UnsafeRow</code></li> <li><code>UnsafeExternalRowSorter</code> is requested to <code>insertRow</code> an <code>UnsafeRow</code></li> <li><code>UnsafeFixedWidthAggregationMap</code> is requested to getAggregationBufferFromUnsafeRow</li> <li><code>UnsafeKVExternalSorter</code> is requested to <code>insertKV</code></li> <li><code>ExternalAppendOnlyUnsafeRowArray</code> is requested to add an UnsafeRow</li> <li><code>UnsafeHashedRelation</code> is requested to get, getValue, getWithKeyIndex, getValueWithKeyIndex, apply</li> <li><code>LongToUnsafeRowMap</code> is requested to <code>append</code></li> <li><code>InMemoryRowQueue</code> is requested to <code>add</code> an <code>UnsafeRow</code></li> </ul>","text":""},{"location":"UnsafeRow/#writetostream","title":"writeToStream <pre><code>void writeToStream(\n  OutputStream out,\n  byte[] writeBuffer)\n</code></pre> <p><code>writeToStream</code> branches off based on whether the baseObject is <code>byte[]</code> or not.</p> <p><code>writeToStream</code>...FIXME</p> <p><code>writeToStream</code> is used when:</p> <ul> <li><code>SparkPlan</code> is requested to compress RDD partitions (of UnsafeRows) to byte arrays</li> <li><code>UnsafeRowSerializerInstance</code> is requested to serializeStream</li> <li><code>Percentile</code> expression is requested to <code>serialize</code></li> </ul>","text":""},{"location":"UnsafeRow/#pointto","title":"pointTo <pre><code>void pointTo(\n  byte[] buf,\n  int sizeInBytes) // (1)\nvoid pointTo(\n  Object baseObject,\n  long baseOffset,\n  int sizeInBytes)\n</code></pre> <ol> <li>Uses <code>Platform.BYTE_ARRAY_OFFSET</code> as <code>baseOffset</code></li> </ol> <p><code>pointTo</code> sets the baseObject, the baseOffset and the sizeInBytes to the given values.</p> <p><code>pointTo</code> asserts the following:</p> <ol> <li>numFields is 0 or greater</li> <li>The given <code>sizeInBytes</code> is a multiple of 8</li> </ol>","text":""},{"location":"UnsafeRow/#copyfrom","title":"copyFrom <pre><code>void copyFrom(\n  UnsafeRow row)\n</code></pre> <p><code>copyFrom</code>...FIXME</p> <p><code>copyFrom</code> is used when:</p> <ul> <li><code>ObjectAggregationIterator</code> is requested to processInputs (using <code>SortBasedAggregator</code>)</li> <li><code>TungstenAggregationIterator</code> is requested to produce the next UnsafeRow and outputForEmptyGroupingKeyWithoutInput</li> </ul>","text":""},{"location":"UnsafeRow/#deserializing-unsaferow","title":"Deserializing UnsafeRow <p>Regardless of whether Java or Kryo are used for deserialization, they read values from the given <code>ObjectInput</code> to assign the internal registries.</p>    Registry Value     baseOffset The offset of the first element in the storage allocation of a byte array (<code>BYTE_ARRAY_OFFSET</code>)   sizeInBytes The first four bytes (Java's <code>int</code>) from the <code>ObjectInput</code>   numFields The next four bytes (Java's <code>int</code>) from the <code>ObjectInput</code>   bitSetWidthInBytes Based on the numFields   baseObject <code>byte[]</code> (of sizeInBytes size)","text":""},{"location":"UnsafeRow/#kryo","title":"Kryo <pre><code>void read(\n  Kryo kryo,\n  Input in)\n</code></pre> <p><code>read</code> is part of the <code>KryoSerializable</code> (Kryo) abstraction.</p>","text":""},{"location":"UnsafeRow/#java","title":"Java <pre><code>void readExternal(\n  ObjectInput in)\n</code></pre> <p><code>readExternal</code> is part of the <code>Externalizable</code> (Java) abstraction.</p>","text":""},{"location":"UnsafeRow/#demo","title":"Demo <pre><code>// Use ExpressionEncoder for simplicity\nimport org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\nval stringEncoder = ExpressionEncoder[String]\nval row = stringEncoder.toRow(\"hello world\")\n\nimport org.apache.spark.sql.catalyst.expressions.UnsafeRow\nval unsafeRow = row match { case ur: UnsafeRow =&gt; ur }\n\nscala&gt; unsafeRow.getBytes\nres0: Array[Byte] = Array(0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 16, 0, 0, 0, 104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 0, 0, 0, 0, 0)\n\nscala&gt; unsafeRow.getUTF8String(0)\nres1: org.apache.spark.unsafe.types.UTF8String = hello world\n</code></pre> <pre><code>// a sample human-readable row representation\n// id (long), txt (string), num (int)\nval id: Long = 0\nval txt: String = \"hello world\"\nval num: Int = 110\nval singleRow = Seq(id, txt, num)\nval numFields = singleRow.size\n\n// that's not enough and I learnt it a few lines down\nval rowDataInBytes = Array(id.toByte) ++ txt.toArray.map(_.toByte) ++ Array(num.toByte)\n\nimport org.apache.spark.sql.catalyst.expressions.UnsafeRow\nval row = new UnsafeRow(numFields)\n</code></pre> <p>sizeInBytes should be a multiple of 8 and it's a coincidence that this pointTo does not catch it. Checking <code>sizeInBytes % 8 == 0</code> passes fine and that's why the demo fails later on.</p> <pre><code>row.pointTo(rowDataInBytes, rowDataInBytes.length)\n</code></pre> <p>The following will certainly fail. Consider it a WIP.</p> <pre><code>assert(row.getLong(0) == id)\n</code></pre>","text":""},{"location":"UnsafeRow/#word_size","title":"WORD_SIZE <p><code>UnsafeRow</code> uses <code>8</code> as the size of a word for <code>MapEntries</code> unary expression to <code>doGenCode</code> and <code>genCodeForPrimitiveElements</code>.</p>","text":""},{"location":"V2SessionCatalog/","title":"V2SessionCatalog","text":"<p><code>V2SessionCatalog</code> is the default session catalog of CatalogManager.</p> <p><code>V2SessionCatalog</code> is a TableCatalog and a SupportsNamespaces.</p>"},{"location":"V2SessionCatalog/#creating-instance","title":"Creating Instance","text":"<p><code>V2SessionCatalog</code> takes the following to be created:</p> <ul> <li> SessionCatalog <li> SQLConf <p><code>V2SessionCatalog</code> is created when <code>BaseSessionStateBuilder</code> is requested for one.</p>"},{"location":"V2SessionCatalog/#default-namespace","title":"Default Namespace <pre><code>defaultNamespace: Array[String]\n</code></pre> <p>The default namespace of <code>V2SessionCatalog</code> is default.</p> <p><code>defaultNamespace</code> is part of the CatalogPlugin abstraction.</p>","text":""},{"location":"V2SessionCatalog/#name","title":"Name  Signature <pre><code>name: String\n</code></pre> <p><code>name</code> is part of the CatalogPlugin abstraction.</p>  <p>The name of <code>V2SessionCatalog</code> is spark_catalog.</p>","text":""},{"location":"V2SessionCatalog/#loading-table","title":"Loading Table  Signature <pre><code>loadTable(\n  ident: Identifier): Table\n</code></pre> <p><code>loadTable</code> is part of the TableCatalog abstraction.</p>  <p><code>loadTable</code> creates a V1Table for a table metadata (from the SessionCatalog).</p>","text":""},{"location":"V2SessionCatalog/#loading-function","title":"Loading Function  Signature <pre><code>loadFunction(\n  ident: Identifier): UnboundFunction\n</code></pre> <p><code>loadFunction</code> is part of the FunctionCatalog abstraction.</p>  <p><code>loadFunction</code>...FIXME</p>","text":""},{"location":"V2SessionCatalog/#creating-table","title":"Creating Table  Signature <pre><code>createTable(\n  ident: Identifier,\n  columns: Array[Column],\n  partitions: Array[Transform],\n  properties: Map[String, String]): Table\ncreateTable(\n  ident: Identifier,\n  schema: StructType,\n  partitions: Array[Transform],\n  properties: util.Map[String, String]): Table // (1)!\n</code></pre> <ol> <li>Deprecated</li> </ol> <p><code>createTable</code> is part of the FunctionCatalog abstraction.</p>  <p><code>createTable</code> creates a CatalogTable and requests the SessionCatalog to createTable (with <code>ignoreIfExists</code> flag disabled so when the table already exists a <code>TableAlreadyExistsException</code> is reported).</p> <p>In the end, <code>createTable</code> loads the table.</p>","text":""},{"location":"WriteConfigMethods/","title":"WriteConfigMethods","text":"<p><code>WriteConfigMethods</code> is...FIXME</p>"},{"location":"caching-and-persistence/","title":"Caching and Persistence","text":"<p>One of the optimizations in Spark SQL is Dataset caching (aka Dataset persistence) which is available using the &lt;&gt; using the following basic actions: <ul> <li> <p>[[cache]] &lt;&gt; <li> <p>[[persist]] &lt;&gt; <li> <p>[[unpersist]] &lt;&gt; <p><code>cache</code> is simply <code>persist</code> with <code>MEMORY_AND_DISK</code> storage level.</p>"},{"location":"caching-and-persistence/#source-scala","title":"[source, scala]","text":"<p>// Cache Dataset -- it is lazy and so nothing really happens val data = spark.range(1).cache</p> <p>// Trigger caching by executing an action // The common idiom is to execute count since it's fairly cheap data.count</p> <p>At this point you could use web UI's Storage tab to review the Datasets persisted. Visit http://localhost:4040/storage.</p> <p>.web UI's Storage tab image::images/spark-webui-storage.png[align=\"center\"]</p> <p><code>persist</code> uses CacheManager for an in-memory cache of structured queries (and InMemoryRelation logical operators), and is used to cache structured queries.</p> <p>At withCachedData phase (of execution of a structured query), <code>QueryExecution</code> requests the <code>CacheManager</code> to replace segments of a logical query plan with their cached data (including &lt;&gt;). <pre><code>scala&gt; println(data.queryExecution.withCachedData.numberedTreeString)\n00 InMemoryRelation [id#9L], StorageLevel(disk, memory, deserialized, 1 replicas)\n01    +- *(1) Range (0, 1, step=1, splits=8)\n</code></pre> <pre><code>// Use the cached Dataset in another query\n// Notice InMemoryRelation in use for cached queries\nscala&gt; df.withColumn(\"newId\", 'id).explain(extended = true)\n== Parsed Logical Plan ==\n'Project [*, 'id AS newId#16]\n+- Range (0, 1, step=1, splits=Some(8))\n\n== Analyzed Logical Plan ==\nid: bigint, newId: bigint\nProject [id#0L, id#0L AS newId#16L]\n+- Range (0, 1, step=1, splits=Some(8))\n\n== Optimized Logical Plan ==\nProject [id#0L, id#0L AS newId#16L]\n+- InMemoryRelation [id#0L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n      +- *Range (0, 1, step=1, splits=Some(8))\n\n== Physical Plan ==\n*Project [id#0L, id#0L AS newId#16L]\n+- InMemoryTableScan [id#0L]\n      +- InMemoryRelation [id#0L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n            +- *Range (0, 1, step=1, splits=Some(8))\n\n// Clear in-memory cache using SQL\n// Equivalent to spark.catalog.clearCache\nscala&gt; sql(\"CLEAR CACHE\").collect\nres1: Array[org.apache.spark.sql.Row] = Array()\n\n// Visit http://localhost:4040/storage to confirm the cleaning\n</code></pre>"},{"location":"caching-and-persistence/#note","title":"[NOTE]","text":"<p>You can also use SQL's <code>CACHE TABLE [tableName]</code> to cache <code>tableName</code> table in memory. Unlike &lt;&gt; and &lt;&gt; operators, <code>CACHE TABLE</code> is an eager operation which is executed as soon as the statement is executed."},{"location":"caching-and-persistence/#sourcescala","title":"[source,scala]","text":""},{"location":"caching-and-persistence/#sqlcache-table-tablename","title":"sql(\"CACHE TABLE [tableName]\")","text":"<p>You could however use <code>LAZY</code> keyword to make caching lazy.</p>"},{"location":"caching-and-persistence/#sourcescala_1","title":"[source,scala]","text":""},{"location":"caching-and-persistence/#sqlcache-lazy-table-tablename","title":"sql(\"CACHE LAZY TABLE [tableName]\")","text":"<p>Use SQL's <code>REFRESH TABLE [tableName]</code> to refresh a cached table.</p> <p>Use SQL's <code>UNCACHE TABLE (IF EXISTS)? [tableName]</code> to remove a table from cache.</p>"},{"location":"caching-and-persistence/#use-sqls-clear-cache-to-remove-all-tables-from-cache","title":"Use SQL's <code>CLEAR CACHE</code> to remove all tables from cache.","text":""},{"location":"caching-and-persistence/#note_1","title":"[NOTE]","text":"<p>Be careful what you cache, i.e. what Dataset is cached, as it gives different queries cached.</p>"},{"location":"caching-and-persistence/#source-scala_1","title":"[source, scala]","text":"<p>// cache after range(5) val q1 = spark.range(5).cache.filter($\"id\" % 2 === 0).select(\"id\") scala&gt; q1.explain == Physical Plan == *Filter ((id#0L % 2) = 0) +- InMemoryTableScan [id#0L], [((id#0L % 2) = 0)]       +- InMemoryRelation [id#0L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)             +- *Range (0, 5, step=1, splits=8)</p> <p>// cache at the end val q2 = spark.range(1).filter($\"id\" % 2 === 0).select(\"id\").cache scala&gt; q2.explain == Physical Plan == InMemoryTableScan [id#17L]    +- InMemoryRelation [id#17L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)          +- *Filter ((id#17L % 2) = 0)             +- *Range (0, 1, step=1, splits=8)</p> <p>====</p>"},{"location":"caching-and-persistence/#tip","title":"[TIP]","text":"<p>You can check whether a Dataset was cached or not using the following code:</p>"},{"location":"caching-and-persistence/#source-scala_2","title":"[source, scala]","text":"<p>scala&gt; :type q2 org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]</p> <p>val cache = spark.sharedState.cacheManager scala&gt; cache.lookupCachedData(q2.queryExecution.logical).isDefined res0: Boolean = false</p> <p>====</p> <p>=== [[cache-table]] SQL's CACHE TABLE</p> <p>SQL's <code>CACHE TABLE</code> corresponds to requesting the session-specific <code>Catalog</code> to caching the table.</p> <p>Internally, <code>CACHE TABLE</code> becomes RunnableCommand.md#CacheTableCommand[CacheTableCommand] runnable command that...FIXME</p>"},{"location":"caching-webui-storage/","title":"User-Friendly Names of Cached Queries in web UI","text":"<p>As you may have noticed, web UI's Storage tab displays some cached queries with user-friendly RDD names (e.g. \"In-memory table [name]\") while others not (e.g. \"Scan JDBCRelation...\").</p> <p></p> <p>\"In-memory table [name]\" RDD names are the result of SQL's CACHE TABLE or when <code>Catalog</code> is requested to cache a table.</p> <pre><code>// register Dataset as temporary view (table)\nspark.range(1).createOrReplaceTempView(\"one\")\n// caching is lazy and won't happen until an action is executed\nval one = spark.table(\"one\").cache\n// The following gives \"*Range (0, 1, step=1, splits=8)\"\n// WHY?!\none.show\n\nscala&gt; spark.catalog.isCached(\"one\")\nres0: Boolean = true\n\none.unpersist\n\nimport org.apache.spark.storage.StorageLevel\n// caching is lazy\nspark.catalog.cacheTable(\"one\", StorageLevel.MEMORY_ONLY)\n// The following gives \"In-memory table one\"\none.show\n\nspark.range(100).createOrReplaceTempView(\"hundred\")\n// SQL's CACHE TABLE is eager\n// The following gives \"In-memory table `hundred`\"\n// WHY single quotes?\nspark.sql(\"CACHE TABLE hundred\")\n\n// register Dataset under name\nval ds = spark.range(20)\nspark.sharedState.cacheManager.cacheQuery(ds, Some(\"twenty\"))\n// trigger an action\nds.head\n</code></pre> <p>The other RDD names are due to caching a Dataset.</p> <pre><code>val ten = spark.range(10).cache\nten.head\n</code></pre>"},{"location":"checkpointing/","title":"Checkpointing","text":"<p>Checkpointing is a feature of Spark SQL to truncate a logical query plan that could specifically be useful for highly iterative data algorithms (e.g. Spark MLlib that uses Spark SQL's <code>Dataset</code> API for data manipulation).</p>"},{"location":"checkpointing/#note","title":"[NOTE]","text":"<p>Checkpointing is actually a feature of Spark Core (that Spark SQL uses for distributed computations) that allows a driver to be restarted on failure with previously computed state of a distributed computation described as an <code>RDD</code>. That has been successfully used in Spark Streaming - the now-obsolete Spark module for stream processing based on RDD API.</p> <p>Checkpointing truncates the lineage of a RDD to be checkpointed. That has been successfully used in Spark MLlib in iterative machine learning algorithms like ALS.</p>"},{"location":"checkpointing/#dataset-checkpointing-in-spark-sql-uses-checkpointing-to-truncate-the-lineage-of-the-underlying-rdd-of-a-dataset-being-checkpointed","title":"Dataset checkpointing in Spark SQL uses checkpointing to truncate the lineage of the underlying RDD of a <code>Dataset</code> being checkpointed.","text":"<p>Checkpointing can be eager or lazy per <code>eager</code> flag of checkpoint operator. Eager checkpointing is the default checkpointing and happens immediately when requested. Lazy checkpointing does not and will only happen when an action is executed.</p> <p>[[checkpoint-directory]] Using Dataset checkpointing requires that you specify the checkpoint directory. The directory stores the checkpoint files for RDDs to be checkpointed. Use &lt;&gt; to set the path to a checkpoint directory. <p>Checkpointing can be local or reliable which defines how reliable the checkpoint directory is. Local checkpointing uses executor storage to write checkpoint files to and due to the executor lifecycle is considered unreliable. Reliable checkpointing uses a reliable data storage like Hadoop HDFS.</p> <p>.Dataset Checkpointing Types [cols=\"1,<sup>1,</sup>2\",options=\"header\",width=\"100%\"] |=== | | Eager | Lazy</p> <p>^| Reliable | checkpoint | checkpoint(eager = false)</p> <p>^| Local | localCheckpoint | localCheckpoint(eager = false) |===</p> <p>A RDD can be recovered from a checkpoint files using &lt;&gt;. You can use SparkSession.md#internalCreateDataFrame[SparkSession.internalCreateDataFrame] method to (re)create the DataFrame from the RDD of internal binary rows. <p>[[logging]] [TIP] ==== Enable <code>INFO</code> logging level for <code>org.apache.spark.rdd.ReliableRDDCheckpointData</code> logger to see what happens while an RDD is checkpointed.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.rdd.ReliableRDDCheckpointData=INFO\n</code></pre>"},{"location":"checkpointing/#refer-to-spark-loggingmdlogging","title":"Refer to spark-logging.md[Logging].","text":"<pre><code>import org.apache.spark.sql.functions.rand\nval nums = spark.range(5).withColumn(\"random\", rand()).filter($\"random\" &gt; 0.5)\nscala&gt; nums.show\n+---+------------------+\n| id|            random|\n+---+------------------+\n|  0| 0.752877642067488|\n|  1|0.5271005540026181|\n+---+------------------+\n\nscala&gt; println(nums.queryExecution.toRdd.toDebugString)\n(8) MapPartitionsRDD[7] at toRdd at &lt;console&gt;:27 []\n |  MapPartitionsRDD[6] at toRdd at &lt;console&gt;:27 []\n |  ParallelCollectionRDD[5] at toRdd at &lt;console&gt;:27 []\n\n// Remember to set the checkpoint directory\nscala&gt; nums.checkpoint\norg.apache.spark.SparkException: Checkpoint directory has not been set in the SparkContext\n  at org.apache.spark.rdd.RDD.checkpoint(RDD.scala:1548)\n  at org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:594)\n  at org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:539)\n  ... 49 elided\n\nspark.sparkContext.setCheckpointDir(\"/tmp/checkpoints\")\n\nval checkpointDir = spark.sparkContext.getCheckpointDir.get\nscala&gt; println(checkpointDir)\nfile:/tmp/checkpoints/b1f413dc-3eaf-46a0-99de-d795252035e0\n\nval numsCheckpointed = nums.checkpoint\nscala&gt; println(numsCheckpointed.queryExecution.toRdd.toDebugString)\n(8) MapPartitionsRDD[11] at toRdd at &lt;console&gt;:27 []\n |  MapPartitionsRDD[9] at checkpoint at &lt;console&gt;:26 []\n |  ReliableCheckpointRDD[10] at checkpoint at &lt;console&gt;:26 []\n\n// Set org.apache.spark.rdd.ReliableRDDCheckpointData logger to INFO\n// to see what happens while an RDD is checkpointed\n// Let's use log4j API\nimport org.apache.log4j.{Level, Logger}\nLogger.getLogger(\"org.apache.spark.rdd.ReliableRDDCheckpointData\").setLevel(Level.INFO)\n\nscala&gt; nums.checkpoint\n18/03/23 00:05:15 INFO ReliableRDDCheckpointData: Done checkpointing RDD 12 to file:/tmp/checkpoints/b1f413dc-3eaf-46a0-99de-d795252035e0/rdd-12, new parent is RDD 13\nres7: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, random: double]\n\n// Save the schema as it is going to use to reconstruct nums dataset from a RDD\nval schema = nums.schema\n\n// Recover nums dataset from the checkpoint files\n// Start from recovering the underlying RDD\n// And create a Dataset based on the RDD\n\n// Get the path to the checkpoint files of the checkpointed RDD of the Dataset\nimport org.apache.spark.sql.execution.LogicalRDD\nval logicalRDD = numsCheckpointed.queryExecution.optimizedPlan.asInstanceOf[LogicalRDD]\nval checkpointFiles = logicalRDD.rdd.getCheckpointFile.get\nscala&gt; println(checkpointFiles)\nfile:/tmp/checkpoints/b1f413dc-3eaf-46a0-99de-d795252035e0/rdd-9\n\n// SparkContext.checkpointFile is a `protected[spark]` method\n// Use :paste -raw mode in Spark shell and define a helper object to \"escape\" the package lock-in\nscala&gt; :paste -raw\n// Entering paste mode (ctrl-D to finish)\n\npackage org.apache.spark\nobject my {\n  import scala.reflect.ClassTag\n  import org.apache.spark.rdd.RDD\n  def recover[T: ClassTag](sc: SparkContext, path: String): RDD[T] = {\n    sc.checkpointFile[T](path)\n  }\n}\n\n// Exiting paste mode, now interpreting.\n\n// Make sure to use the same checkpoint directory\n\nimport org.apache.spark.my\nimport org.apache.spark.sql.catalyst.InternalRow\nval numsRddRecovered = my.recover[InternalRow](spark.sparkContext, checkpointFiles)\nscala&gt; :type numsRddRecovered\norg.apache.spark.rdd.RDD[org.apache.spark.sql.catalyst.InternalRow]\n\n// We have to convert RDD[InternalRow] to DataFrame\n\n// Use :paste -raw again as we use `private[sql]` method\nscala&gt; :pa -raw\n// Entering paste mode (ctrl-D to finish)\n\npackage org.apache.spark.sql\nobject my2 {\n  import org.apache.spark.rdd.RDD\n  import org.apache.spark.sql.{DataFrame, SparkSession}\n  import org.apache.spark.sql.catalyst.InternalRow\n  import org.apache.spark.sql.types.StructType\n  def createDataFrame(spark: SparkSession, catalystRows: RDD[InternalRow], schema: StructType): DataFrame = {\n    spark.internalCreateDataFrame(catalystRows, schema)\n  }\n}\n\n// Exiting paste mode, now interpreting.\n\nimport org.apache.spark.sql.my2\nval numsRecovered = my2.createDataFrame(spark, numsRddRecovered, schema)\nscala&gt; numsRecovered.show\n+---+------------------+\n| id|            random|\n+---+------------------+\n|  0| 0.752877642067488|\n|  1|0.5271005540026181|\n+---+------------------+\n</code></pre> <p>=== [[sparkcontext-setCheckpointDir]] Specifying Checkpoint Directory -- <code>SparkContext.setCheckpointDir</code> Method</p>"},{"location":"checkpointing/#source-scala","title":"[source, scala]","text":""},{"location":"checkpointing/#sparkcontextsetcheckpointdirdirectory-string","title":"SparkContext.setCheckpointDir(directory: String)","text":"<p><code>setCheckpointDir</code> sets the &lt;&gt;. <p>Internally, <code>setCheckpointDir</code>...FIXME</p> <p>=== [[sparkcontext-checkpointFile]] Recovering RDD From Checkpoint Files -- <code>SparkContext.checkpointFile</code> Method</p>"},{"location":"checkpointing/#source-scala_1","title":"[source, scala]","text":""},{"location":"checkpointing/#sparkcontextcheckpointfiledirectory-string","title":"SparkContext.checkpointFile(directory: String)","text":"<p><code>checkpointFile</code> reads (recovers) a RDD from a checkpoint directory.</p> <p>NOTE: <code>SparkContext.checkpointFile</code> is a <code>protected[spark]</code> method so the code to access it has to be in <code>org.apache.spark</code> package.</p> <p>Internally, <code>checkpointFile</code> creates a <code>ReliableCheckpointRDD</code> in a scope.</p>"},{"location":"configuration-properties/","title":"Configuration Properties","text":"<p>Configuration properties (aka settings) allow you to fine-tune a Spark SQL application.</p> <p>Configuration properties are configured in a SparkSession while creating a new instance using config method (e.g. spark.sql.warehouse.dir).</p> <pre><code>import org.apache.spark.sql.SparkSession\nval spark: SparkSession = SparkSession.builder\n.master(\"local[*]\")\n.appName(\"My Spark Application\")\n.config(\"spark.sql.warehouse.dir\", \"c:/Temp\") // (1)!\n.getOrCreate\n</code></pre> <ol> <li>Sets spark.sql.warehouse.dir</li> </ol> <p>You can also set a property using SQL <code>SET</code> command.</p> <pre><code>assert(spark.conf.getOption(\"spark.sql.hive.metastore.version\").isEmpty)\n\nscala&gt; spark.sql(\"SET spark.sql.hive.metastore.version=2.3.2\").show(truncate = false)\n+--------------------------------+-----+\n|key                             |value|\n+--------------------------------+-----+\n|spark.sql.hive.metastore.version|2.3.2|\n+--------------------------------+-----+\n\nassert(spark.conf.get(\"spark.sql.hive.metastore.version\") == \"2.3.2\")\n</code></pre>"},{"location":"configuration-properties/#adaptiveautobroadcastjointhreshold","title":"adaptive.autoBroadcastJoinThreshold <p>spark.sql.adaptive.autoBroadcastJoinThreshold</p> <p>The maximum size (in bytes) of a table to be broadcast when performing a join. <code>-1</code> turns broadcasting off. The default value is same as spark.sql.autoBroadcastJoinThreshold.</p> <p>Used only in Adaptive Query Execution</p> <p>Default: (undefined)</p> <p>Available as SQLConf.ADAPTIVE_AUTO_BROADCASTJOIN_THRESHOLD value.</p>","text":""},{"location":"configuration-properties/#adaptivecustomcostevaluatorclass","title":"adaptive.customCostEvaluatorClass <p>spark.sql.adaptive.customCostEvaluatorClass</p> <p>The fully-qualified class name of the CostEvaluator in Adaptive Query Execution</p> <p>Default: SimpleCostEvaluator</p> <p>Use SQLConf.ADAPTIVE_CUSTOM_COST_EVALUATOR_CLASS method to access the property (in a type-safe way).</p> <p>Used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested for the AQE cost evaluator</li> </ul>","text":""},{"location":"configuration-properties/#adaptiveforceoptimizeskewedjoin","title":"adaptive.forceOptimizeSkewedJoin <p>spark.sql.adaptive.forceOptimizeSkewedJoin</p> <p>Enables OptimizeSkewedJoin physical optimization to be executed even if it introduces extra shuffle</p> <p>Default: <code>false</code></p> <p>Requires spark.sql.adaptive.skewJoin.enabled to be enabled</p> <p>Use SQLConf.ADAPTIVE_FORCE_OPTIMIZE_SKEWED_JOIN to access the property (in a type-safe way).</p> <p>Used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested for the AQE cost evaluator (and creates a SimpleCostEvaluator)</li> <li>OptimizeSkewedJoin physical optimization is executed</li> </ul>","text":""},{"location":"configuration-properties/#adaptiveoptimizerexcludedrules","title":"adaptive.optimizer.excludedRules <p>spark.sql.adaptive.optimizer.excludedRules</p> <p>A comma-separated list of rules (names) to be disabled (excluded) in the AQE Logical Optimizer</p> <p>Default: undefined</p> <p>Use SQLConf.ADAPTIVE_OPTIMIZER_EXCLUDED_RULES to reference the property.</p> <p>Used when:</p> <ul> <li><code>AQEOptimizer</code> is requested for the batches</li> </ul>","text":""},{"location":"configuration-properties/#autobroadcastjointhreshold","title":"autoBroadcastJoinThreshold <p>spark.sql.autoBroadcastJoinThreshold</p> <p>Maximum size (in bytes) for a table that can be broadcast (to all worker nodes) in a join</p> <p>Default: <code>10M</code></p> <p><code>-1</code> (or any negative value) disables broadcasting</p> <p>Use SQLConf.autoBroadcastJoinThreshold method to access the current value.</p>","text":""},{"location":"configuration-properties/#cacheserializer","title":"cache.serializer <p>spark.sql.cache.serializer</p> <p>The name of CachedBatchSerializer implementation to translate SQL data into a format that can more efficiently be cached.</p> <p>Default: org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer</p> <p><code>spark.sql.cache.serializer</code> is a StaticSQLConf</p> <p>Use SQLConf.SPARK_CACHE_SERIALIZER for the name</p> <p>Used when:</p> <ul> <li><code>InMemoryRelation</code> is requested for the CachedBatchSerializer</li> </ul>","text":""},{"location":"configuration-properties/#codegenhugemethodlimit","title":"codegen.hugeMethodLimit <p>spark.sql.codegen.hugeMethodLimit</p> <p>(internal) The maximum bytecode size of a single compiled Java function generated by whole-stage codegen. When the compiled code has a function that exceeds this threshold, the whole-stage codegen is deactivated for this subtree of the query plan.</p> <p>Default: <code>65535</code></p> <p>The default value <code>65535</code> is the largest bytecode size possible for a valid Java method. When running on HotSpot, it may be preferable to set the value to <code>8000</code> (which is the value of <code>HugeMethodLimit</code> in the OpenJDK JVM settings)</p> <p>Use SQLConf.hugeMethodLimit method to access the current value.</p> <p>Used when:</p> <ul> <li><code>WholeStageCodegenExec</code> physical operator is executed</li> </ul>","text":""},{"location":"configuration-properties/#codegenfallback","title":"codegen.fallback <p>spark.sql.codegen.fallback</p> <p>(internal) Whether the whole-stage codegen could be temporary disabled for the part of a query that has failed to compile generated code (<code>true</code>) or not (<code>false</code>).</p> <p>Default: <code>true</code></p> <p>Use SQLConf.wholeStageFallback method to access the current value.</p> <p>Used when:</p> <ul> <li><code>WholeStageCodegenExec</code> physical operator is executed</li> </ul>","text":""},{"location":"configuration-properties/#codegenjoinfulloutershuffledhashjoinenabled","title":"codegen.join.fullOuterShuffledHashJoin.enabled <p>spark.sql.codegen.join.fullOuterShuffledHashJoin.enabled</p> <p>(internal) Enables Whole-Stage Code Generation for FULL OUTER shuffled hash join</p> <p>Default: <code>true</code></p> <p>Use SQLConf.ENABLE_FULL_OUTER_SHUFFLED_HASH_JOIN_CODEGEN to access the property</p>","text":""},{"location":"configuration-properties/#columnvectoroffheapenabled","title":"columnVector.offheap.enabled <p>spark.sql.columnVector.offheap.enabled</p> <p>(internal) Enables OffHeapColumnVector (<code>true</code>) or OnHeapColumnVector (<code>false</code>) in ColumnarBatch</p> <p>Default: <code>false</code></p> <p>Use SQLConf.offHeapColumnVectorEnabled for the current value</p> <p>Used when:</p> <ul> <li><code>RowToColumnarExec</code> physical operator is requested to doExecuteColumnar</li> <li><code>DefaultCachedBatchSerializer</code> is requested to <code>vectorTypes</code> and <code>convertCachedBatchToColumnarBatch</code></li> <li><code>ParquetFileFormat</code> is requested to vectorTypes and buildReaderWithPartitionValues</li> <li><code>ParquetPartitionReaderFactory</code> is created</li> </ul>","text":""},{"location":"configuration-properties/#defaultcolumnenabled","title":"defaultColumn.enabled <p>spark.sql.defaultColumn.enabled</p> <p>(internal) When true, allows CREATE TABLE, REPLACE TABLE, and ALTER COLUMN statements to set or update default values for specific columns. Following INSERT, MERGE, and UPDATE statements may then omit these values and their values will be injected automatically instead.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.enableDefaultColumns for the current value</p> <p>Used when:</p> <ul> <li><code>AstBuilder</code> is requested to visitCreateOrReplaceTableColType, visitQualifiedColTypeWithPosition, visitAlterTableAlterColumn</li> <li>ResolveDefaultColumns logical resolution rule is executed, constantFoldCurrentDefaultsToExistDefaults, validateCatalogForDefaultValue, validateTableProviderForDefaultValue</li> </ul>","text":""},{"location":"configuration-properties/#exchangereuse","title":"exchange.reuse <p>spark.sql.exchange.reuse</p> <p>(internal) When enabled (<code>true</code>), the Spark planner will find duplicated exchanges and subqueries and re-use them.</p> <p>When disabled (<code>false</code>), ReuseExchange and ReuseSubquery physical optimizations (that the Spark planner uses for physical query plan optimization) do nothing.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.exchangeReuseEnabled for the current value</p>","text":""},{"location":"configuration-properties/#executionreplacehashwithsortagg","title":"execution.replaceHashWithSortAgg <p>spark.sql.execution.replaceHashWithSortAgg</p> <p>internal Enables replacing hash aggregate operators with sort aggregate based on children's ordering</p> <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li>ReplaceHashWithSortAgg physical optimization is executed</li> </ul>","text":""},{"location":"configuration-properties/#spark.sql.files","title":"spark.sql.files","text":""},{"location":"configuration-properties/#spark.sql.files.maxPartitionBytes","title":"maxPartitionBytes <p>spark.sql.files.maxPartitionBytes</p> <p>Maximum number of bytes to pack into a single partition when reading files for file-based data sources (e.g., Parquet)</p> <p>Default: <code>128MB</code> (like <code>parquet.block.size</code>)</p> <p>Use SQLConf.filesMaxPartitionBytes for the current value</p> <p>Used when:</p> <ul> <li><code>FilePartition</code> is requested for maxSplitBytes</li> </ul>","text":""},{"location":"configuration-properties/#spark.sql.files.maxRecordsPerFile","title":"maxRecordsPerFile <p>spark.sql.files.maxRecordsPerFile</p> <p>Maximum number of records to write out to a single file. If <code>0</code> or negative, there is no limit.</p> <p>Default: <code>0</code></p> <p>Use SQLConf.maxRecordsPerFile method for the current value</p> <p>Used when:</p> <ul> <li><code>FileFormatWriter</code> is requested to write data out</li> <li><code>FileWrite</code> is requested for a BatchWrite (and creates a WriteJobDescription)</li> </ul>","text":""},{"location":"configuration-properties/#spark.sql.files.minPartitionNum","title":"minPartitionNum <p>spark.sql.files.minPartitionNum</p> <p>Hint about the minimum number of partitions for file-based data sources (e.g., Parquet)</p> <p>Default: spark.sql.leafNodeDefaultParallelism</p> <p>Use SQLConf.filesMinPartitionNum for the current value</p> <p>Used when:</p> <ul> <li><code>FilePartition</code> is requested for maxSplitBytes</li> </ul>","text":""},{"location":"configuration-properties/#spark.sql.files.openCostInBytes","title":"openCostInBytes <p>spark.sql.files.openCostInBytes</p> <p>(internal) The estimated cost to open a file, measured by the number of bytes could be scanned at the same time (to include multiple files into a partition). Effective only for file-based sources such as Parquet, JSON and ORC.</p> <p>Default: <code>4MB</code></p> <p>It's better to over-estimate it, then the partitions with small files will be faster than partitions with bigger files (which is scheduled first).</p> <p>Use SQLConf.filesOpenCostInBytes for the current value</p> <p>Used when:</p> <ul> <li><code>FileSourceScanExec</code> physical operator is requested to create an RDD for a non-bucketed read</li> <li><code>FilePartition</code> is requested to getFilePartitions and maxSplitBytes</li> </ul>","text":""},{"location":"configuration-properties/#hivefilesourcepartitionfilecachesize","title":"hive.filesourcePartitionFileCacheSize <p>spark.sql.hive.filesourcePartitionFileCacheSize</p> <p>When greater than <code>0</code>, enables caching of partition file metadata in memory (using SharedInMemoryCache). All tables share a cache that can use up to specified num bytes for file metadata.</p> <p>Requires spark.sql.hive.manageFilesourcePartitions to be enabled</p> <p>Default: <code>250 * 1024 * 1024</code></p> <p>Use SQLConf.filesourcePartitionFileCacheSize for the current value</p> <p>Used when:</p> <ul> <li><code>FileStatusCache</code> is requested to look up the system-wide FileStatusCache</li> </ul>","text":""},{"location":"configuration-properties/#hivemanagefilesourcepartitions","title":"hive.manageFilesourcePartitions <p>spark.sql.hive.manageFilesourcePartitions</p> <p>Enables metastore partition management for file source tables.</p> <p>This includes both datasource and Hive tables. When partition management is enabled, datasource tables store partition in the Hive metastore, and use the metastore to prune partitions during query planning when spark.sql.hive.metastorePartitionPruning is enabled</p> <p>Default: <code>true</code></p> <p>Use SQLConf.manageFilesourcePartitions for the current value</p> <p>Used when:</p> <ul> <li><code>HiveMetastoreCatalog</code> is requested to convert a HiveTableRelation to a LogicalRelation over a HadoopFsRelation</li> <li>CreateDataSourceTableCommand, CreateDataSourceTableAsSelectCommand and InsertIntoHadoopFsRelationCommand logical commands are executed</li> <li><code>DDLUtils</code> utility is used to <code>verifyPartitionProviderIsHive</code></li> <li><code>DataSource</code> is requested to resolve a BaseRelation (for file-based data source tables and creates a <code>HadoopFsRelation</code>)</li> <li><code>FileStatusCache</code> is created</li> <li><code>V2SessionCatalog</code> is requested to create a table (deprecated)</li> </ul>","text":""},{"location":"configuration-properties/#inmemorycolumnarstoragepartitionpruning","title":"inMemoryColumnarStorage.partitionPruning <p>spark.sql.inMemoryColumnarStorage.partitionPruning</p> <p>(internal) Enables partition pruning for in-memory columnar tables</p> <p>Default: <code>true</code></p> <p>Use SQLConf.inMemoryPartitionPruning for the current value</p> <p>Used when:</p> <ul> <li><code>InMemoryTableScanExec</code> physical operator is requested to filter cached column batches</li> </ul>","text":""},{"location":"configuration-properties/#optimizercanchangecachedplanoutputpartitioning","title":"optimizer.canChangeCachedPlanOutputPartitioning <p>spark.sql.optimizer.canChangeCachedPlanOutputPartitioning</p> <p>(internal) Whether to forcibly enable some optimization rules that can change the output partitioning of a cached query when executing it for caching. If it is set to true, queries may need an extra shuffle to read the cached data. This configuration is disabled by default. Currently, the optimization rules enabled by this configuration are spark.sql.adaptive.enabled and spark.sql.sources.bucketing.autoBucketedScan.enabled.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.CAN_CHANGE_CACHED_PLAN_OUTPUT_PARTITIONING to access the property</p>","text":""},{"location":"configuration-properties/#optimizerdecorrelateinnerqueryenabled","title":"optimizer.decorrelateInnerQuery.enabled <p>spark.sql.optimizer.decorrelateInnerQuery.enabled</p> <p>(internal) Decorrelates inner queries by eliminating correlated references and build domain joins</p> <p>Default: <code>true</code></p> <p>Use SQLConf.decorrelateInnerQueryEnabled for the current value</p>","text":""},{"location":"configuration-properties/#optimizerdynamicpartitionpruningfallbackfilterratio","title":"optimizer.dynamicPartitionPruning.fallbackFilterRatio <p>spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio</p> <p>(internal) When statistics are not available or configured not to be used, this config will be used as the fallback filter ratio for computing the data size of the partitioned table after dynamic partition pruning, in order to evaluate if it is worth adding an extra subquery as the pruning filter if broadcast reuse is not applicable.</p> <p>Default: <code>0.5</code></p> <p>Use SQLConf.dynamicPartitionPruningFallbackFilterRatio method to access the current value.</p>","text":""},{"location":"configuration-properties/#optimizerdynamicpartitionpruningpruningsideextrafilterratio","title":"optimizer.dynamicPartitionPruning.pruningSideExtraFilterRatio <p>spark.sql.optimizer.dynamicPartitionPruning.pruningSideExtraFilterRatio</p> <p>(internal) When filtering side doesn't support broadcast by join type, and doing DPP means running an extra query that may have significant overhead. This config will be used as the extra filter ratio for computing the data size of the pruning side after DPP, in order to evaluate if it is worth adding an extra subquery as the pruning filter.</p> <p>Must be a double between <code>0.0</code> and <code>1.0</code></p> <p>Default: <code>0.04</code></p> <p>Use SQLConf.dynamicPartitionPruningPruningSideExtraFilterRatio to access the current value.</p>","text":""},{"location":"configuration-properties/#optimizerdynamicpartitionpruningusestats","title":"optimizer.dynamicPartitionPruning.useStats <p>spark.sql.optimizer.dynamicPartitionPruning.useStats</p> <p>(internal) When true, distinct count statistics will be used for computing the data size of the partitioned table after dynamic partition pruning, in order to evaluate if it is worth adding an extra subquery as the pruning filter if broadcast reuse is not applicable.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.dynamicPartitionPruningUseStats for the current value</p> <p>Used when:</p> <ul> <li>PartitionPruning logical optimization rule is executed</li> </ul>","text":""},{"location":"configuration-properties/#optimizerdynamicpartitionpruningenabled","title":"optimizer.dynamicPartitionPruning.enabled <p>spark.sql.optimizer.dynamicPartitionPruning.enabled</p> <p>Enables generating predicates for partition columns used as join keys</p> <p>Default: <code>true</code></p> <p>Use SQLConf.dynamicPartitionPruningEnabled for the current value</p> <p>Used to control whether to execute the following optimizations or skip them altogether:</p> <ul> <li>CleanupDynamicPruningFilters logical optimization</li> <li>PartitionPruning logical optimization</li> <li>PlanAdaptiveDynamicPruningFilters physical optimization</li> <li>PlanDynamicPruningFilters physical optimization</li> </ul>","text":""},{"location":"configuration-properties/#optimizerdynamicpartitionpruningreusebroadcastonly","title":"optimizer.dynamicPartitionPruning.reuseBroadcastOnly <p>spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly</p> <p>(internal) When <code>true</code>, dynamic partition pruning will only apply when the broadcast exchange of a broadcast hash join operation can be reused as the dynamic pruning filter.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.dynamicPartitionPruningReuseBroadcastOnly for the current value</p> <p>Used when:</p> <ul> <li>PartitionPruning logical optimization is executed (and requested to insertPredicate)</li> </ul>","text":""},{"location":"configuration-properties/#optimizerenablecsvexpressionoptimization","title":"optimizer.enableCsvExpressionOptimization <p>spark.sql.optimizer.enableCsvExpressionOptimization</p> <p>Whether to optimize CSV expressions in SQL optimizer. It includes pruning unnecessary columns from <code>from_csv</code>.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.csvExpressionOptimization for the current value</p>","text":""},{"location":"configuration-properties/#optimizerexcludedrules","title":"optimizer.excludedRules <p>spark.sql.optimizer.excludedRules</p> <p>Comma-separated list of fully-qualified class names of the optimization rules that should be disabled (excluded) from logical query optimization.</p> <p>Default: <code>(empty)</code></p> <p>Use SQLConf.optimizerExcludedRules method to access the current value.</p>  <p>Important</p> <p>It is not guaranteed that all the rules to be excluded will eventually be excluded, as some rules are non-excludable.</p>","text":""},{"location":"configuration-properties/#optimizerexpressionnestedpruningenabled","title":"optimizer.expression.nestedPruning.enabled <p>spark.sql.optimizer.expression.nestedPruning.enabled</p> <p>(internal) Prune nested fields from expressions in an operator which are unnecessary in satisfying a query. Note that this optimization doesn't prune nested fields from physical data source scanning. For pruning nested fields from scanning, please use spark.sql.optimizer.nestedSchemaPruning.enabled config.</p> <p>Default: <code>true</code></p>","text":""},{"location":"configuration-properties/#optimizerinsetconversionthreshold","title":"optimizer.inSetConversionThreshold <p>spark.sql.optimizer.inSetConversionThreshold</p> <p>(internal) The threshold of set size for <code>InSet</code> conversion.</p> <p>Default: <code>10</code></p> <p>Use SQLConf.optimizerInSetConversionThreshold method to access the current value.</p>","text":""},{"location":"configuration-properties/#optimizerinsetswitchthreshold","title":"optimizer.inSetSwitchThreshold <p>spark.sql.optimizer.inSetSwitchThreshold</p> <p>(internal) Configures the max set size in InSet for which Spark will generate code with switch statements. This is applicable only to bytes, shorts, ints, dates.</p> <p>Must be non-negative and less than or equal to 600.</p> <p>Default: <code>400</code></p>","text":""},{"location":"configuration-properties/#optimizermaxiterations","title":"optimizer.maxIterations <p>spark.sql.optimizer.maxIterations</p> <p>Maximum number of iterations for Analyzer and Logical Optimizer.</p> <p>Default: <code>100</code></p>","text":""},{"location":"configuration-properties/#optimizernestedschemapruningenabled","title":"optimizer.nestedSchemaPruning.enabled <p>spark.sql.optimizer.nestedSchemaPruning.enabled</p> <p>(internal) Prune nested fields from the output of a logical relation that are not necessary in satisfying a query. This optimization allows columnar file format readers to avoid reading unnecessary nested column data.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.nestedSchemaPruningEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#optimizernestedpredicatepushdownsupportedfilesources","title":"optimizer.nestedPredicatePushdown.supportedFileSources <p>spark.sql.optimizer.nestedPredicatePushdown.supportedFileSources</p> <p>(internal) A comma-separated list of data source short names or fully qualified data source implementation class names for which Spark tries to push down predicates for nested columns and/or names containing <code>dots</code> to data sources. This configuration is only effective with file-based data source in DSv1. Currently, Parquet implements both optimizations while ORC only supports predicates for names containing <code>dots</code>. The other data sources don't support this feature yet.</p> <p>Default: <code>parquet,orc</code></p>","text":""},{"location":"configuration-properties/#optimizeroptimizeonerowrelationsubquery","title":"optimizer.optimizeOneRowRelationSubquery <p>spark.sql.optimizer.optimizeOneRowRelationSubquery</p> <p>(internal) When <code>true</code>, the optimizer will inline subqueries with <code>OneRowRelation</code> leaf nodes</p> <p>Default: <code>true</code></p> <p>Use SQLConf.OPTIMIZE_ONE_ROW_RELATION_SUBQUERY method to access the property (in a type-safe way)</p>","text":""},{"location":"configuration-properties/#optimizerplanchangelogbatches","title":"optimizer.planChangeLog.batches <p>spark.sql.optimizer.planChangeLog.batches</p> <p>(internal) Configures a list of batches to be logged in the optimizer, in which the batches are specified by their batch names and separated by comma.</p> <p>Default: <code>(undefined)</code></p>","text":""},{"location":"configuration-properties/#optimizerplanchangeloglevel","title":"optimizer.planChangeLog.level <p>spark.sql.optimizer.planChangeLog.level</p> <p>(internal) Configures the log level for logging the change from the original plan to the new plan after a rule or batch is applied. The value can be <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code> or <code>ERROR</code>.</p> <p>Default: <code>TRACE</code></p>","text":""},{"location":"configuration-properties/#optimizerplanchangelogrules","title":"optimizer.planChangeLog.rules <p>spark.sql.optimizer.planChangeLog.rules</p> <p>(internal) Configures a list of rules to be logged in the optimizer, in which the rules are specified by their rule names and separated by comma.</p> <p>Default: <code>(undefined)</code></p>","text":""},{"location":"configuration-properties/#optimizerreplaceexceptwithfilter","title":"optimizer.replaceExceptWithFilter <p>spark.sql.optimizer.replaceExceptWithFilter</p> <p>(internal) When <code>true</code>, the apply function of the rule verifies whether the right node of the except operation is of type Filter or Project followed by Filter. If yes, the rule further verifies 1) Excluding the filter operations from the right (as well as the left node, if any) on the top, whether both the nodes evaluates to a same result. 2) The left and right nodes don't contain any SubqueryExpressions. 3) The output column names of the left node are distinct. If all the conditions are met, the rule will replace the except operation with a Filter by flipping the filter condition(s) of the right node.</p> <p>Default: <code>true</code></p>","text":""},{"location":"configuration-properties/#optimizerruntimebloomfilterenabled","title":"optimizer.runtime.bloomFilter.enabled <p>spark.sql.optimizer.runtime.bloomFilter.enabled</p> <p>Enables a bloom filter on one side of a shuffle join if the other side has a selective predicate (to reduce the amount of shuffle data)</p> <p>Default: <code>false</code></p> <p>Use SQLConf.runtimeFilterBloomFilterEnabled for the current value</p> <p>Used when:</p> <ul> <li>InjectRuntimeFilter logical optimization is executed</li> </ul>","text":""},{"location":"configuration-properties/#optimizerruntimebloomfilterexpectednumitems","title":"optimizer.runtime.bloomFilter.expectedNumItems <p>spark.sql.optimizer.runtime.bloomFilter.expectedNumItems</p> <p>The default number of expected items for the runtime bloomfilter</p> <p>Default: <code>1000000L</code></p> <p>SQLConf.RUNTIME_BLOOM_FILTER_EXPECTED_NUM_ITEMS</p> <p>Used when:</p> <ul> <li>BloomFilterAggregate expression is created</li> </ul>","text":""},{"location":"configuration-properties/#optimizerruntimebloomfiltermaxnumbits","title":"optimizer.runtime.bloomFilter.maxNumBits <p>spark.sql.optimizer.runtime.bloomFilter.maxNumBits</p> <p>Maximum number of bits for the runtime bloom filter</p> <p>Default: <code>67108864L</code> (8MB)</p> <p>Must be a non-zero positive number</p> <p>SQLConf.RUNTIME_BLOOM_FILTER_MAX_NUM_BITS</p> <p>Used when:</p> <ul> <li><code>BloomFilterAggregate</code> is requested to checkInputDataTypes and for the numBits</li> </ul>","text":""},{"location":"configuration-properties/#optimizerruntimefilternumberthreshold","title":"optimizer.runtimeFilter.number.threshold <p>spark.sql.optimizer.runtimeFilter.number.threshold</p> <p>The total number of injected runtime filters (non-DPP) for a single query. This is to prevent driver OOMs with too many Bloom filters.</p> <p>Default: <code>10</code></p> <p>Must be a non-zero positive number</p> <p>SQLConf.RUNTIME_FILTER_NUMBER_THRESHOLD</p> <p>Used when:</p> <ul> <li>InjectRuntimeFilter logical optimization is executed</li> </ul>","text":""},{"location":"configuration-properties/#optimizerruntimefiltersemijoinreductionenabled","title":"optimizer.runtimeFilter.semiJoinReduction.enabled <p>spark.sql.optimizer.runtimeFilter.semiJoinReduction.enabled</p> <p>Enables inserting a semi join on one side of a shuffle join if the other side has a selective predicate (to reduce the amount of shuffle data)</p> <p>Default: <code>false</code></p> <p>Use SQLConf.runtimeFilterSemiJoinReductionEnabled for the current value</p> <p>Used when:</p> <ul> <li>InjectRuntimeFilter logical optimization is executed</li> </ul>","text":""},{"location":"configuration-properties/#optimizerserializernestedschemapruningenabled","title":"optimizer.serializer.nestedSchemaPruning.enabled <p>spark.sql.optimizer.serializer.nestedSchemaPruning.enabled</p> <p>(internal) Prune nested fields from object serialization operator which are unnecessary in satisfying a query. This optimization allows object serializers to avoid executing unnecessary nested expressions.</p> <p>Default: <code>true</code></p>","text":""},{"location":"configuration-properties/#spark.sql.parquet","title":"spark.sql.parquet","text":""},{"location":"configuration-properties/#aggregatepushdown","title":"aggregatePushdown <p>spark.sql.parquet.aggregatePushdown</p> <p>Controls aggregate pushdown in parquet connector</p> <p>Supports MIN, MAX and COUNT as aggregate expression:</p> <ul> <li>For MIN/MAX, support boolean, integer, float and date types.</li> <li>For COUNT, support all data types.</li> </ul> <p>If statistics is missing from any Parquet file footer, exception would be thrown.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.parquetAggregatePushDown for the current value</p> <p>Used when:</p> <ul> <li><code>ParquetScanBuilder</code> is requested to pushAggregation</li> </ul>","text":""},{"location":"configuration-properties/#columnarreaderbatchsize","title":"columnarReaderBatchSize <p>spark.sql.parquet.columnarReaderBatchSize</p> <p>The number of rows to include in a parquet vectorized reader batch (the capacity of VectorizedParquetRecordReader)</p> <p>Default: <code>4096</code> (4k)</p> <p>The number should be carefully chosen to minimize overhead and avoid OOMs while reading data.</p> <p>Use SQLConf.parquetVectorizedReaderBatchSize for the current value</p> <p>Used when:</p> <ul> <li><code>ParquetFileFormat</code> is requested for a data reader (and creates a VectorizedParquetRecordReader for Vectorized Parquet Decoding)</li> <li><code>ParquetPartitionReaderFactory</code> is created</li> <li><code>WritableColumnVector</code> is requested to <code>reserve</code> required capacity (and fails)</li> </ul>","text":""},{"location":"configuration-properties/#enablenestedcolumnvectorizedreader","title":"enableNestedColumnVectorizedReader <p>spark.sql.parquet.enableNestedColumnVectorizedReader</p> <p>Enables vectorized parquet decoding for nested columns (e.g., arrays, structs and maps). Requires spark.sql.parquet.enableVectorizedReader to be enabled</p> <p>Default: <code>true</code></p> <p>Use SQLConf.parquetVectorizedReaderNestedColumnEnabled for the current value</p> <p>Used when:</p> <ul> <li><code>ParquetUtils</code> is requested to isBatchReadSupported</li> </ul>","text":""},{"location":"configuration-properties/#filterpushdown","title":"filterPushdown <p>spark.sql.parquet.filterPushdown</p> <p>Controls filter predicate push-down optimization for parquet connector</p> <p>Default: <code>true</code></p> <p>Use SQLConf.parquetFilterPushDown for the current value</p> <p>Used when:</p> <ul> <li><code>ParquetFileFormat</code> is created</li> <li><code>ParquetPartitionReaderFactory</code> is created</li> <li><code>ParquetScanBuilder</code> is requested to pushDataFilters</li> </ul>","text":""},{"location":"configuration-properties/#filterpushdownstringpredicate","title":"filterPushdown.stringPredicate <p>spark.sql.parquet.filterPushdown.stringPredicate</p> <p>(internal) Controls Parquet filter push-down optimization for string predicate such as startsWith/endsWith/contains functions. Effective only with spark.sql.parquet.filterPushdown enabled.</p> <p>Default: spark.sql.parquet.filterPushdown.string.startsWith</p> <p>Use SQLConf.parquetFilterPushDownStringPredicate for the current value</p> <p>Used when:</p> <ul> <li><code>ParquetFileFormat</code> is requested to buildReaderWithPartitionValues</li> <li><code>ParquetPartitionReaderFactory</code> is created</li> <li><code>ParquetScanBuilder</code> is requested to pushDataFilters</li> </ul>","text":""},{"location":"configuration-properties/#mergeschema","title":"mergeSchema <p>spark.sql.parquet.mergeSchema</p> <p>Controls whether the Parquet data source merges schemas collected from all data files or not. If <code>false</code>, the schema is picked from the summary file or a random data file if no summary file is available.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.isParquetSchemaMergingEnabled for the current value</p> <p>Parquet option (of higher priority): mergeSchema</p> <p>Used when:</p> <ul> <li><code>ParquetOptions</code> is created (and initializes mergeSchema option)</li> </ul>","text":""},{"location":"configuration-properties/#spark.sql.parquet.output.committer.class","title":"output.committer.class <p>spark.sql.parquet.output.committer.class</p> <p>(internal) The output committer class used by parquet data source. The specified class needs to be a subclass of <code>org.apache.hadoop.mapreduce.OutputCommitter</code>. Typically, it's also a subclass of <code>org.apache.parquet.hadoop.ParquetOutputCommitter</code>. If it is not, then metadata summaries will never be created, irrespective of the value of <code>parquet.summary.metadata.level</code>.</p> <p>Default: <code>org.apache.parquet.hadoop.ParquetOutputCommitter</code></p> <p>Use SQLConf.parquetOutputCommitterClass for the current value</p> <p>Used when:</p> <ul> <li><code>ParquetUtils</code> is requested to prepareWrite</li> </ul>","text":""},{"location":"configuration-properties/#spark.sql.sources","title":"spark.sql.sources","text":""},{"location":"configuration-properties/#spark.sql.sources.bucketing.enabled","title":"bucketing.enabled <p>spark.sql.sources.bucketing.enabled</p> <p>Enables Bucketing</p> <p>Default: <code>true</code></p> <p>When disabled (i.e. <code>false</code>), bucketed tables are considered regular (non-bucketed) tables.</p> <p>Use SQLConf.bucketingEnabled method for the current value</p>","text":""},{"location":"configuration-properties/#spark.sql.sources.commitProtocolClass","title":"commitProtocolClass <p>spark.sql.sources.commitProtocolClass</p> <p>(internal) Fully-qualified class name of a <code>FileCommitProtocol</code> (Spark Core) for Transactional Writes</p> <p>Default: SQLHadoopMapReduceCommitProtocol</p> <p>Use SQLConf.fileCommitProtocolClass method for the current value</p> <p>Used when:</p> <ul> <li><code>FileWrite</code> is requested for a BatchWrite</li> <li>InsertIntoHadoopFsRelationCommand logical command is executed</li> <li><code>SaveAsHiveFile</code> is requested to saveAsHiveFile</li> </ul>","text":""},{"location":"configuration-properties/#spark.sql.sources.outputCommitterClass","title":"outputCommitterClass <p>spark.sql.sources.outputCommitterClass</p> <p>(internal) The fully-qualified class name of the user-defined Hadoop OutputCommitter for SQLHadoopMapReduceCommitProtocol</p> <p>Default: (undefined)</p> <p>Use SQLConf.OUTPUT_COMMITTER_CLASS to access the property</p>  <p>Note</p> <p><code>ParquetUtils</code> uses spark.sql.parquet.output.committer.class or the default <code>ParquetOutputCommitter</code> instead.</p>","text":""},{"location":"configuration-properties/#sparksqlobjecthashaggregatesortbasedfallbackthreshold","title":"spark.sql.objectHashAggregate.sortBased.fallbackThreshold <p>(internal) The number of entires in an in-memory hash map (to store aggregation buffers per grouping keys) before ObjectHashAggregateExec (ObjectAggregationIterator, precisely) falls back to sort-based aggregation</p> <p>Default: <code>128</code> (entries)</p> <p>Use SQLConf.objectAggSortBasedFallbackThreshold for the current value</p> <p>Learn more in Demo: ObjectHashAggregateExec and Sort-Based Fallback Tasks</p>","text":""},{"location":"configuration-properties/#sparksqladaptiveoptimizeskewsinrebalancepartitionsenabled","title":"spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled <p>When <code>true</code> and spark.sql.adaptive.enabled is <code>true</code>, Spark SQL will optimize the skewed shuffle partitions in RebalancePartitions and split them to smaller ones according to the target size (specified by spark.sql.adaptive.advisoryPartitionSizeInBytes), to avoid data skew</p> <p>Default: <code>true</code></p> <p>Use SQLConf.ADAPTIVE_OPTIMIZE_SKEWS_IN_REBALANCE_PARTITIONS_ENABLED method to access the property (in a type-safe way)</p>","text":""},{"location":"configuration-properties/#sparksqlcodegenaggregatefasthashmapcapacitybit","title":"spark.sql.codegen.aggregate.fastHashMap.capacityBit <p>(internal) Capacity for the max number of rows to be held in memory by the fast hash aggregate product operator. The bit is not for actual value, but the actual <code>numBuckets</code> is determined by <code>loadFactor</code> (e.g., the default bit value <code>16</code>, the actual numBuckets is <code>((1 &lt;&lt; 16) / 0.5</code>).</p> <p>Default: <code>16</code></p> <p>Must be in the range of <code>[10, 30]</code> (inclusive)</p> <p>Use SQLConf.fastHashAggregateRowMaxCapacityBit for the current value</p> <p>Used when:</p> <ul> <li><code>HashAggregateExec</code> physical operator is requested to doProduceWithKeys</li> </ul>","text":""},{"location":"configuration-properties/#sparksqlcodegenaggregatemaptwolevelenabled","title":"spark.sql.codegen.aggregate.map.twolevel.enabled <p>(internal) Enable two-level aggregate hash map. When enabled, records will first be inserted/looked-up at a 1<sup>st</sup>-level, small, fast map, and then fallback to a 2<sup>nd</sup>-level, larger, slower map when 1<sup>st</sup> level is full or keys cannot be found. When disabled, records go directly to the 2<sup>nd</sup> level.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.enableTwoLevelAggMap for the current value</p> <p>Used when:</p> <ul> <li><code>HashAggregateExec</code> physical operator is requested to doProduceWithKeys</li> </ul>","text":""},{"location":"configuration-properties/#sparksqlcodegenaggregatemapvectorizedenable","title":"spark.sql.codegen.aggregate.map.vectorized.enable <p>(internal) Enables vectorized aggregate hash map. For testing/benchmarking only.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.enableVectorizedHashMap for the current value</p> <p>Used when:</p> <ul> <li><code>HashAggregateExec</code> physical operator is requested to enableTwoLevelHashMap, doProduceWithKeys</li> </ul>","text":""},{"location":"configuration-properties/#sparksqlcodegenaggregatemaptwolevelpartialonly","title":"spark.sql.codegen.aggregate.map.twolevel.partialOnly <p>(internal) Enables two-level aggregate hash map for partial aggregate only, because final aggregate might get more distinct keys compared to partial aggregate. \"Overhead of looking up 1<sup>st</sup>-level map might dominate when having a lot of distinct keys.</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li>HashAggregateExec physical operator is requested to checkIfFastHashMapSupported</li> </ul>","text":""},{"location":"configuration-properties/#sparksqllegacyallownonemptylocationinctas","title":"spark.sql.legacy.allowNonEmptyLocationInCTAS <p>(internal) When <code>false</code>, CTAS with LOCATION throws an analysis exception if the location is not empty.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.allowNonEmptyLocationInCTAS for the current value</p>","text":""},{"location":"configuration-properties/#sparksqllegacyallowautogeneratedaliasforview","title":"spark.sql.legacy.allowAutoGeneratedAliasForView <p>(internal) When <code>true</code>, it's allowed to use an input query without explicit alias when creating a permanent view.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.allowAutoGeneratedAliasForView for the current value</p>","text":""},{"location":"configuration-properties/#sparksqlsessionwindowbufferspillthreshold","title":"spark.sql.sessionWindow.buffer.spill.threshold <p>(internal) The threshold for number of rows to be spilled by window operator. Note that the buffer is used only for the query Spark SQL cannot apply aggregations on determining session window.</p> <p>Default: spark.shuffle.spill.numElementsForceSpillThreshold</p> <p>Use SQLConf.sessionWindowBufferSpillThreshold for the current value</p>","text":""},{"location":"configuration-properties/#sparksqlexecutionarrowpysparkselfdestructenabled","title":"spark.sql.execution.arrow.pyspark.selfDestruct.enabled <p>(Experimental) When <code>true</code>, make use of Apache Arrow's self-destruct and split-blocks options for columnar data transfers in PySpark, when converting from Arrow to Pandas. This reduces memory usage at the cost of some CPU time. Applies to: <code>pyspark.sql.DataFrame.toPandas</code> when spark.sql.execution.arrow.pyspark.enabled is <code>true</code>.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.arrowPySparkSelfDestructEnabled for the current value</p>","text":""},{"location":"configuration-properties/#sparksqllegacyallowstarwithsingletableidentifierincount","title":"spark.sql.legacy.allowStarWithSingleTableIdentifierInCount <p>(internal) When <code>true</code>, the SQL function <code>count</code> is allowed to take a single <code>tblName.*</code> as parameter</p> <p>Default: <code>false</code></p> <p>Use SQLConf.allowStarWithSingleTableIdentifierInCount for the current value</p>","text":""},{"location":"configuration-properties/#sparksqlsessionwindowbufferinmemorythreshold","title":"spark.sql.sessionWindow.buffer.in.memory.threshold <p>(internal) Threshold for number of windows guaranteed to be held in memory by the session window operator. Note that the buffer is used only for the query Spark SQL cannot apply aggregations on determining session window.</p> <p>Default: <code>4096</code></p> <p>Use SQLConf.sessionWindowBufferInMemoryThreshold for the current value</p>","text":""},{"location":"configuration-properties/#sparksqlorcenablenestedcolumnvectorizedreader","title":"spark.sql.orc.enableNestedColumnVectorizedReader <p>Enables vectorized orc decoding for nested column</p> <p>Default: <code>false</code></p> <p>Use SQLConf.orcVectorizedReaderNestedColumnEnabled for the current value</p>","text":""},{"location":"configuration-properties/#sparksqladaptiveforceapply","title":"spark.sql.adaptive.forceApply <p>(internal) When <code>true</code> (together with spark.sql.adaptive.enabled enabled), Spark will force apply adaptive query execution for all supported queries.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.ADAPTIVE_EXECUTION_FORCE_APPLY method to access the property (in a type-safe way).</p>","text":""},{"location":"configuration-properties/#sparksqladaptivecoalescepartitionsenabled","title":"spark.sql.adaptive.coalescePartitions.enabled <p>Controls coalescing shuffle partitions</p> <p>When <code>true</code> and spark.sql.adaptive.enabled is enabled, Spark will coalesce contiguous shuffle partitions according to the target size (specified by spark.sql.adaptive.advisoryPartitionSizeInBytes), to avoid too many small tasks.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.coalesceShufflePartitionsEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqladaptivecoalescepartitionsminpartitionsize","title":"spark.sql.adaptive.coalescePartitions.minPartitionSize <p>The minimum size (in bytes unless specified) of shuffle partitions after coalescing. This is useful when the adaptively calculated target size is too small during partition coalescing</p> <p>Default: <code>1MB</code></p> <p>Use SQLConf.coalesceShufflePartitionsEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqladaptivecoalescepartitionsparallelismfirst","title":"spark.sql.adaptive.coalescePartitions.parallelismFirst <p>When <code>true</code>, Spark does not respect the target size specified by spark.sql.adaptive.advisoryPartitionSizeInBytes when coalescing contiguous shuffle partitions, but adaptively calculate the target size according to the default parallelism of the Spark cluster. The calculated size is usually smaller than the configured target size. This is to maximize the parallelism and avoid performance regression when enabling adaptive query execution. It's recommended to set this config to <code>false</code> and respect the configured target size.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.coalesceShufflePartitionsEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqladaptiveadvisorypartitionsizeinbytes","title":"spark.sql.adaptive.advisoryPartitionSizeInBytes <p>The advisory size in bytes of the shuffle partition during adaptive optimization (when spark.sql.adaptive.enabled is enabled). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.</p> <p>Default: <code>64MB</code></p> <p>Fallback Property: <code>spark.sql.adaptive.shuffle.targetPostShuffleInputSize</code></p> <p>Use SQLConf.ADVISORY_PARTITION_SIZE_IN_BYTES to reference the name.</p>","text":""},{"location":"configuration-properties/#sparksqladaptivecoalescepartitionsminpartitionsize_1","title":"spark.sql.adaptive.coalescePartitions.minPartitionSize <p>The minimum size (in bytes) of shuffle partitions after coalescing.</p> <p>Useful when the adaptively calculated target size is too small during partition coalescing.</p> <p>Default: <code>(undefined)</code></p> <p>Must be positive</p> <p>Used when:</p> <ul> <li>CoalesceShufflePartitions adaptive physical optimization is executed</li> </ul>","text":""},{"location":"configuration-properties/#sparksqladaptivecoalescepartitionsinitialpartitionnum","title":"spark.sql.adaptive.coalescePartitions.initialPartitionNum <p>The initial number of shuffle partitions before coalescing.</p> <p>By default it equals to spark.sql.shuffle.partitions. If not set, the default value is the default parallelism of the Spark cluster. This configuration only has an effect when spark.sql.adaptive.enabled and spark.sql.adaptive.coalescePartitions.enabled are both enabled.</p> <p>Default: <code>(undefined)</code></p>","text":""},{"location":"configuration-properties/#sparksqladaptiveenabled","title":"spark.sql.adaptive.enabled <p>Enables Adaptive Query Execution</p> <p>Default: <code>true</code></p> <p>Use SQLConf.adaptiveExecutionEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqladaptivefetchshuffleblocksinbatch","title":"spark.sql.adaptive.fetchShuffleBlocksInBatch <p>(internal) Whether to fetch the contiguous shuffle blocks in batch. Instead of fetching blocks one by one, fetching contiguous shuffle blocks for the same map task in batch can reduce IO and improve performance. Note, multiple contiguous blocks exist in single \"fetch request only happen when spark.sql.adaptive.enabled and spark.sql.adaptive.coalescePartitions.enabled are both enabled. This feature also depends on a relocatable serializer, the concatenation support codec in use and the new version shuffle fetch protocol.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.fetchShuffleBlocksInBatch method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqladaptivelocalshufflereaderenabled","title":"spark.sql.adaptive.localShuffleReader.enabled <p>When <code>true</code> (and spark.sql.adaptive.enabled is <code>true</code>), Spark SQL tries to use local shuffle reader to read the shuffle data when the shuffle partitioning is not needed, for example, after converting sort-merge join to broadcast-hash join.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.LOCAL_SHUFFLE_READER_ENABLED to access the property (in a type-safe way)</p>","text":""},{"location":"configuration-properties/#sparksqladaptiveloglevel","title":"spark.sql.adaptive.logLevel <p>(internal) Log level for adaptive execution logging of plan changes. The value can be <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code> or <code>ERROR</code>.</p> <p>Default: <code>DEBUG</code></p> <p>Use SQLConf.adaptiveExecutionLogLevel for the current value</p>","text":""},{"location":"configuration-properties/#sparksqladaptivemaxshuffledhashjoinlocalmapthreshold","title":"spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold <p>The maximum size (in bytes) per partition that can be allowed to build local hash map. If this value is not smaller than spark.sql.adaptive.advisoryPartitionSizeInBytes and all the partition size are not larger than this config, join selection prefer to use shuffled hash join instead of sort merge join regardless of the value of spark.sql.join.preferSortMergeJoin.</p> <p>Default: <code>0</code></p> <p>Available as SQLConf.ADAPTIVE_MAX_SHUFFLE_HASH_JOIN_LOCAL_MAP_THRESHOLD</p>","text":""},{"location":"configuration-properties/#sparksqladaptiveskewjoinenabled","title":"spark.sql.adaptive.skewJoin.enabled <p>When <code>true</code> and spark.sql.adaptive.enabled is enabled, Spark dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed partitions.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.SKEW_JOIN_ENABLED to reference the property.</p>","text":""},{"location":"configuration-properties/#sparksqladaptiveskewjoinskewedpartitionfactor","title":"spark.sql.adaptive.skewJoin.skewedPartitionFactor <p>A partition is considered skewed if its size is larger than this factor multiplying the median partition size and also larger than spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes.</p> <p>Default: <code>5</code></p>","text":""},{"location":"configuration-properties/#sparksqladaptiveskewjoinskewedpartitionthresholdinbytes","title":"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes <p>A partition is considered skewed if its size in bytes is larger than this threshold and also larger than spark.sql.adaptive.skewJoin.skewedPartitionFactor multiplying the median partition size. Ideally this config should be set larger than spark.sql.adaptive.advisoryPartitionSizeInBytes.</p> <p>Default: <code>256MB</code></p>","text":""},{"location":"configuration-properties/#sparksqladaptivenonemptypartitionratioforbroadcastjoin","title":"spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin <p>(internal) A relation with a non-empty partition ratio (the number of non-empty partitions to all partitions) lower than this config will not be considered as the build side of a broadcast-hash join in Adaptive Query Execution regardless of the size.</p> <p>Effective with spark.sql.adaptive.enabled <code>true</code></p> <p>Default: <code>0.2</code></p> <p>Use SQLConf.nonEmptyPartitionRatioForBroadcastJoin method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlanalyzermaxiterations","title":"spark.sql.analyzer.maxIterations <p>(internal) The max number of iterations the analyzer runs.</p> <p>Default: <code>100</code></p>","text":""},{"location":"configuration-properties/#sparksqlanalyzerfailambiguousselfjoin","title":"spark.sql.analyzer.failAmbiguousSelfJoin <p>(internal) When <code>true</code>, fail the Dataset query if it contains ambiguous self-join.</p> <p>Default: <code>true</code></p>","text":""},{"location":"configuration-properties/#sparksqlansienabled","title":"spark.sql.ansi.enabled <p>When <code>true</code>, Spark tries to conform to the ANSI SQL specification:</p> <ol> <li>Spark will throw a runtime exception if an overflow occurs in any operation on integral/decimal field.</li> <li>Spark will forbid using the reserved keywords of ANSI SQL as identifiers in the SQL parser.</li> </ol> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqlcliprintheader","title":"spark.sql.cli.print.header <p>When <code>true</code>, spark-sql CLI prints the names of the columns in query output</p> <p>Default: <code>false</code></p> <p>Use SQLConf.cliPrintHeader for the current value</p>","text":""},{"location":"configuration-properties/#sparksqlcodegenwholestage","title":"spark.sql.codegen.wholeStage <p>(internal) Whether the whole stage (of multiple physical operators) will be compiled into a single Java method (<code>true</code>) or not (<code>false</code>).</p> <p>Default: <code>true</code></p> <p>Use SQLConf.wholeStageEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlcodegenmethodsplitthreshold","title":"spark.sql.codegen.methodSplitThreshold <p>(internal) The threshold of source-code splitting in the codegen. When the number of characters in a single Java function (without comment) exceeds the threshold, the function will be automatically split to multiple smaller ones. We cannot know how many bytecode will be generated, so use the code length as metric. When running on HotSpot, a function's bytecode should not go beyond 8KB, otherwise it will not be JITted; it also should not be too small, otherwise there will be many function calls.</p> <p>Default: <code>1024</code></p> <p>Use SQLConf.methodSplitThreshold for the current value</p>","text":""},{"location":"configuration-properties/#sparksqldebugmaxtostringfields","title":"spark.sql.debug.maxToStringFields <p>Maximum number of fields of sequence-like entries can be converted to strings in debug output. Any elements beyond the limit will be dropped and replaced by a \"... N more fields\" placeholder.</p> <p>Default: <code>25</code></p> <p>Use SQLConf.maxToStringFields method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqldefaultcatalog","title":"spark.sql.defaultCatalog <p>Name of the default catalog</p> <p>Default: spark_catalog</p> <p>Use SQLConf.DEFAULT_CATALOG to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlexecutionarrowpysparkenabled","title":"spark.sql.execution.arrow.pyspark.enabled <p>When true, make use of Apache Arrow for columnar data transfers in PySpark. This optimization applies to:</p> <ol> <li>pyspark.sql.DataFrame.toPandas</li> <li>pyspark.sql.SparkSession.createDataFrame when its input is a Pandas DataFrame</li> </ol> <p>The following data types are unsupported: BinaryType, MapType, ArrayType of TimestampType, and nested StructType.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqlexecutionremoveredundantsorts","title":"spark.sql.execution.removeRedundantSorts <p>(internal) Whether to remove redundant physical sort node</p> <p>Default: <code>true</code></p> <p>Used as SQLConf.REMOVE_REDUNDANT_SORTS_ENABLED</p>","text":""},{"location":"configuration-properties/#sparksqlexecutionreusesubquery","title":"spark.sql.execution.reuseSubquery <p>(internal) When <code>true</code>, the planner will try to find duplicated subqueries and re-use them.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.subqueryReuseEnabled for the current value</p>","text":""},{"location":"configuration-properties/#sparksqlexecutionsortbeforerepartition","title":"spark.sql.execution.sortBeforeRepartition <p>(internal) When perform a repartition following a shuffle, the output row ordering would be nondeterministic. If some downstream stages fail and some tasks of the repartition stage retry, these tasks may generate different data, and that can lead to correctness issues. Turn on this config to insert a local sort before actually doing repartition to generate consistent repartition results. The performance of <code>repartition()</code> may go down since we insert extra local sort before it.</p> <p>Default: <code>true</code></p> <p>Since: <code>2.1.4</code></p> <p>Use SQLConf.sortBeforeRepartition method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlexecutionrangeexchangesamplesizeperpartition","title":"spark.sql.execution.rangeExchange.sampleSizePerPartition <p>(internal) Number of points to sample per partition in order to determine the range boundaries for range partitioning, typically used in global sorting (without limit).</p> <p>Default: <code>100</code></p> <p>Since: <code>2.3.0</code></p> <p>Use SQLConf.rangeExchangeSampleSizePerPartition method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlexecutionarrowpysparkfallbackenabled","title":"spark.sql.execution.arrow.pyspark.fallback.enabled <p>When true, optimizations enabled by spark.sql.execution.arrow.pyspark.enabled will fallback automatically to non-optimized implementations if an error occurs.</p> <p>Default: <code>true</code></p>","text":""},{"location":"configuration-properties/#sparksqlexecutionarrowsparkrenabled","title":"spark.sql.execution.arrow.sparkr.enabled <p>When true, make use of Apache Arrow for columnar data transfers in SparkR. This optimization applies to:</p> <ol> <li>createDataFrame when its input is an R DataFrame</li> <li>collect</li> <li>dapply</li> <li>gapply</li> </ol> <p>The following data types are unsupported: FloatType, BinaryType, ArrayType, StructType and MapType.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqlexecutionpandasudfbuffersize","title":"spark.sql.execution.pandas.udf.buffer.size <p>Same as <code>${BUFFER_SIZE.key}</code> but only applies to Pandas UDF executions. If it is not set, the fallback is <code>${BUFFER_SIZE.key}</code>. Note that Pandas execution requires more than 4 bytes. Lowering this value could make small Pandas UDF batch iterated and pipelined; however, it might degrade performance. See SPARK-27870.</p> <p>Default: <code>65536</code></p>","text":""},{"location":"configuration-properties/#sparksqlexecutionpandasconverttoarrowarraysafely","title":"spark.sql.execution.pandas.convertToArrowArraySafely <p>(internal) When true, Arrow will perform safe type conversion when converting Pandas. Series to Arrow array during serialization. Arrow will raise errors when detecting unsafe type conversion like overflow. When false, disabling Arrow's type check and do type conversions anyway. This config only works for Arrow 0.11.0+.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqlstatisticshistogramenabled","title":"spark.sql.statistics.histogram.enabled <p>Enables generating histograms for ANALYZE TABLE SQL statement</p> <p>Default: <code>false</code></p>  <p>Equi-Height Histogram</p> <p>Histograms can provide better estimation accuracy. Currently, Spark only supports equi-height histogram. Note that collecting histograms takes extra cost. For example, collecting column statistics usually takes only one table scan, but generating equi-height histogram will cause an extra table scan.</p>  <p>Use SQLConf.histogramEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlsessiontimezone","title":"spark.sql.session.timeZone <p>The ID of session-local timezone (e.g. \"GMT\", \"America/Los_Angeles\")</p> <p>Default: Java's <code>TimeZone.getDefault.getID</code></p> <p>Use SQLConf.sessionLocalTimeZone method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlsourcesignoredatalocality","title":"spark.sql.sources.ignoreDataLocality <p>(internal) When <code>true</code>, Spark will not fetch the block locations for each file on listing files. This speeds up file listing, but the scheduler cannot schedule tasks to take advantage of data locality. It can be particularly useful if data is read from a remote cluster so the scheduler could never take advantage of locality anyway.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqlsourcesvalidatepartitioncolumns","title":"spark.sql.sources.validatePartitionColumns <p>(internal) When this option is set to true, partition column values will be validated with user-specified schema. If the validation fails, a runtime exception is thrown. When this option is set to false, the partition column value will be converted to null if it can not be casted to corresponding user-specified schema.</p> <p>Default: <code>true</code></p>","text":""},{"location":"configuration-properties/#sparksqlsourcesusev1sourcelist","title":"spark.sql.sources.useV1SourceList <p>(internal) A comma-separated list of data source short names (DataSourceRegisters) or fully-qualified canonical class names of the data sources (TableProviders) for which DataSource V2 code path is disabled (and Data Source V1 code path used).</p> <p>Default: <code>avro,csv,json,kafka,orc,parquet,text</code></p> <p>Used when:</p> <ul> <li><code>DataSource</code> utility is used to lookupDataSourceV2</li> </ul>","text":""},{"location":"configuration-properties/#sparksqlstoreassignmentpolicy","title":"spark.sql.storeAssignmentPolicy <p>When inserting a value into a column with different data type, Spark will perform type coercion. Currently, we support 3 policies for the type coercion rules: ANSI, legacy and strict. With ANSI policy, Spark performs the type coercion as per ANSI SQL. In practice, the behavior is mostly the same as PostgreSQL. It disallows certain unreasonable type conversions such as converting <code>string</code> to <code>int</code> or <code>double</code> to <code>boolean</code>. With legacy policy, Spark allows the type coercion as long as it is a valid <code>Cast</code>, which is very loose. e.g. converting <code>string</code> to <code>int</code> or <code>double</code> to <code>boolean</code> is allowed. It is also the only behavior in Spark 2.x and it is compatible with Hive. With strict policy, Spark doesn't allow any possible precision loss or data truncation in type coercion, e.g. converting <code>double</code> to <code>int</code> or <code>decimal</code> to <code>double</code> is not allowed.</p> <p>Possible values: <code>ANSI</code>, <code>LEGACY</code>, <code>STRICT</code></p> <p>Default: <code>ANSI</code></p>","text":""},{"location":"configuration-properties/#sparksqlthriftserverinterruptoncancel","title":"spark.sql.thriftServer.interruptOnCancel <p>When <code>true</code>, all running tasks will be interrupted if one cancels a query. When <code>false</code>, all running tasks will remain until finished.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.THRIFTSERVER_FORCE_CANCEL to access the property</p>","text":""},{"location":"configuration-properties/#sparksqlhivetablepropertylengththreshold","title":"spark.sql.hive.tablePropertyLengthThreshold <p>(internal) The maximum length allowed in a single cell when storing Spark-specific information in Hive's metastore as table properties. Currently it covers 2 things: the schema's JSON string, the histogram of column statistics.</p> <p>Default: (undefined)</p> <p>Use SQLConf.dynamicPartitionPruningEnabled to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlorcmergeschema","title":"spark.sql.orc.mergeSchema <p>When true, the Orc data source merges schemas collected from all data files, otherwise the schema is picked from a random data file.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqlsourcesbucketingautobucketedscanenabled","title":"spark.sql.sources.bucketing.autoBucketedScan.enabled <p>When <code>true</code>, decide whether to do bucketed scan on input tables based on query plan automatically. Do not use bucketed scan if 1. query does not have operators to utilize bucketing (e.g. join, group-by, etc), or 2. there's an exchange operator between these operators and table scan.</p> <p>Note when spark.sql.sources.bucketing.enabled is set to <code>false</code>, this configuration does not take any effect.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.autoBucketedScanEnabled to access the property</p>","text":""},{"location":"configuration-properties/#sparksqldatetimejava8apienabled","title":"spark.sql.datetime.java8API.enabled <p>When <code>true</code>, java.time.Instant and java.time.LocalDate classes of Java 8 API are used as external types for Catalyst's TimestampType and DateType. When <code>false</code>, java.sql.Timestamp and java.sql.Date are used for the same purpose.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacyintervalenabled","title":"spark.sql.legacy.interval.enabled <p>(internal) When <code>true</code>, Spark SQL uses the mixed legacy interval type <code>CalendarIntervalType</code> instead of the ANSI compliant interval types <code>YearMonthIntervalType</code> and <code>DayTimeIntervalType</code>. For instance, the date subtraction expression returns <code>CalendarIntervalType</code> when the SQL config is set to <code>true</code> otherwise an ANSI interval.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.legacyIntervalEnabled to access the current value</p>","text":""},{"location":"configuration-properties/#sparksqlsourcesbinaryfilemaxlength","title":"spark.sql.sources.binaryFile.maxLength <p>(internal) The max length of a file that can be read by the binary file data source. Spark will fail fast and not attempt to read the file if its length exceeds this value. The theoretical max is Int.MaxValue, though VMs might implement a smaller max.</p> <p>Default: <code>Int.MaxValue</code></p>","text":""},{"location":"configuration-properties/#sparksqlmapkeydeduppolicy","title":"spark.sql.mapKeyDedupPolicy <p>The policy to deduplicate map keys in builtin function: CreateMap, MapFromArrays, MapFromEntries, StringToMap, MapConcat and TransformKeys. When EXCEPTION, the query fails if duplicated map keys are detected. When LAST_WIN, the map key that is inserted at last takes precedence.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LAST_WIN</code></p> <p>Default: <code>EXCEPTION</code></p>","text":""},{"location":"configuration-properties/#sparksqlmaxconcurrentoutputfilewriters","title":"spark.sql.maxConcurrentOutputFileWriters <p>(internal) Maximum number of output file writers for <code>FileFormatWriter</code> to use concurrently (writing out a query result). If number of writers needed reaches this limit, a task will sort rest of output then writing them.</p> <p>Default: <code>0</code></p> <p>Use SQLConf.maxConcurrentOutputFileWriters for the current value</p>","text":""},{"location":"configuration-properties/#sparksqlmaxmetadatastringlength","title":"spark.sql.maxMetadataStringLength <p>Maximum number of characters to output for a metadata string (e.g., <code>Location</code> in FileScan)</p> <p>Default: <code>100</code></p> <p>Must be bigger than <code>3</code></p> <p>Use SQLConf.maxMetadataStringLength for the current value</p>","text":""},{"location":"configuration-properties/#sparksqlmavenadditionalremoterepositories","title":"spark.sql.maven.additionalRemoteRepositories <p>A comma-delimited string config of the optional additional remote Maven mirror repositories. This is only used for downloading Hive jars in IsolatedClientLoader if the default Maven Central repo is unreachable.</p> <p>Default: <code>https://maven-central.storage-download.googleapis.com/maven2/</code></p>","text":""},{"location":"configuration-properties/#sparksqlmaxplanstringlength","title":"spark.sql.maxPlanStringLength <p>Maximum number of characters to output for a plan string.  If the plan is longer, further output will be truncated. The default setting always generates a full plan. Set this to a lower value such as 8k if plan strings are taking up too much memory or are causing OutOfMemory errors in the driver or UI processes.</p> <p>Default: <code>Integer.MAX_VALUE - 15</code></p>","text":""},{"location":"configuration-properties/#sparksqladdpartitioninbatchsize","title":"spark.sql.addPartitionInBatch.size <p>(internal) The number of partitions to be handled in one turn when use <code>AlterTableAddPartitionCommand</code> to add partitions into table. The smaller batch size is, the less memory is required for the real handler, e.g. Hive Metastore.</p> <p>Default: <code>100</code></p>","text":""},{"location":"configuration-properties/#sparksqlscripttransformationexittimeoutinseconds","title":"spark.sql.scriptTransformation.exitTimeoutInSeconds <p>(internal) Timeout for executor to wait for the termination of transformation script when EOF.</p> <p>Default: <code>10</code> seconds</p>","text":""},{"location":"configuration-properties/#sparksqlavrocompressioncodec","title":"spark.sql.avro.compression.codec <p>The compression codec to use when writing Avro data to disk</p> <p>Default: <code>snappy</code></p> <p>The supported codecs are:</p> <ul> <li><code>uncompressed</code></li> <li><code>deflate</code></li> <li><code>snappy</code></li> <li><code>bzip2</code></li> <li><code>xz</code></li> </ul> <p>Use SQLConf.avroCompressionCodec method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlbroadcasttimeout","title":"spark.sql.broadcastTimeout <p>Timeout in seconds for the broadcast wait time in broadcast joins.</p> <p>Default: <code>5 * 60</code></p> <p>When negative, it is assumed infinite (i.e. <code>Duration.Inf</code>)</p> <p>Use SQLConf.broadcastTimeout method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlbucketingcoalescebucketsinjoinenabled","title":"spark.sql.bucketing.coalesceBucketsInJoin.enabled <p>When enabled (<code>true</code>), if two bucketed tables with the different number of buckets are joined, the side with a bigger number of buckets will be coalesced to have the same number of buckets as the other side. Bigger number of buckets is divisible by the smaller number of buckets. Bucket coalescing is applied to sort-merge joins and shuffled hash join.</p>  <p>Note</p> <p>Coalescing bucketed table can avoid unnecessary shuffling in join, but it also reduces parallelism and could possibly cause OOM for shuffled hash join.</p>  <p>Default: <code>false</code></p> <p>Use SQLConf.coalesceBucketsInJoinEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlcasesensitive","title":"spark.sql.caseSensitive <p>(internal) Controls whether the query analyzer should be case sensitive (<code>true</code>) or not (<code>false</code>).</p> <p>Default: <code>false</code></p> <p>It is highly discouraged to turn on case sensitive mode.</p> <p>Use SQLConf.caseSensitiveAnalysis method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlcatalogspark_catalog","title":"spark.sql.catalog.spark_catalog <p>The CatalogPlugin for <code>spark_catalog</code></p> <p>Default: defaultSessionCatalog</p>","text":""},{"location":"configuration-properties/#sparksqlcboenabled","title":"spark.sql.cbo.enabled <p>Enables Cost-Based Optimization (CBO) for estimation of plan statistics when <code>true</code>.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.cboEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlcbojoinreorderenabled","title":"spark.sql.cbo.joinReorder.enabled <p>Enables join reorder for cost-based optimization (CBO).</p> <p>Default: <code>false</code></p> <p>Use SQLConf.joinReorderEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlcboplanstatsenabled","title":"spark.sql.cbo.planStats.enabled <p>When <code>true</code>, the logical plan will fetch row counts and column statistics from catalog.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqlcbostarschemadetection","title":"spark.sql.cbo.starSchemaDetection <p>Enables join reordering based on star schema detection for cost-based optimization (CBO) in ReorderJoin logical plan optimization.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.starSchemaDetection method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlcodegenaggregatemapvectorizedenable_1","title":"spark.sql.codegen.aggregate.map.vectorized.enable <p>(internal) Enables vectorized aggregate hash map. This is for testing/benchmarking only.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqlcodegenaggregatesplitaggregatefuncenabled","title":"spark.sql.codegen.aggregate.splitAggregateFunc.enabled <p>(internal) When true, the code generator would split aggregate code into individual methods instead of a single big method. This can be used to avoid oversized function that can miss the opportunity of JIT optimization.</p> <p>Default: <code>true</code></p>","text":""},{"location":"configuration-properties/#sparksqlcodegencomments","title":"spark.sql.codegen.comments <p>Controls whether <code>CodegenContext</code> should register comments (<code>true</code>) or not (<code>false</code>).</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqlcodegenfactorymode","title":"spark.sql.codegen.factoryMode <p>(internal) Determines the codegen generator fallback behavior</p> <p>Default: <code>FALLBACK</code></p> <p>Acceptable values:</p> <ul> <li><code>CODEGEN_ONLY</code> - disable fallback mode</li> <li><code>FALLBACK</code> - try codegen first and, if any compile error happens, fallback to interpreted mode</li> <li><code>NO_CODEGEN</code> - skips codegen and always uses interpreted path</li> </ul> <p>Used when <code>CodeGeneratorWithInterpretedFallback</code> is requested to createObject (when <code>UnsafeProjection</code> is requested to create an UnsafeProjection for Catalyst expressions)</p>","text":""},{"location":"configuration-properties/#sparksqlcodegenuseidinclassname","title":"spark.sql.codegen.useIdInClassName <p>(internal) Controls whether to embed the (whole-stage) codegen stage ID into the class name of the generated class as a suffix (<code>true</code>) or not (<code>false</code>)</p> <p>Default: <code>true</code></p> <p>Use SQLConf.wholeStageUseIdInClassName method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlcodegenmaxfields","title":"spark.sql.codegen.maxFields <p>(internal) Maximum number of output fields (including nested fields) that whole-stage codegen supports. Going above the number deactivates whole-stage codegen.</p> <p>Default: <code>100</code></p> <p>Use SQLConf.wholeStageMaxNumFields method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlcodegensplitconsumefuncbyoperator","title":"spark.sql.codegen.splitConsumeFuncByOperator <p>(internal) Controls whether whole stage codegen puts the logic of consuming rows of each physical operator into individual methods, instead of a single big method. This can be used to avoid oversized function that can miss the opportunity of JIT optimization.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.wholeStageSplitConsumeFuncByOperator method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlcolumnnameofcorruptrecord","title":"spark.sql.columnNameOfCorruptRecord","text":""},{"location":"configuration-properties/#sparksqlconstraintpropagationenabled","title":"spark.sql.constraintPropagation.enabled <p>(internal) When true, the query optimizer will infer and propagate data constraints in the query plan to optimize them. Constraint propagation can sometimes be computationally expensive for certain kinds of query plans (such as those with a large number of predicates and aliases) which might negatively impact overall runtime.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.constraintPropagationEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlcsvfilterpushdownenabled","title":"spark.sql.csv.filterPushdown.enabled <p>(internal) When <code>true</code>, enable filter pushdown to CSV datasource.</p> <p>Default: <code>true</code></p>","text":""},{"location":"configuration-properties/#sparksqldefaultsizeinbytes","title":"spark.sql.defaultSizeInBytes <p>(internal) Estimated size of a table or relation used in query planning</p> <p>Default: Java's <code>Long.MaxValue</code></p> <p>Set to Java's <code>Long.MaxValue</code> which is larger than spark.sql.autoBroadcastJoinThreshold to be more conservative. That is to say by default the optimizer will not choose to broadcast a table unless it knows for sure that the table size is small enough.</p> <p>Used by the planner to decide when it is safe to broadcast a relation. By default, the system will assume that tables are too large to broadcast.</p> <p>Use SQLConf.defaultSizeInBytes method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqldialect","title":"spark.sql.dialect","text":""},{"location":"configuration-properties/#executionuseobjecthashaggregateexec","title":"execution.useObjectHashAggregateExec <p>spark.sql.execution.useObjectHashAggregateExec</p> <p>(internal) Prefers ObjectHashAggregateExec (over SortAggregateExec) for aggregation</p> <p>Default: <code>true</code></p> <p>Use SQLConf.useObjectHashAggregation method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlfilesignorecorruptfiles","title":"spark.sql.files.ignoreCorruptFiles <p>Controls whether to ignore corrupt files (<code>true</code>) or not (<code>false</code>). If <code>true</code>, the Spark jobs will continue to run when encountering corrupted files and the contents that have been read will still be returned.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.ignoreCorruptFiles method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlfilesignoremissingfiles","title":"spark.sql.files.ignoreMissingFiles <p>Controls whether to ignore missing files (<code>true</code>) or not (<code>false</code>). If <code>true</code>, the Spark jobs will continue to run when encountering missing files and the contents that have been read will still be returned.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.ignoreMissingFiles method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlinmemorycolumnarstoragecompressed","title":"spark.sql.inMemoryColumnarStorage.compressed <p>When enabled, Spark SQL will automatically select a compression codec for each column based on statistics of the data.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.useCompression method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlinmemorycolumnarstoragebatchsize","title":"spark.sql.inMemoryColumnarStorage.batchSize <p>Controls the size of batches for columnar caching. Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.</p> <p>Default: <code>10000</code></p> <p>Use SQLConf.columnBatchSize method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlinmemorytablescanstatisticsenable","title":"spark.sql.inMemoryTableScanStatistics.enable <p>(internal) When true, enable in-memory table scan accumulators.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqlinmemorycolumnarstorageenablevectorizedreader","title":"spark.sql.inMemoryColumnarStorage.enableVectorizedReader <p>Enables vectorized reader for columnar caching</p> <p>Default: <code>true</code></p> <p>Use SQLConf.cacheVectorizedReaderEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqljoinprefersortmergejoin","title":"spark.sql.join.preferSortMergeJoin <p>(internal) Controls whether JoinSelection execution planning strategy prefers sort merge join over shuffled hash join.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.preferSortMergeJoin method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqljsongeneratorignorenullfields","title":"spark.sql.jsonGenerator.ignoreNullFields <p>Whether to ignore null fields when generating JSON objects in JSON data source and JSON functions such as to_json. If false, it generates null for null fields in JSON objects.</p> <p>Default: <code>true</code></p>","text":""},{"location":"configuration-properties/#sparksqlleafnodedefaultparallelism","title":"spark.sql.leafNodeDefaultParallelism <p>The default parallelism of leaf operators that produce data</p> <p>Must be positive</p> <p>Default: <code>SparkContext.defaultParallelism</code> (Spark Core)</p>","text":""},{"location":"configuration-properties/#sparksqllegacydolooseupcast","title":"spark.sql.legacy.doLooseUpcast <p>(internal) When <code>true</code>, the upcast will be loose and allows string to atomic types.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacycteprecedencepolicy","title":"spark.sql.legacy.ctePrecedencePolicy <p>(internal) This config will be removed in future versions and <code>CORRECTED</code> will be the only behavior.</p> <p>Possible values:</p> <ol> <li><code>CORRECTED</code> - inner CTE definitions take precedence</li> <li><code>EXCEPTION</code> - <code>AnalysisException</code> is thrown while name conflict is detected in nested CTE</li> <li><code>LEGACY</code> - outer CTE definitions takes precedence over inner definitions</li> </ol> <p>Default: <code>EXCEPTION</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacytimeparserpolicy","title":"spark.sql.legacy.timeParserPolicy <p>(internal) When LEGACY, java.text.SimpleDateFormat is used for formatting and parsing dates/timestamps in a locale-sensitive manner, which is the approach before Spark 3.0. When set to CORRECTED, classes from <code>java.time.*</code> packages are used for the same purpose. The default value is EXCEPTION, RuntimeException is thrown when we will get different results.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacyfollowthreevaluedlogicinarrayexists","title":"spark.sql.legacy.followThreeValuedLogicInArrayExists <p>(internal) When true, the ArrayExists will follow the three-valued boolean logic.</p> <p>Default: <code>true</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacyfromdaytimestringenabled","title":"spark.sql.legacy.fromDayTimeString.enabled <p>(internal) When <code>true</code>, the <code>from</code> bound is not taken into account in conversion of a day-time string to an interval, and the <code>to</code> bound is used to skip all interval units out of the specified range. When <code>false</code>, <code>ParseException</code> is thrown if the input does not match to the pattern defined by <code>from</code> and <code>to</code>.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacynotreserveproperties","title":"spark.sql.legacy.notReserveProperties <p>(internal) When <code>true</code>, all database and table properties are not reserved and available for create/alter syntaxes. But please be aware that the reserved properties will be silently removed.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacyaddsinglefileinaddfile","title":"spark.sql.legacy.addSingleFileInAddFile <p>(internal) When <code>true</code>, only a single file can be added using ADD FILE. If false, then users can add directory by passing directory path to ADD FILE.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacyexponentliteralasdecimalenabled","title":"spark.sql.legacy.exponentLiteralAsDecimal.enabled <p>(internal) When <code>true</code>, a literal with an exponent (e.g. 1E-30) would be parsed as Decimal rather than Double.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacyallownegativescaleofdecimal","title":"spark.sql.legacy.allowNegativeScaleOfDecimal <p>(internal) When <code>true</code>, negative scale of Decimal type is allowed. For example, the type of number 1E10BD under legacy mode is DecimalType(2, -9), but is Decimal(11, 0) in non legacy mode.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacybucketedtablescanoutputordering","title":"spark.sql.legacy.bucketedTableScan.outputOrdering <p>(internal) When <code>true</code>, the bucketed table scan will list files during planning to figure out the output ordering, which is expensive and may make the planning quite slow.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacyjsonallowemptystringenabled","title":"spark.sql.legacy.json.allowEmptyString.enabled <p>(internal) When <code>true</code>, the parser of JSON data source treats empty strings as null for some data types such as <code>IntegerType</code>.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacycreateemptycollectionusingstringtype","title":"spark.sql.legacy.createEmptyCollectionUsingStringType <p>(internal) When <code>true</code>, Spark returns an empty collection with <code>StringType</code> as element type if the <code>array</code>/<code>map</code> function is called without any parameters. Otherwise, Spark returns an empty collection with <code>NullType</code> as element type.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacyallowuntypedscalaudf","title":"spark.sql.legacy.allowUntypedScalaUDF <p>(internal) When <code>true</code>, user is allowed to use <code>org.apache.spark.sql.functions.udf(f: AnyRef, dataType: DataType)</code>. Otherwise, an exception will be thrown at runtime.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacydatasetnamenonstructgroupingkeyasvalue","title":"spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue <p>(internal) When <code>true</code>, the key attribute resulted from running <code>Dataset.groupByKey</code> for non-struct key type, will be named as <code>value</code>, following the behavior of Spark version 2.4 and earlier.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacysetcommandrejectssparkcoreconfs","title":"spark.sql.legacy.setCommandRejectsSparkCoreConfs <p>(internal) If it is set to true, SET command will fail when the key is registered as a SparkConf entry.</p> <p>Default: <code>true</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacytypecoerciondatetimetostringenabled","title":"spark.sql.legacy.typeCoercion.datetimeToString.enabled <p>(internal) When <code>true</code>, date/timestamp will cast to string in binary comparisons with String</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacyallowhashonmaptype","title":"spark.sql.legacy.allowHashOnMapType <p>(internal) When <code>true</code>, hash expressions can be applied on elements of MapType. Otherwise, an analysis exception will be thrown.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacyparquetdatetimerebasemodeinwrite","title":"spark.sql.legacy.parquet.datetimeRebaseModeInWrite <p>(internal) When LEGACY, Spark will rebase dates/timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar when writing Parquet files. When CORRECTED, Spark will not do rebase and write the dates/timestamps as it is. When EXCEPTION, which is the default, Spark will fail the writing if it sees ancient dates/timestamps that are ambiguous between the two calendars.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacyparquetdatetimerebasemodeinread","title":"spark.sql.legacy.parquet.datetimeRebaseModeInRead <p>(internal) When LEGACY, Spark will rebase dates/timestamps from the legacy hybrid (Julian + Gregorian) calendar to Proleptic Gregorian calendar when reading Parquet files. When CORRECTED, Spark will not do rebase and read the dates/timestamps as it is. When EXCEPTION, which is the default, Spark will fail the reading if it sees ancient dates/timestamps that are ambiguous between the two calendars. This config is only effective if the writer info (like Spark, Hive) of the Parquet files is unknown.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacyavrodatetimerebasemodeinwrite","title":"spark.sql.legacy.avro.datetimeRebaseModeInWrite <p>(internal) When LEGACY, Spark will rebase dates/timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar when writing Avro files. When CORRECTED, Spark will not do rebase and write the dates/timestamps as it is. When EXCEPTION, which is the default, Spark will fail the writing if it sees ancient dates/timestamps that are ambiguous between the two calendars.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacyavrodatetimerebasemodeinread","title":"spark.sql.legacy.avro.datetimeRebaseModeInRead <p>(internal) When LEGACY, Spark will rebase dates/timestamps from the legacy hybrid (Julian + Gregorian) calendar to Proleptic Gregorian calendar when reading Avro files. When CORRECTED, Spark will not do rebase and read the dates/timestamps as it is. When EXCEPTION, which is the default, Spark will fail the reading if it sees ancient dates/timestamps that are ambiguous between the two calendars. This config is only effective if the writer info (like Spark, Hive) of the Avro files is unknown.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p>","text":""},{"location":"configuration-properties/#sparksqllegacyrddapplyconf","title":"spark.sql.legacy.rdd.applyConf <p>(internal) Enables propagation of SQL configurations when executing operations on the RDD that represents a structured query. This is the (buggy) behavior up to 2.4.4.</p> <p>Default: <code>true</code></p> <p>This is for cases not tracked by SQL execution, when a <code>Dataset</code> is converted to an RDD either using Dataset.md#rdd[rdd] operation or QueryExecution, and then the returned RDD is used to invoke actions on it.</p> <p>This config is deprecated and will be removed in 3.0.0.</p>","text":""},{"location":"configuration-properties/#sparksqllegacyreplacedatabrickssparkavroenabled","title":"spark.sql.legacy.replaceDatabricksSparkAvro.enabled <p>Enables resolving (mapping) the data source provider <code>com.databricks.spark.avro</code> to the built-in (but external) Avro data source module for backward compatibility.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.replaceDatabricksSparkAvroEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqllimitscaleupfactor","title":"spark.sql.limit.scaleUpFactor <p>(internal) Minimal increase rate in the number of partitions between attempts when executing <code>take</code> operator on a structured query. Higher values lead to more partitions read. Lower values might lead to longer execution times as more jobs will be run.</p> <p>Default: <code>4</code></p> <p>Use SQLConf.limitScaleUpFactor method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqloptimizenullawareantijoin","title":"spark.sql.optimizeNullAwareAntiJoin <p>(internal) Enables single-column NULL-aware anti join execution planning into BroadcastHashJoinExec (with flag isNullAwareAntiJoin enabled), optimized from O(M*N) calculation into O(M) calculation using hash lookup instead of looping lookup.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.optimizeNullAwareAntiJoin method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlorcimpl","title":"spark.sql.orc.impl <p>(internal) When <code>native</code>, use the native version of ORC support instead of the ORC library in Hive 1.2.1.</p> <p>Default: <code>native</code></p> <p>Acceptable values:</p> <ul> <li><code>hive</code></li> <li><code>native</code></li> </ul>","text":""},{"location":"configuration-properties/#sparksqlplanchangeloglevel","title":"spark.sql.planChangeLog.level <p>(internal) Log level for logging the change from the original plan to the new plan after a rule or batch is applied.</p> <p>Default: <code>trace</code></p> <p>Supported Values (case-insensitive):</p> <ul> <li>trace</li> <li>debug</li> <li>info</li> <li>warn</li> <li>error</li> </ul> <p>Use SQLConf.planChangeLogLevel method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlplanchangelogbatches","title":"spark.sql.planChangeLog.batches <p>(internal) Comma-separated list of batch names for plan changes logging</p> <p>Default: (undefined)</p> <p>Use SQLConf.planChangeBatches method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlplanchangelogrules","title":"spark.sql.planChangeLog.rules <p>(internal) Comma-separated list of rule names for plan changes logging</p> <p>Default: (undefined)</p> <p>Use SQLConf.planChangeRules method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlpysparkjvmstacktraceenabled","title":"spark.sql.pyspark.jvmStacktrace.enabled <p>When true, it shows the JVM stacktrace in the user-facing PySpark exception together with Python stacktrace. By default, it is disabled and hides JVM stacktrace and shows a Python-friendly exception only.</p> <p>Default: <code>false</code></p>","text":""},{"location":"configuration-properties/#sparksqlparquetbinaryasstring","title":"spark.sql.parquet.binaryAsString <p>Some other Parquet-producing systems, in particular Impala and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.isParquetBinaryAsString method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlparquetcompressioncodec","title":"spark.sql.parquet.compression.codec <p>Sets the compression codec used when writing Parquet files. If either <code>compression</code> or <code>parquet.compression</code> is specified in the table-specific options/properties, the precedence would be <code>compression</code>, <code>parquet.compression</code>, <code>spark.sql.parquet.compression.codec</code>.</p> <p>Acceptable values:</p> <ul> <li><code>brotli</code></li> <li><code>gzip</code></li> <li><code>lz4</code></li> <li><code>lzo</code></li> <li><code>none</code></li> <li><code>snappy</code></li> <li><code>uncompressed</code></li> <li><code>zstd</code></li> </ul> <p>Default: <code>snappy</code></p> <p>Use SQLConf.parquetCompressionCodec method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlparquetenablevectorizedreader","title":"spark.sql.parquet.enableVectorizedReader <p>Enables vectorized parquet decoding.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.parquetVectorizedReaderEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlparquetfilterpushdowndate","title":"spark.sql.parquet.filterPushdown.date <p>(internal) Enables parquet filter push-down optimization for <code>Date</code> type (when spark.sql.parquet.filterPushdown is enabled)</p> <p>Default: <code>true</code></p> <p>Use SQLConf.parquetFilterPushDownDate method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlparquetfilterpushdowndecimal","title":"spark.sql.parquet.filterPushdown.decimal <p>(internal) Enables parquet filter push-down optimization for <code>Decimal</code> type (when spark.sql.parquet.filterPushdown is enabled)</p> <p>Default: <code>true</code></p> <p>Use SQLConf.parquetFilterPushDownDecimal method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlparquetint96rebasemodeinwrite","title":"spark.sql.parquet.int96RebaseModeInWrite <p>(internal) Enables rebasing timestamps while writing Parquet files</p> <p>Formerly known as spark.sql.legacy.parquet.int96RebaseModeInWrite</p> <p>Acceptable values:</p> <ul> <li><code>EXCEPTION</code> - Fail writing parquet files if there are ancient timestamps that are ambiguous between the two calendars</li> <li><code>LEGACY</code> - Rebase <code>INT96</code> timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar (gives maximum interoperability)</li> <li><code>CORRECTED</code> - Write datetime values with no change (rabase). Only when you are 100% sure that the written files will only be read by Spark 3.0+ or other systems that use Proleptic Gregorian calendar</li> </ul> <p>Default: <code>EXCEPTION</code></p> <p>Writing dates before <code>1582-10-15</code> or timestamps before <code>1900-01-01T00:00:00Z</code> can be dangerous, as the files may be read by Spark 2.x or legacy versions of Hive later, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar.</p> <p>See more details in SPARK-31404.</p>","text":""},{"location":"configuration-properties/#sparksqlparquetpushdowninfilterthreshold","title":"spark.sql.parquet.pushdown.inFilterThreshold <p>(internal) For IN predicate, Parquet filter will push-down a set of OR clauses if its number of values not exceeds this threshold. Otherwise, Parquet filter will push-down a value greater than or equal to its minimum value and less than or equal to its maximum value (when spark.sql.parquet.filterPushdown is enabled)</p> <p>Disabled when <code>0</code></p> <p>Default: <code>10</code></p> <p>Use SQLConf.parquetFilterPushDownInFilterThreshold method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlparquetfilterpushdownstringstartswith","title":"spark.sql.parquet.filterPushdown.string.startsWith <p>(internal) Enables parquet filter push-down optimization for <code>startsWith</code> function (when spark.sql.parquet.filterPushdown is enabled)</p> <p>Default: <code>true</code></p> <p>Use SQLConf.parquetFilterPushDownStringStartWith method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlparquetfilterpushdowntimestamp","title":"spark.sql.parquet.filterPushdown.timestamp <p>(internal) Enables parquet filter push-down optimization for <code>Timestamp</code> type. It can only have an effect when the following hold:</p> <ol> <li>spark.sql.parquet.filterPushdown is enabled</li> <li><code>Timestamp</code> stored as <code>TIMESTAMP_MICROS</code> or <code>TIMESTAMP_MILLIS</code> type</li> </ol> <p>Default: <code>true</code></p> <p>Use SQLConf.parquetFilterPushDownTimestamp method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlparquetint96astimestamp","title":"spark.sql.parquet.int96AsTimestamp <p>Some Parquet-producing systems, in particular Impala, store Timestamp into INT96. Spark would also store Timestamp as INT96 because we need to avoid precision lost of the nanoseconds field. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.isParquetINT96AsTimestamp method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlparquetint96timestampconversion","title":"spark.sql.parquet.int96TimestampConversion <p>Controls whether timestamp adjustments should be applied to INT96 data when converting to timestamps, for data written by Impala.</p> <p>Default: <code>false</code></p> <p>This is necessary because Impala stores INT96 data with a different timezone offset than Hive and Spark.</p> <p>Use SQLConf.isParquetINT96TimestampConversion method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlparquetoutputtimestamptype","title":"spark.sql.parquet.outputTimestampType <p>Sets which Parquet timestamp type to use when Spark writes data to Parquet files. INT96 is a non-standard but commonly used timestamp type in Parquet. TIMESTAMP_MICROS is a standard timestamp type in Parquet, which stores number of microseconds from the Unix epoch. TIMESTAMP_MILLIS is also standard, but with millisecond precision, which means Spark has to truncate the microsecond portion of its timestamp value.</p> <p>Acceptable values:</p> <ul> <li><code>INT96</code></li> <li><code>TIMESTAMP_MICROS</code></li> <li><code>TIMESTAMP_MILLIS</code></li> </ul> <p>Default: <code>INT96</code></p> <p>Use SQLConf.parquetOutputTimestampType method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlparquetrecordlevelfilterenabled","title":"spark.sql.parquet.recordLevelFilter.enabled <p>Enables Parquet's native record-level filtering using the pushed down filters (when spark.sql.parquet.filterPushdown is enabled).</p> <p>Default: <code>false</code></p> <p>Use SQLConf.parquetRecordFilterEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlparquetrespectsummaryfiles","title":"spark.sql.parquet.respectSummaryFiles <p>When true, we make assumption that all part-files of Parquet are consistent with summary files and we will ignore them when merging schema. Otherwise, if this is false, which is the default, we will merge all part-files. This should be considered as expert-only option, and shouldn't be enabled before knowing what it means exactly.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.isParquetSchemaRespectSummaries method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlparserquotedregexcolumnnames","title":"spark.sql.parser.quotedRegexColumnNames <p>Controls whether quoted identifiers (using backticks) in SELECT statements should be interpreted as regular expressions.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.supportQuotedRegexColumnName method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlpivotmaxvalues","title":"spark.sql.pivotMaxValues <p>Maximum number of (distinct) values that will be collected without error (when doing a pivot without specifying the values for the pivot column)</p> <p>Default: <code>10000</code></p> <p>Use SQLConf.dataFramePivotMaxValues method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlredactionoptionsregex","title":"spark.sql.redaction.options.regex <p>Regular expression to find options of a Spark SQL command with sensitive information</p> <p>Default: <code>(?i)secret!password</code></p> <p>The values of the options matched will be redacted in the explain output.</p> <p>This redaction is applied on top of the global redaction configuration defined by <code>spark.redaction.regex</code> configuration.</p> <p>Used exclusively when <code>SQLConf</code> is requested to redactOptions.</p>","text":""},{"location":"configuration-properties/#sparksqlredactionstringregex","title":"spark.sql.redaction.string.regex <p>Regular expression to point at sensitive information in text output</p> <p>Default: <code>(undefined)</code></p> <p>When this regex matches a string part, it is replaced by a dummy value (i.e. <code>*********(redacted)</code>). This is currently used to redact the output of SQL explain commands.</p> <p>NOTE: When this conf is not set, the value of <code>spark.redaction.string.regex</code> is used instead.</p> <p>Use SQLConf.stringRedactionPattern method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlretaingroupcolumns","title":"spark.sql.retainGroupColumns <p>Controls whether to retain columns used for aggregation or not (in RelationalGroupedDataset operators).</p> <p>Default: <code>true</code></p> <p>Use SQLConf.dataFrameRetainGroupColumns method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlrunsqlonfiles","title":"spark.sql.runSQLOnFiles <p>(internal) Controls whether Spark SQL could use <code>datasource</code>.<code>path</code> as a table in a SQL query.</p> <p>Default: <code>true</code></p> <p>Use SQLConf.runSQLonFile method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlselfjoinautoresolveambiguity","title":"spark.sql.selfJoinAutoResolveAmbiguity <p>Controls whether to resolve ambiguity in join conditions for self-joins automatically (<code>true</code>) or not (<code>false</code>)</p> <p>Default: <code>true</code></p>","text":""},{"location":"configuration-properties/#sparksqlsortenableradixsort","title":"spark.sql.sort.enableRadixSort <p>(internal) Controls whether to use radix sort (<code>true</code>) or not (<code>false</code>) in ShuffleExchangeExec and SortExec physical operators</p> <p>Default: <code>true</code></p> <p>Radix sort is much faster but requires additional memory to be reserved up-front. The memory overhead may be significant when sorting very small rows (up to 50% more).</p> <p>Use SQLConf.enableRadixSort method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlsourcesdefault","title":"spark.sql.sources.default <p>Default data source to use for loading or saving data</p> <p>Default: parquet</p> <p>Use SQLConf.defaultDataSourceName method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlstatisticsfallbacktohdfs","title":"spark.sql.statistics.fallBackToHdfs <p>Enables automatic calculation of table size statistic by falling back to HDFS if the table statistics are not available from table metadata.</p> <p>Default: <code>false</code></p> <p>This can be useful in determining if a table is small enough for auto broadcast joins in query planning.</p> <p>Use SQLConf.fallBackToHdfsForStatsEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlstatisticshistogramnumbins","title":"spark.sql.statistics.histogram.numBins <p>(internal) The number of bins when generating histograms.</p> <p>Default: <code>254</code></p> <p>NOTE: The number of bins must be greater than 1.</p> <p>Use SQLConf.histogramNumBins method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlstatisticsparallelfilelistinginstatscomputationenabled","title":"spark.sql.statisticsparallelFileListingInStatsComputation.enabled* <p>(internal) Enables parallel file listing in SQL commands, e.g. <code>ANALYZE TABLE</code> (as opposed to single thread listing that can be particularly slow with tables with hundreds of partitions)</p> <p>Default: <code>true</code></p> <p>Use SQLConf.parallelFileListingInStatsComputation method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlstatisticsndvmaxerror","title":"spark.sql.statistics.ndv.maxError <p>(internal) The maximum estimation error allowed in HyperLogLog++ algorithm when generating column level statistics.</p> <p>Default: <code>0.05</code></p>","text":""},{"location":"configuration-properties/#sparksqlstatisticspercentileaccuracy","title":"spark.sql.statistics.percentile.accuracy <p>(internal) Accuracy of percentile approximation when generating equi-height histograms. Larger value means better accuracy. The relative error can be deduced by 1.0 / PERCENTILE_ACCURACY.</p> <p>Default: <code>10000</code></p>","text":""},{"location":"configuration-properties/#sparksqlstatisticssizeautoupdateenabled","title":"spark.sql.statistics.size.autoUpdate.enabled <p>Enables automatic update of the table size statistic of a table after the table has changed.</p> <p>Default: <code>false</code></p> <p>IMPORTANT: If the total number of files of the table is very large this can be expensive and slow down data change commands.</p> <p>Use SQLConf.autoSizeUpdateEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlsubexpressioneliminationenabled","title":"spark.sql.subexpressionElimination.enabled <p>(internal) Enables Subexpression Elimination</p> <p>Default: <code>true</code></p> <p>Use SQLConf.subexpressionEliminationEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlshufflepartitions","title":"spark.sql.shuffle.partitions <p>The default number of partitions to use when shuffling data for joins or aggregations.</p> <p>Default: <code>200</code></p>  <p>Note</p> <p>Corresponds to Apache Hive's mapred.reduce.tasks property that Spark SQL considers deprecated.</p>   <p>Spark Structured Streaming</p> <p><code>spark.sql.shuffle.partitions</code> cannot be changed in Spark Structured Streaming between query restarts from the same checkpoint location.</p>  <p>Use SQLConf.numShufflePartitions method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlsourcesfilecompressionfactor","title":"spark.sql.sources.fileCompressionFactor <p>(internal) When estimating the output data size of a table scan, multiply the file size with this factor as the estimated data size, in case the data is compressed in the file and lead to a heavily underestimated result.</p> <p>Default: <code>1.0</code></p> <p>Use SQLConf.fileCompressionFactor method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlsourcespartitionoverwritemode","title":"spark.sql.sources.partitionOverwriteMode <p>Enables dynamic partition inserts when <code>dynamic</code></p> <p>Default: <code>static</code></p> <p>When <code>INSERT OVERWRITE</code> a partitioned data source table with dynamic partition columns, Spark SQL supports two modes (case-insensitive):</p> <ul> <li> <p>static - Spark deletes all the partitions that match the partition specification (e.g. <code>PARTITION(a=1,b)</code>) in the INSERT statement, before overwriting</p> </li> <li> <p>dynamic - Spark doesn't delete partitions ahead, and only overwrites those partitions that have data written into it</p> </li> </ul> <p>The default <code>STATIC</code> overwrite mode is to keep the same behavior of Spark prior to 2.3. Note that this config doesn't affect Hive serde tables, as they are always overwritten with dynamic mode.</p> <p>Use SQLConf.partitionOverwriteMode method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqltruncatetableignorepermissionaclenabled","title":"spark.sql.truncateTable.ignorePermissionAcl.enabled <p>(internal) Disables setting back original permission and ACLs when re-creating the table/partition paths for TRUNCATE TABLE command.</p> <p>Default: <code>false</code></p> <p>Use SQLConf.truncateTableIgnorePermissionAcl method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqluiretainedexecutions","title":"spark.sql.ui.retainedExecutions <p>Number of <code>SQLExecutionUIData</code>s to keep in <code>failedExecutions</code> and <code>completedExecutions</code> internal registries.</p> <p>Default: <code>1000</code></p> <p>When a query execution finishes, the execution is removed from the internal <code>activeExecutions</code> registry and stored in <code>failedExecutions</code> or <code>completedExecutions</code> given the end execution status. It is when <code>SQLListener</code> makes sure that the number of <code>SQLExecutionUIData</code> entires does not exceed <code>spark.sql.ui.retainedExecutions</code> Spark property and removes the excess of entries.</p>","text":""},{"location":"configuration-properties/#sparksqlvariablesubstitute","title":"spark.sql.variable.substitute <p>Enables Variable Substitution</p> <p>Default: <code>true</code></p> <p>Use SQLConf.variableSubstituteEnabled method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlwindowexecbufferinmemorythreshold","title":"spark.sql.windowExec.buffer.in.memory.threshold <p>(internal) Threshold for number of rows guaranteed to be held in memory by WindowExec physical operator.</p> <p>Default: <code>4096</code></p> <p>Use SQLConf.windowExecBufferInMemoryThreshold method to access the current value.</p>","text":""},{"location":"configuration-properties/#sparksqlwindowexecbufferspillthreshold","title":"spark.sql.windowExec.buffer.spill.threshold <p>(internal) Threshold for number of rows buffered in a WindowExec physical operator.</p> <p>Default: <code>4096</code></p> <p>Use SQLConf.windowExecBufferSpillThreshold method to access the current value.</p>","text":""},{"location":"debugging-query-execution/","title":"Debugging Query Execution","text":"<p><code>debug</code> is a Scala package object with utilities for debugging query execution and an in-depth analysis of structured queries.</p> <pre><code>import org.apache.spark.sql.execution.debug._\n\n// Every Dataset (incl. DataFrame) has now the debug and debugCodegen methods\nval q: DataFrame = ???\nq.debug\nq.debugCodegen\n</code></pre> <p>Package Objects</p> <p>Read up on Package Objects in the Scala programming language.</p> <p>debug and debugCodegen are part of an implicit class (<code>DebugQuery</code>) that takes a Dataset when created (that is the query to execute<code>debug</code> on).</p> <p>Tip</p> <p>Read up on Implicit Classes in the official documentation of the Scala programming language.</p>"},{"location":"debugging-query-execution/#demo","title":"Demo","text":"<pre><code>val q = spark.range(5).join(spark.range(10), Seq(\"id\"), \"inner\")\n</code></pre> <pre><code>import org.apache.spark.sql.execution.debug._\n</code></pre> <pre><code>scala&gt; q.debugCodegen\nFound 0 WholeStageCodegen subtrees.\n</code></pre> <p>What?! \"Found 0 WholeStageCodegen subtrees.\"! Inconceivable!</p> <p>The reason is that the query has not been Adaptive Query Execution-optimized yet (the isFinalPlan flag is <code>false</code>).</p> <pre><code>scala&gt; println(q.queryExecution.executedPlan.numberedTreeString)\n00 AdaptiveSparkPlan isFinalPlan=false\n01 +- Project [id#4L]\n02    +- BroadcastHashJoin [id#4L], [id#6L], Inner, BuildLeft, false\n03       :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#16]\n04       :  +- Range (0, 5, step=1, splits=16)\n05       +- Range (0, 10, step=1, splits=16)\n</code></pre> <p>Execute Adaptive Query Execution optimization.</p> <pre><code>q.queryExecution.executedPlan.executeTake(1)\n</code></pre> <p>Note that the isFinalPlan flag is <code>true</code>.</p> <pre><code>scala&gt; println(q.queryExecution.executedPlan.numberedTreeString)\n00 AdaptiveSparkPlan isFinalPlan=true\n01 +- == Final Plan ==\n02    *(2) Project [id#4L]\n03    +- *(2) BroadcastHashJoin [id#4L], [id#6L], Inner, BuildLeft, false\n04       :- BroadcastQueryStage 0\n05       :  +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#22]\n06       :     +- *(1) Range (0, 5, step=1, splits=16)\n07       +- *(2) Range (0, 10, step=1, splits=16)\n08 +- == Initial Plan ==\n09    Project [id#4L]\n10    +- BroadcastHashJoin [id#4L], [id#6L], Inner, BuildLeft, false\n11       :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#16]\n12       :  +- Range (0, 5, step=1, splits=16)\n13       +- Range (0, 10, step=1, splits=16)\n</code></pre> <pre><code>scala&gt; q.debugCodegen\nFound 2 WholeStageCodegen subtrees.\n== Subtree 1 / 2 (maxMethodCodeSize:282; maxConstantPoolSize:175(0.27% used); numInnerClasses:0) ==\n*(1) Range (0, 5, step=1, splits=16)\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=1\n/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean range_initRange_0;\n/* 010 */   private long range_nextIndex_0;\n/* 011 */   private TaskContext range_taskContext_0;\n/* 012 */   private InputMetrics range_inputMetrics_0;\n/* 013 */   private long range_batchEnd_0;\n/* 014 */   private long range_numElementsTodo_0;\n...\n</code></pre>"},{"location":"debugging-query-execution/#debug","title":"debug <pre><code>debug(): Unit\n</code></pre>","text":""},{"location":"debugging-query-execution/#review-me","title":"Review Me","text":"<p><code>debug</code> requests the &lt;&gt; (of the &lt;&gt;) for the optimized physical query plan. <p><code>debug</code> transforms the optimized physical query plan to add a new &lt;&gt; physical operator for every physical operator. <p><code>debug</code> requests the query plan to &lt;&gt; and then counts the number of rows in the result. It prints out the following message: <pre><code>Results returned: [count]\n</code></pre> <p>In the end, <code>debug</code> requests every <code>DebugExec</code> physical operator (in the query plan) to &lt;&gt;. <pre><code>val q = spark.range(10).where('id === 4)\n\nscala&gt; :type q\norg.apache.spark.sql.Dataset[Long]\n\n// Extend Dataset[Long] with debug and debugCodegen methods\nimport org.apache.spark.sql.execution.debug._\n\nscala&gt; q.debug\nResults returned: 1\n== WholeStageCodegen ==\nTuples output: 1\n id LongType: {java.lang.Long}\n== Filter (id#0L = 4) ==\nTuples output: 0\n id LongType: {}\n== Range (0, 10, step=1, splits=8) ==\nTuples output: 0\n id LongType: {}\n</code></pre>"},{"location":"debugging-query-execution/#debugcodegen","title":"debugCodegen <pre><code>debugCodegen(): Unit\n</code></pre> <p><code>debugCodegen</code> displays the Java source code generated for a structured query in whole-stage code generation (i.e. the output of each WholeStageCodegen subtree in the query plan).</p>","text":""},{"location":"debugging-query-execution/#review-me_1","title":"Review Me","text":"<p><code>debugCodegen</code> requests the QueryExecution (of the structured query) for the optimized physical query plan.</p> <p>In the end, <code>debugCodegen</code> prints out the result to the standard output.</p> <pre><code>scala&gt; spark.range(10).where('id === 4).debugCodegen\nFound 1 WholeStageCodegen subtrees.\n== Subtree 1 / 1 ==\n*Filter (id#29L = 4)\n+- *Range (0, 10, splits=8)\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIterator(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 006 */   private Object[] references;\n...\n</code></pre>"},{"location":"developer-api/","title":"Developer API","text":""},{"location":"developer-api/#developerapi","title":"DeveloperApi","text":"<ul> <li>SparkSessionExtensions</li> <li>CachedBatch</li> <li>CachedBatchSerializer</li> <li>SimpleMetricsCachedBatchSerializer</li> <li>UserDefinedType</li> <li>ColumnarBatch</li> </ul>"},{"location":"dynamic-partition-inserts/","title":"Dynamic Partition Inserts","text":"<p>Partitioning uses partitioning columns to divide a dataset into smaller chunks (based on the values of certain columns) that will be written into separate directories.</p> <p>With a partitioned dataset, Spark SQL can load only the parts (partitions) that are really needed (and avoid doing filtering out unnecessary data on JVM). That leads to faster load time and more efficient memory consumption which gives a better performance overall.</p> <p>With a partitioned dataset, Spark SQL can also be executed over different subsets (directories) in parallel at the same time.</p> <pre><code>spark.range(10)\n  .withColumn(\"p1\", 'id % 2)\n  .write\n  .mode(\"overwrite\")\n  .partitionBy(\"p1\")\n  .saveAsTable(\"partitioned_table\")\n</code></pre> <p>Dynamic Partition Inserts is a feature of Spark SQL that allows for executing <code>INSERT OVERWRITE TABLE</code> SQL statements over partitioned HadoopFsRelations that limits what partitions are deleted to overwrite the partitioned table (and its partitions) with new data.</p> <p>[[dynamic-partitions]] Dynamic partitions are the partition columns that have no values defined explicitly in the PARTITION clause of &lt;&gt; SQL statements (in the <code>partitionSpec</code> part). <p>[[static-partitions]] Static partitions are the partition columns that have values defined explicitly in the PARTITION clause of &lt;&gt; SQL statements (in the <code>partitionSpec</code> part). <pre><code>// Borrowed from https://medium.com/@anuvrat/writing-into-dynamic-partitions-using-spark-2e2b818a007a\n// Note day dynamic partition\nINSERT OVERWRITE TABLE stats\nPARTITION(country = 'US', year = 2017, month = 3, day)\nSELECT ad, SUM(impressions), SUM(clicks), log_day\nFROM impression_logs\nGROUP BY ad;\n</code></pre> <p>NOTE: <code>INSERT OVERWRITE TABLE</code> SQL statement is translated into &lt;&gt; logical operator. <p>Dynamic Partition Inserts is only supported in SQL mode (for &lt;&gt; SQL statements). <p>Dynamic Partition Inserts is not supported for non-file-based data sources (InsertableRelationss).</p> <p>With Dynamic Partition Inserts, the behaviour of <code>OVERWRITE</code> keyword is controlled by spark.sql.sources.partitionOverwriteMode configuration property. The property controls whether Spark should delete all the partitions that match the partition specification regardless of whether there is data to be written to or not or delete only those partitions that will have data written into.</p> <p>When the <code>dynamic</code> overwrite mode is enabled Spark will only delete the partitions for which it has data to be written to. All the other partitions remain intact.</p> <p>From the Writing Into Dynamic Partitions Using Spark:</p> <p>Spark now writes data partitioned just as Hive would\u200a\u2014\u200awhich means only the partitions that are touched by the INSERT query get overwritten and the others are not touched.</p>"},{"location":"hive-integration/","title":"Hive Integration","text":"<p>Spark SQL can read and write data stored in Apache Hive using HiveExternalCatalog.</p> <p>From Wikipedia, the free encyclopedia:</p> <p>Apache Hive supports analysis of large datasets stored in Hadoop's HDFS and compatible file systems such as Amazon S3 filesystem.</p> <p>It provides an SQL-like language called HiveQL with schema on read and transparently converts queries to Hadoop MapReduce, Apache Tez and Apache Spark jobs.</p> <p>All three execution engines can run in Hadoop YARN.</p> <p>Builder.enableHiveSupport is used to enable Hive support (that simply sets spark.sql.catalogImplementation internal configuration property to <code>hive</code> only when the Hive classes are available).</p> <pre><code>import org.apache.spark.sql.SparkSession\nval spark = SparkSession\n  .builder\n  .enableHiveSupport()  // &lt;-- enables Hive support\n  .getOrCreate\n\nscala&gt; sql(\"set spark.sql.catalogImplementation\").show(false)\n+-------------------------------+-----+\n|key                            |value|\n+-------------------------------+-----+\n|spark.sql.catalogImplementation|hive |\n+-------------------------------+-----+\n\nassert(spark.conf.get(\"spark.sql.catalogImplementation\") == \"hive\")\n</code></pre>"},{"location":"hive-integration/#hive-configuration-hive-sitexml","title":"Hive Configuration - hive-site.xml","text":"<p>The configuration for Hive is in <code>hive-site.xml</code> on the classpath.</p> <p>The default configuration uses Hive 1.2.1 with the default warehouse in <code>/user/hive/warehouse</code>.</p> <pre><code>16/04/09 13:37:54 INFO HiveContext: Initializing execution hive, version 1.2.1\n16/04/09 13:37:58 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0\n16/04/09 13:37:58 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n16/04/09 13:37:58 INFO HiveContext: default warehouse location is /user/hive/warehouse\n16/04/09 13:37:58 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.\n16/04/09 13:38:01 DEBUG HiveContext: create HiveContext\n</code></pre>"},{"location":"implicits/","title":"implicits Object -- Implicits Conversions","text":"<p><code>implicits</code> object gives &lt;&gt; for converting Scala objects (incl. RDDs) into a <code>Dataset</code>, <code>DataFrame</code>, <code>Columns</code> or supporting such conversions (through &lt;&gt;). <p>[[methods]] .implicits API [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| <code>localSeqToDatasetHolder</code> a| [[localSeqToDatasetHolder]] Creates a &lt;&gt; with the input <code>Seq[T]</code> converted to a <code>Dataset[T]</code> (using &lt;&gt;). <pre><code>implicit def localSeqToDatasetHolder[T : Encoder](s: Seq[T]): DatasetHolder[T]\n</code></pre> <p>| Encoders | [[Encoders]] Encoders for primitive and object types in Scala and Java (aka boxed types)</p> <p>| <code>StringToColumn</code> a| [[StringToColumn]] Converts <code>$\"name\"</code> into a Column</p>"},{"location":"implicits/#source-scala","title":"[source, scala]","text":""},{"location":"implicits/#implicit-class-stringtocolumnval-sc-stringcontext","title":"implicit class StringToColumn(val sc: StringContext)","text":"<p>| <code>rddToDatasetHolder</code> a| [[rddToDatasetHolder]]</p>"},{"location":"implicits/#source-scala_1","title":"[source, scala]","text":""},{"location":"implicits/#implicit-def-rddtodatasetholdert-encoder-datasetholdert","title":"implicit def rddToDatasetHolderT : Encoder: DatasetHolder[T]","text":"<p>| <code>symbolToColumn</code> a| [[symbolToColumn]]</p>"},{"location":"implicits/#source-scala_2","title":"[source, scala]","text":""},{"location":"implicits/#implicit-def-symboltocolumns-symbol-columnname","title":"implicit def symbolToColumn(s: Symbol): ColumnName","text":"<p>|===</p> <p><code>implicits</code> object is defined inside &lt;&gt; and hence requires that you build a &lt;&gt; instance first before importing <code>implicits</code> conversions."},{"location":"implicits/#source-scala_3","title":"[source, scala]","text":"<p>import org.apache.spark.sql.SparkSession val spark: SparkSession = ... import spark.implicits._</p> <p>scala&gt; val ds = Seq(\"I am a shiny Dataset!\").toDS ds: org.apache.spark.sql.Dataset[String] = [value: string]</p> <p>scala&gt; val df = Seq(\"I am an old grumpy DataFrame!\").toDF df: org.apache.spark.sql.DataFrame = [value: string]</p> <p>scala&gt; val df = Seq(\"I am an old grumpy DataFrame with text column!\").toDF(\"text\") df: org.apache.spark.sql.DataFrame = [text: string]</p> <p>val rdd = sc.parallelize(Seq(\"hello, I'm a very low-level RDD\")) scala&gt; val ds = rdd.toDS ds: org.apache.spark.sql.Dataset[String] = [value: string]</p>"},{"location":"implicits/#tip","title":"[TIP]","text":""},{"location":"implicits/#in-scala-repl-based-environments-eg-spark-shell-use-imports-to-know-what-imports-are-in-scope","title":"In Scala REPL-based environments, e.g. <code>spark-shell</code>, use <code>:imports</code> to know what imports are in scope.","text":""},{"location":"implicits/#source-scala_4","title":"[source, scala]","text":"<p>scala&gt; :help imports</p> <p>show import history, identifying sources of names</p> <p>scala&gt; :imports  1) import org.apache.spark.SparkContext._ (69 terms, 1 are implicit)  2) import spark.implicits._       (1 types, 67 terms, 37 are implicit)  3) import spark.sql               (1 terms)  4) import org.apache.spark.sql.functions._ (354 terms)</p> <p><code>implicits</code> object extends <code>SQLImplicits</code> abstract class.</p> <p>=== [[DatasetHolder]][[toDS]][[toDF]] <code>DatasetHolder</code> Scala Case Class</p> <p>[[ds]] [[creating-instance]] <code>DatasetHolder</code> is a Scala case class that, when created, takes a <code>Dataset[T]</code>.</p> <p><code>DatasetHolder</code> is &lt;&gt; (implicitly) when &lt;&gt; and &lt;&gt; implicit conversions are used. <p><code>DatasetHolder</code> has <code>toDS</code> and <code>toDF</code> methods that simply return the &lt;&gt; (it was created with) or a <code>DataFrame</code> (using &lt;&gt; operator), respectively."},{"location":"implicits/#source-scala_5","title":"[source, scala]","text":"<p>toDS(): Dataset[T] toDF(): DataFrame toDF(colNames: String*): DataFrame</p>"},{"location":"joins/","title":"Join Queries","text":"<p>From PostgreSQL's 2.6. Joins Between Tables:</p> <p>Queries can access multiple tables at once, or access the same table in such a way that multiple rows of the table are being processed at the same time. A query that accesses multiple rows of the same or different tables at one time is called a join query.</p>"},{"location":"joins/#dataset-join-operators","title":"Dataset Join Operators","text":"Operator Return Type Description crossJoin DataFrame Untyped <code>Row</code>-based cross join join DataFrame Untyped <code>Row</code>-based join joinWith Dataset Type-preserving join with two output columns for records for which a join condition holds <p><code>join</code> operators create a <code>DataFrame</code> with a Join logical operator.</p>"},{"location":"joins/#crossjoin","title":"crossJoin <pre><code>crossJoin(\n  right: Dataset[_]): DataFrame\n</code></pre> <p><code>crossJoin</code> creates a Join logical operator with the Cross join type.</p>","text":""},{"location":"joins/#join","title":"join <pre><code>join(\n  right: Dataset[_]): DataFrame\njoin(\n  right: Dataset[_],\n  joinExprs: Column): DataFrame\njoin(\n  right: Dataset[_],\n  joinExprs: Column,\n  joinType: String): DataFrame\njoin(\n  right: Dataset[_],\n  usingColumns: Seq[String]): DataFrame\njoin(\n  right: Dataset[_],\n  usingColumns: Seq[String],\n  joinType: String): DataFrame\njoin(\n  right: Dataset[_],\n  usingColumn: String): DataFrame\n</code></pre> <p><code>join</code> creates a Join logical operator with the given join type or the Inner.</p>","text":""},{"location":"joins/#joinwith","title":"joinWith <pre><code>joinWith[U](\n  other: Dataset[U],\n  condition: Column): Dataset[(T, U)]\njoinWith[U](\n  other: Dataset[U],\n  condition: Column,\n  joinType: String): Dataset[(T, U)]\n</code></pre> <p><code>joinWith</code> creates a Join logical operator with the given join type or the Inner.</p>","text":""},{"location":"joins/#query-execution-planning","title":"Query Execution Planning","text":"<p>JoinSelection execution planning strategy is used to plan Join logical operators.</p>"},{"location":"joins/#join-condition","title":"Join Condition","text":"<p>Join condition (join expression) can be specified using the join operators, where or filter operators.</p> <pre><code>df1.join(df2, $\"df1Key\" === $\"df2Key\")\ndf1.join(df2).where($\"df1Key\" === $\"df2Key\")\ndf1.join(df2).filter($\"df1Key\" === $\"df2Key\")\n</code></pre>"},{"location":"joins/#join-types","title":"Join Types","text":"<p>Join types can be specified using the join operators (using <code>joinType</code> optional parameter).</p> <pre><code>df1.join(df2, $\"df1Key\" === $\"df2Key\", \"inner\")\n</code></pre> <p>Join names are case-insensitive and can use the underscore (<code>_</code>) at any position (e.g. <code>left_anti</code> and <code>L_E_F_T_A_N_T_I</code> are equivalent).</p> SQL JoinType Parameter Name CROSS Cross cross INNER Inner inner FULL OUTER FullOuter outer, full, fullouter LEFT ANTI LeftAnti leftanti LEFT OUTER LeftOuter leftouter, left LEFT SEMI LeftSemi leftsemi RIGHT OUTER RightOuter rightouter, right NATURAL NaturalJoin Special case for Inner, LeftOuter, RightOuter, FullOuter USING UsingJoin Special case for Inner, LeftOuter, LeftSemi, RightOuter, FullOuter, LeftAnti"},{"location":"joins/#existencejoin","title":"ExistenceJoin <p><code>ExistenceJoin</code> is an artifical join type used to express an existential sub-query, that is often referred to as existential join.</p> <p>LeftAnti and ExistenceJoin are special cases of LeftOuter.</p>","text":""},{"location":"joins/#join-families","title":"Join Families","text":""},{"location":"joins/#innerlike","title":"InnerLike <p><code>InnerLike</code> with Inner and Cross</p>","text":""},{"location":"joins/#leftexistence","title":"LeftExistence <p><code>LeftExistence</code> with LeftSemi, LeftAnti and ExistenceJoin</p>","text":""},{"location":"joins/#demo","title":"Demo <pre><code>val left = Seq((0, \"zero\"), (1, \"one\")).toDF(\"id\", \"left\")\nval right = Seq((0, \"zero\"), (2, \"two\"), (3, \"three\")).toDF(\"id\", \"right\")\n</code></pre>","text":""},{"location":"joins/#inner-join","title":"Inner join <pre><code>val q = left.join(right, \"id\")\n</code></pre> <pre><code>+---+----+-----+\n| id|left|right|\n+---+----+-----+\n|  0|zero| zero|\n+---+----+-----+\n</code></pre> <pre><code>== Physical Plan ==\n*(1) Project [id#7, left#8, right#19]\n+- *(1) BroadcastHashJoin [id#7], [id#18], Inner, BuildLeft, false\n   :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#15]\n   :  +- LocalTableScan [id#7, left#8]\n   +- *(1) LocalTableScan [id#18, right#19]\n</code></pre>","text":""},{"location":"joins/#full-outer","title":"Full outer <pre><code>val q = left.join(right, Seq(\"id\"), \"fullouter\")\n</code></pre> <pre><code>+---+----+-----+\n| id|left|right|\n+---+----+-----+\n|  1| one| null|\n|  3|null|three|\n|  2|null|  two|\n|  0|zero| zero|\n+---+----+-----+\n</code></pre> <pre><code>== Physical Plan ==\n*(3) Project [coalesce(id#7, id#18) AS id#25, left#8, right#19]\n+- SortMergeJoin [id#7], [id#18], FullOuter\n   :- *(1) Sort [id#7 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(id#7, 200), ENSURE_REQUIREMENTS, [id=#38]\n   :     +- LocalTableScan [id#7, left#8]\n   +- *(2) Sort [id#18 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(id#18, 200), ENSURE_REQUIREMENTS, [id=#39]\n         +- LocalTableScan [id#18, right#19]\n</code></pre>","text":""},{"location":"joins/#left-anti","title":"Left Anti <pre><code>val q = left.join(right, Seq(\"id\"), \"leftanti\")\n</code></pre> <pre><code>+---+----+\n| id|left|\n+---+----+\n|  1| one|\n+---+----+\n</code></pre> <pre><code>== Physical Plan ==\n*(1) BroadcastHashJoin [id#7], [id#18], LeftAnti, BuildRight, false\n:- *(1) LocalTableScan [id#7, left#8]\n+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#65]\n   +- LocalTableScan [id#18]\n</code></pre>","text":""},{"location":"multi-dimensional-aggregation/","title":"Multi-Dimensional Aggregation","text":"<p>Multi-dimensional aggregate operators are enhanced variants of groupBy operator to create queries for subtotals, grand totals and superset of subtotals in one go.</p> <p>It is assumed that using one of the operators is usually more efficient (than <code>union</code> and <code>groupBy</code>) as it gives more freedom for query optimization.</p> <p>Beside Dataset.cube and Dataset.rollup operators, Spark SQL supports GROUPING SETS clause in SQL mode only.</p> <p>Support for multi-dimensional aggregate operators was added in [SPARK-6356] Support the ROLLUP/CUBE/GROUPING SETS/grouping() in SQLContext.</p>"},{"location":"multi-dimensional-aggregation/#operators","title":"Operators","text":""},{"location":"multi-dimensional-aggregation/#cube","title":"cube <pre><code>cube(\n  cols: Column*): RelationalGroupedDataset\ncube(\n  col1: String,\n  cols: String*): RelationalGroupedDataset\n</code></pre> <pre><code>GROUP BY expressions WITH CUBE\nGROUP BY CUBE(expressions)\n</code></pre> <p><code>cube</code> multi-dimensional aggregate operator returns a RelationalGroupedDataset to calculate subtotals and a grand total for every permutation of the columns specified.</p> <p><code>cube</code> is an extension of groupBy operator that allows calculating subtotals and a grand total across all combinations of specified group of <code>n + 1</code> dimensions (with <code>n</code> being the number of columns as <code>cols</code> and <code>col1</code> and <code>1</code> for where values become <code>null</code>, i.e. undefined).</p> <p><code>cube</code> returns RelationalGroupedDataset that you can use to execute aggregate function or operator.</p>  <p>cube vs rollup</p> <p><code>cube</code> is more than rollup operator, i.e. <code>cube</code> does <code>rollup</code> with aggregation over all the missing combinations given the columns.</p>","text":""},{"location":"multi-dimensional-aggregation/#rollup","title":"rollup <pre><code>rollup(\n  cols: Column*): RelationalGroupedDataset\nrollup(\n  col1: String,\n  cols: String*): RelationalGroupedDataset\n</code></pre> <pre><code>GROUP BY expressions WITH ROLLUP\nGROUP BY ROLLUP(expressions)\n</code></pre> <p><code>rollup</code> gives a RelationalGroupedDataset to calculate subtotals and a grand total over (ordered) combination of groups.</p> <p><code>rollup</code> is an extension of groupBy operator that calculates subtotals and a grand total across specified group of <code>n + 1</code> dimensions (with <code>n</code> being the number of columns as <code>cols</code> and <code>col1</code> and <code>1</code> for where values become <code>null</code>, i.e. undefined).</p>  <p>Note</p> <p><code>rollup</code> operator is commonly used for analysis over hierarchical data; e.g. total salary by department, division, and company-wide total.</p> <p>See PostgreSQL's https://www.postgresql.org/docs/current/static/queries-table-expressions.html#QUERIES-GROUPING-SETS[7.2.4. GROUPING SETS, CUBE, and ROLLUP]</p>   <p>Note</p> <p><code>rollup</code> operator is equivalent to <code>GROUP BY \\... WITH ROLLUP</code> in SQL (which in turn is equivalent to <code>GROUP BY \\... GROUPING SETS \\((a,b,c),(a,b),(a),())</code> when used with 3 columns: <code>a</code>, <code>b</code>, and <code>c</code>).</p>  <p>From Using GROUP BY with ROLLUP, CUBE, and GROUPING SETS in Microsoft's TechNet:</p>  <p>The ROLLUP, CUBE, and GROUPING SETS operators are extensions of the GROUP BY clause. The ROLLUP, CUBE, or GROUPING SETS operators can generate the same result set as when you use UNION ALL to combine single grouping queries; however, using one of the GROUP BY operators is usually more efficient.</p>  <p>From PostgreSQL's 7.2.4. GROUPING SETS, CUBE, and ROLLUP:</p>  <p>References to the grouping columns or expressions are replaced by null values in result rows for grouping sets in which those columns do not appear.</p>  <p>From Summarizing Data Using ROLLUP in Microsoft's TechNet:</p>  <p>The ROLLUP operator is useful in generating reports that contain subtotals and totals. (...) ROLLUP generates a result set that shows aggregates for a hierarchy of values in the selected columns.</p>  <pre><code>// Borrowed from Microsoft's \"Summarizing Data Using ROLLUP\" article\nval inventory = Seq(\n  (\"table\", \"blue\", 124),\n  (\"table\", \"red\", 223),\n  (\"chair\", \"blue\", 101),\n  (\"chair\", \"red\", 210)).toDF(\"item\", \"color\", \"quantity\")\n\nscala&gt; inventory.show\n+-----+-----+--------+\n| item|color|quantity|\n+-----+-----+--------+\n|chair| blue|     101|\n|chair|  red|     210|\n|table| blue|     124|\n|table|  red|     223|\n+-----+-----+--------+\n\n// ordering and empty rows done manually for demo purposes\nscala&gt; inventory.rollup(\"item\", \"color\").sum().show\n+-----+-----+-------------+\n| item|color|sum(quantity)|\n+-----+-----+-------------+\n|chair| blue|          101|\n|chair|  red|          210|\n|chair| null|          311|\n|     |     |             |\n|table| blue|          124|\n|table|  red|          223|\n|table| null|          347|\n|     |     |             |\n| null| null|          658|\n+-----+-----+-------------+\n</code></pre> <p>From Hive's Cubes and Rollups:</p>  <p>WITH ROLLUP is used with the GROUP BY only. ROLLUP clause is used with GROUP BY to compute the aggregate at the hierarchy levels of a dimension.</p> <p>GROUP BY a, b, c with ROLLUP assumes that the hierarchy is \"a\" drilling down to \"b\" drilling down to \"c\".</p> <p>GROUP BY a, b, c, WITH ROLLUP is equivalent to GROUP BY a, b, c GROUPING SETS ( (a, b, c), (a, b), (a), ( )).</p>   <p>Note</p> <p>Read up on ROLLUP in Hive's LanguageManual in Grouping Sets, Cubes, Rollups, and the GROUPING__ID Function.</p>  <pre><code>// Borrowed from http://stackoverflow.com/a/27222655/1305344\nval quarterlyScores = Seq(\n  (\"winter2014\", \"Agata\", 99),\n  (\"winter2014\", \"Jacek\", 97),\n  (\"summer2015\", \"Agata\", 100),\n  (\"summer2015\", \"Jacek\", 63),\n  (\"winter2015\", \"Agata\", 97),\n  (\"winter2015\", \"Jacek\", 55),\n  (\"summer2016\", \"Agata\", 98),\n  (\"summer2016\", \"Jacek\", 97)).toDF(\"period\", \"student\", \"score\")\n\nscala&gt; quarterlyScores.show\n+----------+-------+-----+\n|    period|student|score|\n+----------+-------+-----+\n|winter2014|  Agata|   99|\n|winter2014|  Jacek|   97|\n|summer2015|  Agata|  100|\n|summer2015|  Jacek|   63|\n|winter2015|  Agata|   97|\n|winter2015|  Jacek|   55|\n|summer2016|  Agata|   98|\n|summer2016|  Jacek|   97|\n+----------+-------+-----+\n\n// ordering and empty rows done manually for demo purposes\nscala&gt; quarterlyScores.rollup(\"period\", \"student\").sum(\"score\").show\n+----------+-------+----------+\n|    period|student|sum(score)|\n+----------+-------+----------+\n|winter2014|  Agata|        99|\n|winter2014|  Jacek|        97|\n|winter2014|   null|       196|\n|          |       |          |\n|summer2015|  Agata|       100|\n|summer2015|  Jacek|        63|\n|summer2015|   null|       163|\n|          |       |          |\n|winter2015|  Agata|        97|\n|winter2015|  Jacek|        55|\n|winter2015|   null|       152|\n|          |       |          |\n|summer2016|  Agata|        98|\n|summer2016|  Jacek|        97|\n|summer2016|   null|       195|\n|          |       |          |\n|      null|   null|       706|\n+----------+-------+----------+\n</code></pre> <p>From PostgreSQL's 7.2.4. GROUPING SETS, CUBE, and ROLLUP:</p>  <p>The individual elements of a CUBE or ROLLUP clause may be either individual expressions, or sublists of elements in parentheses. In the latter case, the sublists are treated as single units for the purposes of generating the individual grouping sets.</p>  <pre><code>// using struct function\nscala&gt; inventory.rollup(struct(\"item\", \"color\") as \"(item,color)\").sum().show\n+------------+-------------+\n|(item,color)|sum(quantity)|\n+------------+-------------+\n| [table,red]|          223|\n|[chair,blue]|          101|\n|        null|          658|\n| [chair,red]|          210|\n|[table,blue]|          124|\n+------------+-------------+\n</code></pre> <pre><code>// using expr function\nscala&gt; inventory.rollup(expr(\"(item, color)\") as \"(item, color)\").sum().show\n+-------------+-------------+\n|(item, color)|sum(quantity)|\n+-------------+-------------+\n|  [table,red]|          223|\n| [chair,blue]|          101|\n|         null|          658|\n|  [chair,red]|          210|\n| [table,blue]|          124|\n+-------------+-------------+\n</code></pre> <p>Internally, <code>rollup</code> converts the Dataset into a DataFrame and then creates a RelationalGroupedDataset (with <code>RollupType</code> group type).</p>  <p>Tip</p> <p>Read up on <code>rollup</code> in Deeper into Postgres 9.5 - New Group By Options for Aggregation.</p>","text":""},{"location":"multi-dimensional-aggregation/#grouping-sets-sql-clause","title":"GROUPING SETS SQL Clause <pre><code>GROUP BY (expressions) GROUPING SETS (expressions)\nGROUP BY GROUPING SETS (expressions)\n</code></pre>  <p>Note</p> <p>SQL's <code>GROUPING SETS</code> is the most general aggregate \"operator\" and can generate the same dataset as using a simple groupBy, cube and rollup operators.</p>  <pre><code>import java.time.LocalDate\nimport java.sql.Date\nval expenses = Seq(\n  ((2012, Month.DECEMBER, 12), 5),\n  ((2016, Month.AUGUST, 13), 10),\n  ((2017, Month.MAY, 27), 15))\n  .map { case ((yy, mm, dd), a) =&gt; (LocalDate.of(yy, mm, dd), a) }\n  .map { case (d, a) =&gt; (d.toString, a) }\n  .map { case (d, a) =&gt; (Date.valueOf(d), a) }\n  .toDF(\"date\", \"amount\")\nscala&gt; expenses.show\n+----------+------+\n|      date|amount|\n+----------+------+\n|2012-12-12|     5|\n|2016-08-13|    10|\n|2017-05-27|    15|\n+----------+------+\n\n// rollup time!\nval q = expenses\n  .rollup(year($\"date\") as \"year\", month($\"date\") as \"month\")\n  .agg(sum(\"amount\") as \"amount\")\n  .sort($\"year\".asc_nulls_last, $\"month\".asc_nulls_last)\nscala&gt; q.show\n+----+-----+------+\n|year|month|amount|\n+----+-----+------+\n|2012|   12|     5|\n|2012| null|     5|\n|2016|    8|    10|\n|2016| null|    10|\n|2017|    5|    15|\n|2017| null|    15|\n|null| null|    30|\n+----+-----+------+\n</code></pre> <p><code>GROUPING SETS</code> clause generates a dataset that is equivalent to <code>union</code> operator of multiple groupBy operators.</p> <pre><code>val sales = Seq(\n  (\"Warsaw\", 2016, 100),\n  (\"Warsaw\", 2017, 200),\n  (\"Boston\", 2015, 50),\n  (\"Boston\", 2016, 150),\n  (\"Toronto\", 2017, 50)\n).toDF(\"city\", \"year\", \"amount\")\nsales.createOrReplaceTempView(\"sales\")\n\n// equivalent to rollup(\"city\", \"year\")\nval q = sql(\"\"\"\n  SELECT city, year, sum(amount) as amount\n  FROM sales\n  GROUP BY city, year\n  GROUPING SETS ((city, year), (city), ())\n  ORDER BY city DESC NULLS LAST, year ASC NULLS LAST\n  \"\"\")\nscala&gt; q.show\n+-------+----+------+\n|   city|year|amount|\n+-------+----+------+\n| Warsaw|2016|   100|\n| Warsaw|2017|   200|\n| Warsaw|null|   300|\n|Toronto|2017|    50|\n|Toronto|null|    50|\n| Boston|2015|    50|\n| Boston|2016|   150|\n| Boston|null|   200|\n|   null|null|   550|  &lt;-- grand total across all cities and years\n+-------+----+------+\n\n// equivalent to cube(\"city\", \"year\")\n// note the additional (year) grouping set\nval q = sql(\"\"\"\n  SELECT city, year, sum(amount) as amount\n  FROM sales\n  GROUP BY city, year\n  GROUPING SETS ((city, year), (city), (year), ())\n  ORDER BY city DESC NULLS LAST, year ASC NULLS LAST\n  \"\"\")\nscala&gt; q.show\n+-------+----+------+\n|   city|year|amount|\n+-------+----+------+\n| Warsaw|2016|   100|\n| Warsaw|2017|   200|\n| Warsaw|null|   300|\n|Toronto|2017|    50|\n|Toronto|null|    50|\n| Boston|2015|    50|\n| Boston|2016|   150|\n| Boston|null|   200|\n|   null|2015|    50|  &lt;-- total across all cities in 2015\n|   null|2016|   250|  &lt;-- total across all cities in 2016\n|   null|2017|   250|  &lt;-- total across all cities in 2017\n|   null|null|   550|\n+-------+----+------+\n</code></pre> <p><code>GROUPING SETS</code> clause is parsed in withAggregation parsing handler (in <code>AstBuilder</code>) and becomes a GroupingSets logical operator internally.</p>","text":""},{"location":"multi-dimensional-aggregation/#demo","title":"Demo","text":""},{"location":"multi-dimensional-aggregation/#grouping-sets","title":"GROUPING SETS <pre><code>val sales = Seq(\n  (\"Warsaw\", 2016, 100),\n  (\"Warsaw\", 2017, 200),\n  (\"Boston\", 2015, 50),\n  (\"Boston\", 2016, 150),\n  (\"Toronto\", 2017, 50)\n).toDF(\"city\", \"year\", \"amount\")\n</code></pre> <pre><code>// very labor-intense\n// groupBy's unioned\nval groupByCityAndYear = sales\n  .groupBy(\"city\", \"year\")  // &lt;-- subtotals (city, year)\n  .agg(sum(\"amount\") as \"amount\")\nval groupByCityOnly = sales\n  .groupBy(\"city\")          // &lt;-- subtotals (city)\n  .agg(sum(\"amount\") as \"amount\")\n  .select($\"city\", lit(null) as \"year\", $\"amount\")  // &lt;-- year is null\nval withUnion = groupByCityAndYear\n  .union(groupByCityOnly)\n  .sort($\"city\".desc_nulls_last, $\"year\".asc_nulls_last)\n</code></pre> <pre><code>scala&gt; withUnion.show\n+-------+----+------+\n|   city|year|amount|\n+-------+----+------+\n| Warsaw|2016|   100|\n| Warsaw|2017|   200|\n| Warsaw|null|   300|\n|Toronto|2017|    50|\n|Toronto|null|    50|\n| Boston|2015|    50|\n| Boston|2016|   150|\n| Boston|null|   200|\n+-------+----+------+\n</code></pre> <p>Multi-dimensional aggregate operators are semantically equivalent to <code>union</code> operator (or SQL's <code>UNION ALL</code>) to combine single grouping queries.</p> <pre><code>// Roll up your sleeves!\nval withRollup = sales\n  .rollup(\"city\", \"year\")\n  .agg(sum(\"amount\") as \"amount\", grouping_id() as \"gid\")\n  .sort($\"city\".desc_nulls_last, $\"year\".asc_nulls_last)\n  .filter(grouping_id() =!= 3)\n  .select(\"city\", \"year\", \"amount\")\n</code></pre> <pre><code>scala&gt; withRollup.show\n+-------+----+------+\n|   city|year|amount|\n+-------+----+------+\n| Warsaw|2016|   100|\n| Warsaw|2017|   200|\n| Warsaw|null|   300|\n|Toronto|2017|    50|\n|Toronto|null|    50|\n| Boston|2015|    50|\n| Boston|2016|   150|\n| Boston|null|   200|\n+-------+----+------+\n</code></pre> <pre><code>// Be even more smarter?\n// SQL only, alas.\nsales.createOrReplaceTempView(\"sales\")\nval withGroupingSets = sql(\"\"\"\n  SELECT city, year, SUM(amount) as amount\n  FROM sales\n  GROUP BY city, year\n  GROUPING SETS ((city, year), (city))\n  ORDER BY city DESC NULLS LAST, year ASC NULLS LAST\n  \"\"\")\nscala&gt; withGroupingSets.show\n+-------+----+------+\n|   city|year|amount|\n+-------+----+------+\n| Warsaw|2016|   100|\n| Warsaw|2017|   200|\n| Warsaw|null|   300|\n|Toronto|2017|    50|\n|Toronto|null|    50|\n| Boston|2015|    50|\n| Boston|2016|   150|\n| Boston|null|   200|\n+-------+----+------+\n</code></pre>","text":""},{"location":"multi-dimensional-aggregation/#rollup_1","title":"Rollup <pre><code>val sales = Seq(\n  (\"Warsaw\", 2016, 100),\n  (\"Warsaw\", 2017, 200),\n  (\"Boston\", 2015, 50),\n  (\"Boston\", 2016, 150),\n  (\"Toronto\", 2017, 50)\n).toDF(\"city\", \"year\", \"amount\")\n\nval q = sales\n  .rollup(\"city\", \"year\")\n  .agg(sum(\"amount\") as \"amount\")\n  .sort($\"city\".desc_nulls_last, $\"year\".asc_nulls_last)\nscala&gt; q.show\n+-------+----+------+\n|   city|year|amount|\n+-------+----+------+\n| Warsaw|2016|   100| &lt;-- subtotal for Warsaw in 2016\n| Warsaw|2017|   200|\n| Warsaw|null|   300| &lt;-- subtotal for Warsaw (across years)\n|Toronto|2017|    50|\n|Toronto|null|    50|\n| Boston|2015|    50|\n| Boston|2016|   150|\n| Boston|null|   200|\n|   null|null|   550| &lt;-- grand total\n+-------+----+------+\n\n// The above query is semantically equivalent to the following\nval q1 = sales\n  .groupBy(\"city\", \"year\")  // &lt;-- subtotals (city, year)\n  .agg(sum(\"amount\") as \"amount\")\nval q2 = sales\n  .groupBy(\"city\")          // &lt;-- subtotals (city)\n  .agg(sum(\"amount\") as \"amount\")\n  .select($\"city\", lit(null) as \"year\", $\"amount\")  // &lt;-- year is null\nval q3 = sales\n  .groupBy()                // &lt;-- grand total\n  .agg(sum(\"amount\") as \"amount\")\n  .select(lit(null) as \"city\", lit(null) as \"year\", $\"amount\")  // &lt;-- city and year are null\nval qq = q1\n  .union(q2)\n  .union(q3)\n  .sort($\"city\".desc_nulls_last, $\"year\".asc_nulls_last)\nscala&gt; qq.show\n+-------+----+------+\n|   city|year|amount|\n+-------+----+------+\n| Warsaw|2016|   100|\n| Warsaw|2017|   200|\n| Warsaw|null|   300|\n|Toronto|2017|    50|\n|Toronto|null|    50|\n| Boston|2015|    50|\n| Boston|2016|   150|\n| Boston|null|   200|\n|   null|null|   550|\n+-------+----+------+\n</code></pre>","text":""},{"location":"multi-dimensional-aggregation/#cube_1","title":"Cube <pre><code>val sales = Seq(\n  (\"Warsaw\", 2016, 100),\n  (\"Warsaw\", 2017, 200),\n  (\"Boston\", 2015, 50),\n  (\"Boston\", 2016, 150),\n  (\"Toronto\", 2017, 50)\n).toDF(\"city\", \"year\", \"amount\")\n\nval q = sales.cube(\"city\", \"year\")\n  .agg(sum(\"amount\") as \"amount\")\n  .sort($\"city\".desc_nulls_last, $\"year\".asc_nulls_last)\nscala&gt; q.show\n+-------+----+------+\n|   city|year|amount|\n+-------+----+------+\n| Warsaw|2016|   100|  &lt;-- total in Warsaw in 2016\n| Warsaw|2017|   200|  &lt;-- total in Warsaw in 2017\n| Warsaw|null|   300|  &lt;-- total in Warsaw (across all years)\n|Toronto|2017|    50|\n|Toronto|null|    50|\n| Boston|2015|    50|\n| Boston|2016|   150|\n| Boston|null|   200|\n|   null|2015|    50|  &lt;-- total in 2015 (across all cities)\n|   null|2016|   250|\n|   null|2017|   250|\n|   null|null|   550|  &lt;-- grand total (across cities and years)\n+-------+----+------+\n</code></pre>","text":""},{"location":"overview/","title":"Spark SQL","text":""},{"location":"overview/#structured-data-processing-with-relational-queries-on-massive-scale","title":"Structured Data Processing with Relational Queries on Massive Scale","text":"<p>Spark SQL allows expressing distributed in-memory computations using relational operators.</p> <p>Spark SQL is a relational framework for ingesting, querying and persisting (semi)structured data using structured queries (aka relational queries) that can be expressed in good ol' SQL (incl. HiveQL) and the high-level SQL-like functional declarative Dataset API (Structured Query DSL).</p> <p>Note</p> <p>Semi- and structured data are collections of records that can be described using schema with column names, their types and whether a column can be null or not (nullability).</p> <p>Spark SQL comes with a uniform and pluggable interface for data access in distributed storage systems and formats (e.g. Hadoop DFS, Hive, Parquet, Avro, Apache Kafka) using DataFrameReader and DataFrameWriter APIs.</p> <p>Spark SQL allows you to execute SQL-like queries on large volume of data that can live in Hadoop HDFS or Hadoop-compatible file systems like S3. It can access data from different data sources - files or tables.</p> <p>Whichever query interface you use to describe a structured query, i.e. SQL or Query DSL, the query becomes a Dataset (with a mandatory Encoder).</p> <p>Shark, Spark SQL, Hive on Spark, and the future of SQL on Apache Spark</p> <p>For SQL users, Spark SQL provides state-of-the-art SQL performance and maintains compatibility with Shark/Hive. In particular, like Shark, Spark SQL supports all existing Hive data formats, user-defined functions (UDF), and the Hive metastore.</p> <p>For Spark users, Spark SQL becomes the narrow-waist for manipulating (semi-) structured data as well as ingesting data from sources that provide schema, such as JSON, Parquet, Hive, or EDWs. It truly unifies SQL and sophisticated analysis, allowing users to mix and match SQL and more imperative programming APIs for advanced analytics.</p> <p>For open source hackers, Spark SQL proposes a novel, elegant way of building query planners. It is incredibly easy to add new optimizations under this framework.</p>"},{"location":"overview/#dataset-data-structure","title":"Dataset Data Structure","text":"<p>The main data abstraction of Spark SQL is Dataset that represents a structured data (records with a known schema). This structured data representation <code>Dataset</code> enables compact binary representation using compressed columnar format that is stored in managed objects outside JVM's heap. It is supposed to speed computations up by reducing memory usage and GCs.</p> <p><code>Dataset</code> is a programming interface to the structured query execution pipeline with transformations and actions (as in the good old days of RDD API in Spark Core).</p> <p>Internally, a structured query is a Catalyst tree of (logical and physical) relational operators and expressions.</p>"},{"location":"overview/#spark-sql-high-level-interface","title":"Spark SQL - High-Level Interface","text":"<p>Spark SQL is de facto the primary and feature-rich interface to Spark's underlying in-memory distributed platform (hiding Spark Core's RDDs behind higher-level abstractions that allow for logical and physical query optimization strategies even without your consent).</p> <p>In other words, Spark SQL's <code>Dataset</code> API describes a distributed computation that will eventually be converted to an RDD for execution.</p> <p>Under the covers, structured queries are automatically compiled into corresponding RDD operations.</p> <p>Spark SQL supports structured queries in batch and streaming modes (with the latter as a separate module of Spark SQL called Spark Structured Streaming).</p> <p>Spark SQL supports loading datasets from various data sources including tables in Apache Hive. With Hive support enabled, you can load datasets from existing Apache Hive deployments and save them back to Hive tables if needed.</p>"},{"location":"overview/#query-optimizations","title":"Query Optimizations","text":"<p>Spark SQL offers performance query optimizations using Catalyst Optimizer, Whole-Stage Codegen and Tungsten execution engine.</p> <p>Quoting Apache Drill which applies to Spark SQL perfectly:</p> <p>A SQL query engine for relational and NoSQL databases with direct queries on self-describing and semi-structured data in files, e.g. JSON or Parquet, and HBase tables without needing to specify metadata definitions in a centralized store.</p> <p>Spark SQL supports predicate pushdown to optimize query performance and can also generate optimized code at runtime.</p>"},{"location":"overview/#spark-sql-paper","title":"Spark SQL Paper","text":"<p>Quoting Spark SQL: Relational Data Processing in Spark paper on Spark SQL:</p> <p>Spark SQL is a new module in Apache Spark that integrates relational processing with Spark's functional programming API.</p> <p>Spark SQL lets Spark programmers leverage the benefits of relational processing (e.g., declarative queries and optimized storage), and lets SQL users call complex analytics libraries in Spark (e.g., machine learning).</p>"},{"location":"overview/#further-reading-and-watching","title":"Further Reading and Watching","text":"<ul> <li>Spark SQL home page</li> <li>(video) Spark's Role in the Big Data Ecosystem - Matei Zaharia</li> <li>Introducing Apache Spark 2.0</li> </ul>"},{"location":"spark-logging/","title":"Logging","text":"<p>Apache Spark uses Apache Log4j 2 for logging.</p> <p>Note</p> <p>Learn more in The Internals of Apache Spark online book.</p>"},{"location":"spark-sql-DataFrameNaFunctions/","title":"DataFrameNaFunctions \u2014 Working With Missing Data","text":"<p><code>DataFrameNaFunctions</code> is used to work with &lt;&gt; in a structured query (a DataFrame). <p>[[methods]] .DataFrameNaFunctions API [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| &lt;&gt; a|"},{"location":"spark-sql-DataFrameNaFunctions/#source-scala","title":"[source, scala]","text":"<p>drop(): DataFrame drop(cols: Array[String]): DataFrame drop(minNonNulls: Int): DataFrame drop(minNonNulls: Int, cols: Array[String]): DataFrame drop(minNonNulls: Int, cols: Seq[String]): DataFrame drop(cols: Seq[String]): DataFrame drop(how: String): DataFrame drop(how: String, cols: Array[String]): DataFrame drop(how: String, cols: Seq[String]): DataFrame</p> <p>| &lt;&gt; a|"},{"location":"spark-sql-DataFrameNaFunctions/#source-scala_1","title":"[source, scala]","text":"<p>fill(value: Boolean): DataFrame fill(value: Boolean, cols: Array[String]): DataFrame fill(value: Boolean, cols: Seq[String]): DataFrame fill(value: Double): DataFrame fill(value: Double, cols: Array[String]): DataFrame fill(value: Double, cols: Seq[String]): DataFrame fill(value: Long): DataFrame fill(value: Long, cols: Array[String]): DataFrame fill(value: Long, cols: Seq[String]): DataFrame fill(valueMap: Map[String, Any]): DataFrame fill(value: String): DataFrame fill(value: String, cols: Array[String]): DataFrame fill(value: String, cols: Seq[String]): DataFrame</p> <p>| &lt;&gt; a|"},{"location":"spark-sql-DataFrameNaFunctions/#source-scala_2","title":"[source, scala]","text":"<p>replaceT: DataFrame replaceT: DataFrame</p> <p>|===</p> <p>[[creating-instance]] <code>DataFrameNaFunctions</code> is available using na untyped transformation.</p>"},{"location":"spark-sql-DataFrameNaFunctions/#source-scala_3","title":"[source, scala]","text":"<p>val q: DataFrame = ... q.na</p> <p>=== [[convertToDouble]] <code>convertToDouble</code> Internal Method</p>"},{"location":"spark-sql-DataFrameNaFunctions/#source-scala_4","title":"[source, scala]","text":""},{"location":"spark-sql-DataFrameNaFunctions/#converttodoublev-any-double","title":"convertToDouble(v: Any): Double","text":"<p><code>convertToDouble</code>...FIXME</p> <p>NOTE: <code>convertToDouble</code> is used when...FIXME</p> <p>=== [[drop]] <code>drop</code> Method</p>"},{"location":"spark-sql-DataFrameNaFunctions/#source-scala_5","title":"[source, scala]","text":"<p>drop(): DataFrame drop(cols: Array[String]): DataFrame drop(minNonNulls: Int): DataFrame drop(minNonNulls: Int, cols: Array[String]): DataFrame drop(minNonNulls: Int, cols: Seq[String]): DataFrame drop(cols: Seq[String]): DataFrame drop(how: String): DataFrame drop(how: String, cols: Array[String]): DataFrame drop(how: String, cols: Seq[String]): DataFrame</p> <p><code>drop</code>...FIXME</p> <p>=== [[fill]] <code>fill</code> Method</p>"},{"location":"spark-sql-DataFrameNaFunctions/#source-scala_6","title":"[source, scala]","text":"<p>fill(value: Boolean): DataFrame fill(value: Boolean, cols: Array[String]): DataFrame fill(value: Boolean, cols: Seq[String]): DataFrame fill(value: Double): DataFrame fill(value: Double, cols: Array[String]): DataFrame fill(value: Double, cols: Seq[String]): DataFrame fill(value: Long): DataFrame fill(value: Long, cols: Array[String]): DataFrame fill(value: Long, cols: Seq[String]): DataFrame fill(valueMap: Map[String, Any]): DataFrame fill(value: String): DataFrame fill(value: String, cols: Array[String]): DataFrame fill(value: String, cols: Seq[String]): DataFrame</p> <p><code>fill</code>...FIXME</p> <p>=== [[fillCol]] <code>fillCol</code> Internal Method</p>"},{"location":"spark-sql-DataFrameNaFunctions/#source-scala_7","title":"[source, scala]","text":""},{"location":"spark-sql-DataFrameNaFunctions/#fillcolt-column","title":"fillColT: Column","text":"<p><code>fillCol</code>...FIXME</p> <p>NOTE: <code>fillCol</code> is used when...FIXME</p> <p>=== [[fillMap]] <code>fillMap</code> Internal Method</p>"},{"location":"spark-sql-DataFrameNaFunctions/#source-scala_8","title":"[source, scala]","text":""},{"location":"spark-sql-DataFrameNaFunctions/#fillmapvalues-seqstring-any-dataframe","title":"fillMap(values: Seq[(String, Any)]): DataFrame","text":"<p><code>fillMap</code>...FIXME</p> <p>NOTE: <code>fillMap</code> is used when...FIXME</p> <p>=== [[fillValue]] <code>fillValue</code> Internal Method</p>"},{"location":"spark-sql-DataFrameNaFunctions/#source-scala_9","title":"[source, scala]","text":""},{"location":"spark-sql-DataFrameNaFunctions/#fillvaluet-dataframe","title":"fillValueT: DataFrame","text":"<p><code>fillValue</code>...FIXME</p> <p>NOTE: <code>fillValue</code> is used when...FIXME</p> <p>=== [[replace0]] <code>replace0</code> Internal Method</p>"},{"location":"spark-sql-DataFrameNaFunctions/#source-scala_10","title":"[source, scala]","text":""},{"location":"spark-sql-DataFrameNaFunctions/#replace0t-dataframe","title":"replace0T: DataFrame","text":"<p><code>replace0</code>...FIXME</p> <p>NOTE: <code>replace0</code> is used when...FIXME</p> <p>=== [[replace]] <code>replace</code> Method</p>"},{"location":"spark-sql-DataFrameNaFunctions/#source-scala_11","title":"[source, scala]","text":"<p>replaceT: DataFrame replaceT: DataFrame</p> <p><code>replace</code>...FIXME</p> <p>=== [[replaceCol]] <code>replaceCol</code> Internal Method</p>"},{"location":"spark-sql-DataFrameNaFunctions/#source-scala_12","title":"[source, scala]","text":""},{"location":"spark-sql-DataFrameNaFunctions/#replacecolcol-structfield-replacementmap-map_-_-column","title":"replaceCol(col: StructField, replacementMap: Map[_, _]): Column","text":"<p><code>replaceCol</code>...FIXME</p> <p>NOTE: <code>replaceCol</code> is used when...FIXME</p>"},{"location":"spark-sql-Dataset-actions/","title":"Dataset API -- Actions","text":"<p>Actions are part of the &lt;&gt; for...FIXME <p>NOTE: Actions are the methods in the <code>Dataset</code> Scala class that are grouped in <code>action</code> group name, i.e. <code>@group action</code>.</p> <p>[[methods]] .Dataset API's Actions [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Action | Description</p> <p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-actions/#source-scala","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#collect-arrayt","title":"collect(): Array[T]","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-actions/#source-scala_1","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#count-long","title":"count(): Long","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-actions/#source-scala_2","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#describecols-string-dataframe","title":"describe(cols: String*): DataFrame","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-actions/#source-scala_3","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#first-t","title":"first(): T","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-actions/#source-scala_4","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#foreachf-t-unit-unit","title":"foreach(f: T =&gt; Unit): Unit","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-actions/#source-scala_5","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#foreachpartitionf-iteratort-unit-unit","title":"foreachPartition(f: Iterator[T] =&gt; Unit): Unit","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-actions/#source-scala_6","title":"[source, scala]","text":"<p>head(): T head(n: Int): Array[T]</p> <p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-actions/#source-scala_7","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#reducefunc-t-t-t-t","title":"reduce(func: (T, T) =&gt; T): T","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-actions/#source-scala_8","title":"[source, scala]","text":"<p>show(): Unit show(truncate: Boolean): Unit show(numRows: Int): Unit show(numRows: Int, truncate: Boolean): Unit show(numRows: Int, truncate: Int): Unit show(numRows: Int, truncate: Int, vertical: Boolean): Unit</p> <p>| &lt;&gt; a| Computes specified statistics for numeric and string columns. The default statistics are: <code>count</code>, <code>mean</code>, <code>stddev</code>, <code>min</code>, <code>max</code> and <code>25%</code>, <code>50%</code>, <code>75%</code> percentiles."},{"location":"spark-sql-Dataset-actions/#source-scala_9","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#summarystatistics-string-dataframe","title":"summary(statistics: String*): DataFrame","text":"<p>NOTE: <code>summary</code> is an extended version of the &lt;&gt; action that simply calculates <code>count</code>, <code>mean</code>, <code>stddev</code>, <code>min</code> and <code>max</code> statistics. <p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-actions/#source-scala_10","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#taken-int-arrayt","title":"take(n: Int): Array[T]","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-actions/#source-scala_11","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#tolocaliterator-javautiliteratort","title":"toLocalIterator(): java.util.Iterator[T]","text":"<p>|===</p> <p>=== [[collect]] <code>collect</code> Action</p>"},{"location":"spark-sql-Dataset-actions/#source-scala_12","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#collect-arrayt_1","title":"collect(): Array[T]","text":"<p><code>collect</code>...FIXME</p> <p>=== [[count]] <code>count</code> Action</p>"},{"location":"spark-sql-Dataset-actions/#source-scala_13","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#count-long_1","title":"count(): Long","text":"<p><code>count</code>...FIXME</p> <p>=== [[describe]] Calculating Basic Statistics -- <code>describe</code> Action</p>"},{"location":"spark-sql-Dataset-actions/#source-scala_14","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#describecols-string-dataframe_1","title":"describe(cols: String*): DataFrame","text":"<p><code>describe</code>...FIXME</p> <p>=== [[first]] <code>first</code> Action</p>"},{"location":"spark-sql-Dataset-actions/#source-scala_15","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#first-t_1","title":"first(): T","text":"<p><code>first</code>...FIXME</p> <p>=== [[foreach]] <code>foreach</code> Action</p>"},{"location":"spark-sql-Dataset-actions/#source-scala_16","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#foreachf-t-unit-unit_1","title":"foreach(f: T =&gt; Unit): Unit","text":"<p><code>foreach</code>...FIXME</p> <p>=== [[foreachPartition]] <code>foreachPartition</code> Action</p>"},{"location":"spark-sql-Dataset-actions/#source-scala_17","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#foreachpartitionf-iteratort-unit-unit_1","title":"foreachPartition(f: Iterator[T] =&gt; Unit): Unit","text":"<p><code>foreachPartition</code>...FIXME</p> <p>=== [[head]] <code>head</code> Action</p>"},{"location":"spark-sql-Dataset-actions/#source-scala_18","title":"[source, scala]","text":"<p>head(): T // &lt;1&gt; head(n: Int): Array[T]</p> <p>&lt;1&gt; Calls the other <code>head</code> with <code>n</code> as <code>1</code> and takes the first element</p> <p><code>head</code>...FIXME</p> <p>=== [[reduce]] <code>reduce</code> Action</p>"},{"location":"spark-sql-Dataset-actions/#source-scala_19","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#reducefunc-t-t-t-t_1","title":"reduce(func: (T, T) =&gt; T): T","text":"<p><code>reduce</code>...FIXME</p> <p>=== [[show]] <code>show</code> Action</p>"},{"location":"spark-sql-Dataset-actions/#source-scala_20","title":"[source, scala]","text":"<p>show(): Unit show(truncate: Boolean): Unit show(numRows: Int): Unit show(numRows: Int, truncate: Boolean): Unit show(numRows: Int, truncate: Int): Unit show(numRows: Int, truncate: Int, vertical: Boolean): Unit</p> <p><code>show</code>...FIXME</p> <p>=== [[summary]] Calculating Statistics -- <code>summary</code> Action</p>"},{"location":"spark-sql-Dataset-actions/#source-scala_21","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#summarystatistics-string-dataframe_1","title":"summary(statistics: String*): DataFrame","text":"<p><code>summary</code> calculates specified statistics for numeric and string columns.</p> <p>The default statistics are: <code>count</code>, <code>mean</code>, <code>stddev</code>, <code>min</code>, <code>max</code> and <code>25%</code>, <code>50%</code>, <code>75%</code> percentiles.</p> <p>NOTE: <code>summary</code> accepts arbitrary approximate percentiles specified as a percentage (e.g. <code>10%</code>).</p> <p>Internally, <code>summary</code> uses the <code>StatFunctions</code> to calculate the requested summaries for the <code>Dataset</code>.</p> <p>=== [[take]] Taking First Records -- <code>take</code> Action</p>"},{"location":"spark-sql-Dataset-actions/#source-scala_22","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#taken-int-arrayt_1","title":"take(n: Int): Array[T]","text":"<p><code>take</code> is an action on a <code>Dataset</code> that returns a collection of <code>n</code> records.</p> <p>WARNING: <code>take</code> loads all the data into the memory of the Spark application's driver process and for a large <code>n</code> could result in <code>OutOfMemoryError</code>.</p> <p>Internally, <code>take</code> creates a new <code>Dataset</code> with <code>Limit</code> logical plan for <code>Literal</code> expression and the current <code>LogicalPlan</code>. It then runs the SparkPlan.md[SparkPlan] that produces a <code>Array[InternalRow]</code> that is in turn decoded to <code>Array[T]</code> using a bounded encoder.</p> <p>=== [[toLocalIterator]] <code>toLocalIterator</code> Action</p>"},{"location":"spark-sql-Dataset-actions/#source-scala_23","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-actions/#tolocaliterator-javautiliteratort_1","title":"toLocalIterator(): java.util.Iterator[T]","text":"<p><code>toLocalIterator</code>...FIXME</p>"},{"location":"spark-sql-Dataset-basic-actions/","title":"Dataset API \u2014 Basic Actions","text":"<p>Basic actions are a set of operators (methods) of the &lt;&gt; for transforming a <code>Dataset</code> into a session-scoped or global temporary view and other basic actions (FIXME). <p>NOTE: Basic actions are the methods in the <code>Dataset</code> Scala class that are grouped in <code>basic</code> group name, i.e. <code>@group basic</code>.</p> <p>[[methods]] .Dataset API's Basic Actions [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Action | Description</p> <p>| <code>cache</code> a| [[cache]]</p>"},{"location":"spark-sql-Dataset-basic-actions/#source-scala","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#cache-thistype","title":"cache(): this.type","text":"<p>Marks the <code>Dataset</code> to be persisted (cached) and is actually a synonym of &lt;&gt; basic action <p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_1","title":"[source, scala]","text":"<p>checkpoint(): Dataset[T] checkpoint(eager: Boolean): Dataset[T]</p> <p>Checkpoints the <code>Dataset</code> in a reliable way (using a reliable HDFS-compliant file system, e.g. Hadoop HDFS or Amazon S3)</p> <p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_2","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#columns-arraystring","title":"columns: Array[String]","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_3","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#createglobaltempviewviewname-string-unit","title":"createGlobalTempView(viewName: String): Unit","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_4","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#createorreplaceglobaltempviewviewname-string-unit","title":"createOrReplaceGlobalTempView(viewName: String): Unit","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_5","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#createorreplacetempviewviewname-string-unit","title":"createOrReplaceTempView(viewName: String): Unit","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_6","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#createtempviewviewname-string-unit","title":"createTempView(viewName: String): Unit","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_7","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#dtypes-arraystring-string","title":"dtypes: Array[(String, String)]","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_8","title":"[source, scala]","text":"<p>explain(): Unit explain(extended: Boolean): Unit</p> <p>Displays the logical and physical plans of the <code>Dataset</code>, i.e. displays the logical and physical plans (with optional cost and codegen summaries) to the standard output</p> <p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_9","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#hintname-string-parameters-any-datasett","title":"hint(name: String, parameters: Any*): Dataset[T]","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_10","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#inputfiles-arraystring","title":"inputFiles: Array[String]","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_11","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#isempty-boolean","title":"isEmpty: Boolean","text":"<p>(New in 2.4.0)</p> <p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_12","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#islocal-boolean","title":"isLocal: Boolean","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_13","title":"[source, scala]","text":"<p>localCheckpoint(): Dataset[T] localCheckpoint(eager: Boolean): Dataset[T]</p> <p>Checkpoints the <code>Dataset</code> locally on executors (and therefore unreliably)</p> <p>| <code>persist</code> a| [[persist]]</p>"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_14","title":"[source, scala]","text":"<p>persist(): this.type // &lt;1&gt; persist(newLevel: StorageLevel): this.type</p> <p>&lt;1&gt; Assumes the default storage level <code>MEMORY_AND_DISK</code></p> <p>Marks the <code>Dataset</code> to be persisted the next time an action is executed</p> <p>Internally, <code>persist</code> simply request the <code>CacheManager</code> to cache the structured query.</p> <p>NOTE: <code>persist</code> uses the CacheManager from the &lt;&gt; associated with the &lt;&gt; (of the Dataset). <p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_15","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#printschema-unit","title":"printSchema(): Unit","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_16","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#rdd-rddt","title":"rdd: RDD[T]","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_17","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#schema-structtype","title":"schema: StructType","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_18","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#storagelevel-storagelevel","title":"storageLevel: StorageLevel","text":"<p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_19","title":"[source, scala]","text":"<p>toDF(): DataFrame toDF(colNames: String*): DataFrame</p> <p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_20","title":"[source, scala]","text":"<p>unpersist(): this.type unpersist(blocking: Boolean): this.type</p> <p>Unpersists the <code>Dataset</code></p> <p>| &lt;&gt; a|"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_21","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#write-dataframewritert","title":"write: DataFrameWriter[T]","text":"<p>Returns a DataFrameWriter for saving the content of the (non-streaming) <code>Dataset</code> out to an external storage |===</p> <p>=== [[checkpoint]] Reliably Checkpointing Dataset -- <code>checkpoint</code> Basic Action</p>"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_22","title":"[source, scala]","text":"<p>checkpoint(): Dataset[T]  // &lt;1&gt; checkpoint(eager: Boolean): Dataset[T]  // &lt;2&gt;</p> <p>&lt;1&gt; <code>eager</code> and <code>reliableCheckpoint</code> flags enabled &lt;2&gt; <code>reliableCheckpoint</code> flag enabled</p> <p>NOTE: <code>checkpoint</code> is an experimental operator and the API is evolving towards becoming stable.</p> <p><code>checkpoint</code> simply requests the <code>Dataset</code> to &lt;&gt; with the given <code>eager</code> flag and the <code>reliableCheckpoint</code> flag enabled. <p>=== [[createTempView]] <code>createTempView</code> Basic Action</p>"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_23","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#createtempviewviewname-string-unit_1","title":"createTempView(viewName: String): Unit","text":"<p><code>createTempView</code>...FIXME</p> <p>NOTE: <code>createTempView</code> is used when...FIXME</p> <p>=== [[createOrReplaceTempView]] <code>createOrReplaceTempView</code> Basic Action</p>"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_24","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#createorreplacetempviewviewname-string-unit_1","title":"createOrReplaceTempView(viewName: String): Unit","text":"<p><code>createOrReplaceTempView</code>...FIXME</p> <p>NOTE: <code>createOrReplaceTempView</code> is used when...FIXME</p> <p>=== [[createGlobalTempView]] <code>createGlobalTempView</code> Basic Action</p>"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_25","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#createglobaltempviewviewname-string-unit_1","title":"createGlobalTempView(viewName: String): Unit","text":"<p><code>createGlobalTempView</code>...FIXME</p> <p>NOTE: <code>createGlobalTempView</code> is used when...FIXME</p> <p>=== [[createOrReplaceGlobalTempView]] <code>createOrReplaceGlobalTempView</code> Basic Action</p>"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_26","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#createorreplaceglobaltempviewviewname-string-unit_1","title":"createOrReplaceGlobalTempView(viewName: String): Unit","text":"<p><code>createOrReplaceGlobalTempView</code>...FIXME</p> <p>NOTE: <code>createOrReplaceGlobalTempView</code> is used when...FIXME</p> <p>=== [[createTempViewCommand]] <code>createTempViewCommand</code> Internal Method</p>"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_27","title":"[source, scala]","text":"<p>createTempViewCommand(   viewName: String,   replace: Boolean,   global: Boolean): CreateViewCommand</p> <p><code>createTempViewCommand</code>...FIXME</p> <p>NOTE: <code>createTempViewCommand</code> is used when the following <code>Dataset</code> operators are used: &lt;&gt;, &lt;&gt;, &lt;&gt; and &lt;&gt;. <p>=== [[explain]] Displaying Logical and Physical Plans, Their Cost and Codegen -- <code>explain</code> Basic Action</p>"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_28","title":"[source, scala]","text":"<p>explain(): Unit // &lt;1&gt; explain(extended: Boolean): Unit</p> <p>&lt;1&gt; Turns the <code>extended</code> flag on</p> <p><code>explain</code> prints the spark-sql-LogicalPlan.md[logical] and (with <code>extended</code> flag enabled) SparkPlan.md[physical] plans, their cost and codegen to the console.</p> <p>TIP: Use <code>explain</code> to review the structured queries and optimizations applied.</p> <p>Internally, <code>explain</code> creates a ExplainCommand.md[ExplainCommand] logical command and requests <code>SessionState</code> to SessionState.md#executePlan[execute it] (to get a QueryExecution back).</p> <p>NOTE: <code>explain</code> uses ExplainCommand.md[ExplainCommand] logical command that, when ExplainCommand.md#run[executed], gives different text representations of QueryExecution (for the Dataset's spark-sql-LogicalPlan.md[LogicalPlan]) depending on the flags (e.g. extended, codegen, and cost which are disabled by default).</p> <p><code>explain</code> then requests <code>QueryExecution</code> for the optimized physical query plan and collects the records (as InternalRow objects).</p>"},{"location":"spark-sql-Dataset-basic-actions/#note","title":"[NOTE]","text":""},{"location":"spark-sql-Dataset-basic-actions/#explain-uses-datasets-datasetmdsparksessionsparksession-to-sparksessionmdsessionstateaccess-the-current-sessionstate","title":"<code>explain</code> uses Dataset's Dataset.md#sparkSession[SparkSession] to SparkSession.md#sessionState[access the current <code>SessionState</code>].","text":"<p>In the end, <code>explain</code> goes over the <code>InternalRow</code> records and converts them to lines to display to console.</p> <p>Note</p> <p><code>explain</code> \"converts\" an <code>InternalRow</code> record to a line using getString at position <code>0</code>.</p> <p>TIP: If you are serious about query debugging you could also use the Debugging Query Execution facility.</p> <pre><code>scala&gt; spark.range(10).explain(extended = true)\n== Parsed Logical Plan ==\nRange (0, 10, step=1, splits=Some(8))\n\n== Analyzed Logical Plan ==\nid: bigint\nRange (0, 10, step=1, splits=Some(8))\n\n== Optimized Logical Plan ==\nRange (0, 10, step=1, splits=Some(8))\n\n== Physical Plan ==\n*Range (0, 10, step=1, splits=Some(8))\n</code></pre>"},{"location":"spark-sql-Dataset-basic-actions/#specifying-hint","title":"Specifying Hint <pre><code>hint(\n  name: String,\n  parameters: Any*): Dataset[T]\n</code></pre> <p><code>hint</code> operator is part of Hint Framework to specify a hint (by <code>name</code> and <code>parameters</code>) for a <code>Dataset</code>.</p> <p>Internally, <code>hint</code> simply attaches UnresolvedHint.md[UnresolvedHint] unary logical operator to an \"analyzed\" <code>Dataset</code> (i.e. the Dataset.md#logicalPlan[analyzed logical plan] of a <code>Dataset</code>).</p> <pre><code>val ds = spark.range(3)\nval plan = ds.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 Range (0, 3, step=1, splits=Some(8))\n\n// Attach a hint\nval dsHinted = ds.hint(\"myHint\", 100, true)\nval plan = dsHinted.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'UnresolvedHint myHint, [100, true]\n01 +- Range (0, 3, step=1, splits=Some(8))\n</code></pre>  <p>Note</p> <p><code>hint</code> adds an &lt;&gt; unary logical operator to an analyzed logical plan that indirectly triggers analysis phase that executes logical commands and their unions as well as resolves all hints that have already been added to a logical plan.  <p>=== [[localCheckpoint]] Locally Checkpointing Dataset -- <code>localCheckpoint</code> Basic Action</p>","text":""},{"location":"spark-sql-Dataset-basic-actions/#source-scala_29","title":"[source, scala]","text":"<p>localCheckpoint(): Dataset[T] // &lt;1&gt; localCheckpoint(eager: Boolean): Dataset[T]</p> <p>&lt;1&gt; <code>eager</code> flag enabled</p> <p><code>localCheckpoint</code> simply uses &lt;&gt; operator with the input <code>eager</code> flag and <code>reliableCheckpoint</code> flag disabled (<code>false</code>). <p>=== [[checkpoint-internal]] <code>checkpoint</code> Internal Method</p>"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_30","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#checkpointeager-boolean-reliablecheckpoint-boolean-datasett","title":"checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T]","text":"<p><code>checkpoint</code> requests Dataset.md#queryExecution[QueryExecution] (of the <code>Dataset</code>) to generate an RDD of internal binary rows (<code>internalRdd</code>) and then requests the RDD to make a copy of all the rows (by adding a <code>MapPartitionsRDD</code>).</p> <p>Depending on <code>reliableCheckpoint</code> flag, <code>checkpoint</code> marks the RDD for (reliable) checkpointing (<code>true</code>) or local checkpointing (<code>false</code>).</p> <p>With <code>eager</code> flag on, <code>checkpoint</code> counts the number of records in the RDD (by executing <code>RDD.count</code>) that gives the effect of immediate eager checkpointing.</p> <p><code>checkpoint</code> requests Dataset.md#queryExecution[QueryExecution] (of the <code>Dataset</code>) for optimized physical query plan (the plan is used to get the SparkPlan.md#outputPartitioning[outputPartitioning] and SparkPlan.md#outputOrdering[outputOrdering] for the result <code>Dataset</code>).</p> <p>In the end, <code>checkpoint</code> Dataset.md#ofRows[creates a DataFrame] with a new LogicalRDD.md#creating-instance[logical plan node for scanning data from an RDD of InternalRows] (<code>LogicalRDD</code>).</p> <p>NOTE: <code>checkpoint</code> is used in the <code>Dataset</code> untyped transformations, i.e. checkpoint and localCheckpoint.</p> <p>=== [[rdd]] Generating RDD of Internal Binary Rows -- <code>rdd</code> Basic Action</p>"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_31","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#rdd-rddt_1","title":"rdd: RDD[T]","text":"<p>Whenever you are in need to convert a <code>Dataset</code> into a <code>RDD</code>, executing <code>rdd</code> method gives you the RDD of the proper input object type (not Row as in DataFrames) that sits behind the <code>Dataset</code>.</p>"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_32","title":"[source, scala]","text":"<p>scala&gt; val rdd = tokens.rdd rdd: org.apache.spark.rdd.RDD[Token] = MapPartitionsRDD[11] at rdd at :30 <p>Internally, it looks ExpressionEncoder (for the <code>Dataset</code>) up and accesses the <code>deserializer</code> expression. That gives the DataType of the result of evaluating the expression.</p> <p>NOTE: A deserializer expression is used to decode an InternalRow to an object of type <code>T</code>. See ExpressionEncoder.</p> <p>It then executes a DeserializeToObject.md[<code>DeserializeToObject</code> logical operator] that will produce a <code>RDD[InternalRow]</code> that is converted into the proper <code>RDD[T]</code> using the <code>DataType</code> and <code>T</code>.</p> <p>NOTE: It is a lazy operation that \"produces\" a <code>RDD[T]</code>.</p> <p>=== [[schema]] Accessing Schema -- <code>schema</code> Basic Action</p> <p>A <code>Dataset</code> has a schema.</p>"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_33","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#schema-structtype_1","title":"schema: StructType","text":""},{"location":"spark-sql-Dataset-basic-actions/#tip","title":"[TIP]","text":"<p>You may also use the following methods to learn about the schema:</p> <ul> <li><code>printSchema(): Unit</code></li> <li>"},{"location":"spark-sql-Dataset-basic-actions/#_1","title":"&lt;&gt;   <p>=== [[toDF]] Converting Typed Dataset to Untyped DataFrame -- <code>toDF</code> Basic Action</p>","text":""},{"location":"spark-sql-Dataset-basic-actions/#source-scala_34","title":"[source, scala]","text":"<p>toDF(): DataFrame toDF(colNames: String*): DataFrame</p> <p><code>toDF</code> converts a Dataset.md[Dataset] into a DataFrame.</p> <p>Internally, the empty-argument <code>toDF</code> creates a <code>Dataset[Row]</code> using the <code>Dataset</code>'s SparkSession.md[SparkSession] and QueryExecution with the encoder being RowEncoder.</p> <p>CAUTION: FIXME Describe <code>toDF(colNames: String*)</code></p> <p>=== [[unpersist]] Unpersisting Cached Dataset -- <code>unpersist</code> Basic Action</p>"},{"location":"spark-sql-Dataset-basic-actions/#source-scala_35","title":"[source, scala]","text":"<p>unpersist(): this.type unpersist(blocking: Boolean): this.type</p> <p><code>unpersist</code> uncache the <code>Dataset</code> possibly by <code>blocking</code> the call.</p> <p>Internally, <code>unpersist</code> requests <code>CacheManager</code> to uncache the query.</p> <p>CAUTION: FIXME</p>"},{"location":"spark-sql-Dataset-basic-actions/#accessing-dataframewriter-to-describe-writing-dataset","title":"Accessing DataFrameWriter (to Describe Writing Dataset)","text":""},{"location":"spark-sql-Dataset-basic-actions/#source-scala_36","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#write-dataframewritert_1","title":"write: DataFrameWriter[T] <p><code>write</code> gives DataFrameWriter for records of type <code>T</code>.</p> <pre><code>import org.apache.spark.sql.{DataFrameWriter, Dataset}\nval ints: Dataset[Int] = (0 to 5).toDS\nval writer: DataFrameWriter[Int] = ints.write\n</code></pre> <p>=== [[isEmpty]] <code>isEmpty</code> Typed Transformation</p>","text":""},{"location":"spark-sql-Dataset-basic-actions/#source-scala_37","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#isempty-boolean_1","title":"isEmpty: Boolean <p><code>isEmpty</code>...FIXME</p> <p>=== [[isLocal]] <code>isLocal</code> Typed Transformation</p>","text":""},{"location":"spark-sql-Dataset-basic-actions/#source-scala_38","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-basic-actions/#islocal-boolean_1","title":"isLocal: Boolean <p><code>isLocal</code>...FIXME</p>","text":""},{"location":"spark-sql-Dataset-typed-transformations/","title":"Dataset API \u2014 Typed Transformations","text":"<p>Typed transformations are part of the Dataset API for transforming a <code>Dataset</code> with an Encoder (except the RowEncoder).</p> <p>NOTE: Typed transformations are the methods in the <code>Dataset</code> Scala class that are grouped in <code>typedrel</code> group name, i.e. <code>@group typedrel</code>.</p> <p>=== [[as]] Enforcing Type -- <code>as</code> Typed Transformation</p>"},{"location":"spark-sql-Dataset-typed-transformations/#source-scala","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-typed-transformations/#asu-encoder-datasetu","title":"as[U: Encoder]: Dataset[U]","text":"<p><code>as[T]</code> allows for converting from a weakly-typed <code>Dataset</code> of Rows to <code>Dataset[T]</code> with <code>T</code> being a domain class (that can enforce a stronger schema).</p> <pre><code>// Create DataFrame of pairs\nval df = Seq(\"hello\", \"world!\").zipWithIndex.map(_.swap).toDF(\"id\", \"token\")\n\nscala&gt; df.printSchema\nroot\n |-- id: integer (nullable = false)\n |-- token: string (nullable = true)\n\nscala&gt; val ds = df.as[(Int, String)]\nds: org.apache.spark.sql.Dataset[(Int, String)] = [id: int, token: string]\n\n// It's more helpful to have a case class for the conversion\nfinal case class MyRecord(id: Int, token: String)\n\nscala&gt; val myRecords = df.as[MyRecord]\nmyRecords: org.apache.spark.sql.Dataset[MyRecord] = [id: int, token: string]\n</code></pre> <p>=== [[coalesce]] Repartitioning Dataset with Shuffle Disabled -- <code>coalesce</code> Typed Transformation</p>"},{"location":"spark-sql-Dataset-typed-transformations/#source-scala_1","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-typed-transformations/#coalescenumpartitions-int-datasett","title":"coalesce(numPartitions: Int): Dataset[T]","text":"<p><code>coalesce</code> operator repartitions the <code>Dataset</code> to exactly <code>numPartitions</code> partitions.</p> <p>Internally, <code>coalesce</code> creates a <code>Repartition</code> logical operator with <code>shuffle</code> disabled (which is marked as <code>false</code> in the below <code>explain</code>'s output).</p>"},{"location":"spark-sql-Dataset-typed-transformations/#source-scala_2","title":"[source, scala]","text":"<p>scala&gt; spark.range(5).coalesce(1).explain(extended = true) == Parsed Logical Plan == Repartition 1, false +- Range (0, 5, step=1, splits=Some(8))</p> <p>== Analyzed Logical Plan == id: bigint Repartition 1, false +- Range (0, 5, step=1, splits=Some(8))</p> <p>== Optimized Logical Plan == Repartition 1, false +- Range (0, 5, step=1, splits=Some(8))</p> <p>== Physical Plan == Coalesce 1 +- *Range (0, 5, step=1, splits=Some(8))</p> <p>=== [[flatMap]] Creating Zero or More Records -- <code>flatMap</code> Typed Transformation</p>"},{"location":"spark-sql-Dataset-typed-transformations/#source-scala_3","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-typed-transformations/#flatmapu-encoder-datasetu","title":"flatMapU: Encoder: Dataset[U]","text":"<p><code>flatMap</code> returns a new <code>Dataset</code> (of type <code>U</code>) with all records (of type <code>T</code>) mapped over using the function <code>func</code> and then flattening the results.</p> <p>NOTE: <code>flatMap</code> can create new records. It deprecated <code>explode</code>.</p>"},{"location":"spark-sql-Dataset-typed-transformations/#source-scala_4","title":"[source, scala]","text":"<p>final case class Sentence(id: Long, text: String) val sentences = Seq(Sentence(0, \"hello world\"), Sentence(1, \"witaj swiecie\")).toDS</p> <p>scala&gt; sentences.flatMap(s =&gt; s.text.split(\"\\s+\")).show +-------+ |  value| +-------+ |  hello| |  world| |  witaj| |swiecie| +-------+</p> <p>Internally, <code>flatMap</code> calls &lt;&gt; with the partitions <code>flatMap(ped)</code>. <p>=== [[randomSplit]] Randomly Split Dataset Into Two or More Datasets Per Weight -- <code>randomSplit</code> Typed Transformation</p>"},{"location":"spark-sql-Dataset-typed-transformations/#source-scala_5","title":"[source, scala]","text":"<p>randomSplit(weights: Array[Double]): Array[Dataset[T]] randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]]</p> <p><code>randomSplit</code> randomly splits the <code>Dataset</code> per <code>weights</code>.</p> <p><code>weights</code> doubles should sum up to <code>1</code> and will be normalized if they do not.</p> <p>You can define <code>seed</code> and if you don't, a random <code>seed</code> will be used.</p> <p>NOTE: <code>randomSplit</code> is commonly used in Spark MLlib to split an input Dataset into two datasets for training and validation.</p>"},{"location":"spark-sql-Dataset-typed-transformations/#source-scala_6","title":"[source, scala]","text":"<p>val ds = spark.range(10) scala&gt; ds.randomSplit(ArrayDouble).foreach(_.show) +---+ | id| +---+ |  0| |  1| |  2| +---+</p> <p>+---+ | id| +---+ |  3| |  4| |  5| |  6| |  7| |  8| |  9| +---+</p> <p>=== [[repartition]] Repartitioning Dataset (Shuffle Enabled) -- <code>repartition</code> Typed Transformation</p>"},{"location":"spark-sql-Dataset-typed-transformations/#source-scala_7","title":"[source, scala]","text":"<p>repartition(partitionExprs: Column*): Dataset[T] repartition(numPartitions: Int): Dataset[T] repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T]</p> <p><code>repartition</code> operators repartition the <code>Dataset</code> to exactly <code>numPartitions</code> partitions or using <code>partitionExprs</code> expressions.</p> <p><code>repartition</code> creates a Repartition or RepartitionByExpression logical operators with <code>shuffle</code> enabled (which is <code>true</code> in the below <code>explain</code>'s output beside <code>Repartition</code>).</p> <pre><code>scala&gt; spark.range(5).repartition(1).explain(extended = true)\n== Parsed Logical Plan ==\nRepartition 1, true\n+- Range (0, 5, step=1, splits=Some(8))\n\n== Analyzed Logical Plan ==\nid: bigint\nRepartition 1, true\n+- Range (0, 5, step=1, splits=Some(8))\n\n== Optimized Logical Plan ==\nRepartition 1, true\n+- Range (0, 5, step=1, splits=Some(8))\n\n== Physical Plan ==\nExchange RoundRobinPartitioning(1)\n+- *Range (0, 5, step=1, splits=Some(8))\n</code></pre> <p>NOTE: <code>repartition</code> methods correspond to SQL's spark-sql-SparkSqlAstBuilder.md#withRepartitionByExpression[DISTRIBUTE BY or CLUSTER BY clauses].</p> <p>=== [[sortWithinPartitions]] <code>sortWithinPartitions</code> Typed Transformation</p>"},{"location":"spark-sql-Dataset-typed-transformations/#source-scala_8","title":"[source, scala]","text":"<p>sortWithinPartitions(sortExprs: Column*): Dataset[T] sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T]</p> <p><code>sortWithinPartitions</code> simply calls the internal &lt;&gt; method with the <code>global</code> flag disabled (<code>false</code>). <p>=== [[toJSON]] <code>toJSON</code> Typed Transformation</p>"},{"location":"spark-sql-Dataset-typed-transformations/#source-scala_9","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-typed-transformations/#tojson-datasetstring","title":"toJSON: Dataset[String]","text":"<p><code>toJSON</code> maps the content of <code>Dataset</code> to a <code>Dataset</code> of strings in JSON format.</p> <pre><code>scala&gt; val ds = Seq(\"hello\", \"world\", \"foo bar\").toDS\nds: org.apache.spark.sql.Dataset[String] = [value: string]\n\nscala&gt; ds.toJSON.show\n+-------------------+\n|              value|\n+-------------------+\n|  {\"value\":\"hello\"}|\n|  {\"value\":\"world\"}|\n|{\"value\":\"foo bar\"}|\n+-------------------+\n</code></pre> <p>Internally, <code>toJSON</code> grabs the <code>RDD[InternalRow]</code> (of the QueryExecution.md#toRdd[QueryExecution] of the <code>Dataset</code>) and spark-rdd-transformations.md#mapPartitions[maps the records (per RDD partition)] into JSON.</p> <p>NOTE: <code>toJSON</code> uses Jackson's JSON parser -- https://github.com/FasterXML/jackson-module-scala[jackson-module-scala].</p> <p>=== [[transform]] Transforming Datasets -- <code>transform</code> Typed Transformation</p>"},{"location":"spark-sql-Dataset-typed-transformations/#source-scala_10","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-typed-transformations/#transformu-datasetu","title":"transformU: Dataset[U]","text":"<p><code>transform</code> applies <code>t</code> function to the source <code>Dataset[T]</code> to produce a result <code>Dataset[U]</code>. It is for chaining custom transformations.</p> <pre><code>val dataset = spark.range(5)\n\n// Transformation t\nimport org.apache.spark.sql.Dataset\ndef withDoubled(longs: Dataset[java.lang.Long]) = longs.withColumn(\"doubled\", 'id * 2)\n\nscala&gt; dataset.transform(withDoubled).show\n+---+-------+\n| id|doubled|\n+---+-------+\n|  0|      0|\n|  1|      2|\n|  2|      4|\n|  3|      6|\n|  4|      8|\n+---+-------+\n</code></pre> <p>Internally, <code>transform</code> executes <code>t</code> function on the current <code>Dataset[T]</code>.</p> <p>=== [[withWatermark]] Creating Streaming Dataset with EventTimeWatermark Logical Operator -- <code>withWatermark</code> Streaming Typed Transformation</p>"},{"location":"spark-sql-Dataset-typed-transformations/#source-scala_11","title":"[source, scala]","text":""},{"location":"spark-sql-Dataset-typed-transformations/#withwatermarkeventtime-string-delaythreshold-string-datasett","title":"withWatermark(eventTime: String, delayThreshold: String): Dataset[T]","text":"<p>Internally, <code>withWatermark</code> creates a <code>Dataset</code> with <code>EventTimeWatermark</code> logical plan for Dataset.md#isStreaming[streaming Datasets].</p> <p>NOTE: <code>withWatermark</code> uses <code>EliminateEventTimeWatermark</code> logical rule to eliminate <code>EventTimeWatermark</code> logical plan for non-streaming batch <code>Datasets</code>.</p>"},{"location":"spark-sql-Dataset-typed-transformations/#source-scala_12","title":"[source, scala]","text":"<p>// Create a batch dataset val events = spark.range(0, 50, 10).   withColumn(\"timestamp\", from_unixtime(unix_timestamp - 'id)).   select('timestamp, 'id as \"count\") scala&gt; events.show +-------------------+-----+ |          timestamp|count| +-------------------+-----+ |2017-06-25 21:21:14|    0| |2017-06-25 21:21:04|   10| |2017-06-25 21:20:54|   20| |2017-06-25 21:20:44|   30| |2017-06-25 21:20:34|   40| +-------------------+-----+</p> <p>// the dataset is a non-streaming batch one... scala&gt; events.isStreaming res1: Boolean = false</p> <p>// ...so EventTimeWatermark is not included in the logical plan val watermarked = events.   withWatermark(eventTime = \"timestamp\", delayThreshold = \"20 seconds\") scala&gt; println(watermarked.queryExecution.logical.numberedTreeString) 00 Project [timestamp#284, id#281L AS count#288L] 01 +- Project [id#281L, from_unixtime((unix_timestamp(current_timestamp(), yyyy-MM-dd HHss, Some(America/Chicago)) - id#281L), yyyy-MM-dd HHss, Some(America/Chicago)) AS timestamp#284] 02    +- Range (0, 50, step=10, splits=Some(8))</p> <p>// Let's create a streaming Dataset import org.apache.spark.sql.types.StructType val schema = new StructType().   add(\"timestamp\".timestamp).   add(\"count\".long) scala&gt; schema.printTreeString root  |-- timestamp: timestamp (nullable = true)  |-- count: long (nullable = true)</p> <p>val events = spark.   readStream.   schema(schema).   csv(\"events\").   withWatermark(eventTime = \"timestamp\", delayThreshold = \"20 seconds\") scala&gt; println(events.queryExecution.logical.numberedTreeString) 00 'EventTimeWatermark 'timestamp, interval 20 seconds 01 +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@75abcdd4,csv,List(),Some(StructType(StructField(timestamp,TimestampType,true), StructField(count,LongType,true))),List(),None,Map(path -&gt; events),None), FileSource[events], [timestamp#329, count#330L]</p>"},{"location":"spark-sql-Dataset-typed-transformations/#note","title":"[NOTE]","text":"<p><code>delayThreshold</code> is parsed using <code>CalendarInterval.fromString</code> with interval formatted as described in expressions/TimeWindow.md[TimeWindow] unary expression.</p>"},{"location":"spark-sql-Dataset-typed-transformations/#0-years-0-months-1-week-0-days-0-hours-1-minute-20-seconds-0-milliseconds-0-microseconds","title":"<pre><code>0 years 0 months 1 week 0 days 0 hours 1 minute 20 seconds 0 milliseconds 0 microseconds\n</code></pre>","text":"<p>NOTE: <code>delayThreshold</code> must not be negative (and <code>milliseconds</code> and <code>months</code> should both be equal or greater than <code>0</code>).</p> <p>NOTE: <code>withWatermark</code> is used when...FIXME</p>"},{"location":"spark-sql-column-operators/","title":"Column API -- Column Operators","text":"<p>Column API is a &lt;&gt; to work with values in a column (of a &lt;&gt;). <p>[[methods]] [[operators]] .Column Operators [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Operator | Description</p> <p>| asc a| [[asc]]</p>"},{"location":"spark-sql-column-operators/#source-scala","title":"[source, scala]","text":""},{"location":"spark-sql-column-operators/#asc-column","title":"asc: Column","text":"<p>| asc_nulls_first a| [[asc_nulls_first]]</p>"},{"location":"spark-sql-column-operators/#source-scala_1","title":"[source, scala]","text":""},{"location":"spark-sql-column-operators/#asc_nulls_first-column","title":"asc_nulls_first: Column","text":"<p>| asc_nulls_last a| [[asc_nulls_last]]</p>"},{"location":"spark-sql-column-operators/#source-scala_2","title":"[source, scala]","text":""},{"location":"spark-sql-column-operators/#asc_nulls_last-column","title":"asc_nulls_last: Column","text":"<p>| desc a| [[desc]]</p>"},{"location":"spark-sql-column-operators/#source-scala_3","title":"[source, scala]","text":""},{"location":"spark-sql-column-operators/#desc-column","title":"desc: Column","text":"<p>| desc_nulls_first a| [[desc_nulls_first]]</p>"},{"location":"spark-sql-column-operators/#source-scala_4","title":"[source, scala]","text":""},{"location":"spark-sql-column-operators/#desc_nulls_first-column","title":"desc_nulls_first: Column","text":"<p>| desc_nulls_last a| [[desc_nulls_last]]</p>"},{"location":"spark-sql-column-operators/#source-scala_5","title":"[source, scala]","text":""},{"location":"spark-sql-column-operators/#desc_nulls_last-column","title":"desc_nulls_last: Column","text":"<p>| &lt;&gt; a| [[isin]]"},{"location":"spark-sql-column-operators/#source-scala_6","title":"[source, scala]","text":""},{"location":"spark-sql-column-operators/#isinlist-any-column","title":"isin(list: Any*): Column","text":"<p>| isInCollection a| [[isInCollection]]</p>"},{"location":"spark-sql-column-operators/#source-scala_7","title":"[source, scala]","text":""},{"location":"spark-sql-column-operators/#isincollectionvalues-scalacollectioniterable_-column","title":"isInCollection(values: scala.collection.Iterable[_]): Column","text":"<p>(New in 2.4.0) An expression operator that is <code>true</code> if the value of the column is in the given <code>values</code> collection</p> <p><code>isInCollection</code> is simply a synonym of &lt;&gt; operator. |=== <p>=== [[isin-internals]] <code>isin</code> Operator</p>"},{"location":"spark-sql-column-operators/#source-scala_8","title":"[source, scala]","text":""},{"location":"spark-sql-column-operators/#isinlist-any-column_1","title":"isin(list: Any*): Column","text":"<p>Internally, <code>isin</code> creates a <code>Column</code> with &lt;&gt; predicate expression."},{"location":"spark-sql-column-operators/#source-scala_9","title":"[source, scala]","text":"<p>val ids = Seq((1, 2, 2), (2, 3, 1)).toDF(\"x\", \"y\", \"id\") scala&gt; ids.show +---+---+---+ |  x|  y| id| +---+---+---+ |  1|  2|  2| |  2|  3|  1| +---+---+---+</p> <p>val c = \"id\" isin (\"x\", $\"y\") val q = ids.filter\u00a9 scala&gt; q.show +---+---+---+ |  x|  y| id| +---+---+---+ |  1|  2|  2| +---+---+---+</p> <p>// Note that isin accepts non-Column values val c = $\"id\" isin (\"x\", \"y\") val q = ids.filter\u00a9 scala&gt; q.show +---+---+---+ |  x|  y| id| +---+---+---+ +---+---+---+</p>"},{"location":"spark-sql-dataset-operators/","title":"Dataset API \u2014 Dataset Operators","text":"<p>Dataset API is a set of operators with typed and untyped transformations, and actions to work with a structured query (as a Dataset) as a whole.</p>"},{"location":"spark-sql-dataset-operators/#dataset-operators-transformations-and-actions","title":"Dataset Operators (Transformations and Actions)","text":""},{"location":"spark-sql-dataset-operators/#explain","title":"explain <pre><code>// Uses simple mode\nexplain(): Unit\n// Uses extended or simple mode\nexplain(\n  extended: Boolean): Unit\nexplain(\n  mode: String): Unit\n</code></pre> <p>explain</p> <p>A basic action to display the logical and physical plans of the <code>Dataset</code>, i.e. displays the logical and physical plans (with optional cost and codegen summaries) to the standard output</p> <p>[cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Operator | Description</p> <p>| agg a| [[agg]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala","title":"[source, scala] <p>agg(aggExpr: (String, String), aggExprs: (String, String)): DataFrame agg(expr: Column, exprs: Column): DataFrame agg(exprs: Map[String, String]): DataFrame</p>  <p>An untyped transformation</p> <p>| &lt;&gt; a| [[alias]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_1","title":"[source, scala] <p>alias(alias: String): Dataset[T] alias(alias: Symbol): Dataset[T]</p>  <p>A typed transformation that is a mere synonym of &lt;&gt;. <p>| apply a| [[apply]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_2","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#applycolname-string-column","title":"apply(colName: String): Column <p>An untyped transformation to select a column based on the column name (i.e. maps a <code>Dataset</code> onto a <code>Column</code>)</p> <p>| &lt;&gt; a| [[as-alias]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_3","title":"[source, scala] <p>as(alias: String): Dataset[T] as(alias: Symbol): Dataset[T]</p>  <p>A typed transformation</p> <p>| &lt;&gt; a| [[as-type]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_4","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#asu-encoder-datasetu","title":"as[U : Encoder]: Dataset[U] <p>A typed transformation to enforce a type, i.e. marking the records in the <code>Dataset</code> as of a given data type (data type conversion). <code>as</code> simply changes the view of the data that is passed into typed operations (e.g. &lt;&gt;) and does not eagerly project away any columns that are not present in the specified class. <p>| &lt;&gt; a| [[cache]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_5","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#cache-thistype","title":"cache(): this.type <p>A basic action that is a mere synonym of &lt;&gt;. <p>| &lt;&gt; a| [[checkpoint]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_6","title":"[source, scala] <p>checkpoint(): Dataset[T] checkpoint(eager: Boolean): Dataset[T]</p>  <p>A basic action to checkpoint the <code>Dataset</code> in a reliable way (using a reliable HDFS-compliant file system, e.g. Hadoop HDFS or Amazon S3)</p> <p>| &lt;&gt; a| [[coalesce]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_7","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#coalescenumpartitions-int-datasett","title":"coalesce(numPartitions: Int): Dataset[T] <p>A typed transformation to repartition a Dataset</p> <p>| col a| [[col]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_8","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#colcolname-string-column","title":"col(colName: String): Column <p>An untyped transformation to create a column (reference) based on the column name</p> <p>| &lt;&gt; a| [[collect]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_9","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#collect-arrayt","title":"collect(): Array[T] <p>An action</p> <p>| colRegex a| [[colRegex]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_10","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#colregexcolname-string-column","title":"colRegex(colName: String): Column <p>An untyped transformation to create a column (reference) based on the column name specified as a regex</p> <p>| &lt;&gt; a| [[columns]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_11","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#columns-arraystring","title":"columns: Array[String] <p>A basic action</p> <p>| &lt;&gt; a| [[count]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_12","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#count-long","title":"count(): Long <p>An action to count the number of rows</p> <p>| &lt;&gt; a| [[createGlobalTempView]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_13","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#createglobaltempviewviewname-string-unit","title":"createGlobalTempView(viewName: String): Unit <p>A basic action</p> <p>| &lt;&gt; a| [[createOrReplaceGlobalTempView]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_14","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#createorreplaceglobaltempviewviewname-string-unit","title":"createOrReplaceGlobalTempView(viewName: String): Unit <p>A basic action</p> <p>| &lt;&gt; a| [[createOrReplaceTempView]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_15","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#createorreplacetempviewviewname-string-unit","title":"createOrReplaceTempView(viewName: String): Unit <p>A basic action</p> <p>| &lt;&gt; a| [[createTempView]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_16","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#createtempviewviewname-string-unit","title":"createTempView(viewName: String): Unit <p>A basic action</p> <p>| crossJoin a| [[crossJoin]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_17","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#crossjoinright-dataset_-dataframe","title":"crossJoin(right: Dataset[_]): DataFrame <p>An untyped transformation</p> <p>| cube a| [[cube]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_18","title":"[source, scala] <p>cube(cols: Column*): RelationalGroupedDataset cube(col1: String, cols: String*): RelationalGroupedDataset</p>  <p>An untyped transformation</p> <p>| &lt;&gt; a| [[describe]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_19","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#describecols-string-dataframe","title":"describe(cols: String*): DataFrame <p>An action</p> <p>| &lt;&gt; a| [[distinct]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_20","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#distinct-datasett","title":"distinct(): Dataset[T] <p>A typed transformation that is a mere synonym of &lt;&gt; (with all the columns of the <code>Dataset</code>) <p>| drop a| [[drop]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_21","title":"[source, scala] <p>drop(colName: String): DataFrame drop(colNames: String*): DataFrame drop(col: Column): DataFrame</p>  <p>An untyped transformation</p> <p>| &lt;&gt; a| [[dropDuplicates]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_22","title":"[source, scala] <p>dropDuplicates(): Dataset[T] dropDuplicates(colNames: Array[String]): Dataset[T] dropDuplicates(colNames: Seq[String]): Dataset[T] dropDuplicates(col1: String, cols: String*): Dataset[T]</p>  <p>A typed transformation</p> <p>| &lt;&gt; a| [[dtypes]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_23","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#dtypes-arraystring-string","title":"dtypes: Array[(String, String)] <p>A basic action</p> <p>| &lt;&gt; a| [[except]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_24","title":"[source, scala] <p>except(   other: Dataset[T]): Dataset[T]</p>  <p>A typed transformation</p> <p>| &lt;&gt; a| [[exceptAll]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_25","title":"[source, scala] <p>exceptAll(   other: Dataset[T]): Dataset[T]</p>  <p>(New in 2.4.0) A typed transformation</p> <p>| &lt;&gt; a| [[filter]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_26","title":"[source, scala] <p>filter(condition: Column): Dataset[T] filter(conditionExpr: String): Dataset[T] filter(func: T =&gt; Boolean): Dataset[T]</p>  <p>A typed transformation</p> <p>| &lt;&gt; a| [[first]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_27","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#first-t","title":"first(): T <p>An action that is a mere synonym of &lt;&gt; <p>| &lt;&gt; a| [[flatMap]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_28","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#flatmapu-encoder-datasetu","title":"flatMapU : Encoder: Dataset[U] <p>A typed transformation</p> <p>| &lt;&gt; a| [[foreach]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_29","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#foreachf-t-unit-unit","title":"foreach(f: T =&gt; Unit): Unit <p>An action</p> <p>| &lt;&gt; a| [[foreachPartition]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_30","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#foreachpartitionf-iteratort-unit-unit","title":"foreachPartition(f: Iterator[T] =&gt; Unit): Unit <p>An action</p> <p>| groupBy a| [[groupBy]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_31","title":"[source, scala] <p>groupBy(cols: Column*): RelationalGroupedDataset groupBy(col1: String, cols: String*): RelationalGroupedDataset</p>  <p>An untyped transformation</p> <p>| &lt;&gt; a| [[groupByKey]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_32","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#groupbykeyk-encoder-keyvaluegroupeddatasetk-t","title":"groupByKeyK: Encoder: KeyValueGroupedDataset[K, T] <p>A typed transformation</p> <p>| &lt;&gt; a| [[head]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_33","title":"[source, scala] <p>head(): T // &lt;1&gt; head(n: Int): Array[T]</p>  <p>&lt;1&gt; Uses <code>1</code> for <code>n</code></p> <p>An action</p> <p>| &lt;&gt; a| [[hint]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_34","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#hintname-string-parameters-any-datasett","title":"hint(name: String, parameters: Any*): Dataset[T] <p>A basic action to specify a hint (and optional parameters)</p> <p>| &lt;&gt; a| [[inputFiles]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_35","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#inputfiles-arraystring","title":"inputFiles: Array[String] <p>A basic action</p> <p>| &lt;&gt; a| [[intersect]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_36","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#intersectother-datasett-datasett","title":"intersect(other: Dataset[T]): Dataset[T] <p>A typed transformation</p> <p>| &lt;&gt; a| [[intersectAll]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_37","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#intersectallother-datasett-datasett","title":"intersectAll(other: Dataset[T]): Dataset[T] <p>(New in 2.4.0) A typed transformation</p> <p>| &lt;&gt; a| [[isEmpty]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_38","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#isempty-boolean","title":"isEmpty: Boolean <p>(New in 2.4.0) A basic action</p> <p>| &lt;&gt; a| [[isLocal]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_39","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#islocal-boolean","title":"isLocal: Boolean <p>A basic action</p> <p>| isStreaming a| [[isStreaming]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_40","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#isstreaming-boolean","title":"isStreaming: Boolean <p>| join a| [[join]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_41","title":"[source, scala] <p>join(right: Dataset[]): DataFrame join(right: Dataset[], usingColumn: String): DataFrame join(right: Dataset[], usingColumns: Seq[String]): DataFrame join(right: Dataset[], usingColumns: Seq[String], joinType: String): DataFrame join(right: Dataset[], joinExprs: Column): DataFrame join(right: Dataset[], joinExprs: Column, joinType: String): DataFrame</p>  <p>An untyped transformation</p> <p>| &lt;&gt; a| [[joinWith]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_42","title":"[source, scala] <p>joinWithU: Dataset[(T, U)] joinWithU: Dataset[(T, U)]</p>  <p>A typed transformation</p> <p>| &lt;&gt; a| [[limit]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_43","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#limitn-int-datasett","title":"limit(n: Int): Dataset[T] <p>A typed transformation</p> <p>| &lt;&gt; a| [[localCheckpoint]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_44","title":"[source, scala] <p>localCheckpoint(): Dataset[T] localCheckpoint(eager: Boolean): Dataset[T]</p>  <p>A basic action to checkpoint the <code>Dataset</code> locally on executors (and therefore unreliably)</p> <p>| &lt;&gt; a| [[map]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_45","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#mapu-encoder-datasetu","title":"mapU: Encoder: Dataset[U] <p>A typed transformation</p> <p>| &lt;&gt; a| [[mapPartitions]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_46","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#mappartitionsu-encoder-datasetu","title":"mapPartitionsU : Encoder: Dataset[U] <p>A typed transformation</p> <p>| na a| [[na]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_47","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#na-dataframenafunctions","title":"na: DataFrameNaFunctions <p>An untyped transformation</p> <p>| &lt;&gt; a| [[orderBy]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_48","title":"[source, scala] <p>orderBy(sortExprs: Column*): Dataset[T] orderBy(sortCol: String, sortCols: String*): Dataset[T]</p>  <p>A typed transformation</p> <p>| &lt;&gt; a| [[persist]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_49","title":"[source, scala] <p>persist(): this.type persist(newLevel: StorageLevel): this.type</p>  <p>A basic action to persist the <code>Dataset</code></p> <p>NOTE: Although its category <code>persist</code> is not an action in the common sense that means executing anything in a Spark cluster (i.e. execution on the driver or on executors). It acts only as a marker to perform Dataset persistence once an action is really executed.</p> <p>| &lt;&gt; a| [[printSchema]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_50","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#printschema-unit","title":"printSchema(): Unit <p>A basic action</p> <p>| &lt;&gt; a| [[randomSplit]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_51","title":"[source, scala] <p>randomSplit(weights: Array[Double]): Array[Dataset[T]] randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]]</p>  <p>A typed transformation to split a <code>Dataset</code> randomly into two <code>Datasets</code></p> <p>| &lt;&gt; a| [[rdd]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_52","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#rdd-rddt","title":"rdd: RDD[T] <p>A basic action</p> <p>| &lt;&gt; a| [[reduce]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_53","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#reducefunc-t-t-t-t","title":"reduce(func: (T, T) =&gt; T): T <p>An action to reduce the records of the <code>Dataset</code> using the specified binary function.</p> <p>| &lt;&gt; a| [[repartition]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_54","title":"[source, scala] <p>repartition(partitionExprs: Column*): Dataset[T] repartition(numPartitions: Int): Dataset[T] repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T]</p>  <p>A typed transformation to repartition a Dataset</p> <p>| &lt;&gt; a| [[repartitionByRange]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_55","title":"[source, scala] <p>repartitionByRange(partitionExprs: Column*): Dataset[T] repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T]</p>  <p>A typed transformation</p> <p>| rollup a| [[rollup]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_56","title":"[source, scala] <p>rollup(cols: Column*): RelationalGroupedDataset rollup(col1: String, cols: String*): RelationalGroupedDataset</p>  <p>An untyped transformation</p> <p>| &lt;&gt; a| [[sample]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_57","title":"[source, scala] <p>sample(withReplacement: Boolean, fraction: Double): Dataset[T] sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] sample(fraction: Double): Dataset[T] sample(fraction: Double, seed: Long): Dataset[T]</p>  <p>A typed transformation</p> <p>| &lt;&gt; a| [[schema]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_58","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#schema-structtype","title":"schema: StructType <p>A basic action</p> <p>| select a| [[select]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_59","title":"[source, scala] <p>// Untyped transformations select(cols: Column*): DataFrame select(col: String, cols: String*): DataFrame</p> <p>// Typed transformations selectU1: Dataset[U1] selectU1, U2: Dataset[(U1, U2)] selectU1, U2, U3: Dataset[(U1, U2, U3)] selectU1, U2, U3, U4: Dataset[(U1, U2, U3, U4)] selectU1, U2, U3, U4, U5: Dataset[(U1, U2, U3, U4, U5)]</p>  <p>An (untyped and typed) transformation</p> <p>| selectExpr a| [[selectExpr]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_60","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#selectexprexprs-string-dataframe","title":"selectExpr(exprs: String*): DataFrame <p>An untyped transformation</p> <p>| &lt;&gt; a| [[show]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_61","title":"[source, scala] <p>show(): Unit show(truncate: Boolean): Unit show(numRows: Int): Unit show(numRows: Int, truncate: Boolean): Unit show(numRows: Int, truncate: Int): Unit show(numRows: Int, truncate: Int, vertical: Boolean): Unit</p>  <p>An action</p> <p>| &lt;&gt; a| [[sort]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_62","title":"[source, scala] <p>sort(sortExprs: Column*): Dataset[T] sort(sortCol: String, sortCols: String*): Dataset[T]</p>  <p>A typed transformation to sort elements globally (across partitions). Use &lt;&gt; transformation for partition-local sort <p>| &lt;&gt; a| [[sortWithinPartitions]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_63","title":"[source, scala] <p>sortWithinPartitions(sortExprs: Column*): Dataset[T] sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T]</p>  <p>A typed transformation to sort elements within partitions (aka local sort). Use &lt;&gt; transformation for global sort (across partitions) <p>| stat a| [[stat]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_64","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#stat-dataframestatfunctions","title":"stat: DataFrameStatFunctions <p>An untyped transformation</p> <p>| &lt;&gt; a| [[storageLevel]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_65","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#storagelevel-storagelevel","title":"storageLevel: StorageLevel <p>A basic action</p> <p>| &lt;&gt; a| [[summary]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_66","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#summarystatistics-string-dataframe","title":"summary(statistics: String*): DataFrame <p>An action to calculate statistics (e.g. <code>count</code>, <code>mean</code>, <code>stddev</code>, <code>min</code>, <code>max</code> and <code>25%</code>, <code>50%</code>, <code>75%</code> percentiles)</p> <p>| &lt;&gt; a| [[take]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_67","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#taken-int-arrayt","title":"take(n: Int): Array[T] <p>An action to take the first records of a Dataset</p> <p>| &lt;&gt; a| [[toDF]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_68","title":"[source, scala] <p>toDF(): DataFrame toDF(colNames: String*): DataFrame</p>  <p>A basic action to convert a Dataset to a DataFrame</p> <p>| &lt;&gt; a| [[toJSON]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_69","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#tojson-datasetstring","title":"toJSON: Dataset[String] <p>A typed transformation</p> <p>| &lt;&gt; a| [[toLocalIterator]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_70","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#tolocaliterator-javautiliteratort","title":"toLocalIterator(): java.util.Iterator[T] <p>An action that returns an iterator with all rows in the <code>Dataset</code>. The iterator will consume as much memory as the largest partition in the <code>Dataset</code>.</p> <p>| &lt;&gt; a| [[transform]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_71","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#transformu-datasetu","title":"transformU: Dataset[U] <p>A typed transformation for chaining custom transformations</p> <p>| &lt;&gt; a| [[union]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_72","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#unionother-datasett-datasett","title":"union(other: Dataset[T]): Dataset[T] <p>A typed transformation</p> <p>| &lt;&gt; a| [[unionByName]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_73","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#unionbynameother-datasett-datasett","title":"unionByName(other: Dataset[T]): Dataset[T] <p>A typed transformation</p> <p>| &lt;&gt; a| [[unpersist]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_74","title":"[source, scala] <p>unpersist(): this.type // &lt;1&gt; unpersist(blocking: Boolean): this.type</p>  <p>&lt;1&gt; Uses <code>unpersist</code> with <code>blocking</code> disabled (<code>false</code>)</p> <p>A basic action to unpersist the <code>Dataset</code></p> <p>| &lt;&gt; a| [[where]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_75","title":"[source, scala] <p>where(condition: Column): Dataset[T] where(conditionExpr: String): Dataset[T]</p>  <p>A typed transformation</p> <p>| withColumn a| [[withColumn]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_76","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#withcolumncolname-string-col-column-dataframe","title":"withColumn(colName: String, col: Column): DataFrame <p>An untyped transformation</p> <p>| withColumnRenamed a| [[withColumnRenamed]]</p>","text":""},{"location":"spark-sql-dataset-operators/#source-scala_77","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#withcolumnrenamedexistingname-string-newname-string-dataframe","title":"withColumnRenamed(existingName: String, newName: String): DataFrame <p>An untyped transformation</p> <p>| &lt;&gt; a| [[write]]","text":""},{"location":"spark-sql-dataset-operators/#source-scala_78","title":"[source, scala]","text":""},{"location":"spark-sql-dataset-operators/#write-dataframewritert","title":"write: DataFrameWriter[T] <p>A basic action that returns a DataFrameWriter for saving the content of the (non-streaming) <code>Dataset</code> out to an external storage |===</p>","text":""},{"location":"spark-sql-dataset-rdd/","title":"Datasets, DataFrames and RDDs","text":"<p>Many may have been asking yourself why they should be using Datasets rather than the foundation of all Spark - RDDs using case classes.</p> <p>This document collects advantages of <code>Dataset</code> vs <code>RDD[CaseClass]</code> to answer the question Dan has asked on twitter:</p> <p>\"In #Spark, what is the advantage of a DataSet over an RDD[CaseClass]?\"</p>"},{"location":"spark-sql-dataset-rdd/#saving-to-or-writing-from-data-sources","title":"Saving to or Writing from Data Sources","text":"<p>With Dataset API, loading data from a data source or saving it to one is as simple as using SparkSession.read or Dataset.write methods, appropriately.</p>"},{"location":"spark-sql-dataset-rdd/#accessing-fields-columns","title":"Accessing Fields / Columns","text":"<p>You <code>select</code> columns in a datasets without worrying about the positions of the columns.</p> <p>In RDD, you have to do an additional hop over a case class and access fields by name.</p>"},{"location":"spark-sql-dataset-vs-sql/","title":"Dataset API and SQL","text":"<p>Spark SQL supports two \"modes\" to write structured queries: spark-sql-dataset-operators.md[Dataset API] and SparkSession.md#sql[SQL].</p> <p>SQL Mode is used to express structured queries using SQL statements using SparkSession.md#sql[SparkSession.sql] operator, expr standard function and <code>spark-sql</code> command-line tool.</p> <p>Some structured queries can be expressed much easier using Dataset API, but there are some that are only possible in SQL. In other words, you may find mixing Dataset API and SQL modes challenging yet rewarding.</p> <p>What is important, and one of the reasons why Spark SQL has been so successful, is that there is no performance difference between the modes. Whatever mode you use to write your structured queries, they all end up as a tree of Catalyst relational data structures. And, yes, you could consider writing structured queries using Catalyst directly, but that could quickly become unwieldy for maintenance (i.e. finding Spark SQL developers who could be comfortable with it as well as being fairly low-level and therefore possibly too dependent on a specific Spark SQL version).</p> <p>The takeaway is that SQL queries in Spark SQL are translated to Command.md[Catalyst logical commands].</p> <p>This section describes the differences between Spark SQL features to develop Spark applications using Dataset API and SQL mode.</p> <p>. spark-sql-Expression-RuntimeReplaceable.md#implementations[RuntimeReplaceable Expressions] are only available using SQL mode by means of SQL functions like <code>nvl</code>, <code>nvl2</code>, <code>ifnull</code>, <code>nullif</code>, etc.</p> <p>. &lt;&gt; and sql/AstBuilder.md#withPredicate[SQL IN predicate with a subquery] (and spark-sql-Expression-In.md[In Predicate Expression])"},{"location":"spark-sql-functions-datetime/","title":"Date and Time Functions","text":"<p>[[functions]] .(Subset of) Standard Functions for Date and Time [align=\"center\",cols=\"1,2\",width=\"100%\",options=\"header\"] |=== | Name | Description</p> <p>| &lt;&gt; | Gives current date as a date column &lt;&gt; &lt;&gt; <p>| &lt;&gt; | Converts column to date type (with an optional date format) <p>| &lt;&gt; | Converts column to timestamp type (with an optional timestamp format) <p>| &lt;&gt; | Converts current or specified time to Unix timestamp (in seconds) <p>| &lt;&gt; | Generates time windows (i.e. tumbling, sliding and delayed windows) |=== <p>=== [[current_date]] Current Date As Date Column -- <code>current_date</code> Function</p> <pre><code>current_date(): Column\n</code></pre> <p><code>current_date</code> function gives the current date as a date column.</p> <pre><code>val df = spark.range(1).select(current_date)\nscala&gt; df.show\n+--------------+\n|current_date()|\n+--------------+\n|    2017-09-16|\n+--------------+\n\nscala&gt; df.printSchema\nroot\n |-- current_date(): date (nullable = false)\n</code></pre> <p>Internally, <code>current_date</code> creates a Column with <code>CurrentDate</code> Catalyst leaf expression.</p> <pre><code>val c = current_date()\nimport org.apache.spark.sql.catalyst.expressions.CurrentDate\nval cd = c.expr.asInstanceOf[CurrentDate]\nscala&gt; println(cd.prettyName)\ncurrent_date\n\nscala&gt; println(cd.numberedTreeString)\n00 current_date(None)\n</code></pre>"},{"location":"spark-sql-functions-datetime/#date_format","title":"date_format <pre><code>date_format(dateExpr: Column, format: String): Column\n</code></pre> <p>Internally, <code>date_format</code> creates a Column with <code>DateFormatClass</code> binary expression. <code>DateFormatClass</code> takes the expression from <code>dateExpr</code> column and <code>format</code>.</p> <pre><code>val c = date_format($\"date\", \"dd/MM/yyyy\")\n\nimport org.apache.spark.sql.catalyst.expressions.DateFormatClass\nval dfc = c.expr.asInstanceOf[DateFormatClass]\nscala&gt; println(dfc.prettyName)\ndate_format\n\nscala&gt; println(dfc.numberedTreeString)\n00 date_format('date, dd/MM/yyyy, None)\n01 :- 'date\n02 +- dd/MM/yyyy\n</code></pre> <p>=== [[current_timestamp]] <code>current_timestamp</code> Function</p>","text":""},{"location":"spark-sql-functions-datetime/#source-scala","title":"[source, scala]","text":""},{"location":"spark-sql-functions-datetime/#current_timestamp-column","title":"current_timestamp(): Column <p>CAUTION: FIXME</p> <p>NOTE: <code>current_timestamp</code> is also <code>now</code> function in SQL.</p>","text":""},{"location":"spark-sql-functions-datetime/#unix_timestamp","title":"unix_timestamp <p><pre><code>unix_timestamp(): Column  // &lt;1&gt;\nunix_timestamp(\n  time: Column): Column // &lt;2&gt;\nunix_timestamp(\n  time: Column, format: String): Column\n</code></pre> &lt;1&gt; Gives current timestamp (in seconds) &lt;2&gt; Converts <code>time</code> string in format <code>yyyy-MM-dd HH:mm:ss</code> to Unix timestamp (in seconds)</p> <p><code>unix_timestamp</code> converts the current or specified <code>time</code> in the specified <code>format</code> to a Unix timestamp (in seconds).</p> <p><code>unix_timestamp</code> supports a column of type <code>Date</code>, <code>Timestamp</code> or <code>String</code>.</p> <pre><code>// no time and format =&gt; current time\nscala&gt; spark.range(1).select(unix_timestamp as \"current_timestamp\").show\n+-----------------+\n|current_timestamp|\n+-----------------+\n|       1493362850|\n+-----------------+\n\n// no format so yyyy-MM-dd HH:mm:ss assumed\nscala&gt; Seq(\"2017-01-01 00:00:00\").toDF(\"time\").withColumn(\"unix_timestamp\", unix_timestamp($\"time\")).show\n+-------------------+--------------+\n|               time|unix_timestamp|\n+-------------------+--------------+\n|2017-01-01 00:00:00|    1483225200|\n+-------------------+--------------+\n\nscala&gt; Seq(\"2017/01/01 00:00:00\").toDF(\"time\").withColumn(\"unix_timestamp\", unix_timestamp($\"time\", \"yyyy/MM/dd\")).show\n+-------------------+--------------+\n|               time|unix_timestamp|\n+-------------------+--------------+\n|2017/01/01 00:00:00|    1483225200|\n+-------------------+--------------+\n</code></pre> <p><code>unix_timestamp</code> returns <code>null</code> when conversion fails.</p> <pre><code>// note slashes as date separators\nscala&gt; Seq(\"2017/01/01 00:00:00\").toDF(\"time\").withColumn(\"unix_timestamp\", unix_timestamp($\"time\")).show\n+-------------------+--------------+\n|               time|unix_timestamp|\n+-------------------+--------------+\n|2017/01/01 00:00:00|          null|\n+-------------------+--------------+\n</code></pre> <p><code>unix_timestamp</code> is also supported in SQL mode.</p> <pre><code>scala&gt; spark.sql(\"SELECT unix_timestamp() as unix_timestamp\").show\n+--------------+\n|unix_timestamp|\n+--------------+\n|    1493369225|\n+--------------+\n</code></pre> <p>Internally, <code>unix_timestamp</code> creates a Column with UnixTimestamp binary expression (possibly with <code>CurrentTimestamp</code>).</p> <p>=== [[window]] Generating Time Windows -- <code>window</code> Function</p>","text":""},{"location":"spark-sql-functions-datetime/#source-scala_1","title":"[source, scala] <p>window(   timeColumn: Column,   windowDuration: String): Column  // &lt;1&gt; window(   timeColumn: Column,   windowDuration: String,   slideDuration: String): Column   // &lt;2&gt; window(   timeColumn: Column,   windowDuration: String,   slideDuration: String,   startTime: String): Column       // &lt;3&gt;</p>  <p>&lt;1&gt; Creates a tumbling time window with <code>slideDuration</code> as <code>windowDuration</code> and <code>0 second</code> for <code>startTime</code> &lt;2&gt; Creates a sliding time window with <code>0 second</code> for <code>startTime</code> &lt;3&gt; Creates a delayed time window</p> <p><code>window</code> generates tumbling, sliding or delayed time windows of <code>windowDuration</code> duration given a <code>timeColumn</code> timestamp specifying column.</p>","text":""},{"location":"spark-sql-functions-datetime/#note","title":"[NOTE]","text":"<p>From https://msdn.microsoft.com/en-us/library/azure/dn835055.aspx[Tumbling Window (Azure Stream Analytics)]:</p>"},{"location":"spark-sql-functions-datetime/#tumbling-windows-are-a-series-of-fixed-sized-non-overlapping-and-contiguous-time-intervals","title":"&gt; Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals.","text":""},{"location":"spark-sql-functions-datetime/#note_1","title":"[NOTE]","text":"<p>From https://flink.apache.org/news/2015/12/04/Introducing-windows.html[Introducing Stream Windows in Apache Flink]:</p> <p>Tumbling windows group elements of a stream into finite sets where each set corresponds to an interval.</p>"},{"location":"spark-sql-functions-datetime/#tumbling-windows-discretize-a-stream-into-non-overlapping-windows","title":"&gt; Tumbling windows discretize a stream into non-overlapping windows.","text":""},{"location":"spark-sql-functions-datetime/#source-scala_2","title":"[source, scala] <p>scala&gt; val timeColumn = window('time, \"5 seconds\") timeColumn: org.apache.spark.sql.Column = timewindow(time, 5000000, 5000000, 0) AS <code>window</code></p>  <p><code>timeColumn</code> should be of TimestampType, i.e. with java.sql.Timestamp values.</p>  <p>Tip</p> <p>Use java.sql.Timestamp.from or java.sql.Timestamp.valueOf factory methods to create <code>Timestamp</code> instances.</p>  <pre><code>// https://docs.oracle.com/javase/8/docs/api/java/time/LocalDateTime.html\nimport java.time.LocalDateTime\n// https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html\nimport java.sql.Timestamp\nval levels = Seq(\n  // (year, month, dayOfMonth, hour, minute, second)\n  ((2012, 12, 12, 12, 12, 12), 5),\n  ((2012, 12, 12, 12, 12, 14), 9),\n  ((2012, 12, 12, 13, 13, 14), 4),\n  ((2016, 8,  13, 0, 0, 0), 10),\n  ((2017, 5,  27, 0, 0, 0), 15)).\n  map { case ((yy, mm, dd, h, m, s), a) =&gt; (LocalDateTime.of(yy, mm, dd, h, m, s), a) }.\n  map { case (ts, a) =&gt; (Timestamp.valueOf(ts), a) }.\n  toDF(\"time\", \"level\")\nscala&gt; levels.show\n+-------------------+-----+\n|               time|level|\n+-------------------+-----+\n|2012-12-12 12:12:12|    5|\n|2012-12-12 12:12:14|    9|\n|2012-12-12 13:13:14|    4|\n|2016-08-13 00:00:00|   10|\n|2017-05-27 00:00:00|   15|\n+-------------------+-----+\n\nval q = levels.select(window($\"time\", \"5 seconds\"), $\"level\")\nscala&gt; q.show(truncate = false)\n+---------------------------------------------+-----+\n|window                                       |level|\n+---------------------------------------------+-----+\n|[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|5    |\n|[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|9    |\n|[2012-12-12 13:13:10.0,2012-12-12 13:13:15.0]|4    |\n|[2016-08-13 00:00:00.0,2016-08-13 00:00:05.0]|10   |\n|[2017-05-27 00:00:00.0,2017-05-27 00:00:05.0]|15   |\n+---------------------------------------------+-----+\n\nscala&gt; q.printSchema\nroot\n |-- window: struct (nullable = true)\n |    |-- start: timestamp (nullable = true)\n |    |-- end: timestamp (nullable = true)\n |-- level: integer (nullable = false)\n\n// calculating the sum of levels every 5 seconds\nval sums = levels.\n  groupBy(window($\"time\", \"5 seconds\")).\n  agg(sum(\"level\") as \"level_sum\").\n  select(\"window.start\", \"window.end\", \"level_sum\")\nscala&gt; sums.show\n+-------------------+-------------------+---------+\n|              start|                end|level_sum|\n+-------------------+-------------------+---------+\n|2012-12-12 13:13:10|2012-12-12 13:13:15|        4|\n|2012-12-12 12:12:10|2012-12-12 12:12:15|       14|\n|2016-08-13 00:00:00|2016-08-13 00:00:05|       10|\n|2017-05-27 00:00:00|2017-05-27 00:00:05|       15|\n+-------------------+-------------------+---------+\n</code></pre> <p><code>windowDuration</code> and <code>slideDuration</code> are strings specifying the width of the window for duration and sliding identifiers, respectively.</p>  <p>Tip</p> <p>Use <code>CalendarInterval</code> for valid window identifiers.</p>  <p>Internally, <code>window</code> creates a Column (with TimeWindow expression) available as <code>window</code> alias.</p> <pre><code>// q is the query defined earlier\nscala&gt; q.show(truncate = false)\n+---------------------------------------------+-----+\n|window                                       |level|\n+---------------------------------------------+-----+\n|[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|5    |\n|[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|9    |\n|[2012-12-12 13:13:10.0,2012-12-12 13:13:15.0]|4    |\n|[2016-08-13 00:00:00.0,2016-08-13 00:00:05.0]|10   |\n|[2017-05-27 00:00:00.0,2017-05-27 00:00:05.0]|15   |\n+---------------------------------------------+-----+\n\nscala&gt; println(timeColumn.expr.numberedTreeString)\n00 timewindow('time, 5000000, 5000000, 0) AS window#22\n01 +- timewindow('time, 5000000, 5000000, 0)\n02    +- 'time\n</code></pre> <p>==== [[window-example]] Example -- Traffic Sensor</p> <p>NOTE: The example is borrowed from https://flink.apache.org/news/2015/12/04/Introducing-windows.html[Introducing Stream Windows in Apache Flink].</p> <p>The example shows how to use <code>window</code> function to model a traffic sensor that counts every 15 seconds the number of vehicles passing a certain location.</p>","text":""},{"location":"spark-sql-functions-datetime/#to_date","title":"to_date <pre><code>to_date(\n  e: Column): Column\nto_date(\n  e: Column,\n  fmt: String): Column\n</code></pre> <p><code>to_date</code> converts the column into DateType (by casting to <code>DateType</code>).</p>  <p>Note</p> <p><code>fmt</code> follows the formatting styles.</p>  <p>Internally, <code>to_date</code> creates a Column with ParseToDate expression (and <code>Literal</code> expression for <code>fmt</code>).</p>  <p>Tip</p> <p>Use ParseToDate expression to use a column for the values of <code>fmt</code>.</p>","text":""},{"location":"spark-sql-functions-datetime/#to_timestamp","title":"to_timestamp <pre><code>to_timestamp(\n  s: Column): Column\nto_timestamp(\n  s: Column,\n  fmt: String): Column\n</code></pre> <p><code>to_timestamp</code> converts the column into TimestampType (by casting to <code>TimestampType</code>).</p>  <p>Note</p> <p><code>fmt</code> follows the formatting styles.</p>  <p>Internally, <code>to_timestamp</code> creates a Column with ParseToTimestamp expression (and <code>Literal</code> expression for <code>fmt</code>).</p>  <p>Tip</p> <p>Use ParseToTimestamp expression to use a column for the values of <code>fmt</code>.</p>","text":""},{"location":"spark-sql-functions-regular-functions/","title":"Regular Functions (Non-Aggregate Functions)","text":"<p>[[functions]] .(Subset of) Regular Functions [align=\"center\",cols=\"1,2\",width=\"100%\",options=\"header\"] |=== | Name | Description</p> &lt;&gt; &lt;&gt; <p>| &lt;&gt; | Gives the first non-<code>null</code> value among the given columns or <code>null</code>. <p>| &lt;&gt; and &lt;&gt; | Creating Columns &lt;&gt; &lt;&gt; &lt;&gt; &lt;&gt; &lt;&gt; &lt;&gt; &lt;&gt; === <p>=== [[broadcast]] <code>broadcast</code> Function</p>"},{"location":"spark-sql-functions-regular-functions/#source-scala","title":"[source, scala]","text":""},{"location":"spark-sql-functions-regular-functions/#broadcastt-datasett","title":"broadcastT: Dataset[T]","text":"<p><code>broadcast</code> function marks the input Dataset.md[Dataset] as small enough to be used in broadcast join.</p> <p>TIP: Read up on spark-sql-joins-broadcast.md[Broadcast Joins (aka Map-Side Joins)].</p>"},{"location":"spark-sql-functions-regular-functions/#source-scala_1","title":"[source, scala]","text":"<p>val left = Seq((0, \"aa\"), (0, \"bb\")).toDF(\"id\", \"token\").as[(Int, String)] val right = Seq((\"aa\", 0.99), (\"bb\", 0.57)).toDF(\"token\", \"prob\").as[(String, Double)]</p> <p>scala&gt; left.join(broadcast(right), \"token\").explain(extended = true) == Parsed Logical Plan == 'Join UsingJoin(Inner,List(token)) :- Project [_1#123 AS id#126, _2#124 AS token#127] :  +- LocalRelation [_1#123, _2#124] +- BroadcastHint    +- Project [_1#136 AS token#139, _2#137 AS prob#140]       +- LocalRelation [_1#136, _2#137]</p> <p>== Analyzed Logical Plan == token: string, id: int, prob: double Project [token#127, id#126, prob#140] +- Join Inner, (token#127 = token#139)    :- Project [_1#123 AS id#126, _2#124 AS token#127]    :  +- LocalRelation [_1#123, _2#124]    +- BroadcastHint       +- Project [_1#136 AS token#139, _2#137 AS prob#140]          +- LocalRelation [_1#136, _2#137]</p> <p>== Optimized Logical Plan == Project [token#127, id#126, prob#140] +- Join Inner, (token#127 = token#139)    :- Project [_1#123 AS id#126, _2#124 AS token#127]    :  +- Filter isnotnull(_2#124)    :     +- LocalRelation [_1#123, _2#124]    +- BroadcastHint       +- Project [_1#136 AS token#139, _2#137 AS prob#140]          +- Filter isnotnull(_1#136)             +- LocalRelation [_1#136, _2#137]</p> <p>== Physical Plan == *Project [token#127, id#126, prob#140] +- *BroadcastHashJoin [token#127], [token#139], Inner, BuildRight    :- *Project [_1#123 AS id#126, _2#124 AS token#127]    :  +- *Filter isnotnull(_2#124)    :     +- LocalTableScan [_1#123, _2#124]    +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))       +- *Project [_1#136 AS token#139, _2#137 AS prob#140]          +- *Filter isnotnull(_1#136)             +- LocalTableScan [_1#136, _2#137]</p> <p>NOTE: <code>broadcast</code> standard function is a special case of spark-sql-dataset-operators.md[Dataset.hint] operator that allows for attaching any hint to a logical plan.</p>"},{"location":"spark-sql-functions-regular-functions/#coalesce","title":"coalesce <pre><code>coalesce(\n  e: Column*): Column\n</code></pre> <p><code>coalesce</code> gives the first non-<code>null</code> value among the given columns or <code>null</code>.</p> <p><code>coalesce</code> requires at least one column and all columns have to be of the same or compatible types.</p> <p>Internally, <code>coalesce</code> creates a Column with a <code>Coalesce</code> expression (with the children being the expressions of the input <code>Column</code>).</p>","text":""},{"location":"spark-sql-functions-regular-functions/#demo-coalesce","title":"Demo: coalesce <pre><code>val q = spark.range(2)\n  .select(\n    coalesce(\n      lit(null),\n      lit(null),\n      lit(2) + 2,\n      $\"id\") as \"first non-null value\")\nscala&gt; q.show\n+--------------------+\n|first non-null value|\n+--------------------+\n|                   4|\n|                   4|\n+--------------------+\n</code></pre> <p>=== [[col]][[column]] Creating Columns -- <code>col</code> and <code>column</code> Functions</p> <pre><code>col(colName: String): Column\ncolumn(colName: String): Column\n</code></pre> <p><code>col</code> and <code>column</code> methods create a Column that you can later use to reference a column in a dataset.</p> <pre><code>import org.apache.spark.sql.functions._\n\nscala&gt; val nameCol = col(\"name\")\nnameCol: org.apache.spark.sql.Column = name\n\nscala&gt; val cityCol = column(\"city\")\ncityCol: org.apache.spark.sql.Column = city\n</code></pre> <p>=== [[expr]] <code>expr</code> Function</p>","text":""},{"location":"spark-sql-functions-regular-functions/#source-scala_2","title":"[source, scala]","text":""},{"location":"spark-sql-functions-regular-functions/#exprexpr-string-column","title":"expr(expr: String): Column <p><code>expr</code> function parses the input <code>expr</code> SQL statement to a <code>Column</code> it represents.</p>","text":""},{"location":"spark-sql-functions-regular-functions/#source-scala_3","title":"[source, scala] <p>val ds = Seq((0, \"hello\"), (1, \"world\"))   .toDF(\"id\", \"token\")   .as[(Long, String)]</p> <p>scala&gt; ds.show +---+-----+ | id|token| +---+-----+ |  0|hello| |  1|world| +---+-----+</p> <p>val filterExpr = expr(\"token = 'hello'\")</p> <p>scala&gt; ds.filter(filterExpr).show +---+-----+ | id|token| +---+-----+ |  0|hello| +---+-----+</p>  <p>Internally, <code>expr</code> uses the active session's SessionState.md[sqlParser] or creates a new  sql/SparkSqlParser.md[SparkSqlParser] to call spark-sql-ParserInterface.md#parseExpression[parseExpression] method.</p> <p>=== [[lit]] <code>lit</code> Function</p>","text":""},{"location":"spark-sql-functions-regular-functions/#source-scala_4","title":"[source, scala]","text":""},{"location":"spark-sql-functions-regular-functions/#litliteral-any-column","title":"lit(literal: Any): Column <p><code>lit</code> function...FIXME</p> <p>=== [[struct]] <code>struct</code> Functions</p>","text":""},{"location":"spark-sql-functions-regular-functions/#source-scala_5","title":"[source, scala] <p>struct(cols: Column*): Column struct(colName: String, colNames: String*): Column</p>  <p><code>struct</code> family of functions allows you to create a new struct column based on a collection of <code>Column</code> or their names.</p> <p>NOTE: The difference between <code>struct</code> and another similar <code>array</code> function is that the types of the columns can be different (in <code>struct</code>).</p>","text":""},{"location":"spark-sql-functions-regular-functions/#source-scala_6","title":"[source, scala] <p>scala&gt; df.withColumn(\"struct\", struct($\"name\", $\"val\")).show +---+---+-----+---------+ | id|val| name|   struct| +---+---+-----+---------+ |  0|  1|hello|[hello,1]| |  2|  3|world|[world,3]| |  2|  4|  ala|  [ala,4]| +---+---+-----+---------+</p>  <p>=== [[typedLit]] <code>typedLit</code> Function</p>","text":""},{"location":"spark-sql-functions-regular-functions/#source-scala_7","title":"[source, scala]","text":""},{"location":"spark-sql-functions-regular-functions/#typedlitt-typetag-column","title":"typedLitT : TypeTag: Column <p><code>typedLit</code>...FIXME</p> <p>=== [[array]] <code>array</code> Function</p>","text":""},{"location":"spark-sql-functions-regular-functions/#source-scala_8","title":"[source, scala] <p>array(cols: Column*): Column array(colName: String, colNames: String*): Column</p>  <p><code>array</code>...FIXME</p> <p>=== [[map]] <code>map</code> Function</p>","text":""},{"location":"spark-sql-functions-regular-functions/#source-scala_9","title":"[source, scala]","text":""},{"location":"spark-sql-functions-regular-functions/#mapcols-column-column","title":"map(cols: Column*): Column <p><code>map</code>...FIXME</p> <p>=== [[when]] <code>when</code> Function</p>","text":""},{"location":"spark-sql-functions-regular-functions/#source-scala_10","title":"[source, scala]","text":""},{"location":"spark-sql-functions-regular-functions/#whencondition-column-value-any-column","title":"when(condition: Column, value: Any): Column <p><code>when</code>...FIXME</p> <p>=== [[monotonically_increasing_id]] <code>monotonically_increasing_id</code> Function</p>","text":""},{"location":"spark-sql-functions-regular-functions/#source-scala_11","title":"[source, scala]","text":""},{"location":"spark-sql-functions-regular-functions/#monotonically_increasing_id-column","title":"monotonically_increasing_id(): Column <p><code>monotonically_increasing_id</code> returns monotonically increasing 64-bit integers. The generated IDs are guaranteed to be monotonically increasing and unique, but not consecutive (unless all rows are in the same single partition which you rarely want due to the amount of the data).</p>","text":""},{"location":"spark-sql-functions-regular-functions/#source-scala_12","title":"[source, scala] <p>val q = spark.range(1).select(monotonically_increasing_id) scala&gt; q.show +-----------------------------+ |monotonically_increasing_id()| +-----------------------------+ |                  60129542144| +-----------------------------+</p>  <p>The current implementation uses the partition ID in the upper 31 bits, and the lower 33 bits represent the record number within each partition. That assumes that the data set has less than 1 billion partitions, and each partition has less than 8 billion records.</p> <pre><code>// Demo to show the internals of monotonically_increasing_id function\n// i.e. how MonotonicallyIncreasingID expression works\n\n// Create a dataset with the same number of rows per partition\nval q = spark.range(start = 0, end = 8, step = 1, numPartitions = 4)\n\n// Make sure that every partition has the same number of rows\nq.mapPartitions(rows =&gt; Iterator(rows.size)).foreachPartition(rows =&gt; assert(rows.next == 2))\nq.select(monotonically_increasing_id).show\n\n// Assign consecutive IDs for rows per partition\nimport org.apache.spark.sql.expressions.Window\n// count is the name of the internal registry of MonotonicallyIncreasingID to count rows\n// Could also be \"id\" since it is unique and consecutive in a partition\nimport org.apache.spark.sql.functions.{row_number, shiftLeft, spark_partition_id}\nval rowNumber = row_number over Window.partitionBy(spark_partition_id).orderBy(\"id\")\n// row_number is a sequential number starting at 1 within a window partition\nval count = rowNumber - 1 as \"count\"\nval partitionMask = shiftLeft(spark_partition_id cast \"long\", 33) as \"partitionMask\"\n// FIXME Why does the following sum give \"weird\" results?!\nval sum = (partitionMask + count) as \"partitionMask + count\"\nval demo = q.select(\n  $\"id\",\n  partitionMask,\n  count,\n  // FIXME sum,\n  monotonically_increasing_id)\nscala&gt; demo.orderBy(\"id\").show\n+---+-------------+-----+-----------------------------+\n| id|partitionMask|count|monotonically_increasing_id()|\n+---+-------------+-----+-----------------------------+\n|  0|            0|    0|                            0|\n|  1|            0|    1|                            1|\n|  2|   8589934592|    0|                   8589934592|\n|  3|   8589934592|    1|                   8589934593|\n|  4|  17179869184|    0|                  17179869184|\n|  5|  17179869184|    1|                  17179869185|\n|  6|  25769803776|    0|                  25769803776|\n|  7|  25769803776|    1|                  25769803777|\n+---+-------------+-----+-----------------------------+\n</code></pre> <p>Internally, <code>monotonically_increasing_id</code> creates a Column with a MonotonicallyIncreasingID non-deterministic leaf expression.</p>","text":""},{"location":"spark-sql-functions-windows/","title":"Standard Functions for Window Aggregation (Window Functions)","text":"<p>Window aggregate functions (aka window functions or windowed aggregates) are functions that perform a calculation over a group of records called window that are in some relation to the current record (i.e. can be in the same partition or frame as the current row).</p> <p>In other words, when executed, a window function computes a value for each and every row in a window (per window specification).</p> <p>Window functions are also called over functions due to how they are applied using over operator.</p> <p>Spark SQL supports three kinds of window functions:</p> <ul> <li>ranking functions</li> <li>analytic functions</li> <li>aggregate functions</li> </ul> <p>.Window Aggregate Functions in Spark SQL [align=\"center\",cols=\"1,1,2\",width=\"80%\",options=\"header\"] |=== | | Function | Purpose</p> <p>.5+<sup>.</sup>|Ranking functions</p> &lt;&gt; &lt;&gt; &lt;&gt; &lt;&gt; &lt;&gt; <p>.3+<sup>.</sup>| Analytic functions</p> <p>| &lt;&gt; a| &lt;&gt; &lt;&gt; === <p>For aggregate functions, you can use the existing aggregate functions as window functions, e.g. <code>sum</code>, <code>avg</code>, <code>min</code>, <code>max</code> and <code>count</code>.</p> <pre><code>// Borrowed from 3.5. Window Functions in PostgreSQL documentation\n// Example of window functions using Scala API\n//\ncase class Salary(depName: String, empNo: Long, salary: Long)\nval empsalary = Seq(\n  Salary(\"sales\", 1, 5000),\n  Salary(\"personnel\", 2, 3900),\n  Salary(\"sales\", 3, 4800),\n  Salary(\"sales\", 4, 4800),\n  Salary(\"personnel\", 5, 3500),\n  Salary(\"develop\", 7, 4200),\n  Salary(\"develop\", 8, 6000),\n  Salary(\"develop\", 9, 4500),\n  Salary(\"develop\", 10, 5200),\n  Salary(\"develop\", 11, 5200)).toDS\n\nimport org.apache.spark.sql.expressions.Window\n// Windows are partitions of deptName\nscala&gt; val byDepName = Window.partitionBy('depName)\nbyDepName: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@1a711314\n\nscala&gt; empsalary.withColumn(\"avg\", avg('salary) over byDepName).show\n+---------+-----+------+-----------------+\n|  depName|empNo|salary|              avg|\n+---------+-----+------+-----------------+\n|  develop|    7|  4200|           5020.0|\n|  develop|    8|  6000|           5020.0|\n|  develop|    9|  4500|           5020.0|\n|  develop|   10|  5200|           5020.0|\n|  develop|   11|  5200|           5020.0|\n|    sales|    1|  5000|4866.666666666667|\n|    sales|    3|  4800|4866.666666666667|\n|    sales|    4|  4800|4866.666666666667|\n|personnel|    2|  3900|           3700.0|\n|personnel|    5|  3500|           3700.0|\n+---------+-----+------+-----------------+\n</code></pre> <p>You describe a window using the convenient factory methods in &lt;&gt; that create a &lt;&gt; that you can further refine with partitioning, ordering, and frame boundaries. <p>After you describe a window you can apply &lt;&gt; like ranking functions (e.g. <code>RANK</code>), analytic functions (e.g. <code>LAG</code>), and the regular aggregate functions, e.g. <code>sum</code>, <code>avg</code>, <code>max</code>. <p>NOTE: Window functions are supported in structured queries using &lt;&gt; and Column-based expressions. <p>Although similar to aggregate functions, a window function does not group rows into a single output row and retains their separate identities. A window function can access rows that are linked to the current row.</p> <p>NOTE: The main difference between window aggregate functions and aggregate functions with grouping operators is that the former calculate values for every row in a window while the latter gives you at most the number of input rows, one value per group.</p> <p>TIP: See &lt;&gt; section in this document. <p>You can mark a function window by <code>OVER</code> clause after a function in SQL, e.g. <code>avg(revenue) OVER (...)</code> or over method on a function in the Dataset API, e.g. <code>rank().over(...)</code>.</p> <p>Note</p> <p>Window functions belong to Window functions group in Spark's Scala API.</p> <p>=== [[Window-object]] Window object</p> <p><code>Window</code> object provides functions to define windows (as WindowSpec instances).</p> <p><code>Window</code> object lives in <code>org.apache.spark.sql.expressions</code> package. Import it to use <code>Window</code> functions.</p> <pre><code>import org.apache.spark.sql.expressions.Window\n</code></pre> <p>There are two families of the functions available in <code>Window</code> object that create WindowSpec instance for one or many Column instances:</p> <ul> <li>partitionBy</li> <li>orderBy</li> </ul> <p>==== [[partitionBy]] Partitioning Records -- <code>partitionBy</code> Methods</p>"},{"location":"spark-sql-functions-windows/#source-scala","title":"[source, scala]","text":"<p>partitionBy(colName: String, colNames: String*): WindowSpec partitionBy(cols: Column*): WindowSpec</p> <p><code>partitionBy</code> creates an instance of <code>WindowSpec</code> with partition expression(s) defined for one or more columns.</p>"},{"location":"spark-sql-functions-windows/#source-scala_1","title":"[source, scala]","text":"<p>// partition records into two groups // * tokens starting with \"h\" // * others val byHTokens = Window.partitionBy('token startsWith \"h\")</p> <p>// count the sum of ids in each group val result = tokens.select('*, sum('id) over byHTokens as \"sum over h tokens\").orderBy('id)</p> <p>scala&gt; .show +---+-----+-----------------+ | id|token|sum over h tokens| +---+-----+-----------------+ |  0|hello|                4| |  1|henry|                4| |  2|  and|                2| |  3|harry|                4| +---+-----+-----------------+</p> <p>==== [[orderBy]] Ordering in Windows -- <code>orderBy</code> Methods</p>"},{"location":"spark-sql-functions-windows/#source-scala_2","title":"[source, scala]","text":"<p>orderBy(colName: String, colNames: String*): WindowSpec orderBy(cols: Column*): WindowSpec</p> <p><code>orderBy</code> allows you to control the order of records in a window.</p>"},{"location":"spark-sql-functions-windows/#source-scala_3","title":"[source, scala]","text":"<p>import org.apache.spark.sql.expressions.Window val byDepnameSalaryDesc = Window.partitionBy('depname).orderBy('salary desc)</p> <p>// a numerical rank within the current row's partition for each distinct ORDER BY value scala&gt; val rankByDepname = rank().over(byDepnameSalaryDesc) rankByDepname: org.apache.spark.sql.Column = RANK() OVER (PARTITION BY depname ORDER BY salary DESC UnspecifiedFrame)</p> <p>scala&gt; empsalary.select('*, rankByDepname as 'rank).show +---------+-----+------+----+ |  depName|empNo|salary|rank| +---------+-----+------+----+ |  develop|    8|  6000|   1| |  develop|   10|  5200|   2| |  develop|   11|  5200|   2| |  develop|    9|  4500|   4| |  develop|    7|  4200|   5| |    sales|    1|  5000|   1| |    sales|    3|  4800|   2| |    sales|    4|  4800|   2| |personnel|    2|  3900|   1| |personnel|    5|  3500|   2| +---------+-----+------+----+</p> <p>==== [[rangeBetween]] <code>rangeBetween</code> Method</p>"},{"location":"spark-sql-functions-windows/#source-scala_4","title":"[source, scala]","text":""},{"location":"spark-sql-functions-windows/#rangebetweenstart-long-end-long-windowspec","title":"rangeBetween(start: Long, end: Long): WindowSpec","text":"<p><code>rangeBetween</code> creates a &lt;&gt; with the frame boundaries from <code>start</code> (inclusive) to <code>end</code> (inclusive). <p>NOTE: It is recommended to use <code>Window.unboundedPreceding</code>, <code>Window.unboundedFollowing</code> and <code>Window.currentRow</code> to describe the frame boundaries when a frame is unbounded preceding, unbounded following and at current row, respectively.</p>"},{"location":"spark-sql-functions-windows/#source-scala_5","title":"[source, scala]","text":"<p>import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.expressions.WindowSpec val spec: WindowSpec = Window.rangeBetween(Window.unboundedPreceding, Window.currentRow)</p> <p>Internally, <code>rangeBetween</code> creates a <code>WindowSpec</code> with <code>SpecifiedWindowFrame</code> and <code>RangeFrame</code> type.</p> <p>=== [[frame]] Frame</p> <p>At its core, a window function calculates a return value for every input row of a table based on a group of rows, called the frame. Every input row can have a unique frame associated with it.</p> <p>When you define a frame you have to specify three components of a frame specification - the start and end boundaries, and the type.</p> <p>Types of boundaries (two positions and three offsets):</p> <ul> <li><code>UNBOUNDED PRECEDING</code> - the first row of the partition</li> <li><code>UNBOUNDED FOLLOWING</code> - the last row of the partition</li> <li><code>CURRENT ROW</code></li> <li><code>&lt;value&gt; PRECEDING</code></li> <li><code>&lt;value&gt; FOLLOWING</code></li> </ul> <p>Offsets specify the offset from the current input row.</p> <p>Types of frames:</p> <ul> <li><code>ROW</code> - based on physical offsets from the position of the current input row</li> <li><code>RANGE</code> - based on logical offsets from the position of the current input row</li> </ul> <p>In the current implementation of &lt;&gt; you can use two methods to define a frame: <ul> <li><code>rowsBetween</code></li> <li><code>rangeBetween</code></li> </ul> <p>See &lt;&gt; for their coverage. <p>=== [[sql]] Window Operators in SQL Queries</p> <p>The grammar of windows operators in SQL accepts the following:</p> <ol> <li> <p><code>CLUSTER BY</code> or <code>PARTITION BY</code> or <code>DISTRIBUTE BY</code> for partitions,</p> </li> <li> <p><code>ORDER BY</code> or <code>SORT BY</code> for sorting order,</p> </li> <li> <p><code>RANGE</code>, <code>ROWS</code>, <code>RANGE BETWEEN</code>, and <code>ROWS BETWEEN</code> for window frame types,</p> </li> <li> <p><code>UNBOUNDED PRECEDING</code>, <code>UNBOUNDED FOLLOWING</code>, <code>CURRENT ROW</code> for frame bounds.</p> </li> </ol> <p>TIP: Consult sql/AstBuilder.md#withWindows[withWindows] helper in <code>AstBuilder</code>.</p> <p>=== [[examples]] Examples</p> <p>==== [[example-top-n]] Top N per Group</p> <p>Top N per Group is useful when you need to compute the first and second best-sellers in category.</p> <p>NOTE: This example is borrowed from an excellent article  https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html[Introducing Window Functions in Spark SQL].</p> <p>.Table PRODUCT_REVENUE [align=\"center\",width=\"80%\",options=\"header\"] |=== |product |category |revenue |      Thin|cell phone|   6000 |    Normal|    tablet|   1500 |      Mini|    tablet|   5500 |Ultra thin|cell phone|   5000 | Very thin|cell phone|   6000 |       Big|    tablet|   2500 |  Bendable|cell phone|   3000 |  Foldable|cell phone|   3000 |       Pro|    tablet|   4500 |      Pro2|    tablet|   6500 |===</p> <p>Question: What are the best-selling and the second best-selling products in every category?</p> <pre><code>val dataset = Seq(\n  (\"Thin\",       \"cell phone\", 6000),\n  (\"Normal\",     \"tablet\",     1500),\n  (\"Mini\",       \"tablet\",     5500),\n  (\"Ultra thin\", \"cell phone\", 5000),\n  (\"Very thin\",  \"cell phone\", 6000),\n  (\"Big\",        \"tablet\",     2500),\n  (\"Bendable\",   \"cell phone\", 3000),\n  (\"Foldable\",   \"cell phone\", 3000),\n  (\"Pro\",        \"tablet\",     4500),\n  (\"Pro2\",       \"tablet\",     6500))\n  .toDF(\"product\", \"category\", \"revenue\")\n\nscala&gt; dataset.show\n+----------+----------+-------+\n|   product|  category|revenue|\n+----------+----------+-------+\n|      Thin|cell phone|   6000|\n|    Normal|    tablet|   1500|\n|      Mini|    tablet|   5500|\n|Ultra thin|cell phone|   5000|\n| Very thin|cell phone|   6000|\n|       Big|    tablet|   2500|\n|  Bendable|cell phone|   3000|\n|  Foldable|cell phone|   3000|\n|       Pro|    tablet|   4500|\n|      Pro2|    tablet|   6500|\n+----------+----------+-------+\n\nscala&gt; data.where('category === \"tablet\").show\n+-------+--------+-------+\n|product|category|revenue|\n+-------+--------+-------+\n| Normal|  tablet|   1500|\n|   Mini|  tablet|   5500|\n|    Big|  tablet|   2500|\n|    Pro|  tablet|   4500|\n|   Pro2|  tablet|   6500|\n+-------+--------+-------+\n</code></pre> <p>The question boils down to ranking products in a category based on their revenue, and to pick the best selling and the second best-selling products based the ranking.</p> <pre><code>import org.apache.spark.sql.expressions.Window\nval overCategory = Window.partitionBy('category).orderBy('revenue.desc)\n\nval ranked = data.withColumn(\"rank\", dense_rank.over(overCategory))\n\nscala&gt; ranked.show\n+----------+----------+-------+----+\n|   product|  category|revenue|rank|\n+----------+----------+-------+----+\n|      Pro2|    tablet|   6500|   1|\n|      Mini|    tablet|   5500|   2|\n|       Pro|    tablet|   4500|   3|\n|       Big|    tablet|   2500|   4|\n|    Normal|    tablet|   1500|   5|\n|      Thin|cell phone|   6000|   1|\n| Very thin|cell phone|   6000|   1|\n|Ultra thin|cell phone|   5000|   2|\n|  Bendable|cell phone|   3000|   3|\n|  Foldable|cell phone|   3000|   3|\n+----------+----------+-------+----+\n\nscala&gt; ranked.where('rank &lt;= 2).show\n+----------+----------+-------+----+\n|   product|  category|revenue|rank|\n+----------+----------+-------+----+\n|      Pro2|    tablet|   6500|   1|\n|      Mini|    tablet|   5500|   2|\n|      Thin|cell phone|   6000|   1|\n| Very thin|cell phone|   6000|   1|\n|Ultra thin|cell phone|   5000|   2|\n+----------+----------+-------+----+\n</code></pre> <p>==== Revenue Difference per Category</p> <p>NOTE: This example is the 2<sup>nd</sup> example from an excellent article  https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html[Introducing Window Functions in Spark SQL].</p> <pre><code>import org.apache.spark.sql.expressions.Window\nval reveDesc = Window.partitionBy('category).orderBy('revenue.desc)\nval reveDiff = max('revenue).over(reveDesc) - 'revenue\n\nscala&gt; data.select('*, reveDiff as 'revenue_diff).show\n+----------+----------+-------+------------+\n|   product|  category|revenue|revenue_diff|\n+----------+----------+-------+------------+\n|      Pro2|    tablet|   6500|           0|\n|      Mini|    tablet|   5500|        1000|\n|       Pro|    tablet|   4500|        2000|\n|       Big|    tablet|   2500|        4000|\n|    Normal|    tablet|   1500|        5000|\n|      Thin|cell phone|   6000|           0|\n| Very thin|cell phone|   6000|           0|\n|Ultra thin|cell phone|   5000|        1000|\n|  Bendable|cell phone|   3000|        3000|\n|  Foldable|cell phone|   3000|        3000|\n+----------+----------+-------+------------+\n</code></pre> <p>==== Difference on Column</p> <p>Compute a difference between values in rows in a column.</p> <pre><code>val pairs = for {\n  x &lt;- 1 to 5\n  y &lt;- 1 to 2\n} yield (x, 10 * x * y)\nval ds = pairs.toDF(\"ns\", \"tens\")\n\nscala&gt; ds.show\n+---+----+\n| ns|tens|\n+---+----+\n|  1|  10|\n|  1|  20|\n|  2|  20|\n|  2|  40|\n|  3|  30|\n|  3|  60|\n|  4|  40|\n|  4|  80|\n|  5|  50|\n|  5| 100|\n+---+----+\n\nimport org.apache.spark.sql.expressions.Window\nval overNs = Window.partitionBy('ns).orderBy('tens)\nval diff = lead('tens, 1).over(overNs)\n\nscala&gt; ds.withColumn(\"diff\", diff - 'tens).show\n+---+----+----+\n| ns|tens|diff|\n+---+----+----+\n|  1|  10|  10|\n|  1|  20|null|\n|  3|  30|  30|\n|  3|  60|null|\n|  5|  50|  50|\n|  5| 100|null|\n|  4|  40|  40|\n|  4|  80|null|\n|  2|  20|  20|\n|  2|  40|null|\n+---+----+----+\n</code></pre> <p>Please note that http://stackoverflow.com/a/32379437/1305344[Why do Window functions fail with \"Window function X does not take a frame specification\"?]</p> <p>The key here is to remember that DataFrames are RDDs under the covers and hence aggregation like grouping by a key in DataFrames is RDD's <code>groupBy</code> (or worse, <code>reduceByKey</code> or <code>aggregateByKey</code> transformations).</p> <p>==== [[example-running-total]] Running Total</p> <p>The running total is the sum of all previous lines including the current one.</p>"},{"location":"spark-sql-functions-windows/#source-scala_6","title":"[source, scala]","text":"<p>val sales = Seq(   (0, 0, 0, 5),   (1, 0, 1, 3),   (2, 0, 2, 1),   (3, 1, 0, 2),   (4, 2, 0, 8),   (5, 2, 2, 8))   .toDF(\"id\", \"orderID\", \"prodID\", \"orderQty\")</p> <p>scala&gt; sales.show +---+-------+------+--------+ | id|orderID|prodID|orderQty| +---+-------+------+--------+ |  0|      0|     0|       5| |  1|      0|     1|       3| |  2|      0|     2|       1| |  3|      1|     0|       2| |  4|      2|     0|       8| |  5|      2|     2|       8| +---+-------+------+--------+</p> <p>val orderedByID = Window.orderBy('id)</p> <p>val totalQty = sum('orderQty).over(orderedByID).as('running_total) val salesTotalQty = sales.select('*, totalQty).orderBy('id)</p> <p>scala&gt; salesTotalQty.show 16/04/10 23:01:52 WARN Window: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation. +---+-------+------+--------+-------------+ | id|orderID|prodID|orderQty|running_total| +---+-------+------+--------+-------------+ |  0|      0|     0|       5|            5| |  1|      0|     1|       3|            8| |  2|      0|     2|       1|            9| |  3|      1|     0|       2|           11| |  4|      2|     0|       8|           19| |  5|      2|     2|       8|           27| +---+-------+------+--------+-------------+</p> <p>val byOrderId = orderedByID.partitionBy('orderID) val totalQtyPerOrder = sum('orderQty).over(byOrderId).as('running_total_per_order) val salesTotalQtyPerOrder = sales.select('*, totalQtyPerOrder).orderBy('id)</p> <p>scala&gt; salesTotalQtyPerOrder.show +---+-------+------+--------+-----------------------+ | id|orderID|prodID|orderQty|running_total_per_order| +---+-------+------+--------+-----------------------+ |  0|      0|     0|       5|                      5| |  1|      0|     1|       3|                      8| |  2|      0|     2|       1|                      9| |  3|      1|     0|       2|                      2| |  4|      2|     0|       8|                      8| |  5|      2|     2|       8|                     16| +---+-------+------+--------+-----------------------+</p> <p>==== [[example-rank]] Calculate rank of row</p> <p>See &lt;&gt; for an elaborate example. <p>=== Interval data type for Date and Timestamp types</p> <p>See https://issues.apache.org/jira/browse/SPARK-8943[[SPARK-8943] CalendarIntervalType for time intervals].</p> <p>With the Interval data type, you could use intervals as values specified in <code>&lt;value&gt; PRECEDING</code> and <code>&lt;value&gt; FOLLOWING</code> for <code>RANGE</code> frame. It is specifically suited for time-series analysis with window functions.</p> <p>==== Accessing values of earlier rows</p> <p>FIXME What's the value of rows before current one?</p> <p>==== [[example-moving-average]] Moving Average</p> <p>==== [[example-cumulative-aggregates]] Cumulative Aggregates</p> <p>Eg. cumulative sum</p> <p>=== User-defined aggregate functions</p> <p>See https://issues.apache.org/jira/browse/SPARK-3947[[SPARK-3947] Support Scala/Java UDAF].</p> <p>With the window function support, you could use user-defined aggregate functions as window functions.</p> <p>=== [[explain-windows]] \"Explaining\" Query Plans of Windows</p> <pre><code>import org.apache.spark.sql.expressions.Window\nval byDepnameSalaryDesc = Window.partitionBy('depname).orderBy('salary desc)\n\nscala&gt; val rankByDepname = rank().over(byDepnameSalaryDesc)\nrankByDepname: org.apache.spark.sql.Column = RANK() OVER (PARTITION BY depname ORDER BY salary DESC UnspecifiedFrame)\n\n// empsalary defined at the top of the page\nscala&gt; empsalary.select('*, rankByDepname as 'rank).explain(extended = true)\n== Parsed Logical Plan ==\n'Project [*, rank() windowspecdefinition('depname, 'salary DESC, UnspecifiedFrame) AS rank#9]\n+- LocalRelation [depName#5, empNo#6L, salary#7L]\n\n== Analyzed Logical Plan ==\ndepName: string, empNo: bigint, salary: bigint, rank: int\nProject [depName#5, empNo#6L, salary#7L, rank#9]\n+- Project [depName#5, empNo#6L, salary#7L, rank#9, rank#9]\n   +- Window [rank(salary#7L) windowspecdefinition(depname#5, salary#7L DESC, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank#9], [depname#5], [salary#7L DESC]\n      +- Project [depName#5, empNo#6L, salary#7L]\n         +- LocalRelation [depName#5, empNo#6L, salary#7L]\n\n== Optimized Logical Plan ==\nWindow [rank(salary#7L) windowspecdefinition(depname#5, salary#7L DESC, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank#9], [depname#5], [salary#7L DESC]\n+- LocalRelation [depName#5, empNo#6L, salary#7L]\n\n== Physical Plan ==\nWindow [rank(salary#7L) windowspecdefinition(depname#5, salary#7L DESC, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank#9], [depname#5], [salary#7L DESC]\n+- *Sort [depname#5 ASC, salary#7L DESC], false, 0\n   +- Exchange hashpartitioning(depname#5, 200)\n      +- LocalTableScan [depName#5, empNo#6L, salary#7L]\n</code></pre> <p>=== [[lag]] <code>lag</code> Window Function</p>"},{"location":"spark-sql-functions-windows/#source-scala_7","title":"[source, scala]","text":"<p>lag(e: Column, offset: Int): Column lag(columnName: String, offset: Int): Column lag(columnName: String, offset: Int, defaultValue: Any): Column lag(e: Column, offset: Int, defaultValue: Any): Column</p> <p><code>lag</code> returns the value in <code>e</code> / <code>columnName</code> column that is <code>offset</code> records before the current record. <code>lag</code> returns <code>null</code> value if the number of records in a window partition is less than <code>offset</code> or <code>defaultValue</code>.</p>"},{"location":"spark-sql-functions-windows/#source-scala_8","title":"[source, scala]","text":"<p>val buckets = spark.range(9).withColumn(\"bucket\", 'id % 3) // Make duplicates val dataset = buckets.union(buckets)</p> <p>import org.apache.spark.sql.expressions.Window val windowSpec = Window.partitionBy('bucket).orderBy('id) scala&gt; dataset.withColumn(\"lag\", lag('id, 1) over windowSpec).show +---+------+----+ | id|bucket| lag| +---+------+----+ |  0|     0|null| |  3|     0|   0| |  6|     0|   3| |  1|     1|null| |  4|     1|   1| |  7|     1|   4| |  2|     2|null| |  5|     2|   2| |  8|     2|   5| +---+------+----+</p> <p>scala&gt; dataset.withColumn(\"lag\", lag('id, 2, \"\") over windowSpec).show +---+------+----+ | id|bucket| lag| +---+------+----+ |  0|     0|null| |  3|     0|null| |  6|     0|   0| |  1|     1|null| |  4|     1|null| |  7|     1|   1| |  2|     2|null| |  5|     2|null| |  8|     2|   2| +---+------+----+ <p>CAUTION: FIXME It looks like <code>lag</code> with a default value has a bug -- the default value's not used at all.</p> <p>=== [[lead]] <code>lead</code> Window Function</p>"},{"location":"spark-sql-functions-windows/#source-scala_9","title":"[source, scala]","text":"<p>lead(columnName: String, offset: Int): Column lead(e: Column, offset: Int): Column lead(columnName: String, offset: Int, defaultValue: Any): Column lead(e: Column, offset: Int, defaultValue: Any): Column</p> <p><code>lead</code> returns the value that is <code>offset</code> records after the current records, and <code>defaultValue</code> if there is less than <code>offset</code> records after the current record. <code>lag</code> returns <code>null</code> value if the number of records in a window partition is less than <code>offset</code> or <code>defaultValue</code>.</p>"},{"location":"spark-sql-functions-windows/#source-scala_10","title":"[source, scala]","text":"<p>val buckets = spark.range(9).withColumn(\"bucket\", 'id % 3) // Make duplicates val dataset = buckets.union(buckets)</p> <p>import org.apache.spark.sql.expressions.Window val windowSpec = Window.partitionBy('bucket).orderBy('id) scala&gt; dataset.withColumn(\"lead\", lead('id, 1) over windowSpec).show +---+------+----+ | id|bucket|lead| +---+------+----+ |  0|     0|   0| |  0|     0|   3| |  3|     0|   3| |  3|     0|   6| |  6|     0|   6| |  6|     0|null| |  1|     1|   1| |  1|     1|   4| |  4|     1|   4| |  4|     1|   7| |  7|     1|   7| |  7|     1|null| |  2|     2|   2| |  2|     2|   5| |  5|     2|   5| |  5|     2|   8| |  8|     2|   8| |  8|     2|null| +---+------+----+</p> <p>scala&gt; dataset.withColumn(\"lead\", lead('id, 2, \"\") over windowSpec).show +---+------+----+ | id|bucket|lead| +---+------+----+ |  0|     0|   3| |  0|     0|   3| |  3|     0|   6| |  3|     0|   6| |  6|     0|null| |  6|     0|null| |  1|     1|   4| |  1|     1|   4| |  4|     1|   7| |  4|     1|   7| |  7|     1|null| |  7|     1|null| |  2|     2|   5| |  2|     2|   5| |  5|     2|   8| |  5|     2|   8| |  8|     2|null| |  8|     2|null| +---+------+----+ <p>CAUTION: FIXME It looks like <code>lead</code> with a default value has a bug -- the default value's not used at all.</p> <p>=== [[cume_dist]] Cumulative Distribution of Records Across Window Partitions -- <code>cume_dist</code> Window Function</p>"},{"location":"spark-sql-functions-windows/#source-scala_11","title":"[source, scala]","text":""},{"location":"spark-sql-functions-windows/#cume_dist-column","title":"cume_dist(): Column","text":"<p><code>cume_dist</code> computes the cumulative distribution of the records in window partitions. This is equivalent to SQL's <code>CUME_DIST</code> function.</p>"},{"location":"spark-sql-functions-windows/#source-scala_12","title":"[source, scala]","text":"<p>val buckets = spark.range(9).withColumn(\"bucket\", 'id % 3) // Make duplicates val dataset = buckets.union(buckets)</p> <p>import org.apache.spark.sql.expressions.Window val windowSpec = Window.partitionBy('bucket).orderBy('id) scala&gt; dataset.withColumn(\"cume_dist\", cume_dist over windowSpec).show +---+------+------------------+ | id|bucket|         cume_dist| +---+------+------------------+ |  0|     0|0.3333333333333333| |  3|     0|0.6666666666666666| |  6|     0|               1.0| |  1|     1|0.3333333333333333| |  4|     1|0.6666666666666666| |  7|     1|               1.0| |  2|     2|0.3333333333333333| |  5|     2|0.6666666666666666| |  8|     2|               1.0| +---+------+------------------+</p> <p>=== [[row_number]] Sequential numbering per window partition -- <code>row_number</code> Window Function</p>"},{"location":"spark-sql-functions-windows/#source-scala_13","title":"[source, scala]","text":""},{"location":"spark-sql-functions-windows/#row_number-column","title":"row_number(): Column","text":"<p><code>row_number</code> returns a sequential number starting at <code>1</code> within a window partition.</p>"},{"location":"spark-sql-functions-windows/#source-scala_14","title":"[source, scala]","text":"<p>val buckets = spark.range(9).withColumn(\"bucket\", 'id % 3) // Make duplicates val dataset = buckets.union(buckets)</p> <p>import org.apache.spark.sql.expressions.Window val windowSpec = Window.partitionBy('bucket).orderBy('id) scala&gt; dataset.withColumn(\"row_number\", row_number() over windowSpec).show +---+------+----------+ | id|bucket|row_number| +---+------+----------+ |  0|     0|         1| |  0|     0|         2| |  3|     0|         3| |  3|     0|         4| |  6|     0|         5| |  6|     0|         6| |  1|     1|         1| |  1|     1|         2| |  4|     1|         3| |  4|     1|         4| |  7|     1|         5| |  7|     1|         6| |  2|     2|         1| |  2|     2|         2| |  5|     2|         3| |  5|     2|         4| |  8|     2|         5| |  8|     2|         6| +---+------+----------+</p> <p>=== [[ntile]] <code>ntile</code> Window Function</p>"},{"location":"spark-sql-functions-windows/#source-scala_15","title":"[source, scala]","text":""},{"location":"spark-sql-functions-windows/#ntilen-int-column","title":"ntile(n: Int): Column","text":"<p><code>ntile</code> computes the ntile group id (from <code>1</code> to <code>n</code> inclusive) in an ordered window partition.</p>"},{"location":"spark-sql-functions-windows/#source-scala_16","title":"[source, scala]","text":"<p>val dataset = spark.range(7).select('*, 'id % 3 as \"bucket\")</p> <p>import org.apache.spark.sql.expressions.Window val byBuckets = Window.partitionBy('bucket).orderBy('id) scala&gt; dataset.select('*, ntile(3) over byBuckets as \"ntile\").show +---+------+-----+ | id|bucket|ntile| +---+------+-----+ |  0|     0|    1| |  3|     0|    2| |  6|     0|    3| |  1|     1|    1| |  4|     1|    2| |  2|     2|    1| |  5|     2|    2| +---+------+-----+</p> <p>CAUTION: FIXME How is <code>ntile</code> different from <code>rank</code>? What about performance?</p> <p>=== [[rank]][[dense_rank]][[percent_rank]] Ranking Records per Window Partition -- <code>rank</code> Window Function</p>"},{"location":"spark-sql-functions-windows/#source-scala_17","title":"[source, scala]","text":"<p>rank(): Column dense_rank(): Column percent_rank(): Column</p> <p><code>rank</code> functions assign the sequential rank of each distinct value per window partition. They are equivalent to <code>RANK</code>, <code>DENSE_RANK</code> and <code>PERCENT_RANK</code> functions in the good ol' SQL.</p>"},{"location":"spark-sql-functions-windows/#source-scala_18","title":"[source, scala]","text":"<p>val dataset = spark.range(9).withColumn(\"bucket\", 'id % 3)</p> <p>import org.apache.spark.sql.expressions.Window val byBucket = Window.partitionBy('bucket).orderBy('id)</p> <p>scala&gt; dataset.withColumn(\"rank\", rank over byBucket).show +---+------+----+ | id|bucket|rank| +---+------+----+ |  0|     0|   1| |  3|     0|   2| |  6|     0|   3| |  1|     1|   1| |  4|     1|   2| |  7|     1|   3| |  2|     2|   1| |  5|     2|   2| |  8|     2|   3| +---+------+----+</p> <p>scala&gt; dataset.withColumn(\"percent_rank\", percent_rank over byBucket).show +---+------+------------+ | id|bucket|percent_rank| +---+------+------------+ |  0|     0|         0.0| |  3|     0|         0.5| |  6|     0|         1.0| |  1|     1|         0.0| |  4|     1|         0.5| |  7|     1|         1.0| |  2|     2|         0.0| |  5|     2|         0.5| |  8|     2|         1.0| +---+------+------------+</p> <p><code>rank</code> function assigns the same rank for duplicate rows with a gap in the sequence (similarly to Olympic medal places). <code>dense_rank</code> is like <code>rank</code> for duplicate rows but compacts the ranks and removes the gaps.</p>"},{"location":"spark-sql-functions-windows/#source-scala_19","title":"[source, scala]","text":"<p>// rank function with duplicates // Note the missing/sparse ranks, i.e. 2 and 4 scala&gt; dataset.union(dataset).withColumn(\"rank\", rank over byBucket).show +---+------+----+ | id|bucket|rank| +---+------+----+ |  0|     0|   1| |  0|     0|   1| |  3|     0|   3| |  3|     0|   3| |  6|     0|   5| |  6|     0|   5| |  1|     1|   1| |  1|     1|   1| |  4|     1|   3| |  4|     1|   3| |  7|     1|   5| |  7|     1|   5| |  2|     2|   1| |  2|     2|   1| |  5|     2|   3| |  5|     2|   3| |  8|     2|   5| |  8|     2|   5| +---+------+----+</p> <p>// dense_rank function with duplicates // Note that the missing ranks are now filled in scala&gt; dataset.union(dataset).withColumn(\"dense_rank\", dense_rank over byBucket).show +---+------+----------+ | id|bucket|dense_rank| +---+------+----------+ |  0|     0|         1| |  0|     0|         1| |  3|     0|         2| |  3|     0|         2| |  6|     0|         3| |  6|     0|         3| |  1|     1|         1| |  1|     1|         1| |  4|     1|         2| |  4|     1|         2| |  7|     1|         3| |  7|     1|         3| |  2|     2|         1| |  2|     2|         1| |  5|     2|         2| |  5|     2|         2| |  8|     2|         3| |  8|     2|         3| +---+------+----------+</p> <p>// percent_rank function with duplicates scala&gt; dataset.union(dataset).withColumn(\"percent_rank\", percent_rank over byBucket).show +---+------+------------+ | id|bucket|percent_rank| +---+------+------------+ |  0|     0|         0.0| |  0|     0|         0.0| |  3|     0|         0.4| |  3|     0|         0.4| |  6|     0|         0.8| |  6|     0|         0.8| |  1|     1|         0.0| |  1|     1|         0.0| |  4|     1|         0.4| |  4|     1|         0.4| |  7|     1|         0.8| |  7|     1|         0.8| |  2|     2|         0.0| |  2|     2|         0.0| |  5|     2|         0.4| |  5|     2|         0.4| |  8|     2|         0.8| |  8|     2|         0.8| +---+------+------------+</p> <p>=== [[currentRow]] <code>currentRow</code> Window Function</p>"},{"location":"spark-sql-functions-windows/#source-scala_20","title":"[source, scala]","text":""},{"location":"spark-sql-functions-windows/#currentrow-column","title":"currentRow(): Column","text":"<p><code>currentRow</code>...FIXME</p> <p>=== [[unboundedFollowing]] <code>unboundedFollowing</code> Window Function</p>"},{"location":"spark-sql-functions-windows/#source-scala_21","title":"[source, scala]","text":""},{"location":"spark-sql-functions-windows/#unboundedfollowing-column","title":"unboundedFollowing(): Column","text":"<p><code>unboundedFollowing</code>...FIXME</p> <p>=== [[unboundedPreceding]] <code>unboundedPreceding</code> Window Function</p>"},{"location":"spark-sql-functions-windows/#source-scala_22","title":"[source, scala]","text":""},{"location":"spark-sql-functions-windows/#unboundedpreceding-column","title":"unboundedPreceding(): Column","text":"<p><code>unboundedPreceding</code>...FIXME</p> <p>=== [[i-want-more]] Further Reading and Watching</p> <ul> <li>https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html[Introducing Window Functions in Spark SQL]</li> <li>http://www.postgresql.org/docs/current/static/tutorial-window.html[3.5. Window Functions] in the official documentation of PostgreSQL</li> <li>https://www.simple-talk.com/sql/t-sql-programming/window-functions-in-sql/[Window Functions in SQL]</li> <li>https://www.simple-talk.com/sql/learn-sql-server/working-with-window-functions-in-sql-server/[Working with Window Functions in SQL Server]</li> <li>https://msdn.microsoft.com/en-CA/library/ms189461.aspx[OVER Clause (Transact-SQL)]</li> <li>https://sqlsunday.com/2013/03/31/windowed-functions/[An introduction to windowed functions]</li> <li>https://blog.jooq.org/2013/11/03/probably-the-coolest-sql-feature-window-functions/[Probably the Coolest SQL Feature: Window Functions]</li> <li>https://sqlschool.modeanalytics.com/advanced/window-functions/[Window Functions]</li> </ul>"},{"location":"spark-sql-joins-broadcast/","title":"Broadcast Joins","text":"<p>Spark SQL uses broadcast join (broadcast hash join) instead of hash join to optimize join queries when the size of one side data is below spark.sql.autoBroadcastJoinThreshold.</p> <p>Broadcast join can be very efficient for joins between a large table (fact) with relatively small tables (dimensions) that could then be used to perform a star-schema join. It can avoid sending all data of the large table over the network.</p> <p>You can use broadcast function or SQL's broadcast hints to mark a dataset to be broadcast when used in a join query.</p> <p>NOTE: According to the article http://dmtolpeko.com/2015/02/20/map-side-join-in-spark/[Map-Side Join in Spark], broadcast join is also called a replicated join (in the distributed system community) or a map-side join (in the Hadoop community).</p> <p><code>CanBroadcast</code> object matches a spark-sql-LogicalPlan.md[LogicalPlan] with output small enough for broadcast join.</p> <p>NOTE: Currently statistics are only supported for Hive Metastore tables where the command <code>ANALYZE TABLE [tableName] COMPUTE STATISTICS noscan</code> has been run.</p> <p>JoinSelection execution planning strategy uses spark.sql.autoBroadcastJoinThreshold configuration property to control the size of a dataset before broadcasting it to all worker nodes when performing a join.</p> <pre><code>val threshold =  spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\").toInt\nscala&gt; threshold / 1024 / 1024\nres0: Int = 10\n\nval q = spark.range(100).as(\"a\").join(spark.range(100).as(\"b\")).where($\"a.id\" === $\"b.id\")\nscala&gt; println(q.queryExecution.logical.numberedTreeString)\n00 'Filter ('a.id = 'b.id)\n01 +- Join Inner\n02    :- SubqueryAlias a\n03    :  +- Range (0, 100, step=1, splits=Some(8))\n04    +- SubqueryAlias b\n05       +- Range (0, 100, step=1, splits=Some(8))\n\nscala&gt; println(q.queryExecution.sparkPlan.numberedTreeString)\n00 BroadcastHashJoin [id#0L], [id#4L], Inner, BuildRight\n01 :- Range (0, 100, step=1, splits=8)\n02 +- Range (0, 100, step=1, splits=8)\n\nscala&gt; q.explain\n== Physical Plan ==\n*BroadcastHashJoin [id#0L], [id#4L], Inner, BuildRight\n:- *Range (0, 100, step=1, splits=8)\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]))\n   +- *Range (0, 100, step=1, splits=8)\n\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\nscala&gt; spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\nres1: String = -1\n\nscala&gt; q.explain\n== Physical Plan ==\n*SortMergeJoin [id#0L], [id#4L], Inner\n:- *Sort [id#0L ASC NULLS FIRST], false, 0\n:  +- Exchange hashpartitioning(id#0L, 200)\n:     +- *Range (0, 100, step=1, splits=8)\n+- *Sort [id#4L ASC NULLS FIRST], false, 0\n   +- ReusedExchange [id#4L], Exchange hashpartitioning(id#0L, 200)\n\n// Force BroadcastHashJoin with broadcast hint (as function)\nval qBroadcast = spark.range(100).as(\"a\").join(broadcast(spark.range(100)).as(\"b\")).where($\"a.id\" === $\"b.id\")\nscala&gt; qBroadcast.explain\n== Physical Plan ==\n*BroadcastHashJoin [id#14L], [id#18L], Inner, BuildRight\n:- *Range (0, 100, step=1, splits=8)\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]))\n   +- *Range (0, 100, step=1, splits=8)\n\n// Force BroadcastHashJoin using SQL's BROADCAST hint\n// Supported hints: BROADCAST, BROADCASTJOIN or MAPJOIN\nval qBroadcastLeft = \"\"\"\n  SELECT /*+ BROADCAST (lf) */ *\n  FROM range(100) lf, range(1000) rt\n  WHERE lf.id = rt.id\n\"\"\"\nscala&gt; sql(qBroadcastLeft).explain\n== Physical Plan ==\n*BroadcastHashJoin [id#34L], [id#35L], Inner, BuildRight\n:- *Range (0, 100, step=1, splits=8)\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]))\n   +- *Range (0, 1000, step=1, splits=8)\n\nval qBroadcastRight = \"\"\"\n SELECT /*+ MAPJOIN (rt) */ *\n FROM range(100) lf, range(1000) rt\n WHERE lf.id = rt.id\n\"\"\"\nscala&gt; sql(qBroadcastRight).explain\n== Physical Plan ==\n*BroadcastHashJoin [id#42L], [id#43L], Inner, BuildRight\n:- *Range (0, 100, step=1, splits=8)\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]))\n   +- *Range (0, 1000, step=1, splits=8)\n</code></pre>"},{"location":"spark-sql-performance-tuning-groupBy-aggregation/","title":"Case Study: Number of Partitions for groupBy Aggregation","text":""},{"location":"spark-sql-performance-tuning-groupBy-aggregation/#important","title":"[IMPORTANT]","text":"<p>As it fairly often happens in my life, right after I had described the discovery I found out I was wrong and the \"Aha moment\" was gone.</p> <p>Until I thought about the issue again and took the shortest path possible. See &lt;&gt; for the definitive solution."},{"location":"spark-sql-performance-tuning-groupBy-aggregation/#im-leaving-the-page-with-no-changes-in-between-so-you-can-read-it-and-learn-from-my-mistakes","title":"I'm leaving the page with no changes in-between so you can read it and learn from my mistakes.","text":"<p>The goal of the case study is to fine tune the number of partitions used for <code>groupBy</code> aggregation.</p> <p>Given the following 2-partition dataset the task is to write a structured query so there are no empty partitions (or as little as possible).</p>"},{"location":"spark-sql-performance-tuning-groupBy-aggregation/#source-scala","title":"[source, scala]","text":"<p>// 2-partition dataset val ids = spark.range(start = 0, end = 4, step = 1, numPartitions = 2) scala&gt; ids.show +---+ | id| +---+ |  0| |  1| |  2| |  3| +---+</p> <p>scala&gt; ids.rdd.toDebugString res1: String = (2) MapPartitionsRDD[8] at rdd at :26 []  |  MapPartitionsRDD[7] at rdd at :26 []  |  MapPartitionsRDD[6] at rdd at :26 []  |  MapPartitionsRDD[5] at rdd at :26 []  |  ParallelCollectionRDD[4] at rdd at :26 [] <p>Note</p> <p>By default Spark SQL uses spark.sql.shuffle.partitions number of partitions for aggregations and joins.</p> <p>That often leads to explosion of partitions for nothing that does impact the  performance of a query since these 200 tasks (per partition) have all to start and finish before you get the result.</p> <p>=== [[case_1]] Case 1: Default Number of Partitions -- spark.sql.shuffle.partitions Property</p> <p>This is the moment when you learn that sometimes relying on defaults may lead to poor performance.</p> <p>Think how many partitions the following query really requires?</p>"},{"location":"spark-sql-performance-tuning-groupBy-aggregation/#source-scala_1","title":"[source, scala]","text":"<p>val groupingExpr = 'id % 2 as \"group\" val q = ids.   groupBy(groupingExpr).   agg(count($\"id\") as \"count\")</p> <p>You may have expected to have at most 2 partitions given the number of groups.</p> <p>Wrong!</p>"},{"location":"spark-sql-performance-tuning-groupBy-aggregation/#source-scala_2","title":"[source, scala]","text":"<p>scala&gt; q.explain == Physical Plan == *HashAggregate(keys=[(id#0L % 2)#17L], functions=[count(1)]) +- Exchange hashpartitioning((id#0L % 2)#17L, 200)    +- *HashAggregate(keys=[(id#0L % 2) AS (id#0L % 2)#17L], functions=[partial_count(1)])       +- *Range (0, 4, step=1, splits=2)</p> <p>scala&gt; q.rdd.toDebugString res5: String = (200) MapPartitionsRDD[16] at rdd at :30 []   |   MapPartitionsRDD[15] at rdd at :30 []   |   MapPartitionsRDD[14] at rdd at :30 []   |   ShuffledRowRDD[13] at rdd at :30 []   +-(2) MapPartitionsRDD[12] at rdd at :30 []      |  MapPartitionsRDD[11] at rdd at :30 []      |  MapPartitionsRDD[10] at rdd at :30 []      |  ParallelCollectionRDD[9] at rdd at :30 [] <p>When you execute the query you should see 200 or so partitions in use in web UI.</p>"},{"location":"spark-sql-performance-tuning-groupBy-aggregation/#source-scala_3","title":"[source, scala]","text":"<p>scala&gt; q.show +-----+-----+ |group|count| +-----+-----+ |    0|    2| |    1|    2| +-----+-----+</p> <p>.Case 1's Physical Plan with Default Number of Partitions image::images/spark-sql-performance-tuning-groupBy-aggregation-case1.png[align=\"center\"]</p> <p>NOTE: The number of Succeeded Jobs is 5.</p> <p>=== Case 2: Using repartition Operator</p> <p>Let's rewrite the query to use <code>repartition</code> operator.</p> <p><code>repartition</code> operator is indeed a step in a right direction when used with caution as it may lead to an unnecessary shuffle (aka exchange in Spark SQL's parlance).</p> <p>Think how many partitions the following query really requires?</p>"},{"location":"spark-sql-performance-tuning-groupBy-aggregation/#source-scala_4","title":"[source, scala]","text":"<p>val groupingExpr = 'id % 2 as \"group\" val q = ids.   repartition(groupingExpr). // \u2190 repartition per groupBy expression   groupBy(groupingExpr).   agg(count($\"id\") as \"count\")</p> <p>You may have expected 2 partitions again?!</p> <p>Wrong!</p>"},{"location":"spark-sql-performance-tuning-groupBy-aggregation/#source-scala_5","title":"[source, scala]","text":"<p>scala&gt; q.explain == Physical Plan == *HashAggregate(keys=[(id#6L % 2)#105L], functions=[count(1)]) +- Exchange hashpartitioning((id#6L % 2)#105L, 200)    +- *HashAggregate(keys=[(id#6L % 2) AS (id#6L % 2)#105L], functions=[partial_count(1)])       +- Exchange hashpartitioning((id#6L % 2), 200)          +- *Range (0, 4, step=1, splits=2)</p> <p>scala&gt; q.rdd.toDebugString res1: String = (200) MapPartitionsRDD[57] at rdd at :30 []   |   MapPartitionsRDD[56] at rdd at :30 []   |   MapPartitionsRDD[55] at rdd at :30 []   |   ShuffledRowRDD[54] at rdd at :30 []   +-(200) MapPartitionsRDD[53] at rdd at :30 []       |   MapPartitionsRDD[52] at rdd at :30 []       |   ShuffledRowRDD[51] at rdd at :30 []       +-(2) MapPartitionsRDD[50] at rdd at :30 []          |  MapPartitionsRDD[49] at rdd at :30 []          |  MapPartitionsRDD[48] at rdd at :30 []          |  ParallelCollectionRDD[47] at rdd at :30 [] <p>Compare the physical plans of the two queries and you will surely regret using <code>repartition</code> operator in the latter as you did cause an extra shuffle stage (!)</p> <p>=== Case 3: Using repartition Operator With Explicit Number of Partitions</p> <p>The discovery of the day is to notice that <code>repartition</code> operator accepts an additional parameter for...the number of partitions (!)</p> <p>As a matter of fact, there are two variants of <code>repartition</code> operator with the number of partitions and the trick is to use the one with partition expressions (that will be used for grouping as well as...hash partitioning).</p>"},{"location":"spark-sql-performance-tuning-groupBy-aggregation/#source-scala_6","title":"[source, scala]","text":""},{"location":"spark-sql-performance-tuning-groupBy-aggregation/#repartitionnumpartitions-int-partitionexprs-column-datasett","title":"repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T]","text":"<p>Can you think of the number of partitions the following query uses? I'm sure you have guessed correctly!</p>"},{"location":"spark-sql-performance-tuning-groupBy-aggregation/#source-scala_7","title":"[source, scala]","text":"<p>val groupingExpr = 'id % 2 as \"group\" val q = ids.   repartition(numPartitions = 2, groupingExpr). // \u2190 repartition per groupBy expression   groupBy(groupingExpr).   agg(count($\"id\") as \"count\")</p> <p>You may have expected 2 partitions again?!</p> <p>Correct!</p>"},{"location":"spark-sql-performance-tuning-groupBy-aggregation/#source-scala_8","title":"[source, scala]","text":"<p>scala&gt; q.explain == Physical Plan == *HashAggregate(keys=[(id#6L % 2)#129L], functions=[count(1)]) +- Exchange hashpartitioning((id#6L % 2)#129L, 200)    +- *HashAggregate(keys=[(id#6L % 2) AS (id#6L % 2)#129L], functions=[partial_count(1)])       +- Exchange hashpartitioning((id#6L % 2), 2)          +- *Range (0, 4, step=1, splits=2)</p> <p>scala&gt; q.rdd.toDebugString res14: String = (200) MapPartitionsRDD[78] at rdd at :30 []   |   MapPartitionsRDD[77] at rdd at :30 []   |   MapPartitionsRDD[76] at rdd at :30 []   |   ShuffledRowRDD[75] at rdd at :30 []   +-(2) MapPartitionsRDD[74] at rdd at :30 []      |  MapPartitionsRDD[73] at rdd at :30 []      |  ShuffledRowRDD[72] at rdd at :30 []      +-(2) MapPartitionsRDD[71] at rdd at :30 []         |  MapPartitionsRDD[70] at rdd at :30 []         |  MapPartitionsRDD[69] at rdd at :30 []         |  ParallelCollectionRDD[68] at rdd at :30 [] <p>Congratulations! You are done.</p> <p>Not quite. Read along!</p> <p>=== [[case_4]] Case 4: Remember spark.sql.shuffle.partitions Property? Set It Up Properly</p>"},{"location":"spark-sql-performance-tuning-groupBy-aggregation/#source-scala_9","title":"[source, scala]","text":"<p>import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 2) // spark.conf.set(SHUFFLE_PARTITIONS.key, 2)</p> <p>scala&gt; spark.sessionState.conf.numShufflePartitions res8: Int = 2</p> <p>val q = ids.   groupBy(groupingExpr).   agg(count($\"id\") as \"count\")</p>"},{"location":"spark-sql-performance-tuning-groupBy-aggregation/#source-scala_10","title":"[source, scala]","text":"<p>scala&gt; q.explain == Physical Plan == *HashAggregate(keys=[(id#0L % 2)#40L], functions=[count(1)]) +- Exchange hashpartitioning((id#0L % 2)#40L, 2)    +- *HashAggregate(keys=[(id#0L % 2) AS (id#0L % 2)#40L], functions=[partial_count(1)])       +- *Range (0, 4, step=1, splits=2)</p> <p>scala&gt; q.rdd.toDebugString res10: String = (2) MapPartitionsRDD[31] at rdd at :31 []  |  MapPartitionsRDD[30] at rdd at :31 []  |  MapPartitionsRDD[29] at rdd at :31 []  |  ShuffledRowRDD[28] at rdd at :31 []  +-(2) MapPartitionsRDD[27] at rdd at :31 []     |  MapPartitionsRDD[26] at rdd at :31 []     |  MapPartitionsRDD[25] at rdd at :31 []     |  ParallelCollectionRDD[24] at rdd at :31 [] <p>.Case 4's Physical Plan with Custom Number of Partitions image::images/spark-sql-performance-tuning-groupBy-aggregation-case4.png[align=\"center\"]</p> <p>NOTE: The number of Succeeded Jobs is 2.</p> <p>Congratulations! You are done now.</p>"},{"location":"spark-sql-performance-tuning/","title":"Spark SQL's Performance Tuning Tips and Tricks (aka Case Studies)","text":"<p>From time to time I'm lucky enough to find ways to optimize structured queries in Spark SQL. These findings (or discoveries) usually fall into a study category than a single topic and so the goal of Spark SQL's Performance Tuning Tips and Tricks chapter is to have a single place for the so-called tips and tricks.</p> <p>. spark-sql-performance-tuning-groupBy-aggregation.md[Number of Partitions for groupBy Aggegration]</p>"},{"location":"spark-sql-performance-tuning/#others","title":"Others","text":"<p>. Avoid <code>ObjectType</code> it turns whole-stage Java code generation off.</p> <p>. Keep whole-stage codegen requirements in mind, in particular avoid physical operators with supportCodegen flag off.</p>"},{"location":"spark-sql-udfs-blackbox/","title":"UDFs are Blackbox \u2014 Don't Use Them Unless You've Got No Choice","text":"<p>User-Defined Functions are a blackbox to the Catalyst optimizer and so whatever happens in the UDF, stays in the UDF (paraphrasing the former advertising slogan of Las Vegas, Nevada). With UDFs your queries will likely be slower and memory-inefficient.</p>"},{"location":"spark-sql-udfs-blackbox/#example","title":"Example","text":"<p>Let's review an example with an UDF. This example is converting strings of size 7 characters only and uses the <code>Dataset</code> standard operators first and then custom UDF to do the same transformation.</p> <pre><code>assert(spark.conf.get(\"spark.sql.parquet.filterPushdown\"))\n</code></pre> <p>You are going to use the following <code>cities</code> dataset that is based on Parquet file (as used in Predicate Pushdown / Filter Pushdown for Parquet Data Source section). The reason for parquet is that it is an external data source that does support optimization Spark uses to optimize itself like predicate pushdown.</p> <pre><code>// no optimization as it is a more involved Scala function in filter\n\nval cities6chars = cities.filter(_.name.length == 6).map(_.name.toUpperCase)\n\ncities6chars.explain(true)\n\n// or simpler when only concerned with PushedFilters attribute in Parquet\nscala&gt; cities6chars.queryExecution.optimizedPlan\nres33: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =\nSerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true) AS value#248]\n+- MapElements &lt;function1&gt;, class City, [StructField(id,LongType,false), StructField(name,StringType,true)], obj#247: java.lang.String\n   +- Filter &lt;function1&gt;.apply\n      +- DeserializeToObject newInstance(class City), obj#246: City\n         +- Relation[id#236L,name#237] parquet\n</code></pre> <pre><code>// no optimization for Dataset[City]\n\nval cities6chars = cities.filter(_.name == \"Warsaw\").map(_.name.toUpperCase)\n\ncities6chars.explain(true)\n\n// The filter predicate is pushed down fine for Dataset's Column-based query in where operator\nscala&gt; cities.where('name === \"Warsaw\").queryExecution.executedPlan\nres29: org.apache.spark.sql.execution.SparkPlan =\n*Project [id#128L, name#129]\n+- *Filter (isnotnull(name#129) &amp;&amp; (name#129 = Warsaw))\n   +- *FileScan parquet [id#128L,name#129] Batched: true, Format: ParquetFormat, InputPaths: file:/Users/jacek/dev/oss/spark/cities.parquet, PartitionFilters: [], PushedFilters: [IsNotNull(name), EqualTo(name,Warsaw)], ReadSchema: struct&lt;id:bigint,name:string&gt;\n</code></pre> <pre><code>// Let's define a UDF to do the filtering\nval isWarsaw = udf { (s: String) =&gt; s == \"Warsaw\" }\n\n// Use the UDF in where (replacing the Column-based query)\nscala&gt; cities.where(isWarsaw('name)).queryExecution.executedPlan\nres33: org.apache.spark.sql.execution.SparkPlan =\n*Filter UDF(name#129)\n+- *FileScan parquet [id#128L,name#129] Batched: true, Format: ParquetFormat, InputPaths: file:/Users/jacek/dev/oss/spark/cities.parquet, PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint,name:string&gt;\n</code></pre>"},{"location":"spark-sql-udfs/","title":"User-Defined Functions","text":"<p>User-Defined Functions (aka UDF) is a feature of Spark SQL to define new Column-based functions that extend the vocabulary of Spark SQL's DSL for transforming Dataset.md[Datasets].</p> <p>Important</p> <p>Use the higher-level standard Column-based functions (with Dataset operators) whenever possible before reverting to developing user-defined functions since UDFs are a blackbox for Spark SQL and it cannot (and does not even try to) optimize them.</p> <p>As Reynold Xin from the Apache Spark project has once said on Spark's dev mailing list:</p> <p>There are simple cases in which we can analyze the UDFs byte code and infer what it is doing, but it is pretty difficult to do in general.</p> <p>Check out UDFs are Blackbox -- Don't Use Them Unless You've Got No Choice if you want to know the internals.</p> <p>You define a new UDF by defining a Scala function as an input parameter of udf function. It accepts Scala functions of up to 10 input parameters.</p> <pre><code>val dataset = Seq((0, \"hello\"), (1, \"world\")).toDF(\"id\", \"text\")\n\n// Define a regular Scala function\nval upper: String =&gt; String = _.toUpperCase\n\n// Define a UDF that wraps the upper Scala function defined above\n// You could also define the function in place, i.e. inside udf\n// but separating Scala functions from Spark SQL's UDFs allows for easier testing\nimport org.apache.spark.sql.functions.udf\nval upperUDF = udf(upper)\n\n// Apply the UDF to change the source dataset\nscala&gt; dataset.withColumn(\"upper\", upperUDF('text)).show\n+---+-----+-----+\n| id| text|upper|\n+---+-----+-----+\n|  0|hello|HELLO|\n|  1|world|WORLD|\n+---+-----+-----+\n</code></pre> <p>You can register UDFs to use in SparkSession.md#sql[SQL-based query expressions] via UDFRegistration.md[UDFRegistration] (that is available through SparkSession.md#udf[<code>SparkSession.udf</code> attribute]).</p> <pre><code>val spark: SparkSession = ...\nscala&gt; spark.udf.register(\"myUpper\", (input: String) =&gt; input.toUpperCase)\n</code></pre> <p>You can query for available standard and user-defined functions using the Catalog interface (that is available through SparkSession.md#catalog[<code>SparkSession.catalog</code> attribute]).</p>"},{"location":"spark-sql-udfs/#source-scala","title":"[source, scala]","text":"<p>val spark: SparkSession = ... scala&gt; spark.catalog.listFunctions.filter('name like \"%upper%\").show(false) +-------+--------+-----------+-----------------------------------------------+-----------+ |name   |database|description|className                                      |isTemporary| +-------+--------+-----------+-----------------------------------------------+-----------+ |myupper|null    |null       |null                                           |true       | |upper  |null    |null       |org.apache.spark.sql.catalyst.expressions.Upper|true       | +-------+--------+-----------+-----------------------------------------------+-----------+</p> <p>NOTE: UDFs play a vital role in Spark MLlib to define new spark-mllib/spark-mllib-transformers.md[Transformers] that are function objects that transform <code>DataFrames</code> into <code>DataFrames</code> by introducing new columns.</p> <p>=== [[udf-function]] udf Functions (in functions object)</p>"},{"location":"spark-sql-udfs/#source-scala_1","title":"[source, scala]","text":"<p>udfRT: TypeTag: UserDefinedFunction ... udfRT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag, A7: TypeTag, A8: TypeTag, A9: TypeTag, A10: TypeTag: UserDefinedFunction</p> <p><code>org.apache.spark.sql.functions</code> object comes with <code>udf</code> function to let you define a UDF for a Scala function <code>f</code>.</p> <pre><code>val df = Seq(\n  (0, \"hello\"),\n  (1, \"world\")).toDF(\"id\", \"text\")\n\n// Define a \"regular\" Scala function\n// It's a clone of upper UDF\nval toUpper: String =&gt; String = _.toUpperCase\n\nimport org.apache.spark.sql.functions.udf\nval upper = udf(toUpper)\n\nscala&gt; df.withColumn(\"upper\", upper('text)).show\n+---+-----+-----+\n| id| text|upper|\n+---+-----+-----+\n|  0|hello|HELLO|\n|  1|world|WORLD|\n+---+-----+-----+\n\n// You could have also defined the UDF this way\nval upperUDF = udf { s: String =&gt; s.toUpperCase }\n\n// or even this way\nval upperUDF = udf[String, String](_.toUpperCase)\n\nscala&gt; df.withColumn(\"upper\", upperUDF('text)).show\n+---+-----+-----+\n| id| text|upper|\n+---+-----+-----+\n|  0|hello|HELLO|\n|  1|world|WORLD|\n+---+-----+-----+\n</code></pre> <p>TIP: Define custom UDFs based on \"standalone\" Scala functions (e.g. <code>toUpperUDF</code>) so you can test the Scala functions using Scala way (without Spark SQL's \"noise\") and once they are defined reuse the UDFs in spark-mllib/spark-mllib-transformers.md#UnaryTransformer[UnaryTransformers].</p>"},{"location":"subexpression-elimination/","title":"Subexpression Elimination In Code-Generated Expression Evaluation (Common Expression Reuse)","text":"<p>Subexpression Elimination (aka Common Expression Reuse) is an optimization of a logical query plan that eliminates expressions in code-generated (non-interpreted) expression evaluation.</p> <p>Subexpression Elimination is enabled by default. Use the internal &lt;&gt; configuration property control whether the feature is enabled (<code>true</code>) or not (<code>false</code>). <p>Subexpression Elimination is used (by means of SparkPlan.md#subexpressionEliminationEnabled[subexpressionEliminationEnabled] flag of <code>SparkPlan</code>) when the following physical operators are requested to execute (i.e. moving away from queries to an RDD of internal rows to describe a distributed computation):</p> <ul> <li> <p>ProjectExec</p> </li> <li> <p>HashAggregateExec</p> </li> <li> <p>ObjectHashAggregateExec</p> </li> <li> <p>SortAggregateExec</p> </li> <li> <p>WindowExec (and creates a lookup table for WindowExpressions and factory functions for WindowFunctionFrame)</p> </li> </ul> <p>Internally, subexpression elimination happens when <code>CodegenContext</code> is requested for subexpressionElimination (when <code>CodegenContext</code> is requested to &lt;&gt; with &lt;&gt; enabled)."},{"location":"subexpression-elimination/#sparksqlsubexpressioneliminationenabled-configuration-property","title":"spark.sql.subexpressionElimination.enabled Configuration Property <p>spark.sql.subexpressionElimination.enabled internal configuration property controls whether the subexpression elimination optimization is enabled or not.</p>","text":""},{"location":"variable-substitution/","title":"Variable Substitution","text":"<p>Spark SQL (and Spark Thrift Server) supports Variable Substitution in SQL commands using syntax like <code>${var}</code>, <code>${system:var}</code>, and <code>${env:var}</code>.</p> <p>Note</p> <p><code>VariableSubstitution</code> is meant for SQL commands mainly (if not exclusively) since in programming languages there are other means to achieve it (e.g., String Interpolation in Scala).</p>"},{"location":"variable-substitution/#sparksqlvariablesubstitute","title":"spark.sql.variable.substitute <p>spark.sql.variable.substitute configuration property is used to enable variable substitution.</p>","text":""},{"location":"adaptive-query-execution/","title":"Adaptive Query Execution (AQE)","text":"<p>Adaptive Query Execution (aka Adaptive Query Optimization, Adaptive Optimization, or AQE in short) is an optimization of a physical query execution plan in the middle of query execution for alternative execution plans at runtime.</p> <p>Adaptive Query Execution can only be used for queries with exchanges or sub-queries.</p> <p>Adaptive Query Execution re-optimizes the query plan based on runtime statistics.</p> <p>Adaptive Query Execution is enabled by default based on spark.sql.adaptive.enabled configuration property (since Spark 3.2 and SPARK-33679).</p> <p>Quoting the description of a talk by the authors of Adaptive Query Execution:</p> <p>At runtime, the adaptive execution mode can change shuffle join to broadcast join if it finds the size of one table is less than the broadcast threshold. It can also handle skewed input data for join and change the partition number of the next stage to better fit the data scale. In general, adaptive execution decreases the effort involved in tuning SQL query parameters and improves the execution performance by choosing a better execution plan and parallelism at runtime.</p>"},{"location":"adaptive-query-execution/#insertadaptivesparkplan-physical-optimization","title":"InsertAdaptiveSparkPlan Physical Optimization","text":"<p>Adaptive Query Execution is possible (and applied to a physical query plan) using the InsertAdaptiveSparkPlan physical optimization that inserts AdaptiveSparkPlanExec physical operators.</p>"},{"location":"adaptive-query-execution/#aqe-logical-optimizer","title":"AQE Logical Optimizer","text":"<p>Adaptive Query Execution uses AQEOptimizer logical optimizer to re-optimize logical plans.</p>"},{"location":"adaptive-query-execution/#aqe-cost-evaluator","title":"AQE Cost Evaluator","text":"<p>Adaptive Query Execution uses CostEvaluator to evaluate cost when considering a candidate for an Adaptively-Optimized Physical Query Plan.</p> <p>If a <code>SparkPlan</code> change happens, <code>AdaptiveSparkPlanExec</code> prints out the following message to the logs:</p> <pre><code>Plan changed from [currentPhysicalPlan] to [newPhysicalPlan]\n</code></pre> <p>Adaptive Query Execution uses spark.sql.adaptive.customCostEvaluatorClass configuration property or defaults to SimpleCostEvaluator.</p>"},{"location":"adaptive-query-execution/#aqe-querystage-physical-preparation-rules","title":"AQE QueryStage Physical Preparation Rules","text":"<p>Adaptive Query Execution uses QueryStage Physical Preparation Rules that can be extended using SparkSessionExtensions.</p>"},{"location":"adaptive-query-execution/#sparklistenersqladaptiveexecutionupdates","title":"SparkListenerSQLAdaptiveExecutionUpdates","text":"<p>Adaptive Query Execution notifies Spark listeners about a physical plan change using <code>SparkListenerSQLAdaptiveExecutionUpdate</code> and <code>SparkListenerSQLAdaptiveSQLMetricUpdates</code> events.</p>"},{"location":"adaptive-query-execution/#logging","title":"Logging","text":"<p>Adaptive Query Execution uses logOnLevel to print out diagnostic messages to the log.</p>"},{"location":"adaptive-query-execution/#demo","title":"Demo","text":"<p>Demo: Adaptive Query Execution</p>"},{"location":"adaptive-query-execution/#unsupported","title":"Unsupported","text":""},{"location":"adaptive-query-execution/#cachemanager","title":"CacheManager","text":"<p>Adaptive Query Execution can change number of shuffle partitions and CacheManager makes sure that this configuration is disabled (for to cacheQuery and recacheByCondition)</p>"},{"location":"adaptive-query-execution/#structured-streaming","title":"Structured Streaming","text":"<p>Adaptive Query Execution can change number of shuffle partitions and so is not supported for streaming queries (Spark Structured Streaming).</p>"},{"location":"adaptive-query-execution/#references","title":"References","text":""},{"location":"adaptive-query-execution/#videos","title":"Videos","text":"<ul> <li>An Adaptive Execution Engine For Apache Spark SQL \u2014 Carson Wang</li> </ul>"},{"location":"adaptive-query-execution/AQEOptimizer/","title":"AQEOptimizer \u2014 AQE Logical Optimizer","text":"<p><code>AQEOptimizer</code> is the logical optimizer for optimizing logical plans in Adaptive Query Execution.</p> <p></p> <p><code>AQEOptimizer</code> is used in AdaptiveSparkPlanExec physical operator.</p> <p><code>AQEOptimizer</code> uses spark.sql.adaptive.optimizer.excludedRules configuration property to exclude logical optimizations from the batches of logical optimization rules.</p> <p><code>AQEOptimizer</code> is a Catalyst RuleExecutor of the logical (optimization) rules (<code>RuleExecutor[LogicalPlan]</code>).</p>"},{"location":"adaptive-query-execution/AQEOptimizer/#creating-instance","title":"Creating Instance","text":"<p><code>AQEOptimizer</code> takes the following to be created:</p> <ul> <li> SQLConf <p><code>AQEOptimizer</code> is created alongside AdaptiveSparkPlanExec physical operator.</p>"},{"location":"adaptive-query-execution/AQEOptimizer/#default-batches","title":"Default Batches <pre><code>defaultBatches: Seq[Batch]\n</code></pre> <p><code>AQEOptimizer</code> creates a collection of batches with logical optimizations (in the order of their execution):</p> <ol> <li>Propagate Empty Relations</li> <li>Dynamic Join Selection</li> <li>Eliminate Limits</li> <li>Optimize One Row Plan</li> </ol> <p><code>defaultBatches</code> is used as the batches.</p>","text":""},{"location":"adaptive-query-execution/AQEOptimizer/#dynamic-join-selection","title":"Dynamic Join Selection","text":"<p>Dynamic Join Selection is a once-executed batch of the following rules:</p> <ul> <li>DynamicJoinSelection</li> </ul>"},{"location":"adaptive-query-execution/AQEOptimizer/#eliminate-limits","title":"Eliminate Limits","text":"<p>Eliminate Limits is a fixed-point batch of the following rules:</p> <ul> <li><code>EliminateLimits</code></li> </ul>"},{"location":"adaptive-query-execution/AQEOptimizer/#optimize-one-row-plan","title":"Optimize One Row Plan","text":"<p>Optimize One Row Plan is a fixed-point batch of the following rules:</p> <ul> <li><code>OptimizeOneRowPlan</code></li> </ul>"},{"location":"adaptive-query-execution/AQEOptimizer/#propagate-empty-relations","title":"Propagate Empty Relations","text":"<p>Propagate Empty Relations is a fixed-point batch of the following rules:</p> <ul> <li>AQEPropagateEmptyRelation</li> <li>ConvertToLocalRelation</li> <li>UpdateAttributeNullability</li> </ul>"},{"location":"adaptive-query-execution/AQEOptimizer/#creating-fixedpoint-batch-execution-strategy","title":"Creating FixedPoint Batch Execution Strategy <pre><code>fixedPoint: FixedPoint\n</code></pre> <p><code>fixedPoint</code> creates a <code>FixedPoint</code> batch execution strategy with the following:</p>    Attribute Value     maxIterations spark.sql.optimizer.maxIterations   maxIterationsSetting <code>spark.sql.optimizer.maxIterations</code>","text":""},{"location":"adaptive-query-execution/AQEOptimizer/#batches","title":"Batches <pre><code>batches: Seq[Batch]\n</code></pre> <p><code>batches</code> is part of the RuleExecutor abstraction.</p>  <p><code>batches</code> returns the default rules excluding the ones specified in the spark.sql.adaptive.optimizer.excludedRules configuration property.</p> <p>For excluded rules, <code>batches</code> prints out the following INFO message to the logs:</p> <pre><code>Optimization rule '[ruleName]' is excluded from the optimizer.\n</code></pre> <p>For batches with all rules excluded, <code>batches</code> prints out the following INFO message to the logs:</p> <pre><code>Optimization batch '[name]' is excluded from the optimizer as all enclosed rules have been excluded.\n</code></pre>","text":""},{"location":"adaptive-query-execution/AQEOptimizer/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.adaptive.AQEOptimizer</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.AQEOptimizer.name = org.apache.spark.sql.execution.adaptive.AQEOptimizer\nlogger.AQEOptimizer.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"adaptive-query-execution/AQEUtils/","title":"AQEUtils","text":""},{"location":"adaptive-query-execution/AQEUtils/#getting-required-distribution","title":"Getting Required Distribution <pre><code>getRequiredDistribution(\n  p: SparkPlan): Option[Distribution]\n</code></pre> <p><code>getRequiredDistribution</code> determines the Distribution for the given SparkPlan (if there are any user-specified repartition hints):</p> <ul> <li> <p>For ShuffleExchangeExec physical operators with HashPartitioning and REPARTITION_BY_COL or REPARTITION_BY_NUM shuffle origins, <code>getRequiredDistribution</code> returns a HashClusteredDistribution</p> </li> <li> <p>For FilterExec, (non-global) SortExec and CollectMetricsExec physical operators, <code>getRequiredDistribution</code> skips them and determines the required distribution using their child operator</p> </li> <li> <p>For ProjectExec physical operators, <code>getRequiredDistribution</code> finds a HashClusteredDistribution using the child</p> </li> <li> <p>For all other operators, <code>getRequiredDistribution</code> returns the UnspecifiedDistribution</p> </li> </ul> <p><code>getRequiredDistribution</code> is used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested for the required distribution</li> </ul>","text":""},{"location":"adaptive-query-execution/AdaptiveExecutionContext/","title":"AdaptiveExecutionContext","text":"<p><code>AdaptiveExecutionContext</code> is the execution context to share the following between the main query and all subqueries:</p> <ul> <li>SparkSession</li> <li>QueryExecution</li> <li>Subquery Cache</li> <li>Stage Cache</li> </ul>"},{"location":"adaptive-query-execution/AdaptiveExecutionContext/#creating-instance","title":"Creating Instance","text":"<p><code>AdaptiveExecutionContext</code> takes the following to be created:</p> <ul> <li> SparkSession <li> QueryExecution <p><code>AdaptiveExecutionContext</code> is created when:</p> <ul> <li><code>QueryExecution</code> is requested for the physical preparations rules (and creates an InsertAdaptiveSparkPlan)</li> </ul>"},{"location":"adaptive-query-execution/AdaptiveExecutionContext/#subquery-cache","title":"Subquery Cache <pre><code>subqueryCache: TrieMap[SparkPlan, BaseSubqueryExec]\n</code></pre> <p><code>AdaptiveExecutionContext</code> creates a <code>TrieMap</code> (Scala) of SparkPlans (canonicalized ExecSubqueryExpressions to be precise) and associated BaseSubqueryExecs.</p>  <p><code>subqueryCache</code> is used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested for the adaptive optimizations (and creates a ReuseAdaptiveSubquery)</li> </ul>","text":""},{"location":"adaptive-query-execution/AdaptiveExecutionContext/#stage-cache","title":"Stage Cache <pre><code>stageCache: TrieMap[SparkPlan, QueryStageExec]\n</code></pre> <p><code>AdaptiveExecutionContext</code> creates a <code>TrieMap</code> (Scala) of SparkPlans (canonicalized Exchanges to be precise) and associated QueryStageExecs.</p>  <p><code>stageCache</code> is used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested to createQueryStages</li> </ul>","text":""},{"location":"adaptive-query-execution/CostEvaluator/","title":"CostEvaluator","text":"<p><code>CostEvaluator</code> is an abstraction of cost evaluators in Adaptive Query Execution.</p> <p><code>CostEvaluator</code> is used in AdaptiveSparkPlanExec physical operator based on spark.sql.adaptive.customCostEvaluatorClass configuration property.</p>"},{"location":"adaptive-query-execution/CostEvaluator/#contract","title":"Contract","text":""},{"location":"adaptive-query-execution/CostEvaluator/#evaluating-cost","title":"Evaluating Cost <pre><code>evaluateCost(\n  plan: SparkPlan): Cost\n</code></pre> <p>Evaluates the cost of the given SparkPlan</p> <p>Used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested for the final physical query plan</li> </ul>","text":""},{"location":"adaptive-query-execution/CostEvaluator/#implementations","title":"Implementations","text":"<ul> <li>SimpleCostEvaluator</li> </ul>"},{"location":"adaptive-query-execution/ShufflePartitionsUtil/","title":"ShufflePartitionsUtil","text":""},{"location":"adaptive-query-execution/ShufflePartitionsUtil/#coalescepartitions","title":"coalescePartitions <pre><code>coalescePartitions(\n  mapOutputStatistics: Seq[Option[MapOutputStatistics]],\n  inputPartitionSpecs: Seq[Option[Seq[ShufflePartitionSpec]]],\n  advisoryTargetSize: Long,\n  minNumPartitions: Int,\n  minPartitionSize: Long): Seq[Seq[ShufflePartitionSpec]]\n</code></pre> <p><code>coalescePartitions</code> does nothing and returns an empty result with empty <code>mapOutputStatistics</code>.</p> <p><code>coalescePartitions</code> calculates the total shuffle bytes (<code>totalPostShuffleInputSize</code>) by suming up the<code>bytesByPartitionId</code> (of a <code>shuffleId</code>) for every<code>MapOutputStatistics</code> (in <code>mapOutputStatistics</code>).</p> <p><code>coalescePartitions</code> calculates the maximum target size (<code>maxTargetSize</code>) to be a ratio of the total shuffle bytes and the given <code>minNumPartitions</code>.</p> <p><code>coalescePartitions</code> determines the target size (<code>targetSize</code>) to be not larger than the given <code>minPartitionSize</code>.</p> <p><code>coalescePartitions</code> prints out the following INFO message to the logs:</p> <pre><code>For shuffle([shuffleIds]), advisory target size: [advisoryTargetSize],\nactual target size [targetSize], minimum partition size: [minPartitionSize]\n</code></pre> <p><code>coalescePartitions</code> coalescePartitionsWithoutSkew with all the given <code>inputPartitionSpecs</code> empty or coalescePartitionsWithSkew.</p>  <p>inputPartitionSpecs</p> <p><code>inputPartitionSpecs</code> can be empty for ShuffleQueryStageExecs and available for AQEShuffleReadExecs.</p>  <p><code>coalescePartitions</code> is used when:</p> <ul> <li>CoalesceShufflePartitions adaptive physical optimization is executed</li> </ul>","text":""},{"location":"adaptive-query-execution/ShufflePartitionsUtil/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"adaptive-query-execution/SimpleCostEvaluator/","title":"SimpleCostEvaluator","text":"<p><code>SimpleCostEvaluator</code> is a CostEvaluator in Adaptive Query Execution.</p>"},{"location":"adaptive-query-execution/SimpleCostEvaluator/#evaluating-cost","title":"Evaluating Cost <pre><code>evaluateCost(\n  plan: SparkPlan): Cost\n</code></pre> <p><code>evaluateCost</code>\u00a0is part of the CostEvaluator abstraction.</p>  <p><code>evaluateCost</code> counts the shuffle exchanges unary physical operators in the given SparkPlan.</p>","text":""},{"location":"avro/","title":"Avro Connector","text":"<p>Apache Avro is a data serialization format and provides the following features:</p> <ul> <li>Language-independent (with language bindings for popular programming languages, e.g. Java, Python)</li> <li>Rich data structures</li> <li>A compact, fast, binary data format (encoding)</li> <li>A container file for sequences of Avro data (aka Avro data files)</li> <li>Remote procedure call (RPC)</li> <li>Optional code generation (optimization) to read or write data files, and implement RPC protocols</li> </ul>"},{"location":"avro/AvroDataToCatalyst/","title":"AvroDataToCatalyst Unary Expression","text":"<p><code>AvroDataToCatalyst</code> is a &lt;&gt; that represents from_avro function in a structured query. <p>[[creating-instance]] <code>AvroDataToCatalyst</code> takes the following when created:</p> <ul> <li>[[child]] &lt;&gt; <li>[[jsonFormatSchema]] JSON-encoded Avro schema</li> <p><code>AvroDataToCatalyst</code> &lt;&gt;. <p>=== [[doGenCode]] Generating Java Source Code (ExprCode) For Code-Generated Expression Evaluation -- <code>doGenCode</code> Method</p>"},{"location":"avro/AvroDataToCatalyst/#source-scala","title":"[source, scala]","text":""},{"location":"avro/AvroDataToCatalyst/#dogencodectx-codegencontext-ev-exprcode-exprcode","title":"doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode","text":"<p>NOTE: <code>doGenCode</code> is part of &lt;&gt; to generate a Java source code (<code>ExprCode</code>) for code-generated expression evaluation. <p><code>doGenCode</code> requests the <code>CodegenContext</code> to generate code to reference this AvroDataToCatalyst instance.</p> <p>In the end, <code>doGenCode</code> &lt;&gt; with the function <code>f</code> that uses &lt;&gt;. <p>=== [[nullSafeEval]] <code>nullSafeEval</code> Method</p>"},{"location":"avro/AvroDataToCatalyst/#source-scala_1","title":"[source, scala]","text":""},{"location":"avro/AvroDataToCatalyst/#nullsafeevalinput-any-any","title":"nullSafeEval(input: Any): Any","text":"<p>NOTE: <code>nullSafeEval</code> is part of the &lt;&gt; to...FIXME. <p><code>nullSafeEval</code>...FIXME</p>"},{"location":"avro/AvroFileFormat/","title":"AvroFileFormat","text":"<p><code>AvroFileFormat</code> is a FileFormat for Apache Avro, i.e. a data source format that can read and write Avro-encoded data in files.</p>"},{"location":"avro/AvroOptions/","title":"AvroOptions","text":"<p><code>AvroOptions</code> represents the &lt;&gt; of the Avro data source. <p>[[options]] .Options for Avro Data Source [cols=\"1m,1,2\",options=\"header\",width=\"100%\"] |=== | Option / Key | Default Value | Description</p> <p>| avroSchema | (undefined) | [[avroSchema]] Avro schema in JSON format</p> <p>| compression | (undefined) a| [[compression]] Specifies the compression codec to use when writing Avro data to disk</p> <p>Note</p> <p>If the option is not defined explicitly, Avro data source uses spark.sql.avro.compression.codec configuration property.</p> <p>| ignoreExtension | <code>false</code> a| [[ignoreExtension]] Controls whether Avro data source should read all Avro files regardless of their extension (<code>true</code>) or not (<code>false</code>)</p> <p>By default, Avro data source reads only files with <code>.avro</code> file extension.</p> <p>NOTE: If the option is not defined explicitly, Avro data source uses <code>avro.mapred.ignore.inputs.without.extension</code> Hadoop runtime property.</p> <p>| recordName | <code>topLevelRecord</code> | [[recordName]] Top-level record name when writing Avro data to disk</p> <p>Consult https://avro.apache.org/docs/1.8.2/spec.html#schema_record[Apache Avro\u2122 1.8.2 Specification]</p> <p>| recordNamespace | (empty) | [[recordNamespace]] Record namespace when writing Avro data to disk</p> <p>Consult https://avro.apache.org/docs/1.8.2/spec.html#schema_record[Apache Avro\u2122 1.8.2 Specification] |===</p> <p>NOTE: The &lt;&gt; are case-insensitive."},{"location":"avro/AvroOptions/#creating-instance","title":"Creating Instance","text":"<p><code>AvroOptions</code> takes the following when created:</p> <ul> <li>[[parameters]] Case-insensitive configuration parameters (i.e. <code>Map[String, String]</code>)</li> <li>[[conf]] Hadoop https://hadoop.apache.org/docs/r3.1.1/api/org/apache/hadoop/conf/Configuration.html[Configuration]</li> </ul> <p><code>AvroOptions</code> is created when <code>AvroFileFormat</code> is requested to inferSchema, prepareWrite and buildReader.</p>"},{"location":"avro/CatalystDataToAvro/","title":"CatalystDataToAvro Unary Expression","text":"<p><code>CatalystDataToAvro</code> is a &lt;&gt; that represents to_avro function in a structured query. <p>[[creating-instance]] [[child]] <code>CatalystDataToAvro</code> takes a single &lt;&gt; when created. <p><code>CatalystDataToAvro</code> &lt;&gt;. <pre><code>import org.apache.spark.sql.avro.CatalystDataToAvro\nval catalystDataToAvro = CatalystDataToAvro($\"id\".expr)\n\nimport org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, ExprCode}\nval ctx = new CodegenContext\n// doGenCode is used when Expression.genCode is executed\n// FIXME The following won't work due to https://issues.apache.org/jira/browse/SPARK-26063\nval ExprCode(code, _, _) = catalystDataToAvro.genCode(ctx)\n\n// Helper methods\ndef trim(code: String): String = {\n  code.trim.split(\"\\n\").map(_.trim).filter(line =&gt; line.nonEmpty).mkString(\"\\n\")\n}\ndef prettyPrint(code: String) = println(trim(code))\n// END: Helper methods\n\nscala&gt; println(trim(code))\n// FIXME: Finish me once https://issues.apache.org/jira/browse/SPARK-26063 is fixed\n// See the following example\n</code></pre> <pre><code>// Let's use a workaround to create a CatalystDataToAvro expression\n// with the child resolved\nval q = spark.range(1).withColumn(\"to_avro_id\", to_avro('id))\nimport org.apache.spark.sql.avro.CatalystDataToAvro\nval analyzedPlan = q.queryExecution.analyzed\nval catalystDataToAvro = analyzedPlan.expressions.drop(1).head.children.head.asInstanceOf[CatalystDataToAvro]\n\nimport org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, ExprCode}\nval ctx = new CodegenContext\nval ExprCode(code, _, _) = catalystDataToAvro.genCode(ctx)\n\n// Doh! It does not work either\n// java.lang.UnsupportedOperationException: Cannot evaluate expression: id#38L\n\n// Let's try something else (more serious)\n\nimport org.apache.spark.sql.catalyst.expressions.{BindReferences, Expression}\nval boundExprs = analyzedPlan.expressions.map { e =&gt;\n  BindReferences.bindReference[Expression](e, analyzedPlan.children.head.output)\n}\n// That should trigger doGenCode\nval codes = ctx.generateExpressions(boundExprs)\n\n// The following corresponds to catalystDataToAvro.genCode(ctx)\nval ExprCode(code, _, _) = codes.tail.head\n\n// Helper methods\ndef trim(code: String): String = {\n  code.trim.split(\"\\n\").map(_.trim).filter(line =&gt; line.nonEmpty).mkString(\"\\n\")\n}\ndef prettyPrint(code: String) = println(trim(code))\n// END: Helper methods\n\nscala&gt; println(trim(code.toString))\nlong value_7 = i.getLong(0);\nbyte[] value_6 = null;\nvalue_6 = (byte[]) ((org.apache.spark.sql.avro.CatalystDataToAvro) references[2] /* this */).nullSafeEval(value_7);\n</code></pre> <p>=== [[doGenCode]] Generating Java Source Code (ExprCode) For Code-Generated Expression Evaluation -- <code>doGenCode</code> Method</p>"},{"location":"avro/CatalystDataToAvro/#source-scala","title":"[source, scala]","text":""},{"location":"avro/CatalystDataToAvro/#dogencodectx-codegencontext-ev-exprcode-exprcode","title":"doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode","text":"<p>NOTE: <code>doGenCode</code> is part of &lt;&gt; to generate a Java source code (<code>ExprCode</code>) for code-generated expression evaluation. <p><code>doGenCode</code> requests the <code>CodegenContext</code> to generate code to reference this CatalystDataToAvro instance.</p> <p>In the end, <code>doGenCode</code> &lt;&gt; with the function <code>f</code> that uses &lt;&gt;. <p>=== [[nullSafeEval]] <code>nullSafeEval</code> Method</p>"},{"location":"avro/CatalystDataToAvro/#source-scala_1","title":"[source, scala]","text":""},{"location":"avro/CatalystDataToAvro/#nullsafeevalinput-any-any","title":"nullSafeEval(input: Any): Any","text":"<p>NOTE: <code>nullSafeEval</code> is part of the &lt;&gt; to...FIXME. <p><code>nullSafeEval</code>...FIXME</p>"},{"location":"basic-aggregation/","title":"Basic Aggregation","text":"<p>Basic Aggregation calculates aggregates over a group of rows in a Dataset using aggregate operators (possibly with aggregate functions).</p>"},{"location":"basic-aggregation/#aggregate-operators","title":"Aggregate Operators","text":""},{"location":"basic-aggregation/#agg","title":"agg <p>Aggregates over (applies an aggregate function on) a subset of or the entire <code>Dataset</code> (i.e., considering the entire data set as one group)</p> <p>Creates a RelationalGroupedDataset</p>  <p>Note</p> <p><code>Dataset.agg</code> is simply a shortcut for <code>Dataset.groupBy().agg</code>.</p>","text":""},{"location":"basic-aggregation/#groupby","title":"groupBy <p>Groups the rows in a <code>Dataset</code> by columns (as Column expressions or names).</p> <p>Creates a RelationalGroupedDataset</p> <p>Used for untyped aggregates using <code>DataFrame</code>s. Grouping is described using column expressions or column names.</p> <p>Internally, <code>groupBy</code> resolves column names and creates a RelationalGroupedDataset (with groupType as <code>GroupByType</code>).</p>","text":""},{"location":"basic-aggregation/#groupbykey","title":"groupByKey <p>Groups records (of type <code>T</code>) by the input <code>func</code> and creates a KeyValueGroupedDataset to apply aggregation to.</p> <p>Used for typed aggregates using <code>Dataset</code>s with records grouped by a key-defining discriminator function</p> <pre><code>import org.apache.spark.sql.expressions.scalalang._\nval q = dataset\n  .groupByKey(_.productId).\n  .agg(typed.sum[Token](_.score))\n  .toDF(\"productId\", \"sum\")\n  .orderBy('productId)\n</code></pre> <pre><code>spark\n  .readStream\n  .format(\"rate\")\n  .load\n  .as[(Timestamp, Long)]\n  .groupByKey { case (ts, v) =&gt; v % 2 }\n  .agg()\n  .writeStream\n  .format(\"console\")\n  .trigger(Trigger.ProcessingTime(5.seconds))\n  .outputMode(\"complete\")\n  .start\n</code></pre>","text":""},{"location":"basic-aggregation/KeyValueGroupedDataset/","title":"KeyValueGroupedDataset","text":"<p><code>KeyValueGroupedDataset</code> is an interface for Typed Grouping to calculate aggregates over groups of objects in a typed Dataset.</p> <p><code>KeyValueGroupedDataset</code> represents a grouped dataset as a result of Dataset.groupByKey operator (that aggregates records by a grouping function).</p> <pre><code>// Dataset[T]\ngroupByKey(\n  func: T =&gt; K): KeyValueGroupedDataset[K, T]\n</code></pre> <p><code>KeyValueGroupedDataset</code> works for batch and streaming aggregations.</p> RelationalGroupedDataset <p>RelationalGroupedDataset is used for untyped <code>Row</code>-based aggregates.</p>"},{"location":"basic-aggregation/KeyValueGroupedDataset/#creating-instance","title":"Creating Instance","text":"<p><code>KeyValueGroupedDataset</code> takes the following to be created:</p> <ul> <li> Key Encoder <li> Value Encoder <li> QueryExecution <li> Data Attributes <li> Grouping Attributes <p><code>KeyValueGroupedDataset</code> is created for the following high-level operators:</p> <ul> <li>Dataset.groupByKey</li> <li>KeyValueGroupedDataset.keyAs</li> <li>KeyValueGroupedDataset.mapValues</li> <li>RelationalGroupedDataset.as</li> </ul>"},{"location":"basic-aggregation/KeyValueGroupedDataset/#flatmapgroupswithstate","title":"flatMapGroupsWithState <pre><code>flatMapGroupsWithState[S: Encoder, U: Encoder](\n  outputMode: OutputMode,\n  timeoutConf: GroupStateTimeout)(\n  func: (K, Iterator[V], GroupState[S]) =&gt; Iterator[U]): Dataset[U]\nflatMapGroupsWithState[S: Encoder, U: Encoder](\n  outputMode: OutputMode,\n  timeoutConf: GroupStateTimeout,\n  initialState: KeyValueGroupedDataset[K, S])(\n  func: (K, Iterator[V], GroupState[S]) =&gt; Iterator[U]): Dataset[U]\n</code></pre> <p><code>flatMapGroupsWithState</code> creates a Dataset with a FlatMapGroupsWithState logical operator (with the isMapGroupsWithState disabled).</p> <p><code>flatMapGroupsWithState</code> accepts <code>Append</code> and <code>Update</code> output modes only, and throws an <code>IllegalArgumentException</code> for the others:</p> <pre><code>The output mode of function should be append or update\n</code></pre>","text":""},{"location":"basic-aggregation/KeyValueGroupedDataset/#spark-structured-streaming","title":"Spark Structured Streaming <p><code>KeyValueGroupedDataset</code> can be used in streaming queries in Spark Structured Streaming:</p> <ul> <li>Streaming Aggregation</li> <li>Arbitrary Stateful Streaming Aggregation</li> </ul>","text":""},{"location":"basic-aggregation/KeyValueGroupedDataset/#demo","title":"Demo <pre><code>import java.sql.Timestamp\nval numGroups = spark.\n  readStream.\n  format(\"rate\").\n  load.\n  as[(Timestamp, Long)].\n  groupByKey { case (time, value) =&gt; value % 2 }\n\nscala&gt; :type numGroups\norg.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)]\n</code></pre>","text":""},{"location":"basic-aggregation/RelationalGroupedDataset/","title":"RelationalGroupedDataset","text":"<p><code>RelationalGroupedDataset</code> is an interface for Untyped (Row-based) Grouping to calculate aggregates over groups of rows in a DataFrame.</p> <p>Note</p> <p>KeyValueGroupedDataset is used for typed aggregates over groups of custom Scala objects (not Rows).</p> <p><code>RelationalGroupedDataset</code> is a result of executing the following grouping operators:</p> <ul> <li>groupBy</li> <li>rollup</li> <li>cube</li> <li>pivot</li> </ul> <p>[[operators]] .RelationalGroupedDataset's Aggregate Operators [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Operator | Description</p> <p>| &lt;&gt; a| <p>| <code>avg</code> a| [[avg]]</p> <p>| <code>count</code> a| [[count]]</p> <p>| <code>max</code> a| [[max]]</p> <p>| <code>mean</code> a| [[mean]]</p> <p>| <code>min</code> a| [[min]]</p> <p>| &lt;&gt; a| [[pivot]]"},{"location":"basic-aggregation/RelationalGroupedDataset/#source-scala","title":"[source, scala]","text":"<p>pivot(pivotColumn: String): RelationalGroupedDataset pivot(pivotColumn: String, values: Seq[Any]): RelationalGroupedDataset pivot(pivotColumn: Column): RelationalGroupedDataset // &lt;1&gt; pivot(pivotColumn: Column, values: Seq[Any]): RelationalGroupedDataset // &lt;1&gt;</p> <p>&lt;1&gt; New in 2.4.0</p> <p>Pivots on a column (with new columns per distinct value)</p> <p>| <code>sum</code> a| [[sum]] |===</p> <p>Note</p> <p>spark.sql.retainGroupColumns configuration property controls whether to retain columns used for aggregation or not (../in <code>RelationalGroupedDataset</code> operators).</p> <p>=== [[agg]] Computing Aggregates Using Aggregate Column Expressions or Function Names -- <code>agg</code> Operator</p> <p><code>agg</code> creates a DataFrame with the rows being the result of executing grouping expressions (../specified using Columns or names) over row groups.</p> <p>Note</p> <p>There are untyped and typed column expressions.</p> <pre><code>val countsAndSums = spark\n.range(10)\n.withColumn(\"group\", 'id % 2)\n.groupBy(\"group\")\n.agg(count(\"id\") as \"count\", sum(\"id\") as \"sum\")\n</code></pre> <p>Internally, <code>agg</code> creates a DataFrame with <code>Aggregate</code> or <code>Pivot</code> logical operators.</p> <pre><code>// groupBy above\nscala&gt; println(countsAndSums.queryExecution.logical.numberedTreeString)\n00 'Aggregate [group#179L], [group#179L, count('id) AS count#188, sum('id) AS sum#190]\n01 +- Project [id#176L, (id#176L % cast(2 as bigint)) AS group#179L]\n02    +- Range (0, 10, step=1, splits=Some(8))\n\n// rollup operator\nval rollupQ = spark.range(2).rollup('id).agg(count('id))\nscala&gt; println(rollupQ.queryExecution.logical.numberedTreeString)\n00 'Aggregate [rollup('id)], [unresolvedalias('id, None), count('id) AS count(id)#267]\n01 +- Range (0, 2, step=1, splits=Some(8))\n\n// cube operator\nval cubeQ = spark.range(2).cube('id).agg(count('id))\nscala&gt; println(cubeQ.queryExecution.logical.numberedTreeString)\n00 'Aggregate [cube('id)], [unresolvedalias('id, None), count('id) AS count(id)#280]\n01 +- Range (0, 2, step=1, splits=Some(8))\n\n// pivot operator\nval pivotQ = spark.\n  range(10).\n  withColumn(\"group\", 'id % 2).\n  groupBy(\"group\").\n  pivot(\"group\").\n  agg(count(\"id\"))\nscala&gt; println(pivotQ.queryExecution.logical.numberedTreeString)\n00 'Pivot [group#296L], group#296: bigint, [0, 1], [count('id)]\n01 +- Project [id#293L, (id#293L % cast(2 as bigint)) AS group#296L]\n02    +- Range (0, 10, step=1, splits=Some(8))\n</code></pre> <p>=== [[creating-instance]] Creating RelationalGroupedDataset Instance</p> <p><code>RelationalGroupedDataset</code> takes the following when created:</p> <ul> <li>[[df]] DataFrame</li> <li>[[groupingExprs]] Grouping expressions/Expression.md[expressions]</li> <li>[[groupType]] Group type (to indicate the \"source\" operator)</li> </ul> <p>** <code>GroupByType</code> for groupBy</p> <p>** <code>CubeType</code></p> <p>** <code>RollupType</code></p> <p>** <code>PivotType</code></p> <p>=== [[pivot-internals]] <code>pivot</code> Operator</p>"},{"location":"basic-aggregation/RelationalGroupedDataset/#source-scala_1","title":"[source, scala]","text":"<p>pivot(pivotColumn: String): RelationalGroupedDataset // &lt;1&gt; pivot(pivotColumn: String, values: Seq[Any]): RelationalGroupedDataset // &lt;2&gt; pivot(pivotColumn: Column): RelationalGroupedDataset // &lt;3&gt; pivot(pivotColumn: Column, values: Seq[Any]): RelationalGroupedDataset // &lt;3&gt;</p> <p>&lt;1&gt; Selects distinct and sorted values on <code>pivotColumn</code> and calls the other <code>pivot</code> (that results in 3 extra \"scanning\" jobs) &lt;2&gt; Preferred as more efficient because the unique values are aleady provided &lt;3&gt; New in 2.4.0</p> <p><code>pivot</code> pivots on a <code>pivotColumn</code> column, i.e. adds new columns per distinct values in <code>pivotColumn</code>.</p> <p>NOTE: <code>pivot</code> is only supported after groupBy operation.</p> <p>NOTE: Only one <code>pivot</code> operation is supported on a <code>RelationalGroupedDataset</code>.</p> <pre><code>val visits = Seq(../\n  (../0, \"Warsaw\", 2015),\n  (../1, \"Warsaw\", 2016),\n  (../2, \"Boston\", 2017)\n).toDF(../\"id\", \"city\", \"year\")\n\nval q = visits\n  .groupBy(../\"city\")  // &lt;-- rows in pivot table\n  .pivot(../\"year\")    // &lt;-- columns (../unique values queried)\n  .count(../)          // &lt;-- values in cells\nscala&gt; q.show\n+------+----+----+----+\n|  city|2015|2016|2017|\n+------+----+----+----+\n|Warsaw|   1|   1|null|\n|Boston|null|null|   1|\n+------+----+----+----+\n\nscala&gt; q.explain\n== Physical Plan ==\nHashAggregate(../keys=[city#8], functions=[pivotfirst(../year#9, count(../1) AS `count`#222L, 2015, 2016, 2017, 0, 0)])\n+- Exchange hashpartitioning(../city#8, 200)\n   +- HashAggregate(../keys=[city#8], functions=[partial_pivotfirst(../year#9, count(../1) AS `count`#222L, 2015, 2016, 2017, 0, 0)])\n      +- *HashAggregate(../keys=[city#8, year#9], functions=[count(../1)])\n         +- Exchange hashpartitioning(../city#8, year#9, 200)\n            +- *HashAggregate(../keys=[city#8, year#9], functions=[partial_count(../1)])\n               +- LocalTableScan [city#8, year#9]\n\nscala&gt; visits\n  .groupBy(../'city)\n  .pivot(../\"year\", Seq(../\"2015\")) // &lt;-- one column in pivot table\n  .count\n  .show\n+------+----+\n|  city|2015|\n+------+----+\n|Warsaw|   1|\n|Boston|null|\n+------+----+\n</code></pre> <p>IMPORTANT: Use <code>pivot</code> with a list of distinct values to pivot on so Spark does not have to compute the list itself (and run three extra \"scanning\" jobs).</p> <p></p> <p></p> <p>Note</p> <p>spark.sql.pivotMaxValues controls the maximum number of (distinct) values that will be collected without error (when doing <code>pivot</code> without specifying the values for the pivot column).</p> <p>Internally, <code>pivot</code> creates a <code>RelationalGroupedDataset</code> with <code>PivotType</code> group type and <code>pivotColumn</code> resolved using the DataFrame's columns with <code>values</code> as <code>Literal</code> expressions.</p> <p>Note</p> <p>&lt;&gt; internal method maps <code>PivotType</code> group type to a <code>DataFrame</code> with Pivot.md[Pivot] unary logical operator. <pre><code>scala&gt; q.queryExecution.logical\nres0: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =\nPivot [city#8], year#9: int, [2015, 2016, 2017], [count(../1) AS count#24L]\n+- Project [_1#3 AS id#7, _2#4 AS city#8, _3#5 AS year#9]\n  +- LocalRelation [_1#3, _2#4, _3#5]\n</code></pre>"},{"location":"bucketing/","title":"Bucketing","text":"<p>Bucketing is an optimization technique that uses buckets (and bucketing columns) to determine data partitioning and avoid data shuffle in join queries.</p> <p>The motivation is to optimize performance of a join query by avoiding shuffles (exchanges) of tables participating in the join. Bucketing results in fewer exchanges (and so stages).</p> <p>Note</p> <p>Bucketing can show the biggest benefit when pre-shuffled bucketed tables are used more than once as bucketing itself takes time (that you will offset executing multiple join queries later).</p> <p>Bucketing is supported for permanent tables in the following catalogs (metastores):</p> <ul> <li>HiveExternalCatalog</li> <li>TransformHelper</li> </ul> <p>Bucketing is configured using spark.sql.sources.bucketing.enabled configuration property.</p> <pre><code>assert(spark.sessionState.conf.bucketingEnabled, \"Bucketing disabled?!\")\n</code></pre> <p>Bucketing is used exclusively in FileSourceScanExec physical operator (when requested for the input RDD and to determine the partitioning and ordering of the output).</p>"},{"location":"bucketing/#delta-lake","title":"Delta Lake","text":"<p>Bucketing is not supported by Delta Lake (so it's not really true to say that \"all file-based data sources are supported, unfortunatelly).</p>"},{"location":"bucketing/#create-bucketed-tables","title":"Create Bucketed Tables","text":"<p>Bucketed tables can be created using the following higher-level operators:</p> <ul> <li>AstBuilder</li> <li>CreateTableAsSelectExec</li> <li><code>CreateTableExec</code></li> <li>DataFrameWriter</li> <li><code>ReplaceTableAsSelectExec</code></li> <li><code>ReplaceTableExec</code></li> </ul>"},{"location":"bucketing/#show-create-table","title":"SHOW CREATE TABLE","text":"<p>SHOW CREATE TABLE SQL statement is used to display bucketing specification of a table.</p>"},{"location":"bucketing/#demo-sortmergejoin-of-two-filescans","title":"Demo: SortMergeJoin of Two FileScans","text":"<pre><code>import org.apache.spark.sql.SaveMode\nspark.range(10e4.toLong).write.mode(SaveMode.Overwrite).saveAsTable(\"t10e4\")\nspark.range(10e6.toLong).write.mode(SaveMode.Overwrite).saveAsTable(\"t10e6\")\n</code></pre> <p>Bucketing is enabled by default. Let's check it out, anyway.</p> <pre><code>assert(spark.sessionState.conf.bucketingEnabled, \"Bucketing disabled?!\")\n</code></pre> <p>Make sure that you don't end up with a BroadcastHashJoinExec and a BroadcastExchangeExec. Disable auto broadcasting.</p> <pre><code>spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n</code></pre> <pre><code>val tables = spark.catalog.listTables.where($\"name\" startsWith \"t10e\")\n</code></pre> <pre><code>scala&gt; tables.show\n+-----+--------+-----------+---------+-----------+\n| name|database|description|tableType|isTemporary|\n+-----+--------+-----------+---------+-----------+\n|t10e4| default|       null|  MANAGED|      false|\n|t10e6| default|       null|  MANAGED|      false|\n+-----+--------+-----------+---------+-----------+\n</code></pre> <pre><code>val t4 = spark.table(\"t10e4\")\nval t6 = spark.table(\"t10e6\")\n\nassert(t4.count == 10e4)\nassert(t6.count == 10e6)\n\n// trigger execution of the join query\nt4.join(t6, \"id\").foreach(_ =&gt; ())\n</code></pre> <p>The above join query is a fine example of a SortMergeJoinExec (SortMergeJoin) of two FileSourceScanExecs (the two Scans at the top). The join query uses ShuffleExchangeExec physical operators (Exchange) to shuffle the tables for the <code>SortMergeJoin</code>.</p> <p></p>"},{"location":"bucketing/#creating-bucketed-tables","title":"Creating Bucketed Tables","text":"<p>One way to avoid the exchanges (and so optimize the join query) is to use table bucketing that is applicable for all file-based data sources (e.g. Parquet, ORC, JSON, CSV) that are saved as a table using DataFrameWrite.saveAsTable or simply available in a catalog by SparkSession.table.</p> <p>Note</p> <p>Bucketing is not supported for DataFrameWriter.save, DataFrameWriter.insertInto and DataFrameWriter.jdbc methods.</p> <p>DataFrameWriter.bucketBy method is used to specify the number of buckets and the bucketing columns.</p> <p>You can optionally sort the output rows in buckets using DataFrameWriter.sortBy method.</p> <pre><code>val large = spark.range(10e6.toLong)\nimport org.apache.spark.sql.SaveMode\nlarge.write\n.bucketBy(4, \"id\")\n.sortBy(\"id\")\n.mode(SaveMode.Overwrite)\n.saveAsTable(\"bucketed_4_id\")\n</code></pre> <p>Note</p> <p>DataFrameWriter.bucketBy and DataFrameWriter.sortBy are used to define a bucketing specification.</p> <p>Unlike bucketing in Apache Hive, Spark SQL creates the bucket files per the number of buckets and partitions. In other words, the number of bucketing files is the number of buckets multiplied by the number of task writers (one per partition).</p> <pre><code>// the number of partitions depends on the cluster manager\n// In local[*], the default parallelism is the number of CPU cores\n// I've got a 16-core laptop\nscala&gt; println(large.queryExecution.toRdd.getNumPartitions)\n16\n</code></pre> <pre><code>// That gives 16 (partitions/task writers) x 4 (buckets) = 64 files\n// With _SUCCESS extra file and the ls -l header \"total 794624\" that gives 66 files\n$ ls -tlr spark-warehouse/bucketed_4_id | wc -l\n      66\n</code></pre> <p>With bucketing, the <code>Exchange</code>s will no longer be necessary (as the tables are already pre-shuffled). Let's create bucketed tables (using <code>DataFrameWriter.bucketBy</code>).</p> <pre><code>import org.apache.spark.sql.SaveMode\nspark.range(10e4.toLong)\n.write\n.bucketBy(4, \"id\")\n.sortBy(\"id\")\n.mode(SaveMode.Overwrite)\n.saveAsTable(\"bucketed_4_10e4\")\nspark.range(10e6.toLong)\n.write\n.bucketBy(4, \"id\")\n.sortBy(\"id\")\n.mode(SaveMode.Overwrite)\n.saveAsTable(\"bucketed_4_10e6\")\n</code></pre> <pre><code>val bucketed_4_10e4 = spark.table(\"bucketed_4_10e4\")\nval bucketed_4_10e6 = spark.table(\"bucketed_4_10e6\")\n</code></pre> <pre><code>// trigger execution of the join query\nbucketed_4_10e4.join(bucketed_4_10e6, \"id\").foreach(_ =&gt; ())\n</code></pre> <p>The above join query of the bucketed tables shows no ShuffleExchangeExec physical operators (Exchanges) as the shuffling has already been executed (before the query was run).</p> <p></p> <p>The number of partitions of a bucketed table is exactly the number of buckets.</p> <pre><code>val bucketed_4_10e4 = spark.table(\"bucketed_4_10e4\")\nval numPartitions = bucketed_4_10e4.queryExecution.toRdd.getNumPartitions\nassert(numPartitions == 16)\n</code></pre>"},{"location":"bucketing/#describe-extended","title":"DESCRIBE EXTENDED","text":"<p>Use SessionCatalog or <code>DESCRIBE EXTENDED</code> SQL command to find the bucketing information.</p> <pre><code>val bucketed_tables = spark\n.catalog\n.listTables\n.where($\"name\" startsWith \"bucketed_\")\n</code></pre> <pre><code>scala&gt; bucketed_tables.show\n+---------------+--------+-----------+---------+-----------+\n|           name|database|description|tableType|isTemporary|\n+---------------+--------+-----------+---------+-----------+\n|bucketed_4_10e4| default|       null|  MANAGED|      false|\n|bucketed_4_10e6| default|       null|  MANAGED|      false|\n|  bucketed_4_id| default|       null|  MANAGED|      false|\n+---------------+--------+-----------+---------+-----------+\n</code></pre> <pre><code>val demoTable = \"bucketed_4_10e4\"\n</code></pre> <pre><code>// DESC EXTENDED or DESC FORMATTED would also work\nval describeSQL = sql(s\"DESCRIBE EXTENDED $demoTable\")\nscala&gt; describeSQL.show(numRows = Integer.MAX_VALUE, truncate = false)\n+----------------------------+---------------------------------------------------------------+-------+\n|col_name                    |data_type                                                      |comment|\n+----------------------------+---------------------------------------------------------------+-------+\n|id                          |bigint                                                         |null   |\n|                            |                                                               |       |\n|# Detailed Table Information|                                                               |       |\n|Database                    |default                                                        |       |\n|Table                       |bucketed_4_10e4                                                |       |\n|Owner                       |jacek                                                          |       |\n|Created Time                |Wed Feb 09 21:29:12 CET 2022                                   |       |\n|Last Access                 |UNKNOWN                                                        |       |\n|Created By                  |Spark 3.2.1                                                    |       |\n|Type                        |MANAGED                                                        |       |\n|Provider                    |parquet                                                        |       |\n|Num Buckets                 |4                                                              |       |\n|Bucket Columns              |[`id`]                                                         |       |\n|Sort Columns                |[`id`]                                                         |       |\n|Statistics                  |432802 bytes                                                   |       |\n|Location                    |file:/Users/jacek/dev/oss/spark/spark-warehouse/bucketed_4_10e4|       |\n|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe    |       |\n|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat  |       |\n|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat |       |\n+----------------------------+---------------------------------------------------------------+-------+\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.TableIdentifier\nval metadata = spark\n.sessionState\n.catalog\n.getTableMetadata(TableIdentifier(demoTable))\n</code></pre> <pre><code>scala&gt; metadata.bucketSpec.foreach(println)\n4 buckets, bucket columns: [id], sort columns: [id]\n</code></pre> <p>The number of buckets has to be between <code>0</code> and <code>100000</code> exclusive or Spark SQL throws an <code>AnalysisException</code>:</p> <pre><code>Number of buckets should be greater than 0 but less than 100000. Got `[numBuckets]`\n</code></pre> <p>There are however requirements that have to be met before SparkOptimizer gives a no-Exchange query plan:</p> <ol> <li>The number of partitions on both sides of a join has to be exactly the same</li> <li>Both join operators have to use HashPartitioning partitioning scheme</li> </ol> <p>It is acceptable to use bucketing for one side of a join.</p> <pre><code>// Make sure that you don't end up with a BroadcastHashJoin and a BroadcastExchange\n// For this, let's disable auto broadcasting\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n\nval bucketedTableName = \"bucketed_4_id\"\nval large = spark.range(10e5.toLong)\nimport org.apache.spark.sql.SaveMode\nlarge.write\n.bucketBy(4, \"id\")\n.sortBy(\"id\")\n.mode(SaveMode.Overwrite)\n.saveAsTable(bucketedTableName)\nval bucketedTable = spark.table(bucketedTableName)\n\nval t1 = spark\n.range(4)\n.repartition(4, $\"id\")  // Make sure that the number of partitions matches the other side\n\nval q = t1.join(bucketedTable, \"id\")\n</code></pre> <pre><code>scala&gt; q.explain\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [id#151L]\n   +- SortMergeJoin [id#151L], [id#149L], Inner\n      :- Sort [id#151L ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(id#151L, 4), REPARTITION_BY_NUM, [id=#385]\n      :     +- Range (0, 4, step=1, splits=16)\n      +- Sort [id#149L ASC NULLS FIRST], false, 0\n         +- Filter isnotnull(id#149L)\n            +- FileScan parquet default.bucketed_4_id[id#149L] Batched: true, DataFilters: [isnotnull(id#149L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/jacek/dev/oss/spark/spark-warehouse/bucketed_4_id], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct&lt;id:bigint&gt;, SelectedBucketsCount: 4 out of 4\n</code></pre> <pre><code>q.foreach(_ =&gt; ())\n</code></pre> <p></p>"},{"location":"bucketing/#bucket-pruning","title":"Bucket Pruning","text":"<p>As of Spark 2.4.0, Spark SQL supports bucket pruning to optimize filtering on a bucketed column (by reducing the number of bucket files to scan).</p> <p>Bucket pruning supports the following predicate expressions:</p> <ul> <li>EqualTo</li> <li>EqualNullSafe</li> <li>In</li> <li>InSet</li> <li><code>And</code> and <code>Or</code> of the above</li> </ul> <p>FileSourceStrategy execution planning strategy is responsible for selecting only LogicalRelations over HadoopFsRelation with the bucketing specification with the following:</p> <ol> <li>There is exactly one bucketing column</li> <li>The number of buckets is greater than 1</li> </ol>"},{"location":"bucketing/#demo","title":"Demo","text":"<p>Enable <code>INFO</code> logging level of FileSourceStrategy logger to look under the covers.</p> <pre><code>import org.apache.spark.sql.execution.datasources.FileSourceStrategy\nval logger = FileSourceStrategy.getClass.getName.replace(\"$\", \"\")\nimport org.apache.log4j.{Level, Logger}\nLogger.getLogger(logger).setLevel(Level.INFO)\n</code></pre> <pre><code>val q57 = q.where($\"id\" isin (50, 70))\nscala&gt; val sparkPlan57 = q57.queryExecution.executedPlan\n22/02/09 21:35:06 INFO FileSourceStrategy: Pruned 2 out of 4 buckets.\n22/02/09 21:35:06 INFO FileSourceStrategy: Pushed Filters: In(id, [50,70]),IsNotNull(id)\n22/02/09 21:35:06 INFO FileSourceStrategy: Post-Scan Filters: id#149L IN (50,70),isnotnull(id#149L)\n22/02/09 21:35:06 INFO FileSourceStrategy: Output Data Schema: struct&lt;id: bigint&gt;\n...\n</code></pre> <pre><code>scala&gt; println(sparkPlan57.numberedTreeString)\n00 AdaptiveSparkPlan isFinalPlan=false\n01 +- Project [id#151L]\n02    +- SortMergeJoin [id#151L], [id#149L], Inner\n03       :- Sort [id#151L ASC NULLS FIRST], false, 0\n04       :  +- Exchange hashpartitioning(id#151L, 4), REPARTITION_BY_NUM, [id=#514]\n05       :     +- Filter id#151L IN (50,70)\n06       :        +- Range (0, 4, step=1, splits=16)\n07       +- Sort [id#149L ASC NULLS FIRST], false, 0\n08          +- Filter (id#149L IN (50,70) AND isnotnull(id#149L))\n09             +- FileScan parquet default.bucketed_4_id[id#149L] Batched: true, DataFilters: [id#149L IN (50,70), isnotnull(id#149L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/jacek/dev/oss/spark/spark-warehouse/bucketed_4_id], PartitionFilters: [], PushedFilters: [In(id, [50,70]), IsNotNull(id)], ReadSchema: struct&lt;id:bigint&gt;, SelectedBucketsCount: 2 out of 4\n</code></pre> <p>Execute the query to trigger Adaptive Query Execution.</p> <pre><code>sparkPlan57.executeTake(0)\n</code></pre> <pre><code>import org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec\nval initialPlan = sparkPlan57.asInstanceOf[AdaptiveSparkPlanExec].initialPlan\n</code></pre> <pre><code>import org.apache.spark.sql.execution.FileSourceScanExec\nval scan57 = initialPlan.collectFirst { case exec: FileSourceScanExec =&gt; exec }.get\n\nimport org.apache.spark.sql.execution.datasources.FileScanRDD\nval rdd57 = scan57.inputRDDs.head.asInstanceOf[FileScanRDD]\n\nimport org.apache.spark.sql.execution.datasources.FilePartition\nval bucketFiles57 = for {\nFilePartition(bucketId, files) &lt;- rdd57.filePartitions\nf &lt;- files\n} yield s\"Bucket $bucketId =&gt; $f\"\n</code></pre> <pre><code>scala&gt; println(bucketFiles57.size)\n32\n</code></pre>"},{"location":"bucketing/#sorting","title":"Sorting","text":"<p>Disable auto broadcasting.</p> <pre><code>spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n</code></pre> <pre><code>val bucketedTableName = \"bucketed_4_id\"\nval large = spark.range(10e5.toLong)\nimport org.apache.spark.sql.SaveMode\nlarge.write\n.bucketBy(4, \"id\")\n.sortBy(\"id\")\n.mode(SaveMode.Overwrite)\n.saveAsTable(bucketedTableName)\n\n// Describe the table and include bucketing spec only\nval descSQL = sql(s\"DESC FORMATTED $bucketedTableName\")\n.filter($\"col_name\".contains(\"Bucket\") || $\"col_name\" === \"Sort Columns\")\n</code></pre> <pre><code>scala&gt; descSQL.show\n+--------------+---------+-------+\n|      col_name|data_type|comment|\n+--------------+---------+-------+\n|   Num Buckets|        4|       |\n|Bucket Columns|   [`id`]|       |\n|  Sort Columns|   [`id`]|       |\n+--------------+---------+-------+\n</code></pre> <pre><code>val bucketedTable = spark.table(bucketedTableName)\n\nval t1 = spark.range(4)\n.repartition(2, $\"id\")  // Use just 2 partitions\n.sortWithinPartitions(\"id\") // sort partitions\n\nval q = t1.join(bucketedTable, \"id\")\n</code></pre> <pre><code>// Note two exchanges and sorts\nscala&gt; q.explain\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [id#183L]\n   +- SortMergeJoin [id#183L], [id#181L], Inner\n      :- Sort [id#183L ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(id#183L, 200), ENSURE_REQUIREMENTS, [id=#597]\n      :     +- Range (0, 4, step=1, splits=16)\n      +- Sort [id#181L ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(id#181L, 200), ENSURE_REQUIREMENTS, [id=#605]\n            +- Filter isnotnull(id#181L)\n               +- FileScan parquet default.bucketed_4_id[id#181L] Batched: true, DataFilters: [isnotnull(id#181L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/jacek/dev/oss/spark/spark-warehouse/bucketed_4_id], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct&lt;id:bigint&gt;\n</code></pre> <pre><code>q.foreach(_ =&gt; ())\n</code></pre> <p>Warn</p> <p>There are two exchanges and sorts which makes the above use case almost unusable. I filed an issue at SPARK-24025 Join of bucketed and non-bucketed tables can give two exchanges and sorts for non-bucketed side.</p> <p></p>"},{"location":"bucketing/BucketSpec/","title":"BucketSpec","text":"<p><code>BucketSpec</code> is the bucketing specification of a table (the metadata of a bucketed table).</p> <p><code>BucketSpec</code> is a SQLConfHelper</p>"},{"location":"bucketing/BucketSpec/#creating-instance","title":"Creating Instance","text":"<p><code>BucketSpec</code> takes the following to be created:</p> <ul> <li>Number of buckets</li> <li> Bucketing Columns <li> Sorting Columns <p><code>BucketSpec</code> is created when:</p> <ul> <li><code>CatalogUtils</code> is requested to normalizeBucketSpec</li> <li><code>AstBuilder</code> is requested to parse bucketing specification</li> <li><code>TransformHelper</code> is requested to convertTransforms</li> <li><code>HiveExternalCatalog</code> is requested to restoreTableMetadata (and getBucketSpecFromTableProperties)</li> <li><code>HiveClientImpl</code> is requested to convertHiveTableToCatalogTable</li> <li><code>DataFrameWriter</code> is requested for the BucketSpec</li> <li><code>ShowCreateTableExec</code> is requested to showTablePartitioning</li> </ul>"},{"location":"bucketing/BucketSpec/#number-of-buckets","title":"Number of Buckets <p><code>BucketSpec</code> is given the number of buckets when created.</p> <p>The number of buckets has to be between <code>0</code> and spark.sql.sources.bucketing.maxBuckets (inclusive) or an <code>AnalysisException</code> is reported:</p> <pre><code>Number of buckets should be greater than 0 but less than or equal to bucketing.maxBuckets (`[bucketingMaxBuckets]`). Got `[numBuckets]`.\n</code></pre>","text":""},{"location":"cache-serialization/","title":"Cache Serialization","text":"<p>SPARK-32274 introduced pluggable Cache Serialization based upon the following abstractions:</p> <ul> <li>CachedBatch</li> <li>CachedBatchSerializer</li> </ul>"},{"location":"cache-serialization/CachedBatch/","title":"CachedBatch","text":"<p><code>CachedBatch</code> is an abstraction of cached batches of data with the numRows and sizeInBytes metrics.</p>","tags":["DeveloperApi"]},{"location":"cache-serialization/CachedBatch/#contract","title":"Contract","text":"","tags":["DeveloperApi"]},{"location":"cache-serialization/CachedBatch/#numrows","title":"numRows <pre><code>numRows: Int\n</code></pre> <p>Used when:</p> <ul> <li><code>CachedRDDBuilder</code> is requested to buildBuffers</li> <li><code>InMemoryTableScanExec</code> physical operator is requested for the inputRDD</li> </ul>","text":"","tags":["DeveloperApi"]},{"location":"cache-serialization/CachedBatch/#sizeinbytes","title":"sizeInBytes <pre><code>sizeInBytes: Long\n</code></pre> <p>Used when:</p> <ul> <li><code>CachedRDDBuilder</code> is requested to buildBuffers</li> </ul>","text":"","tags":["DeveloperApi"]},{"location":"cache-serialization/CachedBatch/#implementations","title":"Implementations","text":"<ul> <li>SimpleMetricsCachedBatch</li> </ul>","tags":["DeveloperApi"]},{"location":"cache-serialization/CachedBatchSerializer/","title":"CachedBatchSerializer","text":"<p><code>CachedBatchSerializer</code> is an abstraction of serializers of CachedBatches.</p> <p><code>CachedBatchSerializer</code> is configured using spark.sql.cache.serializer configuration property (for InMemoryRelation).</p> <p><code>CachedBatchSerializer</code> is used to create a CachedRDDBuilder.</p> <p><code>CachedBatchSerializer</code> is a <code>Serializable</code>.</p>","tags":["DeveloperApi"]},{"location":"cache-serialization/CachedBatchSerializer/#contract-subset","title":"Contract (Subset)","text":"","tags":["DeveloperApi"]},{"location":"cache-serialization/CachedBatchSerializer/#buildfilter","title":"buildFilter <pre><code>buildFilter(\n  predicates: Seq[Expression],\n  cachedAttributes: Seq[Attribute]): (Int, Iterator[CachedBatch]) =&gt; Iterator[CachedBatch]\n</code></pre> <p>See:</p> <ul> <li>SimpleMetricsCachedBatchSerializer</li> </ul> <p>Used when:</p> <ul> <li><code>InMemoryTableScanExec</code> physical operator is requested to filteredCachedBatches (with spark.sql.inMemoryColumnarStorage.partitionPruning enabled)</li> </ul>","text":"","tags":["DeveloperApi"]},{"location":"cache-serialization/CachedBatchSerializer/#supportscolumnarinput","title":"supportsColumnarInput <pre><code>supportsColumnarInput(\n  schema: Seq[Attribute]): Boolean\n</code></pre> <p>See:</p> <ul> <li>DefaultCachedBatchSerializer</li> </ul> <p>Used when:</p> <ul> <li><code>CachedRDDBuilder</code> is requested to buildBuffers</li> <li><code>InMemoryRelation</code> is created</li> </ul>","text":"","tags":["DeveloperApi"]},{"location":"cache-serialization/CachedBatchSerializer/#implementations","title":"Implementations","text":"<ul> <li>SimpleMetricsCachedBatchSerializer</li> </ul>","tags":["DeveloperApi"]},{"location":"cache-serialization/CachedRDDBuilder/","title":"CachedRDDBuilder","text":""},{"location":"cache-serialization/CachedRDDBuilder/#creating-instance","title":"Creating Instance","text":"<p><code>CachedRDDBuilder</code> takes the following to be created:</p> <ul> <li> CachedBatchSerializer <li> <code>StorageLevel</code> (Spark Core) <li> Cached SparkPlan <li> (optional) Table Name <p><code>CachedRDDBuilder</code> is created alongside InMemoryRelation leaf logical operator.</p>"},{"location":"cache-serialization/DefaultCachedBatchSerializer/","title":"DefaultCachedBatchSerializer","text":"<p><code>DefaultCachedBatchSerializer</code> is a SimpleMetricsCachedBatchSerializer.</p>"},{"location":"cache-serialization/DefaultCachedBatchSerializer/#supportscolumnarinput","title":"supportsColumnarInput  Signature <pre><code>supportsColumnarInput(\n  schema: Seq[Attribute]): Boolean\n</code></pre> <p><code>supportsColumnarInput</code> is part of the CachedBatchSerializer abstraction.</p>  <p><code>supportsColumnarInput</code> is <code>false</code>.</p>","text":""},{"location":"cache-serialization/DefaultCachedBatchSerializer/#supportscolumnaroutput","title":"supportsColumnarOutput  Signature <pre><code>supportsColumnarOutput(\n  schema: StructType): Boolean\n</code></pre> <p><code>supportsColumnarOutput</code> is part of the CachedBatchSerializer abstraction.</p>  <p><code>supportsColumnarOutput</code> is <code>true</code> when all the fields of the given schema are of the following types:</p> <ul> <li><code>BooleanType</code></li> <li><code>ByteType</code></li> <li><code>DoubleType</code></li> <li><code>FloatType</code></li> <li><code>IntegerType</code></li> <li><code>LongType</code></li> <li><code>ShortType</code></li> </ul>","text":""},{"location":"cache-serialization/SimpleMetricsCachedBatch/","title":"SimpleMetricsCachedBatch","text":"<p><code>SimpleMetricsCachedBatch</code> is...FIXME</p>"},{"location":"cache-serialization/SimpleMetricsCachedBatchSerializer/","title":"SimpleMetricsCachedBatchSerializer","text":"<p><code>SimpleMetricsCachedBatchSerializer</code> is a base abstraction of the CachedBatchSerializer abstraction for serializers with the default buildFilter.</p>","tags":["DeveloperApi"]},{"location":"cache-serialization/SimpleMetricsCachedBatchSerializer/#implementations","title":"Implementations","text":"<ul> <li>DefaultCachedBatchSerializer</li> </ul>","tags":["DeveloperApi"]},{"location":"cache-serialization/SimpleMetricsCachedBatchSerializer/#building-batch-filter","title":"Building Batch Filter  Signature <pre><code>buildFilter(\n  predicates: Seq[Expression],\n  cachedAttributes: Seq[Attribute]): (Int, Iterator[CachedBatch]) =&gt; Iterator[CachedBatch]\n</code></pre> <p><code>buildFilter</code> is part of the CachedBatchSerializer abstraction.</p>  <p><code>buildFilter</code>...FIXME</p>","text":"","tags":["DeveloperApi"]},{"location":"catalyst/","title":"Catalyst Tree Manipulation Framework","text":"<p>Catalyst is an execution-agnostic framework to represent and manipulate a dataflow graph as trees of relational operators and expressions.</p> <p>The Catalyst framework was introduced in [SPARK-1251] Support for optimizing and executing structured queries.</p> <p>Spark SQL uses the Catalyst framework to build an extensible Optimizer with a number of built-in logical query plan optimizations.</p> <p>Catalyst supports both rule-based and cost-based optimizations.</p>"},{"location":"catalyst/GenericStrategy/","title":"GenericStrategy \u2014 Planning Strategies","text":"<p><code>GenericStrategy</code> is an abstraction of planning strategies of QueryPlanner.</p>"},{"location":"catalyst/GenericStrategy/#type-constructor-and-physicalplan-type","title":"Type Constructor and PhysicalPlan Type","text":"<p><code>GenericStrategy</code> is a type constructor in Scala (generic class in Java) with the following definition:</p> <pre><code>abstract class GenericStrategy[PhysicalPlan &lt;: TreeNode[PhysicalPlan]]\n</code></pre> <p><code>GenericStrategy</code> uses <code>PhysicalPlan</code> as the name of a type that is a subtype of TreeNode and for which a concrete class can be created (e.g. SparkStrategy).</p>"},{"location":"catalyst/GenericStrategy/#contract","title":"Contract","text":""},{"location":"catalyst/GenericStrategy/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): Seq[PhysicalPlan]\n</code></pre> <p>Executes the planning strategy (to generate a TreeNode)</p>","text":""},{"location":"catalyst/GenericStrategy/#planlater","title":"planLater <pre><code>planLater(\n  plan: LogicalPlan): PhysicalPlan\n</code></pre>","text":""},{"location":"catalyst/GenericStrategy/#implementations","title":"Implementations","text":"<ul> <li>SparkStrategy</li> </ul>"},{"location":"catalyst/Optimizer/","title":"Optimizer \u2014 Generic Logical Query Plan Optimizer","text":"<p><code>Optimizer</code> (Catalyst Optimizer) is an extension of the RuleExecutor abstraction for logical query plan optimizers.</p> <pre><code>Optimizer: Analyzed Logical Plan ==&gt; Optimized Logical Plan\n</code></pre>"},{"location":"catalyst/Optimizer/#implementations","title":"Implementations","text":"<ul> <li>SparkOptimizer</li> </ul>"},{"location":"catalyst/Optimizer/#creating-instance","title":"Creating Instance","text":"<p><code>Optimizer</code> takes the following to be created:</p> <ul> <li> CatalogManager <p>Abstract Class</p> <p><code>Optimizer</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete Optimizers.</p>"},{"location":"catalyst/Optimizer/#default-batches","title":"Default Batches <p><code>Optimizer</code> defines the default rule batches of logical optimizations that transform the query plan of a structured query to produce the optimized logical query plan.</p> <p>The default rule batches can be further refined (extended or with rules excluded).</p>","text":""},{"location":"catalyst/Optimizer/#eliminate-distinct","title":"Eliminate Distinct","text":"<p>Rules:</p> <ul> <li>EliminateDistinct</li> </ul> <p>Strategy: <code>Once</code></p>"},{"location":"catalyst/Optimizer/#finish-analysis","title":"Finish Analysis <p>Rules:</p> <ul> <li>EliminateResolvedHint</li> <li>EliminateSubqueryAliases</li> <li>EliminateView</li> <li>InlineCTE</li> <li>ReplaceExpressions</li> <li>RewriteNonCorrelatedExists</li> <li>PullOutGroupingExpressions</li> <li>ComputeCurrentTime</li> <li>ReplaceCurrentLike</li> </ul> <p>Strategy: <code>Once</code></p>","text":""},{"location":"catalyst/Optimizer/#union","title":"Union <p>Rules:</p> <ul> <li>CombineUnions</li> </ul> <p>Strategy: <code>Once</code></p>","text":""},{"location":"catalyst/Optimizer/#optimizelimitzero","title":"OptimizeLimitZero <p>Rules:</p> <ul> <li>OptimizeLimitZero</li> </ul> <p>Strategy: <code>Once</code></p>","text":""},{"location":"catalyst/Optimizer/#localrelation-early","title":"LocalRelation early <p>Rules:</p> <ul> <li>ConvertToLocalRelation</li> <li>PropagateEmptyRelation</li> </ul> <p>Strategy: fixedPoint</p>","text":""},{"location":"catalyst/Optimizer/#pullup-correlated-expressions","title":"Pullup Correlated Expressions <p>Rules:</p> <ul> <li>PullupCorrelatedPredicates</li> </ul> <p>Strategy: <code>Once</code></p>","text":""},{"location":"catalyst/Optimizer/#subquery","title":"Subquery <p>Rules:</p> <ul> <li>OptimizeSubqueries</li> </ul> <p>Strategy: <code>FixedPoint(1)</code></p>","text":""},{"location":"catalyst/Optimizer/#replace-operators","title":"Replace Operators <p>Rules:</p> <ul> <li>RewriteExceptAll</li> <li>RewriteIntersectAll</li> <li>ReplaceIntersectWithSemiJoin</li> <li>ReplaceExceptWithFilter</li> <li>ReplaceExceptWithAntiJoin</li> <li>ReplaceDistinctWithAggregate</li> </ul> <p>Strategy: fixedPoint</p>","text":""},{"location":"catalyst/Optimizer/#aggregate","title":"Aggregate <p>Rules:</p> <ul> <li>RemoveLiteralFromGroupExpressions</li> <li>RemoveRepetitionFromGroupExpressions</li> </ul> <p>Strategy: fixedPoint</p>","text":""},{"location":"catalyst/Optimizer/#operator-optimization-before-inferring-filters","title":"Operator Optimization before Inferring Filters <p>Rules:</p> <ul> <li>PushProjectionThroughUnion</li> <li>ReorderJoin</li> <li>EliminateOuterJoin</li> <li>PushDownPredicates</li> <li>PushDownLeftSemiAntiJoin</li> <li>PushLeftSemiLeftAntiThroughJoin</li> <li>LimitPushDown</li> <li>ColumnPruning</li> <li>CollapseRepartition</li> <li>CollapseProject</li> <li>CollapseWindow</li> <li>CombineFilters</li> <li>CombineLimits</li> <li>CombineUnions</li> <li>TransposeWindow</li> <li>NullPropagation</li> <li>ConstantPropagation</li> <li>FoldablePropagation</li> <li>OptimizeIn</li> <li>ConstantFolding</li> <li>ReorderAssociativeOperator</li> <li>LikeSimplification</li> <li>BooleanSimplification</li> <li>SimplifyConditionals</li> <li>RemoveDispensableExpressions</li> <li>SimplifyBinaryComparison</li> <li>ReplaceNullWithFalseInPredicate</li> <li>PruneFilters</li> <li>SimplifyCasts</li> <li>SimplifyCaseConversionExpressions</li> <li>RewriteCorrelatedScalarSubquery</li> <li>EliminateSerialization</li> <li>RemoveRedundantAliases</li> <li>RemoveNoopOperators</li> <li>SimplifyExtractValueOps</li> <li>CombineConcats</li> <li>extendedOperatorOptimizationRules</li> </ul> <p>Strategy: <code>fixedPoint</code></p>","text":""},{"location":"catalyst/Optimizer/#infer-filters","title":"Infer Filters <p>Rules:</p> <ul> <li>InferFiltersFromConstraints</li> </ul> <p>Strategy: <code>Once</code></p>","text":""},{"location":"catalyst/Optimizer/#operator-optimization-after-inferring-filters","title":"Operator Optimization after Inferring Filters <p>Rules:</p> <ul> <li>The same as in Operator Optimization before Inferring Filters batch</li> </ul> <p>Strategy: <code>fixedPoint</code></p>","text":""},{"location":"catalyst/Optimizer/#early-filter-and-projection-push-down","title":"Early Filter and Projection Push-Down <p>Rules:</p> <ul> <li>As defined by the earlyScanPushDownRules extension point</li> </ul> <p>Strategy: <code>Once</code></p>","text":""},{"location":"catalyst/Optimizer/#update-cte-relation-stats","title":"Update CTE Relation Stats <p>Rules:</p> <ul> <li>UpdateCTERelationStats</li> </ul> <p>Strategy: <code>Once</code></p>","text":""},{"location":"catalyst/Optimizer/#join-reorder","title":"Join Reorder <p>Rules:</p> <ul> <li>CostBasedJoinReorder</li> </ul> <p>Strategy: <code>FixedPoint(1)</code></p>","text":""},{"location":"catalyst/Optimizer/#eliminate-sorts","title":"Eliminate Sorts <p>Rules:</p> <ul> <li>EliminateSorts</li> </ul> <p>Strategy: <code>Once</code></p>","text":""},{"location":"catalyst/Optimizer/#decimal-optimizations","title":"Decimal Optimizations <p>Rules:</p> <ul> <li>DecimalAggregates</li> </ul> <p>Strategy: <code>fixedPoint</code></p>","text":""},{"location":"catalyst/Optimizer/#object-expressions-optimization","title":"Object Expressions Optimization <p>Rules:</p> <ul> <li>EliminateMapObjects</li> <li>CombineTypedFilters</li> <li>ObjectSerializerPruning</li> <li>ReassignLambdaVariableID</li> </ul> <p>Strategy: <code>fixedPoint</code></p>","text":""},{"location":"catalyst/Optimizer/#localrelation","title":"LocalRelation <p>Rules:</p> <ul> <li>ConvertToLocalRelation</li> <li>PropagateEmptyRelation</li> </ul> <p>Strategy: <code>fixedPoint</code></p>","text":""},{"location":"catalyst/Optimizer/#check-cartesian-products","title":"Check Cartesian Products <p>Rules:</p> <ul> <li>CheckCartesianProducts</li> </ul> <p>Strategy: <code>Once</code></p>","text":""},{"location":"catalyst/Optimizer/#rewritesubquery","title":"RewriteSubquery <p>Rules:</p> <ul> <li>RewritePredicateSubquery</li> <li>ColumnPruning</li> <li>CollapseProject</li> <li>RemoveNoopOperators</li> </ul> <p>Strategy: <code>Once</code></p>","text":""},{"location":"catalyst/Optimizer/#normalizefloatingnumbers","title":"NormalizeFloatingNumbers <p>Rules:</p> <ul> <li>NormalizeFloatingNumbers</li> </ul> <p>Strategy: <code>Once</code></p>","text":""},{"location":"catalyst/Optimizer/#excluded-rules","title":"Excluded Rules <p><code>Optimizer</code> uses spark.sql.optimizer.excludedRules configuration property to control what rules in the defaultBatches to exclude.</p>","text":""},{"location":"catalyst/Optimizer/#non-excludable-rules","title":"Non-Excludable Rules <pre><code>nonExcludableRules: Seq[String]\n</code></pre> <p><code>nonExcludableRules</code> is a collection of non-excludable optimization rules.</p> <p>Non-Excludable Rules are so critical for query optimization that they can never be excluded (even using spark.sql.optimizer.excludedRules configuration property).</p> <ul> <li><code>FinishAnalysis</code></li> <li><code>RewriteDistinctAggregates</code></li> <li><code>ReplaceDeduplicateWithAggregate</code></li> <li><code>ReplaceIntersectWithSemiJoin</code></li> <li>ReplaceExceptWithFilter</li> <li>ReplaceExceptWithAntiJoin</li> <li>RewriteExceptAll</li> <li><code>RewriteIntersectAll</code></li> <li><code>ReplaceDistinctWithAggregate</code></li> <li>PullupCorrelatedPredicates</li> <li>RewriteCorrelatedScalarSubquery</li> <li>RewritePredicateSubquery</li> <li><code>NormalizeFloatingNumbers</code></li> <li><code>ReplaceUpdateFieldsExpression</code></li> <li><code>RewriteLateralSubquery</code></li> <li>OptimizeSubqueries</li> </ul>","text":""},{"location":"catalyst/Optimizer/#accessing-optimizer","title":"Accessing Optimizer <p><code>Optimizer</code> is available as the optimizer property of a session-specific <code>SessionState</code>.</p> <pre><code>scala&gt; :type spark.sessionState.optimizer\norg.apache.spark.sql.catalyst.optimizer.Optimizer\n</code></pre> <p>You can access the optimized logical plan of a structured query (as a Dataset) using Dataset.explain basic action (with <code>extended</code> flag enabled) or SQL's <code>EXPLAIN EXTENDED</code> SQL command.</p> <pre><code>// sample structured query\nval inventory = spark\n  .range(5)\n  .withColumn(\"new_column\", 'id + 5 as \"plus5\")\n\n// Using explain operator (with extended flag enabled)\nscala&gt; inventory.explain(extended = true)\n== Parsed Logical Plan ==\n'Project [id#0L, ('id + 5) AS plus5#2 AS new_column#3]\n+- AnalysisBarrier\n      +- Range (0, 5, step=1, splits=Some(8))\n\n== Analyzed Logical Plan ==\nid: bigint, new_column: bigint\nProject [id#0L, (id#0L + cast(5 as bigint)) AS new_column#3L]\n+- Range (0, 5, step=1, splits=Some(8))\n\n== Optimized Logical Plan ==\nProject [id#0L, (id#0L + 5) AS new_column#3L]\n+- Range (0, 5, step=1, splits=Some(8))\n\n== Physical Plan ==\n*(1) Project [id#0L, (id#0L + 5) AS new_column#3L]\n+- *(1) Range (0, 5, step=1, splits=8)\n</code></pre> <p>Alternatively, you can access the analyzed logical plan using <code>QueryExecution</code> and its optimizedPlan property  (that together with <code>numberedTreeString</code> method is a very good \"debugging\" tool).</p> <pre><code>val optimizedPlan = inventory.queryExecution.optimizedPlan\nscala&gt; println(optimizedPlan.numberedTreeString)\n00 Project [id#0L, (id#0L + 5) AS new_column#3L]\n01 +- Range (0, 5, step=1, splits=Some(8))\n</code></pre>","text":""},{"location":"catalyst/Optimizer/#fixedpoint-strategy","title":"FixedPoint Strategy <p><code>FixedPoint</code> strategy with the number of iterations as defined by spark.sql.optimizer.maxIterations configuration property.</p>","text":""},{"location":"catalyst/Optimizer/#extended-operator-optimization-rules-extension-point","title":"Extended Operator Optimization Rules (Extension Point) <pre><code>extendedOperatorOptimizationRules: Seq[Rule[LogicalPlan]] = Nil\n</code></pre> <p><code>extendedOperatorOptimizationRules</code> extension point defines additional rules for the Operator Optimization rule batch.</p> <p><code>extendedOperatorOptimizationRules</code> rules are executed right after Operator Optimization before Inferring Filters and Operator Optimization after Inferring Filters.</p>","text":""},{"location":"catalyst/Optimizer/#earlyscanpushdownrules-extension-point","title":"earlyScanPushDownRules (Extension Point) <pre><code>earlyScanPushDownRules: Seq[Rule[LogicalPlan]] = Nil\n</code></pre> <p><code>earlyScanPushDownRules</code> extension point...FIXME</p>","text":""},{"location":"catalyst/Optimizer/#blacklistedoncebatches","title":"blacklistedOnceBatches <pre><code>blacklistedOnceBatches: Set[String]\n</code></pre> <p><code>blacklistedOnceBatches</code>...FIXME</p>","text":""},{"location":"catalyst/Optimizer/#batches","title":"Batches  Signature <pre><code>batches: Seq[Batch]\n</code></pre> <p><code>batches</code> is part of the RuleExecutor abstraction.</p>  <p><code>batches</code> uses spark.sql.optimizer.excludedRules configuration property for the rules to be excluded.</p> <p><code>batches</code> filters out non-excludable rules from the rules to be excluded. For any filtered-out rule, <code>batches</code> prints out the following WARN message to the logs:</p> <pre><code>Optimization rule '[ruleName]' was not excluded from the optimizer because this rule is a non-excludable rule.\n</code></pre> <p><code>batches</code> filters out the excluded rules from all defaultBatches. In case a batch is left with no rules, <code>batches</code> prints out the following INFO message to the logs:</p> <pre><code>Optimization batch '[name]' is excluded from the optimizer as all enclosed rules have been excluded.\n</code></pre>","text":""},{"location":"catalyst/PlanChangeLogger/","title":"PlanChangeLogger","text":"<p><code>PlanChangeLogger</code> is a logging utility for rule executors to log plan changes (at rule and batch level).</p>"},{"location":"catalyst/PlanChangeLogger/#creating-instance","title":"Creating Instance","text":"<p><code>PlanChangeLogger</code> takes no arguments to be created.</p> <p><code>PlanChangeLogger</code> is created when:</p> <ul> <li><code>RuleExecutor</code> is requested to execute rules</li> <li><code>QueryExecution</code> is requested to prepare for execution</li> <li><code>AdaptiveSparkPlanExec</code> physical operator is created</li> </ul>"},{"location":"catalyst/PlanChangeLogger/#treetype","title":"TreeType <pre><code>PlanChangeLogger[TreeType &lt;: TreeNode[_]]\n</code></pre> <p><code>PlanChangeLogger</code> is a Scala type constructor (generic class) with <code>TreeType</code> type alias of a subclass of TreeNode.</p>","text":""},{"location":"catalyst/PlanChangeLogger/#logging-plan-changes-by-rule","title":"Logging Plan Changes by Rule <pre><code>logRule(\n  ruleName: String,\n  oldPlan: TreeType,\n  newPlan: TreeType): Unit\n</code></pre> <p><code>logRule</code> prints out the following message to the logs when the given <code>newPlan</code> and <code>oldPlan</code> are different and the <code>ruleName</code> is included in the spark.sql.planChangeLog.rules configuration property.</p> <pre><code>=== Applying Rule [ruleName] ===\n[oldPlan] [newPlan]\n</code></pre> <p><code>logRule</code> is used when:</p> <ul> <li><code>RuleExecutor</code> is requested to execute</li> <li><code>QueryExecution</code> is requested to prepare for execution</li> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested to applyPhysicalRules</li> </ul>","text":""},{"location":"catalyst/PlanChangeLogger/#logging-plan-changes-by-batch","title":"Logging Plan Changes by Batch <pre><code>logBatch(\n  batchName: String,\n  oldPlan: TreeType,\n  newPlan: TreeType): Unit\n</code></pre> <p><code>logBatch</code> prints out one of the following messages to the logs when the given <code>batchName</code> is included in the spark.sql.planChangeLog.batches configuration property.</p> <p>When the given <code>oldPlan</code> and <code>newPlan</code> are different, <code>logBatch</code> prints out the following message:</p> <pre><code>=== Result of Batch [batchName] ===\n[oldPlan] [newPlan]\n</code></pre> <p>Otherwise, <code>logBatch</code> prints out the following message:</p> <pre><code>Batch [batchName] has no effect.\n</code></pre> <p><code>logBatch</code> is used when:</p> <ul> <li><code>RuleExecutor</code> is requested to execute</li> <li><code>QueryExecution</code> is requested to prepare for execution</li> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested to applyPhysicalRules</li> </ul>","text":""},{"location":"catalyst/PlanChangeLogger/#logging-metrics","title":"Logging Metrics <pre><code>logMetrics(\n  metrics: QueryExecutionMetrics): Unit\n</code></pre> <p><code>logMetrics</code> prints out the following message to the logs:</p> <pre><code>=== Metrics of Executed Rules ===\nTotal number of runs: [numRuns]\nTotal time: [totalTime] seconds\nTotal number of effective runs: [numEffectiveRuns]\nTotal time of effective runs: [totalTimeEffective] seconds\n</code></pre> <p><code>logMetrics</code> is used when:</p> <ul> <li><code>RuleExecutor</code> is requested to execute</li> </ul>","text":""},{"location":"catalyst/PlanChangeLogger/#logbasedonlevel","title":"logBasedOnLevel <pre><code>logBasedOnLevel(\n  f: =&gt; String): Unit\n</code></pre> <p><code>logBasedOnLevel</code> uses the spark.sql.planChangeLog.level configuration property for the log level and prints out the given <code>f</code> message to the logs.</p>","text":""},{"location":"catalyst/QueryPlan/","title":"QueryPlan \u2014 Structured Query Plan","text":"<p><code>QueryPlan</code> is an extension of the TreeNode abstraction for query plans in Catalyst Framework.</p> <p><code>QueryPlan</code> is used to build a tree of relational operators of a structured query. <code>QueryPlan</code> is a tree of (logical or physical) operators that have a tree of expressions.</p> <p><code>QueryPlan</code> has an output attributes, expressions and a schema.</p> <p><code>QueryPlan</code> has statePrefix that is used when displaying a plan with <code>!</code> to indicate an invalid plan, and <code>'</code> to indicate an unresolved plan.</p> <p>A <code>QueryPlan</code> is invalid if there are missing input attributes and <code>children</code> subnodes are non-empty.</p> <p>A <code>QueryPlan</code> is unresolved if the column names have not been verified and column types have not been looked up in the Catalog.</p>"},{"location":"catalyst/QueryPlan/#contract","title":"Contract","text":""},{"location":"catalyst/QueryPlan/#output-schema-attributes","title":"Output (Schema) Attributes <pre><code>output: Seq[Attribute]\n</code></pre> <p>Output Attributes</p> <pre><code>val q = spark.range(3)\n\nscala&gt; q.queryExecution.analyzed.output\nres0: Seq[org.apache.spark.sql.catalyst.expressions.Attribute] = List(id#0L)\n\nscala&gt; q.queryExecution.withCachedData.output\nres1: Seq[org.apache.spark.sql.catalyst.expressions.Attribute] = List(id#0L)\n\nscala&gt; q.queryExecution.optimizedPlan.output\nres2: Seq[org.apache.spark.sql.catalyst.expressions.Attribute] = List(id#0L)\n\nscala&gt; q.queryExecution.sparkPlan.output\nres3: Seq[org.apache.spark.sql.catalyst.expressions.Attribute] = List(id#0L)\n\nscala&gt; q.queryExecution.executedPlan.output\nres4: Seq[org.apache.spark.sql.catalyst.expressions.Attribute] = List(id#0L)\n</code></pre>  <p>Tip</p> <p>You can build a StructType from <code>output</code> attributes using toStructType.</p> <pre><code>scala&gt; q.queryExecution.analyzed.output.toStructType\nres5: org.apache.spark.sql.types.StructType = StructType(StructField(id,LongType,false))\n</code></pre>","text":""},{"location":"catalyst/QueryPlan/#implementations","title":"Implementations","text":"<ul> <li> AnalysisHelper <li> LogicalPlan <li> SparkPlan"},{"location":"catalyst/QueryPlan/#expressions","title":"Expressions <pre><code>expressions: Seq[Expression]\n</code></pre> <p><code>expressions</code> is all of the expressions present in this query plan operator.</p>","text":""},{"location":"catalyst/QueryPlan/#expression-references","title":"Expression References <pre><code>references: AttributeSet\n</code></pre>  Lazy Value <p><code>references</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>references</code> is an <code>AttributeSet</code> of all the Attributes that are referenced by the expressions of this operator (except the produced attributes).</p>  <p><code>references</code> is used when:</p> <ul> <li><code>QueryPlan</code> is requested for the missing input attributes, to transformUpWithNewOutput</li> <li><code>CodegenSupport</code> is requested for the used input attributes</li> <li>others (less interesting?)</li> </ul>","text":""},{"location":"catalyst/QueryPlan/#transforming-expressions","title":"Transforming Expressions <pre><code>transformExpressions(\n  rule: PartialFunction[Expression, Expression]): this.type\n</code></pre> <p><code>transformExpressions</code> executes transformExpressionsDown with the input rule.</p> <p><code>transformExpressions</code> is used when...FIXME</p>","text":""},{"location":"catalyst/QueryPlan/#transforming-expressions-down-the-tree","title":"Transforming Expressions (Down The Tree) <pre><code>transformExpressionsDown(\n  rule: PartialFunction[Expression, Expression]): this.type\n</code></pre> <p><code>transformExpressionsDown</code> applies the given rule to each expression in the query operator.</p> <p><code>transformExpressionsDown</code> is used when...FIXME</p>","text":""},{"location":"catalyst/QueryPlan/#output-schema-attribute-set","title":"Output Schema Attribute Set <pre><code>outputSet: AttributeSet\n</code></pre> <p><code>outputSet</code> simply returns an <code>AttributeSet</code> for the output attributes.</p> <p><code>outputSet</code> is used when...FIXME</p>","text":""},{"location":"catalyst/QueryPlan/#missing-input-attributes","title":"Missing Input Attributes <pre><code>missingInput: AttributeSet\n</code></pre> <p><code>missingInput</code> are attributes that are referenced in expressions but not provided by this node's children (as <code>inputSet</code>) and are not produced by this node (as <code>producedAttributes</code>).</p>","text":""},{"location":"catalyst/QueryPlan/#output-schema","title":"Output Schema <p>You can request the schema of a <code>QueryPlan</code> using <code>schema</code> that builds StructType from the output attributes.</p> <pre><code>// the query\nval dataset = spark.range(3)\n\nscala&gt; dataset.queryExecution.analyzed.schema\nres6: org.apache.spark.sql.types.StructType = StructType(StructField(id,LongType,false))\n</code></pre>","text":""},{"location":"catalyst/QueryPlan/#simple-basic-description-with-state-prefix","title":"Simple (Basic) Description with State Prefix <pre><code>simpleString: String\n</code></pre> <p><code>simpleString</code> adds a state prefix to the node's simple text description.</p> <p><code>simpleString</code> is part of the TreeNode abstraction.</p>","text":""},{"location":"catalyst/QueryPlan/#state-prefix","title":"State Prefix <pre><code>statePrefix: String\n</code></pre> <p>Internally, <code>statePrefix</code> gives <code>!</code> (exclamation mark) when the node is invalid, i.e. missingInput is not empty, and the node is a parent node. Otherwise, <code>statePrefix</code> gives an empty string.</p> <p><code>statePrefix</code> is used when <code>QueryPlan</code> is requested for the simple text node description.</p>","text":""},{"location":"catalyst/QueryPlan/#simple-basic-description-with-state-prefix_1","title":"Simple (Basic) Description with State Prefix <pre><code>verboseString: String\n</code></pre> <p><code>verboseString</code> simply returns the simple (basic) description with state prefix.</p> <p><code>verboseString</code> is part of the TreeNode abstraction.</p>","text":""},{"location":"catalyst/QueryPlan/#innerchildren","title":"innerChildren <pre><code>innerChildren: Seq[QueryPlan[_]]\n</code></pre> <p><code>innerChildren</code> simply returns the subqueries.</p> <p><code>innerChildren</code> is part of the TreeNode abstraction.</p>","text":""},{"location":"catalyst/QueryPlan/#subqueries","title":"subqueries <pre><code>subqueries: Seq[PlanType]\n</code></pre> <p><code>subqueries</code>...FIXME</p> <p><code>subqueries</code> is used when...FIXME</p>","text":""},{"location":"catalyst/QueryPlan/#simplestringwithnodeid","title":"simpleStringWithNodeId <pre><code>simpleStringWithNodeId(): String\n</code></pre> <p><code>simpleStringWithNodeId</code> is part of the TreeNode abstraction.</p> <p><code>simpleStringWithNodeId</code> finds the operatorId tag or defaults to <code>unknown</code>.</p> <p><code>simpleStringWithNodeId</code> uses the nodeName to return the following text:</p> <pre><code>[nodeName] ([operatorId])\n</code></pre>","text":""},{"location":"catalyst/QueryPlan/#append","title":"append <pre><code>append[T &lt;: QueryPlan[T]](\n  plan: =&gt; QueryPlan[T],\n  append: String =&gt; Unit,\n  verbose: Boolean,\n  addSuffix: Boolean,\n  maxFields: Int = SQLConf.get.maxToStringFields,\n  printOperatorId: Boolean = false): Unit\n</code></pre> <p><code>append</code>...FIXME</p> <p><code>append</code> is used when:</p> <ul> <li><code>QueryExecution</code> is requested to simpleString, writePlans and stringWithStats</li> <li><code>ExplainUtils</code> utility is requested to <code>processPlanSkippingSubqueries</code></li> </ul>","text":""},{"location":"catalyst/QueryPlan/#detailed-description-with-operator-id","title":"Detailed Description (with Operator Id) <pre><code>verboseStringWithOperatorId(): String\n</code></pre> <p><code>verboseStringWithOperatorId</code> returns the following text (with spark.sql.debug.maxToStringFields configuration property for the number of arguments to this node, if there are any, and the formatted node name):</p> <pre><code>[formattedNodeName]\nArguments: [argumentString]\n</code></pre> <p><code>verboseStringWithOperatorId</code> is used when:</p> <ul> <li><code>QueryExecution</code> is requested for simple description (and <code>ExplainUtils</code> utility is requested to <code>processPlanSkippingSubqueries</code>)</li> </ul>","text":""},{"location":"catalyst/QueryPlan/#formatted-node-name","title":"Formatted Node Name <pre><code>formattedNodeName: String\n</code></pre> <p><code>formattedNodeName</code>...FIXME</p> <p><code>formattedNodeName</code> is used when:</p> <ul> <li><code>QueryPlan</code> is requested for verboseStringWithOperatorId</li> </ul>","text":""},{"location":"catalyst/QueryPlan/#transformallexpressionswithpruning","title":"transformAllExpressionsWithPruning <pre><code>transformAllExpressionsWithPruning(\n  cond: TreePatternBits =&gt; Boolean,\n  ruleId: RuleId = UnknownRuleId)(\n  rule: PartialFunction[Expression, Expression]): this.type\n</code></pre> <p><code>transformAllExpressionsWithPruning</code>...FIXME</p>  <p><code>transformAllExpressionsWithPruning</code> is used when:</p> <ul> <li><code>QueryPlan</code> is requested for transformAllExpressions and normalizeExpressions</li> <li><code>AnalysisHelper</code> is requested to <code>transformAllExpressionsWithPruning</code></li> <li><code>PlanSubqueries</code> physical optimization is executed</li> <li><code>PlanDynamicPruningFilters</code> physical optimization is executed</li> <li><code>PlanAdaptiveDynamicPruningFilters</code> physical optimization is executed</li> <li><code>PlanAdaptiveSubqueries</code> physical optimization is executed</li> <li><code>ReuseAdaptiveSubquery</code> physical optimization is executed</li> </ul>","text":""},{"location":"catalyst/QueryPlan/#produced-attributes","title":"Produced Attributes <pre><code>producedAttributes: AttributeSet\n</code></pre> <p><code>producedAttributes</code> is empty (and can be overriden by implementations).</p>  <p><code>producedAttributes</code> is used when:</p> <ul> <li><code>NestedColumnAliasing</code> is requested to <code>unapply</code> (destructure a logical operator)</li> <li><code>QueryPlan</code> is requested for the references</li> </ul>","text":""},{"location":"catalyst/QueryPlanner/","title":"QueryPlanner","text":"<p><code>QueryPlanner</code> &lt;&gt;, i.e. converts a spark-sql-LogicalPlan.md[logical plan] to one or more SparkPlan.md[physical plans] using &lt;&gt;. <p>NOTE: <code>QueryPlanner</code> &lt;&gt; at least one physical plan. <p><code>QueryPlanner</code>'s main method is &lt;&gt; that defines the extension points, i.e. &lt;&gt;, &lt;&gt; and &lt;&gt;. <p><code>QueryPlanner</code> is part of Catalyst Framework.</p> <p>=== [[contract]] QueryPlanner Contract</p>"},{"location":"catalyst/QueryPlanner/#source-scala","title":"[source, scala]","text":"<p>abstract class QueryPlanner[PhysicalPlan &lt;: TreeNode[PhysicalPlan]] {   def collectPlaceholders(plan: PhysicalPlan): Seq[(PhysicalPlan, LogicalPlan)]   def prunePlans(plans: Iterator[PhysicalPlan]): Iterator[PhysicalPlan]   def strategies: Seq[GenericStrategy[PhysicalPlan]] }</p> <p>.QueryPlanner Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| [[strategies]] <code>strategies</code> | GenericStrategy planning strategies</p> <p>Used exclusively as an extension point in &lt;&gt;. <p>| [[collectPlaceholders]] <code>collectPlaceholders</code> | Collection of \"placeholder\" physical plans and the corresponding spark-sql-LogicalPlan.md[logical plans].</p> <p>Used exclusively as an extension point in &lt;&gt;. <p>Overriden in SparkPlanner</p> <p>| [[prunePlans]] <code>prunePlans</code> | Prunes physical plans (e.g. bad or somehow incorrect plans).</p> <p>Used exclusively as an extension point in &lt;&gt;. |=== <p>=== [[plan]] Planning Logical Plan -- <code>plan</code> Method</p>"},{"location":"catalyst/QueryPlanner/#source-scala_1","title":"[source, scala]","text":""},{"location":"catalyst/QueryPlanner/#planplan-logicalplan-iteratorphysicalplan","title":"plan(plan: LogicalPlan): Iterator[PhysicalPlan]","text":"<p><code>plan</code> converts the input <code>plan</code> spark-sql-LogicalPlan.md[logical plan] to zero or more <code>PhysicalPlan</code> plans.</p> <p>Internally, <code>plan</code> applies &lt;&gt; to the input <code>plan</code> (one by one collecting all as the plan candidates). <p><code>plan</code> then walks over the plan candidates to &lt;&gt;. <p>If a plan does not contain a placeholder, the plan is returned as is. Otherwise, <code>plan</code> walks over placeholders (as pairs of <code>PhysicalPlan</code> and unplanned spark-sql-LogicalPlan.md[logical plan]) and (recursively) &lt;&gt; the child logical plan. <code>plan</code> then replaces the placeholders with the planned child logical plan. <p>In the end, <code>plan</code> &lt;&gt;. <p>NOTE: <code>plan</code> is used exclusively (through the concrete SparkPlanner) when a <code>QueryExecution</code> is requested for a physical plan.</p>"},{"location":"catalyst/Rule/","title":"Rule","text":"<p><code>Rule</code> is an abstraction of named transformations of TreeNodes.</p> <p><code>Rule</code> can be executed on a <code>TreeNode</code> to produce a new <code>TreeNode</code>.</p> <p><code>Rule</code> is primarily used to create a batch of rules for a RuleExecutor.</p>"},{"location":"catalyst/Rule/#treetype","title":"TreeType","text":"<p><code>Rule</code> is a Scala abstract class constructor (generic class) with <code>TreeType</code> type that is a subtype of TreeNode (e.g. LogicalPlan, SparkPlan, Expression).</p> <pre><code>abstract class Rule[TreeType &lt;: TreeNode[_]]\n</code></pre>"},{"location":"catalyst/Rule/#contract","title":"Contract","text":""},{"location":"catalyst/Rule/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: TreeType): TreeType\n</code></pre> <p>Applies the rule to a TreeType</p> <p>Used when:</p> <ul> <li><code>QueryExecution</code> utility is used to prepareForExecution</li> <li><code>AdaptiveSparkPlanExec</code> utility is used to applyPhysicalRules</li> </ul>","text":""},{"location":"catalyst/Rule/#name","title":"Name <pre><code>ruleName: String\n</code></pre> <p><code>ruleName</code> is the name of a rule that is a class name with no ending <code>$</code> (that Scala generates for objects).</p>","text":""},{"location":"catalyst/Rule/#notable-use-cases","title":"Notable Use Cases <p>The other notable use cases of <code>Rule</code> are as follows:</p> <ul> <li> <p>SparkSessionExtensions</p> </li> <li> <p>When <code>ExperimentalMethods</code> is requested for extraOptimizations</p> </li> <li> <p>When <code>BaseSessionStateBuilder</code> is requested for customResolutionRules, customPostHocResolutionRules, customOperatorOptimizationRules, and the Optimizer</p> </li> <li> <p>When <code>Analyzer</code> is requested for extendedResolutionRules and postHocResolutionRules (see BaseSessionStateBuilder and HiveSessionStateBuilder)</p> </li> <li> <p>When <code>Optimizer</code> is requested for extendedOperatorOptimizationRules</p> </li> <li> <p>When <code>QueryExecution</code> is requested for preparations</p> </li> </ul>","text":""},{"location":"catalyst/RuleExecutor/","title":"RuleExecutors","text":"<p><code>RuleExecutor</code> is an abstraction of rule executors that can execute batches of rules (that transform TreeNodes).</p>"},{"location":"catalyst/RuleExecutor/#contract","title":"Contract","text":""},{"location":"catalyst/RuleExecutor/#batches-of-rules","title":"Batches of Rules <pre><code>batches: Seq[Batch]\n</code></pre> <p>A sequence of batches of rules</p> <p>Used when <code>RuleExecutor</code> is executed</p>","text":""},{"location":"catalyst/RuleExecutor/#implementations","title":"Implementations","text":"<ul> <li>Logical Adaptive Optimizer</li> <li>Logical Analyzer</li> <li><code>ExpressionCanonicalizer</code></li> <li>Logical Optimizers</li> </ul>"},{"location":"catalyst/RuleExecutor/#executing-batches-of-rules","title":"Executing Batches of Rules <pre><code>execute(\n  plan: TreeType): TreeType\n</code></pre> <p><code>execute</code> iterates over rule batches and applies rules sequentially to the input <code>plan</code>.</p> <p><code>execute</code> tracks the number of iterations and the time of executing each rule (with a plan).</p> <p>When a rule changes a plan, you should see the following TRACE message in the logs:</p> <pre><code>=== Applying Rule [ruleName] ===\n[currentAndModifiedPlansSideBySide]\n</code></pre> <p>After the number of iterations has reached the number of iterations for the batch's <code>Strategy</code> it stops execution and prints out the following WARN message to the logs:</p> <pre><code>Max iterations ([iteration]) reached for batch [batchName]\n</code></pre> <p>When the plan has not changed (after applying rules), you should see the following TRACE message in the logs and <code>execute</code> moves on to applying the rules in the next batch. The moment is called fixed point (i.e. when the execution converges).</p> <pre><code>Fixed point reached for batch [batchName] after [iteration] iterations.\n</code></pre> <p>After the batch finishes, if the plan has been changed by the rules, you should see the following DEBUG message in the logs:</p> <pre><code>=== Result of Batch [batchName] ===\n[currentAndModifiedPlansSideBySide]\n</code></pre> <p>Otherwise, when the rules had no changes to a plan, you should see the following TRACE message in the logs:</p> <pre><code>Batch [batchName] has no effect.\n</code></pre>","text":""},{"location":"catalyst/RuleExecutor/#tracking-time-of-executing-batches-of-rules","title":"Tracking Time of Executing Batches of Rules <pre><code>executeAndTrack(\n  plan: TreeType,\n  tracker: QueryPlanningTracker): TreeType\n</code></pre> <p><code>executeAndTrack</code>...FIXME</p> <p><code>executeAndTrack</code> is used when:</p> <ul> <li><code>Analyzer</code> is requested to executeAndCheck</li> <li><code>QueryExecution</code> is requested for the optimized logical plan</li> </ul>","text":""},{"location":"catalyst/RuleExecutor/#blacklistedoncebatches","title":"blacklistedOnceBatches <pre><code>blacklistedOnceBatches: Set[String]\n</code></pre> <p><code>blacklistedOnceBatches</code> is empty by default (<code>Set.empty</code>).</p> <p><code>blacklistedOnceBatches</code> is used when <code>RuleExecutor</code> is executed.</p>","text":""},{"location":"catalyst/RuleExecutor/#checkbatchidempotence-internal-method","title":"checkBatchIdempotence Internal Method <pre><code>checkBatchIdempotence(\n  batch: Batch,\n  plan: TreeType): Unit\n</code></pre> <p><code>checkBatchIdempotence</code>...FIXME</p> <p><code>checkBatchIdempotence</code> is used when <code>RuleExecutor</code> is executed.</p>","text":""},{"location":"catalyst/RuleExecutor/#isplanintegral","title":"isPlanIntegral <pre><code>isPlanIntegral(\n  plan: TreeType): Boolean\n</code></pre> <p><code>isPlanIntegral</code> is <code>true</code> by default.</p> <p><code>isPlanIntegral</code> is used when <code>RuleExecutor</code> is executed.</p>","text":""},{"location":"catalyst/RuleExecutor/#scala-definition","title":"Scala Definition <pre><code>abstract class RuleExecutor[TreeType &lt;: TreeNode[_]] {\n  // body omitted\n}\n</code></pre> <p>The definition of the <code>RuleExecutor</code> abstract class uses <code>TreeType</code> type parameter that is constrained by a upper type bound (<code>TreeNode[_]</code>). It says that <code>TreeType</code> type variable can only be a subtype of type TreeNode.</p>  <p>Tip</p> <p>Read up on <code>&lt;:</code> type operator in Scala in Upper Type Bounds.</p>","text":""},{"location":"catalyst/RuleExecutor/#rule-batch-collection-of-rules","title":"Rule Batch \u2014 Collection of Rules <p><code>Batch</code> is a named collection of rules with an execution strategy.</p> <p><code>Batch</code> is defined by the following:</p> <ul> <li> Batch Name <li> Execution Strategy <li> Collection of rules","text":""},{"location":"catalyst/RuleExecutor/#execution-strategy","title":"Execution Strategy <p><code>Strategy</code> is an abstraction of execution strategies.</p>","text":""},{"location":"catalyst/RuleExecutor/#maxiterations","title":"maxIterations <pre><code>maxIterations: Int\n</code></pre> <p>Used when...FIXME</p>","text":""},{"location":"catalyst/RuleExecutor/#erroronexceed","title":"errorOnExceed <pre><code>errorOnExceed: Boolean = false\n</code></pre> <p>Used when...FIXME</p>","text":""},{"location":"catalyst/RuleExecutor/#maxiterationssetting","title":"maxIterationsSetting <pre><code>maxIterationsSetting: String = null\n</code></pre> <p>Used when...FIXME</p>","text":""},{"location":"catalyst/RuleExecutor/#implementations_1","title":"Implementations","text":""},{"location":"catalyst/RuleExecutor/#fixedpoint","title":"FixedPoint","text":"<p>An execution strategy that runs until fix point (and converge) or <code>maxIterations</code> times, whichever comes first</p>"},{"location":"catalyst/RuleExecutor/#once","title":"Once","text":"<p>An execution strategy that runs only once (with <code>maxIterations</code> as <code>1</code>)</p>"},{"location":"catalyst/TreeNode/","title":"TreeNode \u2014 Node in Catalyst Tree","text":"<p><code>TreeNode</code> is an abstraction of named nodes in Catalyst with zero, one or more children.</p>"},{"location":"catalyst/TreeNode/#contract","title":"Contract","text":""},{"location":"catalyst/TreeNode/#children","title":"children <pre><code>children: Seq[BaseType]\n</code></pre> <p>Zero, one or more child nodes of the node</p>","text":""},{"location":"catalyst/TreeNode/#simplestringwithnodeid","title":"simpleStringWithNodeId <pre><code>simpleStringWithNodeId(): String\n</code></pre> <p>One-line description of this node with the node identifier</p> <p>Used when:</p> <ul> <li><code>TreeNode</code> is requested to generateTreeString (with node ID)</li> </ul>","text":""},{"location":"catalyst/TreeNode/#verbosestring","title":"verboseString <pre><code>verboseString(\n  maxFields: Int): String\n</code></pre> <p>One-line verbose description</p> <p>Used when <code>TreeNode</code> is requested to verboseStringWithSuffix and generateTreeString (with <code>verbose</code> flag enabled)</p>","text":""},{"location":"catalyst/TreeNode/#implementations","title":"Implementations","text":"<ul> <li> Block <li> Expression <li> QueryPlan"},{"location":"catalyst/TreeNode/#simple-description","title":"Simple Description <pre><code>simpleString: String\n</code></pre> <p><code>simpleString</code> gives a simple one-line description of a <code>TreeNode</code>.</p> <p>Internally, <code>simpleString</code> is the &lt;&gt; followed by &lt;&gt; separated by a single white space. <p><code>simpleString</code> is used when <code>TreeNode</code> is requested for &lt;&gt; (of child nodes) and &lt;&gt; (with <code>verbose</code> flag off).","text":""},{"location":"catalyst/TreeNode/#numbered-string-representation","title":"Numbered String Representation <pre><code>numberedTreeString: String\n</code></pre> <p><code>numberedTreeString</code> adds numbers to the string representation of this node tree.</p>  <p><code>numberedTreeString</code> is used primarily for interactive debugging (using apply and p methods).</p>","text":""},{"location":"catalyst/TreeNode/#getting-n-th-treenode-in-tree-for-interactive-debugging","title":"Getting n-th TreeNode in Tree (for Interactive Debugging) <pre><code>apply(\n  number: Int): TreeNode[_]\n</code></pre> <p><code>apply</code> gives <code>number</code>-th tree node in a tree.</p> <p><code>apply</code> can be used for interactive debugging.</p> <p>Internally, <code>apply</code> &lt;&gt; at <code>number</code> position or <code>null</code>.","text":""},{"location":"catalyst/TreeNode/#getting-n-th-basetype-in-tree-for-interactive-debugging","title":"Getting n-th BaseType in Tree (for Interactive Debugging) <pre><code>p(\n  number: Int): BaseType\n</code></pre> <p><code>p</code> gives <code>number</code>-th tree node in a tree as <code>BaseType</code> for interactive debugging.</p>  <p>Note</p> <p><code>p</code> can be used for interactive debugging.</p>  <p><code>BaseType</code> is the base type of a tree and in Spark SQL can be:</p> <ul> <li> <p>LogicalPlan for logical plan trees</p> </li> <li> <p>SparkPlan for physical plan trees</p> </li> <li> <p>Expression for expression trees</p> </li> </ul>","text":""},{"location":"catalyst/TreeNode/#string-representation","title":"String Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code> is part of Java's java.lang.Object for the string representation of an object, e.g. <code>TreeNode</code>.</p>  <p><code>toString</code> is a synonym of treeString.</p>","text":""},{"location":"catalyst/TreeNode/#string-representation-of-all-nodes-in-tree","title":"String Representation of All Nodes in Tree <pre><code>treeString: String // (1)\ntreeString(\n  verbose: Boolean,\n  addSuffix: Boolean = false,\n  maxFields: Int = SQLConf.get.maxToStringFields,\n  printOperatorId: Boolean = false): String\ntreeString(\n  append: String =&gt; Unit,\n  verbose: Boolean,\n  addSuffix: Boolean,\n  maxFields: Int,\n  printOperatorId: Boolean): Unit\n</code></pre> <ol> <li><code>verbose</code> flag is enabled (<code>true</code>)</li> </ol>  <p>printOperatorId</p> <p><code>printOperatorId</code> argument is <code>false</code> by default and seems turned on only when:</p> <ul> <li><code>ExplainUtils</code> utility is used to <code>processPlanSkippingSubqueries</code></li> </ul>  <p><code>treeString</code> returns the string representation of all the nodes in the <code>TreeNode</code>.</p>  <p><code>treeString</code> is used when:</p> <ul> <li><code>QueryPlan</code> is requested to append</li> <li><code>TreeNode</code>is requested for a string representation and numbered string representation</li> </ul>","text":""},{"location":"catalyst/TreeNode/#demo","title":"Demo <pre><code>import org.apache.spark.sql.{functions =&gt; f}\nval q = spark.range(10).withColumn(\"rand\", f.rand())\nval executedPlan = q.queryExecution.executedPlan\n\nval output = executedPlan.treeString(verbose = true)\n\nscala&gt; println(output)\n*(1) Project [id#0L, rand(6790207094253656854) AS rand#2]\n+- *(1) Range (0, 10, step=1, splits=8)\n</code></pre>","text":""},{"location":"catalyst/TreeNode/#verbose-description-with-suffix","title":"Verbose Description with Suffix <pre><code>verboseStringWithSuffix: String\n</code></pre> <p><code>verboseStringWithSuffix</code> simply returns &lt;&gt;. <p><code>verboseStringWithSuffix</code> is used when <code>TreeNode</code> is requested to &lt;&gt; (with <code>verbose</code> and <code>addSuffix</code> flags enabled).","text":""},{"location":"catalyst/TreeNode/#generating-text-representation","title":"Generating Text Representation <pre><code>generateTreeString(\n  depth: Int,\n  lastChildren: Seq[Boolean],\n  append: String =&gt; Unit,\n  verbose: Boolean,\n  prefix: String = \"\",\n  addSuffix: Boolean = false,\n  maxFields: Int,\n  printNodeId: Boolean,\n  indent: Int = 0): Unit\n</code></pre> <p><code>generateTreeString</code>...FIXME</p>  <p><code>generateTreeString</code> is used when:</p> <ul> <li><code>TreeNode</code> is requested for the text representation of all nodes in the tree</li> </ul>","text":""},{"location":"catalyst/TreeNode/#inner-child-nodes","title":"Inner Child Nodes <pre><code>innerChildren: Seq[TreeNode[_]]\n</code></pre> <p><code>innerChildren</code> returns the inner nodes that should be shown as an inner nested tree of this node.</p> <p><code>innerChildren</code> simply returns an empty collection of <code>TreeNodes</code>.</p> <p><code>innerChildren</code> is used when <code>TreeNode</code> is requested to &lt;&gt;, &lt;&gt; and &lt;&gt;.","text":""},{"location":"catalyst/TreeNode/#allchildren","title":"allChildren <pre><code>allChildren: Set[TreeNode[_]]\n</code></pre> <p>NOTE: <code>allChildren</code> is a Scala lazy value which is computed once when accessed and cached afterwards.</p> <p><code>allChildren</code>...FIXME</p> <p><code>allChildren</code> is used when...FIXME</p>","text":""},{"location":"catalyst/TreeNode/#foreach","title":"foreach <pre><code>foreach(f: BaseType =&gt; Unit): Unit\n</code></pre> <p><code>foreach</code> applies the input function <code>f</code> to itself (<code>this</code>) first and then (recursively) to the &lt;&gt;.","text":""},{"location":"catalyst/TreeNode/#node-name","title":"Node Name <pre><code>nodeName: String\n</code></pre> <p><code>nodeName</code> returns the name of the class with <code>Exec</code> suffix removed (that is used as a naming convention for the class name of physical operators).</p> <p><code>nodeName</code> is used when:</p> <ul> <li><code>TreeNode</code> is requested for simpleString and asCode</li> </ul>","text":""},{"location":"catalyst/TreeNode/#scala-definition","title":"Scala Definition <pre><code>abstract class TreeNode[BaseType &lt;: TreeNode[BaseType]] extends Product {\n  self: BaseType =&gt;\n  // ...\n}\n</code></pre> <p><code>TreeNode</code> is a recursive data structure that can have one or many &lt;&gt; that are again <code>TreeNodes</code>.  <p>Tip</p> <p>Read up on <code>&lt;:</code> type operator in Scala in Upper Type Bounds.</p>  <p>Scala-specific, <code>TreeNode</code> is an abstract class that is the &lt;&gt; of Catalyst &lt;&gt; and &lt;&gt; abstract classes. <p><code>TreeNode</code> therefore allows for building entire trees of <code>TreeNodes</code>, e.g. generic &lt;&gt; with concrete &lt;&gt; and physical operators that both use &lt;&gt; (which are <code>TreeNodes</code> again). <p>NOTE: Spark SQL uses <code>TreeNode</code> for &lt;&gt; and &lt;&gt; that can further be used together to build more advanced trees, e.g. Catalyst expressions can have query plans as &lt;&gt;. <p><code>TreeNode</code> can itself be a node in a tree or a collection of nodes, i.e. itself and the &lt;&gt; nodes. Not only does <code>TreeNode</code> come with the &lt;&gt; that you may have used in https://docs.scala-lang.org/overviews/collections/overview.html[Scala Collection API] (e.g. &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;), but also specialized ones for more advanced tree manipulation, e.g. &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;. <p><code>TreeNode</code> abstract type is a fairly advanced Scala type definition (at least comparing to the other Scala types in Spark) so understanding its behaviour even outside Spark might be worthwhile by itself.</p>","text":""},{"location":"catalyst/TreeNode/#node-patterns","title":"Node Patterns <p><code>TreeNode</code>s can optionally define node patterns for faster query planning (offering a so-called tree traversal pruning as part of SPARK-35042).</p> <p>Node Patterns are a new feature in Apache Spark 3.2.0.</p>","text":""},{"location":"catalyst/TreeNode/#nodepatterns","title":"nodePatterns <pre><code>nodePatterns: Seq[TreePattern]\n</code></pre> <p><code>nodePatterns</code> is a collection of TreePatterns.</p> <p><code>nodePatterns</code> is empty by default (and is supposed to be overriden by the implementations).</p>","text":""},{"location":"catalyst/TreeNode/#tree-pattern-bits","title":"Tree Pattern Bits <pre><code>treePatternBits: BitSet\n</code></pre> <p><code>treePatternBits</code> is the default tree pattern bits.</p>  Lazy Value <p><code>treePatternBits</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p>  <p><code>treePatternBits</code>\u00a0is part of the TreePatternBits abstraction.</p>","text":""},{"location":"catalyst/TreeNode/#default-tree-pattern-bits","title":"Default Tree Pattern Bits <pre><code>getDefaultTreePatternBits: BitSet\n</code></pre> <p><code>getDefaultTreePatternBits</code> is a <code>BitSet</code> with the nodePatterns bits on (<code>true</code>) unioned with the treePatternBits of the children (if any).</p> <p><code>getDefaultTreePatternBits</code>\u00a0is used when:</p> <ul> <li><code>PlanExpression</code> is requested for the treePatternBits</li> <li><code>QueryPlan</code> is requested for the treePatternBits</li> <li><code>TreeNode</code> is requested for the treePatternBits</li> </ul>","text":""},{"location":"catalyst/TreeNode/#tags","title":"Tags <pre><code>tags: Map[TreeNodeTag[_], Any]\n</code></pre> <p><code>TreeNode</code> can have a metadata assigned (as a mutable map of tags and their values).</p> <p><code>tags</code> can be set, unset and looked up.</p> <p><code>tags</code> are copied (from another <code>TreeNode</code>) only when a <code>TreeNode</code> has none defined.</p>","text":""},{"location":"catalyst/TreeNode/#copying-tags","title":"Copying Tags <pre><code>copyTagsFrom(\n  other: BaseType): Unit\n</code></pre> <p><code>copyTagsFrom</code> is used when:</p> <ul> <li><code>ResolveRelations</code> logical resolution rule is requested to lookupRelation</li> <li><code>ResolveReferences</code> logical resolution rule is requested to collectConflictPlans</li> <li><code>OneRowRelation</code> leaf logical operator is requested to <code>makeCopy</code></li> <li><code>TreeNode</code> is requested to transformDown, transformUp and makeCopy</li> </ul>","text":""},{"location":"catalyst/TreeNode/#looking-up-tag","title":"Looking Up Tag <pre><code>getTagValue[T](\n  tag: TreeNodeTag[T]): Option[T]\n</code></pre>","text":""},{"location":"catalyst/TreeNode/#setting-tag","title":"Setting Tag <pre><code>setTagValue[T](\n  tag: TreeNodeTag[T], value: T): Unit\n</code></pre>","text":""},{"location":"catalyst/TreeNode/#unsetting-tag","title":"Unsetting Tag <pre><code>unsetTagValue[T](\n  tag: TreeNodeTag[T]): Unit\n</code></pre> <p><code>unsetTagValue</code> is used when:</p> <ul> <li><code>ExplainUtils</code> utility is used to <code>removeTags</code></li> <li><code>AdaptiveSparkPlanExec</code> leaf physical operator is requested to cleanUpTempTags</li> </ul>","text":""},{"location":"catalyst/TreeNode/#node-arguments-comma-separated-text","title":"Node Arguments (Comma-Separated Text) <pre><code>argString(\n  maxFields: Int): String\n</code></pre> <p><code>argString</code> concatenates node arguments (based on the stringArgs).</p> <p><code>argString</code>\u00a0is used when:</p> <ul> <li><code>TreeNode</code> is requested for the simple description</li> <li><code>QueryPlan</code> is requested for the detailed description (with operator id)</li> </ul>","text":""},{"location":"catalyst/TreeNode/#string-arguments","title":"String Arguments <pre><code>stringArgs: Iterator[Any]\n</code></pre> <p><code>stringArgs</code> returns all the elements of this node (using Scala's Product.productIterator by default).</p>  <p><code>stringArgs</code>\u00a0is used when:</p> <ul> <li><code>TreeNode</code> is requested for the argString</li> </ul>","text":""},{"location":"catalyst/TreePattern/","title":"TreePattern","text":"<p><code>TreePattern</code>s are part of TreeNodes.</p>"},{"location":"catalyst/TreePattern/#cte","title":"CTE <p>Used as a node pattern:</p> <ul> <li>CTERelationDef</li> <li>CTERelationRef</li> <li>WithCTE</li> </ul>","text":""},{"location":"catalyst/TreePattern/#dynamic_pruning_expression","title":"DYNAMIC_PRUNING_EXPRESSION <p>Used as a node pattern:</p> <ul> <li>DynamicPruningExpression</li> </ul> <p>Used to transform query plans in the following rules:</p> <ul> <li>PlanAdaptiveDynamicPruningFilters</li> <li>CleanupDynamicPruningFilters</li> </ul>","text":""},{"location":"catalyst/TreePattern/#exchange","title":"EXCHANGE <p>Used as a node pattern:</p> <ul> <li>Exchange</li> </ul> <p>Used to transform query plans in the following rules:</p> <ul> <li>ReuseExchangeAndSubquery</li> </ul>","text":""},{"location":"catalyst/TreePattern/#plan_expression","title":"PLAN_EXPRESSION <p>Used as a node pattern:</p> <ul> <li>PlanExpression</li> </ul>","text":""},{"location":"catalyst/TreePattern/#unresolved_hint","title":"UNRESOLVED_HINT <p>Used as a node pattern:</p> <ul> <li>UnresolvedHint</li> </ul>","text":""},{"location":"catalyst/TreePatternBits/","title":"TreePatternBits","text":"<p><code>TreePatternBits</code> is an abstraction of trees with the treePatternBits.</p>"},{"location":"catalyst/TreePatternBits/#contract","title":"Contract","text":""},{"location":"catalyst/TreePatternBits/#treepatternbits_1","title":"treePatternBits <pre><code>treePatternBits: BitSet\n</code></pre> <p>Used when:</p> <ul> <li><code>TreePatternBits</code> is requested to containsPattern</li> </ul>","text":""},{"location":"catalyst/TreePatternBits/#implementations","title":"Implementations","text":"<ul> <li>TreeNode</li> </ul>"},{"location":"catalyst/TreePatternBits/#containspattern","title":"containsPattern <pre><code>containsPattern(\n  t: TreePattern): Boolean\n</code></pre> <p><code>containsPattern</code> is <code>true</code> when the given <code>TreePattern</code> is among the treePatternBits.</p>","text":""},{"location":"catalyst-dsl/","title":"Catalyst DSL","text":"<p>Catalyst DSL is a collection of Scala implicit conversions for constructing Catalyst data structures more easily.</p> <p>The goal of Catalyst DSL is to make working with Spark SQL's building blocks easier (e.g. for testing or Spark SQL internals exploration).</p>"},{"location":"catalyst-dsl/#expressionconversions","title":"ExpressionConversions <p>Creates Catalyst expressions:</p> <ul> <li>Literals</li> <li>UnresolvedAttribute and UnresolvedReference</li> <li>...</li> </ul>","text":""},{"location":"catalyst-dsl/#type-conversions-to-literal-expressions","title":"Type Conversions to Literal Expressions","text":"<p><code>ExpressionConversions</code> adds conversions of Scala native types (e.g. <code>Boolean</code>, <code>Long</code>, <code>String</code>, <code>Date</code>, <code>Timestamp</code>) and Spark SQL types (i.e. <code>Decimal</code>) to Literal expressions.</p>"},{"location":"catalyst-dsl/#converting-symbols-to-unresolvedattribute-and-attributereference-expressions","title":"Converting Symbols to UnresolvedAttribute and AttributeReference Expressions","text":"<p><code>ExpressionConversions</code> adds conversions of Scala's <code>Symbol</code> to UnresolvedAttribute and <code>AttributeReference</code> expressions.</p>"},{"location":"catalyst-dsl/#converting-prefixed-string-literals-to-unresolvedattribute-expressions","title":"Converting $-Prefixed String Literals to UnresolvedAttribute Expressions","text":"<p><code>ExpressionConversions</code> adds conversions of <code>$\"col name\"</code> to an UnresolvedAttribute expression.</p>"},{"location":"catalyst-dsl/#at","title":"at <pre><code>at(\n  ordinal: Int): BoundReference\n</code></pre> <p><code>ExpressionConversions</code> adds <code>at</code> method to <code>AttributeReferences</code> to create a BoundReference expression.</p> <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\nval boundRef = 'hello.string.at(4)\nscala&gt; println(boundRef)\ninput[4, string, true]\n</code></pre>","text":""},{"location":"catalyst-dsl/#star","title":"star <pre><code>star(names: String*): Expression\n</code></pre> <p><code>ExpressionConversions</code> adds the aggregate and non-aggregate functions to Catalyst expressions (e.g. <code>sum</code>, <code>count</code>, <code>upper</code>, <code>star</code>, <code>callFunction</code>, <code>windowSpec</code>, <code>windowExpr</code>)</p> <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\nval s = star()\n\nimport org.apache.spark.sql.catalyst.analysis.UnresolvedStar\nassert(s.isInstanceOf[UnresolvedStar])\n\nval s = star(\"a\", \"b\")\nscala&gt; println(s)\nWrappedArray(a, b).*\n</code></pre>","text":""},{"location":"catalyst-dsl/#function-and-distinctfunction","title":"function and distinctFunction <p><code>ExpressionConversions</code> allows creating UnresolvedFunction expressions with <code>function</code> and <code>distinctFunction</code> operators.</p> <pre><code>function(exprs: Expression*): UnresolvedFunction\ndistinctFunction(exprs: Expression*): UnresolvedFunction\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\n\n// Works with Scala Symbols only\nval f = 'f.function()\nscala&gt; :type f\norg.apache.spark.sql.catalyst.analysis.UnresolvedFunction\n\nscala&gt; f.isDistinct\nres0: Boolean = false\n\nval g = 'g.distinctFunction()\nscala&gt; g.isDistinct\nres1: Boolean = true\n</code></pre>","text":""},{"location":"catalyst-dsl/#notnull-and-canbenull","title":"notNull and canBeNull <p><code>ExpressionConversions</code> adds <code>canBeNull</code> and <code>notNull</code> operators to create a <code>AttributeReference</code> with <code>nullability</code> turned on or off, respectively.</p> <pre><code>notNull: AttributeReference\ncanBeNull: AttributeReference\n</code></pre>","text":""},{"location":"catalyst-dsl/#implicitoperators","title":"ImplicitOperators <p>Adds operators to Catalyst expressions for complex expressions</p>","text":""},{"location":"catalyst-dsl/#implicit-conversions-for-logical-plans","title":"Implicit Conversions for Logical Plans <pre><code>import org.apache.spark.sql.catalyst.dsl.plans._\n</code></pre>","text":""},{"location":"catalyst-dsl/#dsllogicalplan","title":"DslLogicalPlan <p>DslLogicalPlan</p>","text":""},{"location":"catalyst-dsl/#table","title":"table <p><code>table</code> creates a <code>UnresolvedRelation</code> logical operator.</p> <pre><code>table(\n  ref: String): LogicalPlan\ntable(\n  db: String,\n  ref: String): LogicalPlan\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.dsl.plans._\n\nval t1 = table(\"t1\")\nscala&gt; println(t1.treeString)\n'UnresolvedRelation `t1`\n</code></pre>","text":""},{"location":"catalyst-dsl/#package-object","title":"Package Object <p>Catalyst DSL is part of <code>org.apache.spark.sql.catalyst.dsl</code> package object.</p> <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\n</code></pre>","text":""},{"location":"catalyst-dsl/#spark-shell","title":"spark-shell <p>Some implicit conversions from the Catalyst DSL interfere with the implicits conversions from <code>SQLImplicits</code> that are imported automatically in <code>spark-shell</code> (through <code>spark.implicits._</code>).</p> <pre><code>scala&gt; 'hello.decimal\n&lt;console&gt;:30: error: type mismatch;\n found   : Symbol\n required: ?{def decimal: ?}\nNote that implicit conversions are not applicable because they are ambiguous:\n both method symbolToColumn in class SQLImplicits of type (s: Symbol)org.apache.spark.sql.ColumnName\n and method DslSymbol in trait ExpressionConversions of type (sym: Symbol)org.apache.spark.sql.catalyst.dsl.expressions.DslSymbol\n are possible conversion functions from Symbol to ?{def decimal: ?}\n       'hello.decimal\n       ^\n&lt;console&gt;:30: error: value decimal is not a member of Symbol\n       'hello.decimal\n              ^\n</code></pre> <p>Use <code>sbt console</code> with Spark libraries defined (in <code>build.sbt</code>) instead.</p> <p>You can also disable an implicit conversion using a trick described in How can an implicit be unimported from the Scala repl?.</p> <pre><code>// HACK: Disable symbolToColumn implicit conversion\n// It is imported automatically in spark-shell (and makes demos impossible)\n// implicit def symbolToColumn(s: Symbol): org.apache.spark.sql.ColumnName\ntrait ThatWasABadIdea\nimplicit def symbolToColumn(ack: ThatWasABadIdea) = ack\n\n// HACK: Disable $ string interpolator\n// It is imported automatically in spark-shell (and makes demos impossible)\nimplicit class StringToColumn(val sc: StringContext) {}\n</code></pre>","text":""},{"location":"catalyst-dsl/#demo","title":"Demo <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\nimport org.apache.spark.sql.catalyst.dsl.plans._\n\n// ExpressionConversions\n\nimport org.apache.spark.sql.catalyst.expressions.Literal\nscala&gt; val trueLit: Literal = true\ntrueLit: org.apache.spark.sql.catalyst.expressions.Literal = true\n\nimport org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute\nscala&gt; val name: UnresolvedAttribute = 'name\nname: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute = 'name\n\n// NOTE: This conversion may not work, e.g. in spark-shell\n// There is another implicit conversion StringToColumn in SQLImplicits\n// It is automatically imported in spark-shell\n// See :imports\nval id: UnresolvedAttribute = $\"id\"\n\nimport org.apache.spark.sql.catalyst.expressions.Expression\nscala&gt; val expr: Expression = sum('id)\nexpr: org.apache.spark.sql.catalyst.expressions.Expression = sum('id)\n\n// implicit class DslSymbol\nscala&gt; 'hello.s\nres2: String = hello\n\nscala&gt; 'hello.attr\nres4: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute = 'hello\n\n// implicit class DslString\nscala&gt; \"helo\".expr\nres0: org.apache.spark.sql.catalyst.expressions.Expression = helo\n\nscala&gt; \"helo\".attr\nres1: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute = 'helo\n\n// logical plans\n\nscala&gt; val t1 = table(\"t1\")\nt1: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =\n'UnresolvedRelation `t1`\n\nscala&gt; val p = t1.select('*).serialize[String].where('id % 2 == 0)\np: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =\n'Filter false\n+- 'SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true) AS value#1]\n   +- 'Project ['*]\n      +- 'UnresolvedRelation `t1`\n\n// FIXME Does not work because SimpleAnalyzer's catalog is empty\n// the p plan references a t1 table\nimport org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer\nscala&gt; p.analyze\n</code></pre>","text":""},{"location":"catalyst-dsl/DslLogicalPlan/","title":"DslLogicalPlan","text":"<p><code>DslLogicalPlan</code> is part of Catalyst DSL to build entire logical plans.</p> <pre><code>implicit class DslLogicalPlan(logicalPlan: LogicalPlan)\n</code></pre> <p><code>DslLogicalPlan</code> is part of the <code>org.apache.spark.sql.catalyst.dsl.plans</code> object.</p> <pre><code>import org.apache.spark.sql.catalyst.dsl.plans._\n</code></pre>"},{"location":"catalyst-dsl/DslLogicalPlan/#analyze","title":"analyze <pre><code>analyze: LogicalPlan\n</code></pre> <p>Resolves attribute references (using EliminateSubqueryAliases logical optimization and <code>SimpleAnalyzer</code> logical analyzer)</p>","text":""},{"location":"catalyst-dsl/DslLogicalPlan/#operators","title":"Operators","text":""},{"location":"catalyst-dsl/DslLogicalPlan/#deserialize","title":"deserialize <pre><code>deserialize[T : Encoder]: LogicalPlan\n</code></pre> <p><code>deserialize</code> creates an DeserializeToObject logical operator</p>","text":""},{"location":"catalyst-dsl/DslLogicalPlan/#groupby","title":"groupBy <pre><code>groupBy(\n  groupingExprs: Expression*)(\n  aggregateExprs: Expression*): LogicalPlan\n</code></pre> <p><code>groupBy</code> creates an Aggregate logical operator</p>","text":""},{"location":"catalyst-dsl/DslLogicalPlan/#hint","title":"hint <pre><code>hint(\n  name: String,\n  parameters: Any*): LogicalPlan\n</code></pre> <p>Creates an UnresolvedHint logical operator</p>","text":""},{"location":"catalyst-dsl/DslLogicalPlan/#join","title":"join <pre><code>join(\n  otherPlan: LogicalPlan,\n  joinType: JoinType = Inner,\n  condition: Option[Expression] = None): LogicalPlan\n</code></pre> <p>Creates a Join logical operator</p>","text":""},{"location":"catalyst-dsl/DslLogicalPlan/#orderby","title":"orderBy <pre><code>orderBy(\n  sortExprs: SortOrder*): LogicalPlan\n</code></pre> <p>Creates a Sort logical operator (with the global sorting)</p>","text":""},{"location":"catalyst-dsl/DslLogicalPlan/#sortby","title":"sortBy <pre><code>sortBy(\n  sortExprs: SortOrder*): LogicalPlan\n</code></pre> <p>Creates a Sort logical operator (without global sorting)</p>","text":""},{"location":"catalyst-dsl/DslLogicalPlan/#demo","title":"Demo <pre><code>// Import plans object\n// That loads implicit class DslLogicalPlan\n// And so every LogicalPlan is the \"target\" of the DslLogicalPlan methods\nimport org.apache.spark.sql.catalyst.dsl.plans._\n\nval t1 = table(ref = \"t1\")\n\n// HACK: Disable symbolToColumn implicit conversion\n// It is imported automatically in spark-shell (and makes demos impossible)\n// implicit def symbolToColumn(s: Symbol): org.apache.spark.sql.ColumnName\ntrait ThatWasABadIdea\nimplicit def symbolToColumn(ack: ThatWasABadIdea) = ack\n\nimport org.apache.spark.sql.catalyst.dsl.expressions._\nval id = 'id.long\nval logicalPlan = t1.select(id)\nscala&gt; println(logicalPlan.numberedTreeString)\n00 'Project [id#1L]\n01 +- 'UnresolvedRelation `t1`\n\nval t2 = table(\"t2\")\nimport org.apache.spark.sql.catalyst.plans.LeftSemi\nval logicalPlan = t1.join(t2, joinType = LeftSemi, condition = Some(id))\nscala&gt; println(logicalPlan.numberedTreeString)\n00 'Join LeftSemi, id#1: bigint\n01 :- 'UnresolvedRelation `t1`\n02 +- 'UnresolvedRelation `t2`\n</code></pre>","text":""},{"location":"columnar-execution/","title":"Columnar Execution","text":"<p>Columnar Execution (Columnar Processing) uses the following:</p> <ul> <li>ColumnarRule</li> <li>ApplyColumnarRulesAndInsertTransitions physical optimization</li> <li>ColumnarToRowExec and ColumnarToRowExec physical operators</li> </ul> <p>Physical operators that want to participate in Columnar Execution are expected to override supportsColumnar method.</p> <p>Columnar Execution was introduced to Apache Spark 3.0.0 as SPARK-27396.</p>"},{"location":"columnar-execution/#whole-stage-java-code-generation","title":"Whole-Stage Java Code Generation","text":"<p>Columnar Execution is similar and a kind of \"opposite\" at the same time to Whole-Stage Java Code Generation (which is row-based). It is assumed that if a plan supports columnar execution, it can't support whole-stage-codegen at the same time (see the comment in the source code).</p>"},{"location":"columnar-execution/#references","title":"References","text":""},{"location":"columnar-execution/#articles","title":"Articles","text":"<ul> <li>[DISCUSS] Spark Columnar Processing</li> </ul>"},{"location":"common-table-expressions/","title":"Common Table Expressions","text":"<p>Common Table Expressions (CTEs) are defined using the WITH clause:</p> <pre><code>WITH namedQuery (',' namedQuery)*\n\nnamedQuery\n    : name (columnAliases)? AS? '(' query ')'\n    ;\n</code></pre> <p>CTEs allow for statement scoped views that a user can reference (possibly multiple times) within the scope of a SQL statement.</p>"},{"location":"common-table-expressions/#references","title":"References","text":"<ul> <li>Common Table Expression (CTE) by the offical Spark documentation</li> <li>Common table expression by Wikipedia</li> <li>with \u2014 Organize Complex Queries</li> </ul>"},{"location":"connect/","title":"Spark Connect","text":"<p>Apache Spark 3.4 introduces Spark Connect that is a client and server interface for Apache Spark.</p> <p>The Spark Connect server can be started using <code>sbin/start-connect-server.sh</code> shell script.</p>"},{"location":"connect/CommandPlugin/","title":"CommandPlugin","text":"<p><code>CommandPlugin</code> is...FIXME</p>"},{"location":"connect/ExpressionPlugin/","title":"ExpressionPlugin","text":"<p><code>ExpressionPlugin</code> is...FIXME</p>"},{"location":"connect/RelationPlugin/","title":"RelationPlugin","text":"<p><code>RelationPlugin</code> is an abstraction of extensions that can transform.</p>"},{"location":"connect/RelationPlugin/#contract","title":"Contract","text":""},{"location":"connect/RelationPlugin/#transform","title":"transform <pre><code>transform(\n  relation: protobuf.Any,\n  planner: SparkConnectPlanner): Option[LogicalPlan]\n</code></pre> <p>Used when:</p> <ul> <li><code>SparkConnectPlanner</code> is requested to transformRelationPlugin</li> </ul>","text":""},{"location":"connect/RelationPlugin/#implementations","title":"Implementations","text":"<p>Note</p> <p>No built-in implementations available.</p>"},{"location":"connect/SimpleSparkConnectService/","title":"SimpleSparkConnectService","text":"<p><code>SimpleSparkConnectService</code> is...FIXME</p>"},{"location":"connect/SparkConnectAnalyzeHandler/","title":"SparkConnectAnalyzeHandler","text":""},{"location":"connect/SparkConnectAnalyzeHandler/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectAnalyzeHandler</code> takes the following to be created:</p> <ul> <li> <code>StreamObserver</code> of <code>AnalyzePlanResponse</code>s <p><code>SparkConnectAnalyzeHandler</code> is created when:</p> <ul> <li><code>SparkConnectService</code> is requested to handle analyzePlan request</li> </ul>"},{"location":"connect/SparkConnectInterceptorRegistry/","title":"SparkConnectInterceptorRegistry","text":"<p><code>SparkConnectInterceptorRegistry</code> is...FIXME</p>"},{"location":"connect/SparkConnectPlanner/","title":"SparkConnectPlanner","text":""},{"location":"connect/SparkConnectPlanner/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectPlanner</code> takes the following to be created:</p> <ul> <li> SparkSession <p><code>SparkConnectPlanner</code> is created when:</p> <ul> <li><code>SparkConnectAnalyzeHandler</code> is requested to process a request</li> <li><code>SparkConnectStreamHandler</code> is requested to handlePlan and handleCommand</li> </ul>"},{"location":"connect/SparkConnectPlanner/#transformRelation","title":"transformRelation","text":"<pre><code>transformRelation(\nrel: proto.Relation): LogicalPlan\n</code></pre> <p><code>transformRelation</code>...FIXME</p> <p><code>transformRelation</code> is used when:</p> <ul> <li><code>SparkConnectAnalyzeHandler</code> is requested to process a request</li> <li><code>SparkConnectStreamHandler</code> is requested to handlePlan</li> </ul>"},{"location":"connect/SparkConnectPlanner/#transformRelationPlugin","title":"transformRelationPlugin","text":"<pre><code>transformRelationPlugin(\nextension: ProtoAny): LogicalPlan\n</code></pre> <p><code>transformRelationPlugin</code>...FIXME</p>"},{"location":"connect/SparkConnectPlugin/","title":"SparkConnectPlugin","text":"<p><code>SparkConnectPlugin</code> is...FIXME</p>"},{"location":"connect/SparkConnectServer/","title":"SparkConnectServer Standalone Application","text":"<p><code>SparkConnectServer</code> is a standalone application to start a Spark Connect server from command line.</p> <p><code>SparkConnectServer</code> can be started using <code>sbin/start-connect-server.sh</code> shell script.</p> <p><code>SparkConnectServer</code> can be stopped using <code>sbin/stop-connect-server.sh</code> shell script.</p>"},{"location":"connect/SparkConnectServer/#main","title":"Launching Application","text":"<pre><code>main(\nargs: Array[String]): Unit\n</code></pre> <p><code>main</code> prints out the following INFO message to the logs:</p> <pre><code>Starting Spark session.\n</code></pre> <p><code>main</code> creates a SparkSession.</p> <p><code>main</code> starts a SparkConnectService.</p> <p><code>main</code> prints out the following INFO message to the logs:</p> <pre><code>Spark Connect server started.\n</code></pre> <p>In the end, <code>main</code> is paused until the SparkConnectService is terminated.</p>"},{"location":"connect/SparkConnectService/","title":"SparkConnectService","text":"<p><code>SparkConnectService</code> is a <code>BindableService</code> (gRPC).</p>"},{"location":"connect/SparkConnectService/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectService</code> takes the following to be created:</p> <ul> <li> <code>debug</code> flag <p><code>SparkConnectService</code> is created when:</p> <ul> <li><code>SparkConnectService</code> is requested to startGRPCService</li> </ul>"},{"location":"connect/SparkConnectService/#start","title":"start","text":"<pre><code>start(): Unit\n</code></pre> <p><code>start</code> startGRPCService.</p> <p><code>start</code> is used when:</p> <ul> <li>SimpleSparkConnectService standalone application is started</li> <li><code>SparkConnectPlugin</code> is requested to init</li> <li>SparkConnectServer standalone application is started</li> </ul>"},{"location":"connect/SparkConnectService/#startGRPCService","title":"startGRPCService","text":"<pre><code>startGRPCService(): Unit\n</code></pre> <p><code>startGRPCService</code> reads the values of the following configuration properties:</p> Configuration Property Default Value <code>spark.connect.grpc.debug.enabled</code> <code>true</code> <code>spark.connect.grpc.binding.port</code> <code>15002</code> <p><code>startGRPCService</code> builds a <code>NettyServerBuilder</code> with the <code>spark.connect.grpc.binding.port</code> and a SparkConnectService.</p> <p><code>startGRPCService</code> registers interceptors.</p> <p><code>startGRPCService</code> builds the server and starts it.</p>"},{"location":"connect/SparkConnectStreamHandler/","title":"SparkConnectStreamHandler","text":""},{"location":"connect/SparkConnectStreamHandler/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectStreamHandler</code> takes the following to be created:</p> <ul> <li> <code>StreamObserver</code> of <code>ExecutePlanResponse</code>s <p><code>SparkConnectStreamHandler</code> is created when:</p> <ul> <li><code>SparkConnectService</code> is requested to handle executePlan request</li> </ul>"},{"location":"connector/","title":"Connector API","text":"<p>Connector API is a new API in Spark 3 for Spark SQL developers to create connectors (data sources).</p> <p>Connector API is meant to replace the older (and soon deprecated) DataSource v1 and v2.</p>"},{"location":"connector/ApplyTransform/","title":"ApplyTransform","text":"<p><code>ApplyTransform</code> is...FIXME</p>"},{"location":"connector/Batch/","title":"Batch","text":"<p><code>Batch</code> is an abstraction of data source scans for batch queries.</p>"},{"location":"connector/Batch/#contract","title":"Contract","text":""},{"location":"connector/Batch/#creating-partitionreaderfactory","title":"Creating PartitionReaderFactory <pre><code>PartitionReaderFactory createReaderFactory()\n</code></pre> <p>PartitionReaderFactory to create a PartitionReader to read records from the input partitions</p> <p>See:</p> <ul> <li>ParquetScan</li> </ul> <p>Used when:</p> <ul> <li><code>BatchScanExec</code> is requested for a PartitionReaderFactory</li> </ul>","text":""},{"location":"connector/Batch/#planning-input-partitions","title":"Planning Input Partitions <pre><code>InputPartition[] planInputPartitions()\n</code></pre> <p>InputPartitions to scan this data source with</p> <p>See:</p> <ul> <li>FileScan</li> </ul> <p>Used when:</p> <ul> <li><code>BatchScanExec</code> is requested for the input partitions and filtered input partitions</li> </ul>","text":""},{"location":"connector/Batch/#implementations","title":"Implementations","text":"<ul> <li>FileScan</li> <li>KafkaBatch</li> </ul>"},{"location":"connector/BatchWrite/","title":"BatchWrite","text":"<p><code>BatchWrite</code> is an abstraction of writers to a batch data source.</p>"},{"location":"connector/BatchWrite/#contract","title":"Contract","text":""},{"location":"connector/BatchWrite/#aborting-write-job","title":"Aborting Write Job <pre><code>void abort(\n  WriterCommitMessage[] messages)\n</code></pre> <p>Used when:</p> <ul> <li><code>V2TableWriteExec</code> physical command is requested to writeWithV2</li> </ul>","text":""},{"location":"connector/BatchWrite/#committing-write-job","title":"Committing Write Job <pre><code>void commit(\n  WriterCommitMessage[] messages)\n</code></pre> <p>Used when:</p> <ul> <li><code>V2TableWriteExec</code> physical command is requested to writeWithV2</li> </ul>","text":""},{"location":"connector/BatchWrite/#creating-batch-datawriterfactory","title":"Creating Batch DataWriterFactory <pre><code>DataWriterFactory createBatchWriterFactory(\n  PhysicalWriteInfo info)\n</code></pre> <p>Used when:</p> <ul> <li><code>V2TableWriteExec</code> physical command is requested to writeWithV2</li> </ul>","text":""},{"location":"connector/BatchWrite/#ondatawritercommit","title":"onDataWriterCommit <pre><code>void onDataWriterCommit(\n  WriterCommitMessage message)\n</code></pre> <p><code>onDataWriterCommit</code> does nothing by default (noop).</p> <p>Used when:</p> <ul> <li><code>V2TableWriteExec</code> physical command is requested to writeWithV2</li> </ul>","text":""},{"location":"connector/BatchWrite/#usecommitcoordinator","title":"useCommitCoordinator <pre><code>boolean useCommitCoordinator()\n</code></pre> <p>Controls whether this writer requires a Commit Coordinator to coordinate writing tasks (and ensure that at most one task for each partition commits).</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>V2TableWriteExec</code> physical command is requested to writeWithV2</li> </ul>","text":""},{"location":"connector/BatchWrite/#implementations","title":"Implementations","text":"<ul> <li>FileBatchWrite</li> <li>KafkaBatchWrite</li> <li>MicroBatchWrite (Spark Structured Streaming)</li> <li>NoopBatchWrite</li> </ul>"},{"location":"connector/CustomMetric/","title":"CustomMetric","text":"<p><code>CustomMetric</code> is...FIXME</p>"},{"location":"connector/DataSourceV2Implicits/","title":"DataSourceV2Implicits","text":"<p><code>DataSourceV2Implicits</code> is a Scala object with the following Scala implicit classes:</p> <ul> <li>MetadataColumnHelper</li> <li>MetadataColumnsHelper</li> <li>OptionsHelper</li> <li>PartitionSpecsHelper</li> <li>TableHelper</li> </ul> <p><code>DataSourceV2Implicits</code> is part of the <code>org.apache.spark.sql.execution.datasources.v2</code> package.</p> <pre><code>import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Implicits\n</code></pre>"},{"location":"connector/DataSourceV2Implicits/#__metadata_col","title":"__metadata_col <p><code>DataSourceV2Implicits</code> defines <code>__metadata_col</code> key that is used by the implicit classes:</p> <ul> <li>MetadataColumnsHelper when requested to asStruct</li> <li>MetadataColumnHelper when requested to isMetadataCol</li> </ul>","text":""},{"location":"connector/DataWriter/","title":"DataWriter","text":"<p><code>DataWriter</code> is an abstraction of data writers (of data of type <code>T</code>).</p>"},{"location":"connector/DataWriter/#contract","title":"Contract","text":""},{"location":"connector/DataWriter/#aborting-write","title":"Aborting Write <pre><code>void abort()\n</code></pre> <p>See KafkaDataWriter</p> <p>Used when:</p> <ul> <li><code>FileFormatWriter</code> utility is used to executeTask</li> <li><code>DataWritingSparkTask</code> utility is used to process a partition</li> <li><code>ContinuousWriteRDD</code> (Spark Structured Streaming) is requested to <code>compute</code> a partition</li> </ul>","text":""},{"location":"connector/DataWriter/#committing-successful-write","title":"Committing Successful Write <pre><code>WriterCommitMessage commit()\n</code></pre> <p>See KafkaDataWriter</p> <p>Used when:</p> <ul> <li><code>DataWritingSparkTask</code> utility is used to process a partition</li> <li><code>ContinuousWriteRDD</code> (Spark Structured Streaming) is requested to <code>compute</code> a partition</li> </ul>","text":""},{"location":"connector/DataWriter/#currentmetricsvalues","title":"currentMetricsValues <pre><code>CustomTaskMetric[] currentMetricsValues()\n</code></pre> <p>See KafkaDataWriter</p> <p>Used when:</p> <ul> <li><code>FileFormatWriter</code> utility is used to executeTask</li> <li><code>DataWritingSparkTask</code> utility is used to process a partition</li> <li><code>ContinuousWriteRDD</code> (Spark Structured Streaming) is requested to <code>compute</code> a partition</li> </ul>","text":""},{"location":"connector/DataWriter/#writing-out-record","title":"Writing Out Record <pre><code>void write(\n  T record)\n</code></pre> <p>See KafkaDataWriter</p> <p>Used when:</p> <ul> <li><code>DataWritingSparkTask</code> utility is used to process a partition</li> <li><code>ContinuousWriteRDD</code> (Spark Structured Streaming) is requested to <code>compute</code> a partition</li> </ul>","text":""},{"location":"connector/DataWriter/#implementations","title":"Implementations","text":"<ul> <li>FileFormatDataWriter</li> <li>KafkaDataWriter</li> <li>others</li> </ul>"},{"location":"connector/DataWriterFactory/","title":"DataWriterFactory","text":"<p><code>DataWriterFactory</code> is an abstraction of DataWriter factories.</p> <p><code>DataWriterFactory</code> is <code>Serializable</code>.</p>"},{"location":"connector/DataWriterFactory/#contract","title":"Contract","text":""},{"location":"connector/DataWriterFactory/#creating-datawriter","title":"Creating DataWriter <pre><code>DataWriter&lt;InternalRow&gt; createWriter(\n  int partitionId,\n  long taskId)\n</code></pre> <p>Creates a DataWriter (for the given <code>partitionId</code> and <code>taskId</code>)</p> <p>Used when:</p> <ul> <li><code>DataWritingSparkTask</code> is requested to run</li> </ul>","text":""},{"location":"connector/DataWriterFactory/#implementations","title":"Implementations","text":"<ul> <li>FileWriterFactory</li> <li><code>KafkaBatchWriterFactory</code></li> <li><code>MemoryWriterFactory</code> (Spark Structured Streaming)</li> <li><code>MicroBatchWriterFactory</code> (Spark Structured Streaming)</li> <li><code>NoopWriterFactory</code></li> </ul>"},{"location":"connector/Expression/","title":"Expression","text":"<p><code>Expression</code> is...FIXME</p>"},{"location":"connector/InputPartition/","title":"InputPartition","text":"<p><code>InputPartition</code> is an abstraction of input partitions in Connector API with optional location preferences.</p> <p><code>InputPartition</code> is a Java Serializable.</p>"},{"location":"connector/InputPartition/#contract","title":"Contract","text":""},{"location":"connector/InputPartition/#preferredlocations","title":"preferredLocations <pre><code>String[] preferredLocations()\n</code></pre> <p>Specifies preferred locations (executor hosts)</p> <p>By default, <code>preferredLocations</code> defines no location preferences (is simply empty).</p> <p>See:</p> <ul> <li>FilePartition</li> </ul> <p>Used when:</p> <ul> <li><code>FileScanRDD</code> is requested for preferred locations</li> <li><code>DataSourceRDD</code> is requested for preferred locations</li> <li><code>ContinuousDataSourceRDD</code> (Spark Structured Streaming) is requested for preferred locations</li> </ul>","text":""},{"location":"connector/InputPartition/#implementations","title":"Implementations","text":"<ul> <li><code>ContinuousMemoryStreamInputPartition</code></li> <li>FilePartition</li> <li><code>HasPartitionKey</code></li> <li><code>KafkaBatchInputPartition</code></li> <li><code>KafkaContinuousInputPartition</code></li> <li><code>MemoryStreamInputPartition</code></li> <li><code>RatePerMicroBatchStreamInputPartition</code></li> <li><code>RateStreamContinuousInputPartition</code></li> <li><code>RateStreamMicroBatchInputPartition</code></li> <li><code>TextSocketContinuousInputPartition</code></li> <li><code>TextSocketInputPartition</code></li> </ul>"},{"location":"connector/LocalScan/","title":"LocalScan","text":"<p><code>LocalScan</code> is an extension of the Scan abstraction for local scans.</p> <p><code>LocalScan</code> is planned as LocalTableScanExec physical operator at execution planning.</p>"},{"location":"connector/LocalScan/#contract","title":"Contract","text":""},{"location":"connector/LocalScan/#rows","title":"rows <pre><code>InternalRow[] rows()\n</code></pre> <p>Used when:</p> <ul> <li>DataSourceV2Strategy execution planning strategy is executed (on a DataSourceV2ScanRelation logical operator with a <code>LocalScan</code>)</li> </ul>","text":""},{"location":"connector/LocalScan/#implementations","title":"Implementations","text":"<p>Note</p> <p>No built-in implementations available.</p>"},{"location":"connector/MetadataColumn/","title":"MetadataColumn","text":"<p><code>MetadataColumn</code> is an abstraction of metadata columns that can expose additional metadata about a row.</p>"},{"location":"connector/MetadataColumn/#contract","title":"Contract","text":""},{"location":"connector/MetadataColumn/#comment","title":"comment <pre><code>String comment()\n</code></pre> <p>Documentation of this metadata column</p> <p>Default: <code>null</code></p>","text":""},{"location":"connector/MetadataColumn/#datatype","title":"dataType <pre><code>DataType dataType()\n</code></pre>","text":""},{"location":"connector/MetadataColumn/#isnullable","title":"isNullable <pre><code>boolean isNullable()\n</code></pre> <p>Default: <code>true</code></p>","text":""},{"location":"connector/MetadataColumn/#name","title":"Name <pre><code>String name()\n</code></pre> <p>Used when:</p> <ul> <li><code>MetadataColumnsHelper</code> implicit class is requested to asStruct</li> <li><code>DataSourceV2Relation</code> is requested for the metadata columns</li> <li><code>DescribeTableExec</code> is requested to addMetadataColumns</li> </ul>","text":""},{"location":"connector/MetadataColumn/#transform","title":"transform <pre><code>Transform transform()\n</code></pre> <p>Transform to produce values for this metadata column from data rows</p> <p>Default: <code>null</code></p>","text":""},{"location":"connector/MetadataColumn/#metadatacolumnshelper","title":"MetadataColumnsHelper <p><code>MetadataColumn</code>s can be converted (implicitly) to StructTypes or <code>AttributeReference</code>s using MetadataColumnsHelper implicit class.</p>","text":""},{"location":"connector/MetadataColumnHelper/","title":"MetadataColumnHelper Implicit Class","text":"<p><code>MetadataColumnHelper</code> is a Scala implicit class for Attribute.</p>"},{"location":"connector/MetadataColumnHelper/#creating-instance","title":"Creating Instance","text":"<p><code>MetadataColumnHelper</code> takes the following to be created:</p> <ul> <li> Attribute"},{"location":"connector/MetadataColumnHelper/#ismetadatacol","title":"isMetadataCol <pre><code>isMetadataCol: Boolean\n</code></pre> <p><code>isMetadataCol</code> takes the Metadata of the Attribute and checks if there is the __metadata_col key with <code>true</code> value.</p> <p><code>isMetadataCol</code>\u00a0is used when:</p> <ul> <li>AddMetadataColumns logical resolution rule is executed</li> </ul>","text":""},{"location":"connector/MetadataColumnsHelper/","title":"MetadataColumnsHelper Implicit Class","text":"<p><code>MetadataColumnsHelper</code> is a Scala implicit class for Array[MetadataColumn].</p>"},{"location":"connector/MetadataColumnsHelper/#creating-instance","title":"Creating Instance","text":"<p><code>MetadataColumnsHelper</code> takes the following to be created:</p> <ul> <li> MetadataColumns"},{"location":"connector/MetadataColumnsHelper/#asstruct","title":"asStruct <pre><code>asStruct: StructType\n</code></pre> <p><code>asStruct</code> creates a StructType for the MetadataColumns.</p> <p><code>asStruct</code>\u00a0is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"connector/OptionsHelper/","title":"OptionsHelper Implicit Class","text":"<p><code>OptionsHelper</code> is a Scala implicit class for Map[String, String].</p>"},{"location":"connector/OptionsHelper/#creating-instance","title":"Creating Instance","text":"<p><code>OptionsHelper</code> takes the following to be created:</p> <ul> <li> Options (<code>Map[String, String]</code>)"},{"location":"connector/OptionsHelper/#asoptions","title":"asOptions <pre><code>asOptions: CaseInsensitiveStringMap\n</code></pre> <p><code>asOptions</code> creates a new <code>CaseInsensitiveStringMap</code> with the options.</p> <p><code>asOptions</code>\u00a0is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"connector/PartitionReader/","title":"PartitionReader","text":"<p><code>PartitionReader</code> is an abstraction of partition readers (that PartitionReaderFactory creates for reading partitions in columnar or row-based fashion).</p>"},{"location":"connector/PartitionReader/#generic-interface","title":"Generic Interface","text":"<p><code>PartitionReader</code> is a generic interface with the following Java definition:</p> <pre><code>public interface PartitionReader&lt;T&gt;\n</code></pre> <p><code>PartitionReader</code> uses <code>T</code> as the name of the type parameter.</p> Generic Types <p>Find out more on generic types in The Java Tutorials.</p>"},{"location":"connector/PartitionReader/#contract","title":"Contract","text":""},{"location":"connector/PartitionReader/#next-record","title":"Next Record <pre><code>T get()\n</code></pre> <p>Retrieves the current record</p> <p>Used when:</p> <ul> <li><code>DataSourceRDD</code> is requested to compute a partition</li> <li><code>FilePartitionReader</code> is requested to <code>get</code> the current record</li> <li><code>PartitionReaderWithPartitionValues</code> is requested to <code>get</code> the current record</li> </ul>","text":""},{"location":"connector/PartitionReader/#proceeding-to-next-record","title":"Proceeding to Next Record <pre><code>boolean next()\n</code></pre> <p>Proceeds to next record if available (<code>true</code>) or <code>false</code></p> <p>Used when:</p> <ul> <li><code>DataSourceRDD</code> is requested to compute a partition</li> <li><code>FilePartitionReader</code> is requested to proceed to <code>next</code> record if available</li> <li><code>PartitionReaderWithPartitionValues</code> is requested to proceed to <code>next</code> record if available</li> </ul>","text":""},{"location":"connector/PartitionReader/#implementations","title":"Implementations","text":"<ul> <li>EmptyPartitionReader</li> <li>FilePartitionReader</li> <li>PartitionedFileReader</li> <li>PartitionReaderFromIterator</li> <li>PartitionReaderWithPartitionValues</li> <li>PartitionRecordReader</li> <li>others</li> </ul>"},{"location":"connector/PartitionReaderFactory/","title":"PartitionReaderFactory","text":"<p><code>PartitionReaderFactory</code> is an abstraction of partition reader factories that can create partition or columnar partition readers.</p>"},{"location":"connector/PartitionReaderFactory/#contract","title":"Contract","text":""},{"location":"connector/PartitionReaderFactory/#creating-columnar-partitionreader","title":"Creating Columnar PartitionReader <pre><code>PartitionReader&lt;ColumnarBatch&gt; createColumnarReader(\n    InputPartition partition)\n</code></pre> <p>Creates a PartitionReader for a columnar scan (to read data) from the given InputPartition</p> <p>By default, <code>createColumnarReader</code> throws an <code>UnsupportedOperationException</code>:</p> <pre><code>Cannot create columnar reader.\n</code></pre> <p>See:</p> <ul> <li>FilePartitionReaderFactory</li> </ul> <p>Used when:</p> <ul> <li><code>DataSourceRDD</code> is requested to compute a partition (with columnarReads enabled)</li> </ul>","text":""},{"location":"connector/PartitionReaderFactory/#creating-partitionreader","title":"Creating PartitionReader <pre><code>PartitionReader&lt;InternalRow&gt; createReader(\n    InputPartition partition)\n</code></pre> <p>Creates a PartitionReader for a row-based scan (to read data) from the given InputPartition</p> <p>Used when:</p> <ul> <li><code>DataSourceRDD</code> is requested to compute a partition</li> <li><code>ContinuousDataSourceRDD</code> (Spark Structured Streaming) is requested to <code>compute</code> a partition</li> </ul>","text":""},{"location":"connector/PartitionReaderFactory/#supportcolumnarreads","title":"supportColumnarReads <pre><code>boolean supportColumnarReads(\n    InputPartition partition)\n</code></pre> <p>Controls whether columnar scan can be used (and hence createColumnarReader) or not</p> <p>By default, <code>supportColumnarReads</code> indicates no support for columnar scans (and returns <code>false</code>).</p> <p>See:</p> <ul> <li>ParquetPartitionReaderFactory</li> </ul> <p>Used when:</p> <ul> <li><code>DataSourceV2ScanExecBase</code> is requested to supportsColumnar</li> </ul>","text":""},{"location":"connector/PartitionReaderFactory/#implementations","title":"Implementations","text":"<ul> <li><code>ContinuousPartitionReaderFactory</code></li> <li>FilePartitionReaderFactory</li> <li><code>KafkaBatchReaderFactory</code></li> <li><code>MemoryStreamReaderFactory</code></li> <li><code>RateStreamMicroBatchReaderFactory</code></li> </ul>"},{"location":"connector/PartitionSpecsHelper/","title":"PartitionSpecsHelper Implicit Class","text":"<p><code>PartitionSpecsHelper</code> is a Scala implicit class for Seq[PartitionSpec].</p>"},{"location":"connector/PartitionSpecsHelper/#creating-instance","title":"Creating Instance","text":"<p><code>PartitionSpecsHelper</code> takes the following to be created:</p> <ul> <li> <code>PartitionSpec</code>s"},{"location":"connector/Partitioning/","title":"Partitioning","text":"<p><code>Partitioning</code> is an abstraction of output data partitioning requirements (data distribution) of a Spark SQL connector.</p> <p>Note</p> <p>This <code>Partitioning</code> interface for Spark SQL developers mimics the internal Catalyst Partitioning that is converted into with the help of DataSourcePartitioning.</p>"},{"location":"connector/Partitioning/#contract","title":"Contract","text":""},{"location":"connector/Partitioning/#number-of-partitions","title":"Number of Partitions <pre><code>int numPartitions()\n</code></pre> <p>Used when:</p> <ul> <li>DataSourcePartitioning is requested for the number of partitions</li> </ul>","text":""},{"location":"connector/Partitioning/#satisfying-distribution","title":"Satisfying Distribution <pre><code>boolean satisfy(\n  Distribution distribution)\n</code></pre> <p>Used when:</p> <ul> <li>DataSourcePartitioning is asked whether it satisfies a given data distribution</li> </ul>","text":""},{"location":"connector/Partitioning/#implementations","title":"Implementations","text":"<ul> <li><code>KeyGroupedPartitioning</code></li> <li><code>UnknownPartitioning</code></li> </ul>"},{"location":"connector/Predicate/","title":"Predicate","text":"<p><code>Predicate</code> is...FIXME</p>"},{"location":"connector/RewritableTransform/","title":"RewritableTransform","text":"<p><code>RewritableTransform</code> is...FIXME</p>"},{"location":"connector/Scan/","title":"Scan","text":"<p><code>Scan</code> is an abstraction of logical scans over data sources.</p>"},{"location":"connector/Scan/#contract","title":"Contract","text":""},{"location":"connector/Scan/#description","title":"Description <pre><code>String description()\n</code></pre> <p>Human-readable description of this scan (e.g. for logging purposes).</p> <p>default: the fully-qualified class name</p> <p>Used when:</p> <ul> <li><code>BatchScanExec</code> physical operator is requested for the simpleString</li> <li><code>DataSourceV2ScanExecBase</code> physical operator is requested for the simpleString and verboseStringWithOperatorId</li> </ul>","text":""},{"location":"connector/Scan/#read-schema","title":"Read Schema <pre><code>StructType readSchema()\n</code></pre> <p>Read schema of this scan</p> <p>Used when:</p> <ul> <li><code>FileScan</code> is requested for the partition and data filters</li> <li><code>GroupBasedRowLevelOperationScanPlanning</code> is executed</li> <li><code>PushDownUtils</code> utility is used to pruneColumns</li> <li>V2ScanRelationPushDown logical optimization is executed (and requested to pushDownAggregates)</li> </ul>","text":""},{"location":"connector/Scan/#supported-custom-metrics","title":"Supported Custom Metrics <pre><code>CustomMetric[] supportedCustomMetrics()\n</code></pre> <p>CustomMetrics</p> <p>Empty by default and expected to be overriden by implementations</p> <p>See:</p> <ul> <li>KafkaScan</li> </ul> <p>Used when:</p> <ul> <li><code>DataSourceV2ScanExecBase</code> physical operator is requested for the custom metrics</li> </ul>","text":""},{"location":"connector/Scan/#physical-representation-for-batch-query","title":"Physical Representation for Batch Query <pre><code>Batch toBatch()\n</code></pre> <p>By default, <code>toBatch</code> throws an <code>UnsupportedOperationException</code> (with the description):</p> <pre><code>[description]: Batch scan are not supported\n</code></pre> <p>See:</p> <ul> <li>FileScan</li> </ul>  <p>Must be implemented (overriden), if the Table that created this <code>Scan</code> has BATCH_READ capability (among the capabilities).</p>  <p>Used when:</p> <ul> <li><code>BatchScanExec</code> physical operator is requested for the Batch and the filteredPartitions</li> </ul>","text":""},{"location":"connector/Scan/#converting-to-continuousstream","title":"Converting to ContinuousStream <pre><code>ContinuousStream toContinuousStream(\n    String checkpointLocation)\n</code></pre> <p>By default, <code>toContinuousStream</code> throws an <code>UnsupportedOperationException</code> (with the description):</p> <pre><code>[description]: Continuous scan are not supported\n</code></pre>  <p>Must be implemented (overriden), if the Table that created this <code>Scan</code> has CONTINUOUS_READ capability (among the capabilities).</p>  <p>Used when:</p> <ul> <li><code>ContinuousExecution</code> (Spark Structured Streaming) is requested for the logical plan (WriteToContinuousDataSource)</li> </ul>","text":""},{"location":"connector/Scan/#converting-to-microbatchstream","title":"Converting to MicroBatchStream <pre><code>MicroBatchStream toMicroBatchStream(\n    String checkpointLocation)\n</code></pre> <p>By default, <code>toMicroBatchStream</code> throws an <code>UnsupportedOperationException</code> (with the description):</p> <pre><code>[description]: Micro-batch scan are not supported\n</code></pre>  <p>Must be implemented (overriden), if the Table that created this <code>Scan</code> has MICRO_BATCH_READ capability (among the capabilities).</p>  <p>Used when:</p> <ul> <li><code>MicroBatchExecution</code> (Spark Structured Streaming) is requested for the logical plan</li> </ul>","text":""},{"location":"connector/Scan/#implementations","title":"Implementations","text":"<ul> <li>FileScan</li> <li>KafkaScan</li> <li>SupportsReportPartitioning</li> <li>SupportsReportStatistics</li> <li>V1Scan</li> <li>others</li> </ul>"},{"location":"connector/ScanBuilder/","title":"ScanBuilder","text":"<p><code>ScanBuilder</code> is an abstraction of scan builders.</p>"},{"location":"connector/ScanBuilder/#contract","title":"Contract","text":""},{"location":"connector/ScanBuilder/#building-scan","title":"Building Scan <pre><code>Scan build()\n</code></pre> <p>Builds a Scan</p> <p>See:</p> <ul> <li>ParquetScanBuilder</li> </ul> <p>Used when:</p> <ul> <li><code>DataSourceV2Relation</code> logical operator is requested to computeStats</li> <li><code>DescribeColumnExec</code> physical command is executed</li> <li><code>PushDownUtils</code> is requested to pruneColumns</li> <li>V2ScanRelationPushDown logical optimization is executed (to buildScanWithPushedAggregate)</li> <li><code>MicroBatchExecution</code> (Spark Structured Streaming) is requested for the <code>logicalPlan</code></li> <li><code>ContinuousExecution</code> (Spark Structured Streaming) is requested for the <code>logicalPlan</code></li> </ul>","text":""},{"location":"connector/ScanBuilder/#implementations","title":"Implementations","text":"<ul> <li>FileScanBuilder</li> <li>JDBCScanBuilder</li> <li><code>MemoryStreamScanBuilder</code> (Spark Structured Streaming)</li> <li>SupportsPushDownAggregates</li> <li>SupportsPushDownFilters</li> <li><code>SupportsPushDownLimit</code></li> <li>SupportsPushDownRequiredColumns</li> <li><code>SupportsPushDownTableSample</code></li> <li><code>SupportsPushDownTopN</code></li> <li>SupportsPushDownV2Filters</li> </ul>"},{"location":"connector/SessionConfigSupport/","title":"SessionConfigSupport","text":"<p><code>SessionConfigSupport</code> is an extension of the TableProvider abstraction for table providers that use custom key prefix for spark.datasource configuration options.</p> <p><code>SessionConfigSupport</code> connectors can be configured by additional (session-scoped) configuration options that are specified in SparkSession to extend user-defined options.</p>"},{"location":"connector/SessionConfigSupport/#contract","title":"Contract","text":""},{"location":"connector/SessionConfigSupport/#configuration-key-prefix","title":"Configuration Key Prefix <pre><code>String keyPrefix()\n</code></pre> <p>The prefix of the configuration keys of this connector that is added to <code>spark.datasource</code> prefix</p> <pre><code>spark.datasource.[keyPrefix]\n</code></pre> <p>Must not be <code>null</code></p> <p>Used when:</p> <ul> <li><code>DataSourceV2Utils</code> is requested to extract session configuration options</li> </ul>","text":""},{"location":"connector/SessionConfigSupport/#implementations","title":"Implementations","text":"<p>Note</p> <p>No built-in implementations available.</p>"},{"location":"connector/SimpleTableProvider/","title":"SimpleTableProvider","text":"<p><code>SimpleTableProvider</code>\u00a0is an extension of the TableProvider abstraction for table providers that do not support custom table schema and partitioning.</p>"},{"location":"connector/SimpleTableProvider/#contract","title":"Contract","text":""},{"location":"connector/SimpleTableProvider/#creating-table","title":"Creating Table <pre><code>getTable(\n  options: CaseInsensitiveStringMap): Table\n</code></pre> <p>Creates a Table</p> <p>Used for getOrLoadTable</p>","text":""},{"location":"connector/SimpleTableProvider/#implementations","title":"Implementations","text":"<ul> <li><code>ConsoleSinkProvider</code> (Spark Structured Streaming)</li> <li>KafkaSourceProvider</li> <li><code>MemoryStreamTableProvider</code> (Spark Structured Streaming)</li> <li>NoopDataSource</li> <li><code>RateStreamProvider</code> (Spark Structured Streaming)</li> <li><code>TextSocketSourceProvider</code> (Spark Structured Streaming)</li> </ul>"},{"location":"connector/SimpleTableProvider/#inferring-schema","title":"Inferring Schema <pre><code>inferSchema(\n  options: CaseInsensitiveStringMap): StructType\n</code></pre> <p><code>inferSchema</code> creates a table and requests it for the schema.</p> <p><code>inferSchema</code>\u00a0is part of the TableProvider abstraction.</p>","text":""},{"location":"connector/SimpleTableProvider/#creating-table_1","title":"Creating Table <pre><code>getTable(\n  schema: StructType,\n  partitioning: Array[Transform],\n  properties: util.Map[String, String]): Table\n</code></pre> <p><code>getTable</code> creates a table (with the given <code>properties</code>).</p> <p><code>getTable</code> asserts that there are no <code>Transform</code>s in given <code>partitioning</code>.</p> <p><code>getTable</code>\u00a0is part of the TableProvider abstraction.</p>","text":""},{"location":"connector/SimpleTableProvider/#looking-up-table-once","title":"Looking Up Table Once <pre><code>getOrLoadTable(\n  options: CaseInsensitiveStringMap): Table\n</code></pre> <p><code>getOrLoadTable</code> creates a table and caches it for later use (in an internal variable).</p>","text":""},{"location":"connector/StagedTable/","title":"StagedTable","text":"<p><code>StagedTable</code>\u00a0is an extension of the Table abstraction for tables that can abort or commit staged changes.</p>"},{"location":"connector/StagedTable/#contract","title":"Contract","text":""},{"location":"connector/StagedTable/#abortstagedchanges","title":"abortStagedChanges <pre><code>void abortStagedChanges()\n</code></pre> <p>Used when:</p> <ul> <li><code>AtomicReplaceTableExec</code> physical command is executed</li> <li><code>TableWriteExecHelper</code> is requested to writeToTable</li> </ul>","text":""},{"location":"connector/StagedTable/#commitstagedchanges","title":"commitStagedChanges <pre><code>void commitStagedChanges()\n</code></pre> <p>Used when:</p> <ul> <li><code>AtomicReplaceTableExec</code> physical command is <code>executed</code></li> <li><code>TableWriteExecHelper</code> is requested to writeToTable</li> </ul>","text":""},{"location":"connector/SupportsAtomicPartitionManagement/","title":"SupportsAtomicPartitionManagement","text":"<p><code>SupportsAtomicPartitionManagement</code>\u00a0is an extension of the SupportsPartitionManagement abstraction for partitioned tables.</p>"},{"location":"connector/SupportsAtomicPartitionManagement/#contract","title":"Contract","text":""},{"location":"connector/SupportsAtomicPartitionManagement/#createpartitions","title":"createPartitions <pre><code>void createPartitions(\n  InternalRow[] idents,\n  Map&lt;String, String&gt;[] properties)\n</code></pre> <p>Used when:</p> <ul> <li><code>AlterTableAddPartitionExec</code> physical operator is executed</li> </ul>","text":""},{"location":"connector/SupportsAtomicPartitionManagement/#droppartitions","title":"dropPartitions <pre><code>boolean dropPartitions(\n  InternalRow[] idents)\n</code></pre> <p>Used when:</p> <ul> <li><code>AlterTableDropPartitionExec</code> physical operator is executed</li> </ul>","text":""},{"location":"connector/SupportsDelete/","title":"SupportsDelete","text":"<p><code>SupportsDelete</code> is...FIXME</p>"},{"location":"connector/SupportsDeleteV2/","title":"SupportsDeleteV2 Tables","text":"<p><code>SupportsDeleteV2</code> is an extension of the TruncatableTable abstraction for truncatable tables that can deleteWhere.</p>"},{"location":"connector/SupportsDeleteV2/#contract","title":"Contract","text":""},{"location":"connector/SupportsDeleteV2/#deletewhere","title":"deleteWhere <pre><code>void deleteWhere(\n  Predicate[] predicates)\n</code></pre> <p>See:</p> <ul> <li>SupportsDelete</li> </ul> <p>Used when:</p> <ul> <li><code>SupportsDeleteV2</code> is requested to truncateTable (with canDeleteWhere enabled)</li> <li>DeleteFromTableExec physical operator is executed</li> </ul>","text":""},{"location":"connector/SupportsDeleteV2/#implementations","title":"Implementations","text":"<ul> <li>SupportsDelete</li> </ul>"},{"location":"connector/SupportsDeleteV2/#candeletewhere","title":"canDeleteWhere <pre><code>boolean canDeleteWhere(\n    Predicate[] predicates)\n</code></pre> <p><code>canDeleteWhere</code> is enabled (<code>true</code>).</p>  <p><code>canDeleteWhere</code> is used when:</p> <ul> <li><code>SupportsDeleteV2</code> is requested to truncateTable</li> <li>DataSourceV2Strategy execution planning strategy is executed (to plan a DeleteFromTable over a <code>SupportsDeleteV2</code>)</li> <li><code>OptimizeMetadataOnlyDeleteFromTable</code> logical optimization is executed</li> </ul>","text":""},{"location":"connector/SupportsDynamicOverwrite/","title":"SupportsDynamicOverwrite","text":"<p><code>SupportsDynamicOverwrite</code> is...FIXME</p>"},{"location":"connector/SupportsMetadata/","title":"SupportsMetadata","text":"<p><code>SupportsMetadata</code> is an abstraction of file-based scan operators that can report custom metadata for formatted explain (e.g., Dataset.explain or EXPLAIN FORMATTED SQL statement).</p> Spark ShellSpark SQL CLI <pre><code>val query = spark.read.parquet(\"demo.parquet\")\nquery.explain(mode = \"formatted\")\n</code></pre> <pre><code>== Physical Plan ==\n* ColumnarToRow (2)\n+- Scan parquet  (1)\n\n\n(1) Scan parquet\nOutput [1]: [id#14L]\nBatched: true\nLocation: InMemoryFileIndex [file:/Users/jacek/dev/oss/spark/demo.parquet]\nReadSchema: struct&lt;id:bigint&gt;\n\n(2) ColumnarToRow [codegen id : 1]\nInput [1]: [id#14L]\n</code></pre> <pre><code>EXPLAIN FORMATTED\nSELECT * FROM parquet.`demo.parquet`;\n</code></pre> <pre><code>== Physical Plan ==\n\n* ColumnarToRow (2)\n+- Scan parquet  (1)\n\n(1) Scan parquet\nOutput [1]: [id#22L]\nBatched: true\nLocation: InMemoryFileIndex [file:/Users/jacek/dev/oss/spark/demo.parquet]\nReadSchema: struct&lt;id:bigint&gt;\n\n(2) ColumnarToRow [codegen id : 1]\nInput [1]: [id#22L]\n</code></pre>"},{"location":"connector/SupportsMetadata/#contract","title":"Contract","text":""},{"location":"connector/SupportsMetadata/#custom-metadata","title":"Custom Metadata <pre><code>getMetaData(): Map[String, String]\n</code></pre> <p>See:</p> <ul> <li>FileScan</li> <li>ParquetScan</li> </ul> <p>Used when:</p> <ul> <li>DataSourceV2ScanExecBase physical operator is executed (and verboseStringWithOperatorId)</li> </ul>","text":""},{"location":"connector/SupportsMetadata/#implementations","title":"Implementations","text":"<ul> <li>FileScan</li> </ul>"},{"location":"connector/SupportsMetadataColumns/","title":"SupportsMetadataColumns","text":"<p><code>SupportsMetadataColumns</code>\u00a0is an extension of the Table abstraction for tables with metadata columns.</p>"},{"location":"connector/SupportsMetadataColumns/#contract","title":"Contract","text":""},{"location":"connector/SupportsMetadataColumns/#metadatacolumns","title":"metadataColumns <pre><code>MetadataColumn[] metadataColumns()\n</code></pre> <p>MetadataColumns of this table</p> <p>Used when:</p> <ul> <li><code>DataSourceV2Relation</code> is requested for the metadata output</li> <li>DescribeTableExec physical command is executed</li> </ul>","text":""},{"location":"connector/SupportsOverwrite/","title":"SupportsOverwrite","text":"<p><code>SupportsOverwrite</code> is...FIXME</p>"},{"location":"connector/SupportsPartitionManagement/","title":"SupportsPartitionManagement","text":"<p><code>SupportsPartitionManagement</code>\u00a0is an extension of the Table abstraction for partitioned tables.</p>"},{"location":"connector/SupportsPartitionManagement/#contract","title":"Contract","text":""},{"location":"connector/SupportsPartitionManagement/#createpartition","title":"createPartition <pre><code>void createPartition(\n  InternalRow ident,\n  Map&lt;String, String&gt; properties)\n</code></pre> <p>Used when:</p> <ul> <li><code>AlterTableAddPartitionExec</code> physical command is executed</li> </ul>","text":""},{"location":"connector/SupportsPartitionManagement/#droppartition","title":"dropPartition <pre><code>boolean dropPartition(\n  InternalRow ident)\n</code></pre> <p>Used when:</p> <ul> <li><code>AlterTableDropPartitionExec</code> physical command is executed</li> </ul>","text":""},{"location":"connector/SupportsPartitionManagement/#listpartitionidentifiers","title":"listPartitionIdentifiers <pre><code>InternalRow[] listPartitionIdentifiers(\n  String[] names,\n  InternalRow ident)\n</code></pre> <p>Used when:</p> <ul> <li><code>ShowPartitionsExec</code> physical command is executed</li> </ul>","text":""},{"location":"connector/SupportsPartitionManagement/#loadpartitionmetadata","title":"loadPartitionMetadata <pre><code>Map&lt;String, String&gt; loadPartitionMetadata(\n  InternalRow ident)\n</code></pre>","text":""},{"location":"connector/SupportsPartitionManagement/#partitionexists","title":"partitionExists <pre><code>boolean partitionExists(\n  InternalRow ident)\n</code></pre> <p>Used when:</p> <ul> <li><code>AlterTableAddPartitionExec</code> and <code>AlterTableDropPartitionExec</code> physical commands are executed</li> </ul>","text":""},{"location":"connector/SupportsPartitionManagement/#partitionschema","title":"partitionSchema <pre><code>StructType partitionSchema()\n</code></pre> <p>Used when:</p> <ul> <li><code>AlterTableAddPartitionExec</code>, <code>AlterTableDropPartitionExec</code> and <code>ShowPartitionsExec</code> physical commands are executed</li> <li><code>SupportsPartitionManagement</code> is requested to partitionExists</li> <li><code>CheckAnalysis</code> is requested to checkShowPartitions</li> <li><code>ResolvePartitionSpec</code> logical analysis rule is executed</li> </ul>","text":""},{"location":"connector/SupportsPartitionManagement/#replacepartitionmetadata","title":"replacePartitionMetadata <pre><code>void replacePartitionMetadata(\n  InternalRow ident,\n  Map&lt;String, String&gt; properties)\n</code></pre>","text":""},{"location":"connector/SupportsPartitionManagement/#implementations","title":"Implementations","text":"<ul> <li>SupportsAtomicPartitionManagement</li> </ul>"},{"location":"connector/SupportsPushDownAggregates/","title":"SupportsPushDownAggregates","text":"<p><code>SupportsPushDownAggregates</code> is an extension of the ScanBuilder abstraction for scan builders with support for complete aggregate push-down optimization.</p>"},{"location":"connector/SupportsPushDownAggregates/#contract","title":"Contract","text":""},{"location":"connector/SupportsPushDownAggregates/#pushaggregation","title":"pushAggregation <pre><code>boolean pushAggregation(\n  Aggregation aggregation)\n</code></pre> <p>See:</p> <ul> <li>JDBCScanBuilder</li> <li>ParquetScanBuilder</li> </ul> <p>Used when:</p> <ul> <li>V2ScanRelationPushDown logical optimization is executed (to rewriteAggregate)</li> </ul>","text":""},{"location":"connector/SupportsPushDownAggregates/#supportcompletepushdown","title":"supportCompletePushDown <pre><code>boolean supportCompletePushDown(\n  Aggregation aggregation)\n</code></pre> <p>Default: <code>false</code></p> <p>See:</p> <ul> <li>JDBCScanBuilder</li> </ul> <p>Used when:</p> <ul> <li>V2ScanRelationPushDown logical optimization is executed (to rewriteAggregate)</li> </ul>","text":""},{"location":"connector/SupportsPushDownAggregates/#implementations","title":"Implementations","text":"<ul> <li>JDBCScanBuilder</li> <li><code>OrcScanBuilder</code></li> <li>ParquetScanBuilder</li> </ul>"},{"location":"connector/SupportsPushDownFilters/","title":"SupportsPushDownFilters","text":"<p><code>SupportsPushDownFilters</code>\u00a0is an extension of the ScanBuilder abstraction for scan builders that can pushFilters and pushedFilters (for filter pushdown performance optimization and thus reduce the size of the data to be read).</p> <p>Obsolete as of Spark 3.3.0</p> <p>SupportsPushDownV2Filters is preferred as it uses the modern Predicate expression.</p>"},{"location":"connector/SupportsPushDownFilters/#contract","title":"Contract","text":""},{"location":"connector/SupportsPushDownFilters/#pushedfilters","title":"pushedFilters <pre><code>Filter[] pushedFilters()\n</code></pre> <p>Data source filters that were pushed down to the data source (in pushFilters)</p> <p>Used when:</p> <ul> <li><code>PushDownUtils</code> is requested to pushFilters</li> <li>V2ScanRelationPushDown logical optimization is executed (and getWrappedScan for V1Scans)</li> </ul>","text":""},{"location":"connector/SupportsPushDownFilters/#pushfilters","title":"pushFilters <pre><code>Filter[] pushFilters(\n  Filter[] filters)\n</code></pre> <p>Data source filters that need to be evaluated again after scanning (so Spark can plan an extra filter operator)</p> <p>Used when:</p> <ul> <li><code>PushDownUtils</code> is requested to pushFilters</li> </ul>","text":""},{"location":"connector/SupportsPushDownFilters/#implementations","title":"Implementations","text":"<p>Note</p> <p>No built-in implementations available.</p>"},{"location":"connector/SupportsPushDownRequiredColumns/","title":"SupportsPushDownRequiredColumns","text":"<p><code>SupportsPushDownRequiredColumns</code> is...FIXME</p>"},{"location":"connector/SupportsPushDownV2Filters/","title":"SupportsPushDownV2Filters","text":"<p><code>SupportsPushDownV2Filters</code> is an extension of the ScanBuilder abstraction for scan builders that support pushPredicates (using \"modern\" Predicates).</p>"},{"location":"connector/SupportsPushDownV2Filters/#contract","title":"Contract","text":""},{"location":"connector/SupportsPushDownV2Filters/#pushedpredicates","title":"pushedPredicates <pre><code>Predicate[] pushedPredicates()\n</code></pre> <p>Used when:</p> <ul> <li><code>PushDownUtils</code> is requested to pushFilters</li> </ul>","text":""},{"location":"connector/SupportsPushDownV2Filters/#pushpredicates","title":"pushPredicates <pre><code>Predicate[] pushPredicates(\n  Predicate[] predicates)\n</code></pre> <p>Used when:</p> <ul> <li><code>PushDownUtils</code> is requested to pushFilters</li> </ul>","text":""},{"location":"connector/SupportsPushDownV2Filters/#implementations","title":"Implementations","text":"<ul> <li>JDBCScanBuilder</li> </ul>"},{"location":"connector/SupportsRead/","title":"SupportsRead Tables","text":"<p><code>SupportsRead</code>\u00a0is an extension of the Table abstraction for readable tables.</p>"},{"location":"connector/SupportsRead/#contract","title":"Contract","text":""},{"location":"connector/SupportsRead/#creating-scanbuilder","title":"Creating ScanBuilder <pre><code>ScanBuilder newScanBuilder(\n  CaseInsensitiveStringMap options)\n</code></pre> <p>Creates a ScanBuilder</p> <p>See:</p> <ul> <li>ParquetTable</li> </ul> <p>Used when:</p> <ul> <li><code>DataSourceV2Relation</code> logical operator is requested to computeStats</li> <li>GroupBasedRowLevelOperationScanPlanning logical optimization is executed</li> <li>V2ScanRelationPushDown logical optimization is executed</li> <li><code>MicroBatchExecution</code> (Spark Structured Streaming) is requested for a logical query plan</li> <li><code>ContinuousExecution</code> (Spark Structured Streaming) is created (and initializes a logical query plan)</li> </ul>","text":""},{"location":"connector/SupportsRead/#implementations","title":"Implementations","text":"<ul> <li>FileTable</li> <li><code>JDBCTable</code></li> <li>KafkaTable</li> <li><code>MemoryStreamTable</code> (Spark Structured Streaming)</li> <li><code>RatePerMicroBatchTable</code> (Spark Structured Streaming)</li> <li><code>RateStreamTable</code> (Spark Structured Streaming)</li> <li><code>RowLevelOperationTable</code></li> <li><code>TextSocketTable</code> (Spark Structured Streaming)</li> </ul>"},{"location":"connector/SupportsReportOrdering/","title":"SupportsReportOrdering","text":"<p><code>SupportsReportOrdering</code> is an extension of the Scan abstraction for scans that report the order of data (in each partition).</p>"},{"location":"connector/SupportsReportOrdering/#contract","title":"Contract","text":""},{"location":"connector/SupportsReportOrdering/#outputordering","title":"outputOrdering <pre><code>SortOrder[] outputOrdering()\n</code></pre> <p>SortOrders of the output ordering of this scan</p> <p>Used when:</p> <ul> <li><code>V2ScanPartitioningAndOrdering</code> logical optimization is executed (and <code>ordering</code>)</li> </ul>","text":""},{"location":"connector/SupportsReportOrdering/#implementations","title":"Implementations","text":"<p>Note</p> <p>No built-in implementations available.</p>"},{"location":"connector/SupportsReportPartitioning/","title":"SupportsReportPartitioning","text":"<p><code>SupportsReportPartitioning</code> is an extension of the Scan abstraction for scans with custom data partitioning.</p>"},{"location":"connector/SupportsReportPartitioning/#contract","title":"Contract","text":""},{"location":"connector/SupportsReportPartitioning/#outputordering","title":"outputOrdering <pre><code>Partitioning outputPartitioning()\n</code></pre> <p>Partitionings of the data of this scan</p> <p>Used when:</p> <ul> <li><code>V2ScanPartitioningAndOrdering</code> logical optimization is executed (and <code>partitioning</code>)</li> </ul>","text":""},{"location":"connector/SupportsReportPartitioning/#implementations","title":"Implementations","text":"<p>Note</p> <p>No built-in implementations available.</p>"},{"location":"connector/SupportsReportStatistics/","title":"SupportsReportStatistics","text":"<p><code>SupportsReportStatistics</code> is...FIXME</p>"},{"location":"connector/SupportsRowLevelOperations/","title":"SupportsRowLevelOperations Tables","text":"<p><code>SupportsRowLevelOperations</code> is an extension of the Table abstraction for tables that can create a new RowLevelOperationBuilder.</p>"},{"location":"connector/SupportsRowLevelOperations/#contract","title":"Contract","text":""},{"location":"connector/SupportsRowLevelOperations/#newrowleveloperationbuilder","title":"newRowLevelOperationBuilder <pre><code>RowLevelOperationBuilder newRowLevelOperationBuilder(\n  RowLevelOperationInfo info)\n</code></pre> <p><code>RowLevelOperationBuilder</code> for the <code>RowLevelOperationInfo</code></p> <p>Used when:</p> <ul> <li><code>RewriteRowLevelCommand</code> analysis rule is requested to buildOperationTable</li> </ul>","text":""},{"location":"connector/SupportsRowLevelOperations/#implementations","title":"Implementations","text":"<p>Note</p> <p>No built-in implementations available.</p>"},{"location":"connector/SupportsStreamingUpdate/","title":"SupportsStreamingUpdate","text":"<p><code>SupportsStreamingUpdate</code>\u00a0is an extension of the WriteBuilder abstraction for tables that support streaming update.</p>"},{"location":"connector/SupportsStreamingUpdate/#contract","title":"Contract","text":""},{"location":"connector/SupportsStreamingUpdate/#streaming-update","title":"Streaming Update <pre><code>update(): WriteBuilder\n</code></pre> <p>WriteBuilder</p> <p>Used when:</p> <ul> <li><code>StreamExecution</code> stream execution engine (Spark Structured Streaming) is requested to <code>createStreamingWrite</code></li> </ul>","text":""},{"location":"connector/SupportsStreamingUpdate/#implementations","title":"Implementations","text":"<ul> <li>ConsoleTable</li> <li>ForeachWriterTable</li> <li>KafkaTable</li> <li>MemorySink</li> <li>NoopWriteBuilder</li> </ul>"},{"location":"connector/SupportsTruncate/","title":"SupportsTruncate","text":"<p><code>SupportsTruncate</code>\u00a0is an extension of the WriteBuilder abstraction for tables that support truncation.</p>"},{"location":"connector/SupportsTruncate/#contract","title":"Contract","text":""},{"location":"connector/SupportsTruncate/#truncation","title":"Truncation <pre><code>WriteBuilder truncate()\n</code></pre> <p>WriteBuilder that can replace all existing data with data committed in the write</p> <p>Used when:</p> <ul> <li>OverwriteByExpressionExec and <code>OverwriteByExpressionExecV1</code> physical operators are executed</li> <li><code>StreamExecution</code> stream execution engine (Spark Structured Streaming) is requested to <code>createStreamingWrite</code></li> </ul>","text":""},{"location":"connector/SupportsTruncate/#implementations","title":"Implementations","text":"<ul> <li>ConsoleTable (Spark Structured Streaming)</li> <li>ForeachWriterTable (Spark Structured Streaming)</li> <li>KafkaTable</li> <li>MemorySink (Spark Structured Streaming)</li> <li>NoopWriteBuilder</li> <li>SupportsOverwrite</li> </ul>"},{"location":"connector/SupportsWrite/","title":"SupportsWrite Tables","text":"<p><code>SupportsWrite</code>\u00a0is an extension of the Table abstraction for writable tables.</p>"},{"location":"connector/SupportsWrite/#contract","title":"Contract","text":""},{"location":"connector/SupportsWrite/#creating-writebuilder","title":"Creating WriteBuilder <pre><code>WriteBuilder newWriteBuilder(\n  LogicalWriteInfo info)\n</code></pre> <p>Creates a WriteBuilder for writing (batch and streaming)</p> <p>See:</p> <ul> <li>ParquetTable</li> </ul> <p>Used when:</p> <ul> <li><code>V1FallbackWriters</code> physical operator is requested to <code>newWriteBuilder</code></li> <li>CreateTableAsSelectExec, <code>ReplaceTableAsSelectExec</code> physical commands are executed</li> <li><code>BatchWriteHelper</code> physical operator is requested to newWriteBuilder</li> <li><code>AtomicTableWriteExec</code> physical command is requested to writeToStagedTable</li> <li><code>StreamExecution</code> stream execution engine (Spark Structured Streaming) is requested to <code>createStreamingWrite</code></li> </ul>","text":""},{"location":"connector/SupportsWrite/#implementations","title":"Implementations","text":"<ul> <li>ConsoleTable (Spark Structured Streaming)</li> <li>FileTable</li> <li>ForeachWriterTable (Spark Structured Streaming)</li> <li>KafkaTable</li> <li>MemorySink (Spark Structured Streaming)</li> <li>NoopTable</li> </ul>"},{"location":"connector/Table/","title":"Table","text":"<p><code>Table</code> is an abstraction of logical structured data set:</p> <ul> <li>a directory or files on a file system</li> <li>a topic of Apache Kafka</li> <li>a table in a catalog</li> </ul> <p><code>Table</code> can be loaded using TableCatalog.</p>"},{"location":"connector/Table/#contract","title":"Contract","text":""},{"location":"connector/Table/#capabilities","title":"Capabilities <pre><code>Set&lt;TableCapability&gt; capabilities()\n</code></pre> <p>TableCapabilities of the table</p> <p>Used when <code>Table</code> is asked whether or not it supports a given capability</p>","text":""},{"location":"connector/Table/#columns","title":"Columns <pre><code>Column[] columns()\n</code></pre> <p>Columns of this table</p> <p>Default: structTypeToV2Columns based on the schema</p> <p>Used when <code>Table</code> is asked whether or not it supports a given capability</p>","text":""},{"location":"connector/Table/#name","title":"Name <pre><code>String name()\n</code></pre> <p>Name of the table</p>","text":""},{"location":"connector/Table/#partitioning","title":"Partitioning <pre><code>Transform[] partitioning()\n</code></pre> <p>Partitions of the table (as Transforms)</p> <p>Default: (empty)</p> <p>Used when:</p> <ul> <li>ResolveInsertInto logical analysis rule is executed</li> <li><code>DataFrameWriter</code> is requested to insertInto and save</li> <li>DescribeTableExec physical operator is executed</li> </ul>","text":""},{"location":"connector/Table/#properties","title":"Properties <pre><code>Map&lt;String, String&gt; properties()\n</code></pre> <p>Table properties</p> <p>Default: (empty)</p> <p>Used when:</p> <ul> <li>DescribeTableExec and ShowTablePropertiesExec physical operators are executed</li> </ul>","text":""},{"location":"connector/Table/#schema","title":"Schema <pre><code>StructType schema()\n</code></pre>  <p>Deprecated</p> <p>Use columns instead.</p>  <p>StructType of the table</p> <p>Used when:</p> <ul> <li><code>DataSourceV2Relation</code> utility is used to create a DataSourceV2Relation logical operator</li> <li><code>SimpleTableProvider</code> is requested to inferSchema</li> <li>DataSourceV2Strategy execution planning strategy is executed</li> <li>DescribeTableExec physical operator is executed</li> <li><code>FileDataSourceV2</code> is requested to inferSchema</li> <li>(Spark Structured Streaming) <code>TextSocketTable</code> is requested for a <code>ScanBuilder</code> with a read schema</li> <li>(Spark Structured Streaming) <code>DataStreamReader</code> is requested to load data</li> </ul>","text":""},{"location":"connector/Table/#implementations","title":"Implementations","text":"<ul> <li>FileTable</li> <li>KafkaTable</li> <li>NoopTable</li> <li>StagedTable</li> <li>SupportsRead</li> <li>SupportsWrite</li> <li>V1Table</li> <li>others</li> </ul>"},{"location":"connector/TableCapability/","title":"TableCapability","text":"<p><code>TableCapability</code> represents capabilities that can be provided (supported) by a Table.</p>"},{"location":"connector/TableCapability/#accept_any_schema","title":"ACCEPT_ANY_SCHEMA <p>Indicates that a table accepts input of any schema in a write operation</p> <p>Used when:</p> <ul> <li><code>DataSourceV2Relation</code> is requested to skipSchemaResolution</li> <li><code>KafkaTable</code> is requested for the capabilities</li> <li><code>NoopTable</code> is requested for the capabilities</li> </ul>","text":""},{"location":"connector/TableCapability/#batch_read","title":"BATCH_READ","text":""},{"location":"connector/TableCapability/#batch_write","title":"BATCH_WRITE","text":""},{"location":"connector/TableCapability/#continuous_read","title":"CONTINUOUS_READ","text":""},{"location":"connector/TableCapability/#micro_batch_read","title":"MICRO_BATCH_READ <p>Marks the table to support reads / scans in micro-batch streaming execution mode</p> <p>Used when:</p> <ul> <li><code>MicroBatchExecution</code> stream execution engine is requested for a logical query plan</li> <li><code>DataStreamReader</code> is requested to load data</li> </ul>","text":""},{"location":"connector/TableCapability/#overwrite_by_filter","title":"OVERWRITE_BY_FILTER","text":""},{"location":"connector/TableCapability/#overwrite_dynamic","title":"OVERWRITE_DYNAMIC","text":""},{"location":"connector/TableCapability/#streaming_write","title":"STREAMING_WRITE","text":""},{"location":"connector/TableCapability/#truncate","title":"TRUNCATE","text":""},{"location":"connector/TableCapability/#v1_batch_write","title":"V1_BATCH_WRITE","text":""},{"location":"connector/TableHelper/","title":"TableHelper Implicit Class","text":"<p><code>TableHelper</code> is a Scala implicit class for Table.</p>"},{"location":"connector/TableHelper/#creating-instance","title":"Creating Instance","text":"<p><code>TableHelper</code> takes the following to be created:</p> <ul> <li> Table"},{"location":"connector/TableHelper/#asdeletable","title":"asDeletable <pre><code>asDeletable: SupportsDelete\n</code></pre> <p><code>asDeletable</code>...FIXME</p> <p><code>asDeletable</code>\u00a0is used when...FIXME</p>","text":""},{"location":"connector/TableHelper/#asreadable","title":"asReadable <pre><code>asReadable: SupportsRead\n</code></pre> <p><code>asReadable</code>...FIXME</p> <p><code>asReadable</code>\u00a0is used when...FIXME</p>","text":""},{"location":"connector/TableHelper/#aswritable","title":"asWritable <pre><code>asWritable: SupportsWrite\n</code></pre> <p><code>asWritable</code>...FIXME</p> <p><code>asWritable</code>\u00a0is used when...FIXME</p>","text":""},{"location":"connector/TableHelper/#supports","title":"supports <pre><code>supports(\n  capability: TableCapability): Boolean\n</code></pre> <p><code>supports</code> returns <code>true</code> when the given TableCapability is amongst the capabilities of the Table. Otherwise, <code>supports</code> returns <code>false</code>.</p> <p><code>supports</code>\u00a0is used when:</p> <ul> <li><code>Table</code> is requested to supportsAny</li> <li><code>DataSourceV2Relation</code> is requested to skipSchemaResolution</li> <li><code>DataFrameReader</code> is requested to load data</li> <li><code>DataFrameWriter</code> is requested to save data</li> <li>DataSourceV2Strategy execution planning strategy is executed (for AppendData and OverwriteByExpression logical operators)</li> <li>TableCapabilityCheck extended analysis check rule is executed</li> <li><code>MicroBatchExecution</code> (Spark Structured Streaming) is requested for a <code>LogicalPlan</code></li> <li><code>ContinuousExecution</code> (Spark Structured Streaming) is created</li> <li><code>DataStreamWriter</code> (Spark Structured Streaming) is requested to start a streaming query</li> </ul>","text":""},{"location":"connector/TableHelper/#supportsany","title":"supportsAny <pre><code>supportsAny(\n  capabilities: TableCapability*): Boolean\n</code></pre> <p><code>supportsAny</code>...FIXME</p> <p><code>supportsAny</code>\u00a0is used when...FIXME</p>","text":""},{"location":"connector/TableProvider/","title":"TableProvider","text":"<p><code>TableProvider</code> is an abstraction of table providers (for <code>DataSourceV2Utils</code> utility when requested for a Table).</p>"},{"location":"connector/TableProvider/#contract","title":"Contract","text":""},{"location":"connector/TableProvider/#table","title":"Table <pre><code>Table getTable(\n  StructType schema,\n  Transform[] partitioning,\n  Map&lt;String, String&gt; properties)\n</code></pre> <p>Creates a Table for the given <code>schema</code>, partitioning (as Transforms) and properties.</p> <p>Used when:</p> <ul> <li><code>DataFrameWriter</code> is requested to save data</li> <li><code>DataSourceV2Utils</code> utility is used to getTableFromProvider</li> </ul>","text":""},{"location":"connector/TableProvider/#inferring-partitioning","title":"Inferring Partitioning <pre><code>Transform[] inferPartitioning(\n  CaseInsensitiveStringMap options)\n</code></pre> <p>Default: No partitions (as Transforms)</p> <p>Used when:</p> <ul> <li><code>DataSourceV2Utils</code> utility is used to getTableFromProvider</li> </ul>","text":""},{"location":"connector/TableProvider/#inferring-schema","title":"Inferring Schema <pre><code>StructType inferSchema(\n  CaseInsensitiveStringMap options)\n</code></pre> <p>Used when:</p> <ul> <li><code>DataSourceV2Utils</code> utility is used to getTableFromProvider</li> </ul>","text":""},{"location":"connector/TableProvider/#supportsexternalmetadata","title":"supportsExternalMetadata <pre><code>boolean supportsExternalMetadata()\n</code></pre> <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li><code>DataSourceV2Utils</code> utility is used to getTableFromProvider</li> </ul>","text":""},{"location":"connector/TableProvider/#implementations","title":"Implementations","text":"<ul> <li>FileDataSourceV2</li> <li>SessionConfigSupport</li> <li>SimpleTableProvider</li> <li>SupportsCatalogOptions</li> </ul>"},{"location":"connector/Transform/","title":"Transform Function Expressions","text":"<p><code>Transform</code>\u00a0is an extension of the Expression abstraction for transform functions.</p>"},{"location":"connector/Transform/#contract","title":"Contract","text":""},{"location":"connector/Transform/#arguments","title":"Arguments <pre><code>Expression[] arguments()\n</code></pre> <p>Expressions of the arguments of this transform function</p>","text":""},{"location":"connector/Transform/#name","title":"Name <pre><code>String name()\n</code></pre>","text":""},{"location":"connector/Transform/#references","title":"References <pre><code>NamedReference[] references()\n</code></pre>","text":""},{"location":"connector/Transform/#implementations","title":"Implementations","text":"<ul> <li>ApplyTransform</li> <li>RewritableTransform</li> </ul>"},{"location":"connector/TransformHelper/","title":"TransformHelper","text":"<p><code>TransformHelper</code> is a Scala implicit class to extend Seq[Transform] with convertTransforms extension method.</p>"},{"location":"connector/TransformHelper/#creating-instance","title":"Creating Instance","text":"<p><code>TransformHelper</code> takes the following to be created:</p> <ul> <li> Transforms"},{"location":"connector/TransformHelper/#converttransforms","title":"convertTransforms <pre><code>convertTransforms: (Seq[String], Option[BucketSpec])\n</code></pre> <p><code>convertTransforms</code>...FIXME</p>  <p><code>convertTransforms</code> is used when:</p> <ul> <li>ResolveSessionCatalog logical analysis rule is executed</li> <li><code>V2SessionCatalog</code> is requested to createTable (deprecated)</li> <li><code>CatalogImpl</code> is requested to listColumns</li> </ul>","text":""},{"location":"connector/TruncatableTable/","title":"TruncatableTable","text":"<p><code>TruncatableTable</code> is an extension of the Table abstraction for tables that can truncateTable.</p>"},{"location":"connector/TruncatableTable/#contract","title":"Contract","text":""},{"location":"connector/TruncatableTable/#truncatetable","title":"truncateTable <pre><code>boolean truncateTable()\n</code></pre> <p>See:</p> <ul> <li>SupportsDeleteV2</li> </ul> <p>Used when:</p> <ul> <li>TruncateTableExec physical operator is executed</li> </ul>","text":""},{"location":"connector/TruncatableTable/#implementations","title":"Implementations","text":"<ul> <li>SupportsDeleteV2</li> </ul>"},{"location":"connector/V1Scan/","title":"V1Scan","text":"<p><code>V1Scan</code>\u00a0is an extension of the Scan abstraction for V1 DataSources that would like to participate in the DataSource V2 read code paths.</p>"},{"location":"connector/V1Scan/#contract","title":"Contract","text":""},{"location":"connector/V1Scan/#tov1tablescan","title":"toV1TableScan <pre><code>&lt;T extends BaseRelation &amp; TableScan&gt; T toV1TableScan(\n  SQLContext context)\n</code></pre> <p>BaseRelation with TableScan to scan data from a DataSource v1 (to <code>RDD[Row]</code>)</p> <p>Used when:</p> <ul> <li>DataSourceV2Strategy execution planning strategy is executed (to plan DataSourceV2ScanRelation with <code>V1Scan</code> to RowDataSourceScanExec)</li> </ul>","text":""},{"location":"connector/V1Scan/#implementations","title":"Implementations","text":"<ul> <li>JDBCScan</li> </ul>"},{"location":"connector/V1Table/","title":"V1Table","text":"<p><code>V1Table</code> is a Table that acts as an adapter to expose v1 table metadata.</p>"},{"location":"connector/V1Table/#creating-instance","title":"Creating Instance","text":"<p><code>V1Table</code> takes the following to be created:</p> <ul> <li> CatalogTable <p><code>V1Table</code> is created\u00a0when <code>V2SessionCatalog</code> is requested to load a table.</p>"},{"location":"connector/V1WriteBuilder/","title":"V1WriteBuilder","text":"<p><code>V1WriteBuilder</code>\u00a0is an extension of the WriteBuilder abstraction for V1 DataSources that would like to leverage the DataSource V2 write code paths.</p>"},{"location":"connector/V1WriteBuilder/#contract","title":"Contract","text":""},{"location":"connector/V1WriteBuilder/#buildforv1write","title":"buildForV1Write <pre><code>InsertableRelation buildForV1Write()\n</code></pre> <p>InsertableRelation</p> <p>Used when:</p> <ul> <li><code>AppendDataExecV1</code>, <code>OverwriteByExpressionExecV1</code>, CreateTableAsSelectExec, <code>ReplaceTableAsSelectExec</code> and AtomicTableWriteExec physical commands are executed</li> </ul>","text":""},{"location":"connector/V1WriteBuilder/#implementations","title":"Implementations","text":"<p>Note</p> <p>No known native Spark SQL implementations.</p>"},{"location":"connector/Write/","title":"Write","text":"<p><code>Write</code> is an abstraction of writers.</p>"},{"location":"connector/Write/#contract","title":"Contract","text":""},{"location":"connector/Write/#description","title":"Description <pre><code>String description()\n</code></pre> <p>Defaults to the name of this <code>Write</code> class</p> <p>Used when:</p> <ul> <li><code>Write</code> is requested to toBatch and toStreaming (for reporting purposes)</li> </ul>","text":""},{"location":"connector/Write/#supported-custommetrics","title":"Supported CustomMetrics <pre><code>CustomMetric[] supportedCustomMetrics()\n</code></pre> <p>Defaults to no CustomMetrics</p> <p>Used when:</p> <ul> <li><code>V2ExistingTableWriteExec</code> is requested for customMetrics</li> <li><code>StreamExecution</code> (Spark Structured Streaming) is requested for a <code>StreamingWrite</code></li> </ul>","text":""},{"location":"connector/Write/#toBatch","title":"Creating BatchWrite <pre><code>BatchWrite toBatch()\n</code></pre> <p>BatchWrite of this connector</p>  UnsupportedOperationException by default <p><code>toBatch</code> throws an <code>UnsupportedOperationException</code> by default and is expected to be overriden by implementations.</p> <pre><code>[description]: Batch write is not supported\n</code></pre>  <p><code>toBatch</code> should be implemented for Tables that create <code>Write</code>s that reports BATCH_WRITE support in their capabilities.</p> <p>See:</p> <ul> <li>KafkaWrite</li> <li>FileWrite</li> </ul> <p>Used when:</p> <ul> <li><code>V2ExistingTableWriteExec</code> physical command is executed</li> <li><code>TableWriteExecHelper</code> is requested to writeToTable</li> </ul>","text":""},{"location":"connector/Write/#creating-streamingwrite","title":"Creating StreamingWrite <pre><code>StreamingWrite toStreaming()\n</code></pre> <p><code>toStreaming</code> throws an <code>UnsupportedOperationException</code> by default:</p> <pre><code>[description]: Streaming write is not supported\n</code></pre> <p><code>toStreaming</code> should be implemented for Tables that create <code>Write</code>s that reports STREAMING_WRITE support in their capabilities.</p> <p>Used when:</p> <ul> <li><code>StreamExecution</code> (Spark Structured Streaming) is requested for a <code>StreamingWrite</code></li> </ul>","text":""},{"location":"connector/Write/#implementations","title":"Implementations","text":"<ul> <li><code>ConsoleTable</code> (Spark Structured Streaming)</li> <li>WriteBuilder</li> <li>FileWrite</li> <li><code>ForeachWrite</code> (Spark Structured Streaming)</li> <li>KafkaWrite</li> <li><code>MemoryWrite</code> (Spark Structured Streaming)</li> <li><code>NoopWrite</code></li> <li><code>RequiresDistributionAndOrdering</code></li> <li><code>V1Write</code></li> </ul>"},{"location":"connector/WriteBuilder/","title":"WriteBuilder","text":"<p><code>WriteBuilder</code> is an abstraction of write builders for batch and streaming.</p>"},{"location":"connector/WriteBuilder/#contract","title":"Contract","text":""},{"location":"connector/WriteBuilder/#batchwrite","title":"BatchWrite <pre><code>BatchWrite buildForBatch()\n</code></pre> <p>BatchWrite for writing data to a batch source</p> <p>Used when:</p> <ul> <li>CreateTableAsSelectExec, <code>ReplaceTableAsSelectExec</code>, <code>AppendDataExec</code>, OverwriteByExpressionExec, <code>OverwritePartitionsDynamicExec</code>, AtomicTableWriteExec physical commands are executed</li> </ul>","text":""},{"location":"connector/WriteBuilder/#streamingwrite","title":"StreamingWrite <pre><code>StreamingWrite buildForStreaming()\n</code></pre> <p><code>StreamingWrite</code> for writing data to a streaming source</p> <p>Used when:</p> <ul> <li><code>StreamExecution</code> stream execution engine (Spark Structured Streaming) is requested to <code>createStreamingWrite</code></li> </ul>","text":""},{"location":"connector/WriteBuilder/#implementations","title":"Implementations","text":"<ul> <li>ConsoleTable (Spark Structured Streaming)</li> <li>ForeachWriterTable (Spark Structured Streaming)</li> <li>KafkaTable</li> <li>MemorySink (Spark Structured Streaming)</li> <li>NoopWriteBuilder</li> <li>SupportsDynamicOverwrite</li> <li>SupportsOverwrite</li> <li>SupportsStreamingUpdate</li> <li>SupportsTruncate</li> <li>V1WriteBuilder</li> </ul>"},{"location":"connector/catalog/","title":"Catalog Plugin API","text":"<p>Main abstractions:</p> <ul> <li>CatalogManager</li> <li>CatalogPlugin</li> </ul>"},{"location":"connector/catalog/CatalogExtension/","title":"CatalogExtension","text":"<p><code>CatalogExtension</code> is an extension of the TableCatalog and SupportsNamespaces abstractions for session catalog extensions that setDelegateCatalog.</p>"},{"location":"connector/catalog/CatalogExtension/#contract","title":"Contract","text":""},{"location":"connector/catalog/CatalogExtension/#setdelegatecatalog","title":"setDelegateCatalog <pre><code>void setDelegateCatalog(\n  CatalogPlugin delegate)\n</code></pre> <p>Used when <code>CatalogManager</code> is requested to loadV2SessionCatalog</p>","text":""},{"location":"connector/catalog/CatalogExtension/#implementations","title":"Implementations","text":"<ul> <li>DelegatingCatalogExtension</li> </ul>"},{"location":"connector/catalog/CatalogHelper/","title":"CatalogHelper","text":"<p><code>CatalogHelper</code> is a Scala implicit class that adds extensions methods to CatalogPlugin.</p> <p>Tip</p> <p>Learn more on implicit classes in the official documentation of Scala 2.</p>"},{"location":"connector/catalog/CatalogHelper/#creating-instance","title":"Creating Instance","text":"<p><code>CatalogHelper</code> takes the following to be created:</p> <ul> <li> CatalogPlugin"},{"location":"connector/catalog/CatalogHelper/#asnamespacecatalog","title":"asNamespaceCatalog <pre><code>asNamespaceCatalog: SupportsNamespaces\n</code></pre> <p><code>asNamespaceCatalog</code> returns the CatalogPlugin if it is a SupportsNamespaces or throws an <code>AnalysisException</code> otherwise:</p> <pre><code>Cannot use catalog [name]: does not support namespaces\n</code></pre> <p><code>asNamespaceCatalog</code>\u00a0is used when:</p> <ul> <li>ResolveCatalogs logical resolution rule is executed</li> <li>DataSourceV2Strategy execution planning strategy is executed</li> <li>DropNamespaceExec physical command is executed</li> </ul>","text":""},{"location":"connector/catalog/CatalogHelper/#astablecatalog","title":"asTableCatalog <pre><code>asTableCatalog: TableCatalog\n</code></pre> <p><code>asTableCatalog</code> returns the CatalogPlugin if it is a TableCatalog or throws an <code>AnalysisException</code> otherwise:</p> <pre><code>Cannot use catalog [name]: not a TableCatalog\n</code></pre> <p><code>asTableCatalog</code>\u00a0is used when:</p> <ul> <li>ResolveTables, ResolveRelations, ResolveCatalogs and ResolveSessionCatalog logical resolution rules are executed</li> <li><code>CatalogV2Util</code> utility is used to load a table, createAlterTable and getTableProviderCatalog</li> <li><code>DataFrameWriter</code> is requested to insertInto and saveAsTable</li> <li><code>DataFrameWriterV2</code> is created</li> <li>DataSourceV2Strategy execution planning strategy is executed</li> <li>DropNamespaceExec physical command is executed</li> </ul>","text":""},{"location":"connector/catalog/CatalogManager/","title":"CatalogManager","text":""},{"location":"connector/catalog/CatalogManager/#creating-instance","title":"Creating Instance","text":"<p><code>CatalogManager</code> takes the following to be created:</p> <ul> <li> SQLConf <li>Default Session Catalog</li> <li> SessionCatalog <p><code>CatalogManager</code> is created when:</p> <ul> <li><code>BaseSessionStateBuilder</code> is requested for a CatalogManager</li> </ul>"},{"location":"connector/catalog/CatalogManager/#default-session-catalog","title":"Default Session Catalog <pre><code>defaultSessionCatalog: CatalogPlugin\n</code></pre> <p><code>CatalogManager</code> is given a CatalogPlugin when created for the default session catalog.</p> <p><code>defaultSessionCatalog</code> is used as the delegate catalog when <code>CatalogManager</code> is requested to load a V2SessionCatalog.</p> <p><code>defaultSessionCatalog</code> is used as the fall-back catalog when <code>CatalogManager</code> is requested to load a custom V2CatalogPlugin.</p>","text":""},{"location":"connector/catalog/CatalogManager/#default-catalog-name","title":"Default Catalog Name <p><code>CatalogManager</code> defines <code>spark_catalog</code> as the name of the default catalog (V2SessionCatalog).</p> <p><code>spark_catalog</code> is used as the default value of spark.sql.defaultCatalog configuration property.</p>","text":""},{"location":"connector/catalog/CatalogManager/#current-catalog-name","title":"Current Catalog Name <pre><code>_currentCatalogName: Option[String]\n</code></pre> <p><code>_currentCatalogName</code> is the name of the current catalog and is undefined by default (<code>None</code>).</p> <p><code>_currentCatalogName</code> can be changed using setCurrentCatalog.</p>","text":""},{"location":"connector/catalog/CatalogManager/#current-catalogplugin","title":"Current CatalogPlugin <pre><code>currentCatalog: CatalogPlugin\n</code></pre> <p><code>currentCatalog</code> uses the current CatalogPlugin if defined or falls back on spark.sql.defaultCatalog configuration property.</p> <p><code>currentCatalog</code> is used when:</p> <ul> <li> <p><code>CatalogManager</code> is requested for the current namespace, setCurrentNamespace or setCurrentCatalog</p> </li> <li> <p><code>LookupCatalog</code> is requested to <code>currentCatalog</code></p> </li> <li> <p><code>ViewHelper</code> utility is requested to <code>generateViewProperties</code></p> </li> </ul>","text":""},{"location":"connector/catalog/CatalogManager/#current-namespace","title":"Current Namespace <pre><code>currentNamespace: Array[String]\n</code></pre> <p><code>currentNamespace</code>...FIXME</p> <p><code>currentNamespace</code> is used when:</p> <ul> <li><code>ResolveNamespace</code> analysis rule is executed</li> <li><code>GetCurrentDatabase</code> analysis rule is executed</li> <li><code>CatalogAndIdentifier</code> extractor utility is requested to <code>unapply</code></li> <li><code>ViewHelper</code> utility is requested to <code>generateViewProperties</code></li> </ul>","text":""},{"location":"connector/catalog/CatalogManager/#setting-current-namespace","title":"Setting Current Namespace <pre><code>setCurrentNamespace(\n  namespace: Array[String]): Unit\n</code></pre> <p><code>setCurrentNamespace</code>...FIXME</p> <p><code>setCurrentNamespace</code> is used when <code>SetCatalogAndNamespaceExec</code> physical command is executed.</p>","text":""},{"location":"connector/catalog/CatalogManager/#changing-current-catalog-name","title":"Changing Current Catalog Name <pre><code>setCurrentCatalog(\n  catalogName: String): Unit\n</code></pre> <p><code>setCurrentCatalog</code> checks out whether the given catalog name is different from the currentCatalog's.</p> <p>Only if the names are different, <code>setCurrentCatalog</code> makes it _currentCatalogName and \"resets\" _currentNamespace (<code>None</code>). In the end, <code>setCurrentCatalog</code> requests the SessionCatalog to setCurrentDatabase as default.</p>  <p><code>setCurrentCatalog</code> is used when:</p> <ul> <li><code>SetCatalogCommand</code> logical command is executed</li> <li>SetCatalogAndNamespaceExec physical command is executed</li> </ul>","text":""},{"location":"connector/catalog/CatalogManager/#finding-catalogplugin-by-name","title":"Finding CatalogPlugin by Name <pre><code>catalog(\n  name: String): CatalogPlugin\n</code></pre> <p><code>catalog</code> returns the v2 session catalog when the given name is spark_catalog.</p> <p>Otherwise, <code>catalog</code> looks up the name in catalogs internal registry. When not found, <code>catalog</code> tries to load a CatalogPlugin by name (and registers it in catalogs internal registry).</p> <p><code>catalog</code> is used when:</p> <ul> <li> <p><code>CatalogManager</code> is requested to isCatalogRegistered and currentCatalog</p> </li> <li> <p><code>CatalogV2Util</code> utility is requested to getTableProviderCatalog</p> </li> <li> <p><code>CatalogAndMultipartIdentifier</code>, <code>CatalogAndNamespace</code> and <code>CatalogAndIdentifier</code> utilities are requested to extract a CatalogPlugin (<code>unapply</code>)</p> </li> </ul>","text":""},{"location":"connector/catalog/CatalogManager/#iscatalogregistered","title":"isCatalogRegistered <pre><code>isCatalogRegistered(\n  name: String): Boolean\n</code></pre> <p><code>isCatalogRegistered</code>...FIXME</p> <p><code>isCatalogRegistered</code> is used when <code>Analyzer</code> is requested to expandRelationName.</p>","text":""},{"location":"connector/catalog/CatalogManager/#v2sessioncatalog","title":"v2SessionCatalog <pre><code>v2SessionCatalog: CatalogPlugin\n</code></pre> <p><code>v2SessionCatalog</code>...FIXME</p> <p><code>v2SessionCatalog</code> is used when:</p> <ul> <li> <p><code>CatalogManager</code> is requested to look up a CatalogPlugin by name</p> </li> <li> <p><code>CatalogV2Util</code> is requested to <code>getTableProviderCatalog</code></p> </li> <li> <p><code>CatalogAndIdentifier</code> utility is requested to extract a CatalogPlugin and an identifier from a multi-part name (<code>unapply</code>)</p> </li> </ul>","text":""},{"location":"connector/catalog/CatalogManager/#loadv2sessioncatalog","title":"loadV2SessionCatalog <pre><code>loadV2SessionCatalog(): CatalogPlugin\n</code></pre> <p><code>loadV2SessionCatalog</code> loads the default spark_catalog.</p> <p>If it is of type CatalogExtension, <code>loadV2SessionCatalog</code> requests it to setDelegateCatalog with the defaultSessionCatalog.</p> <p><code>loadV2SessionCatalog</code> is used when:</p> <ul> <li><code>CatalogManager</code> is requested for a CatalogPlugin</li> </ul>","text":""},{"location":"connector/catalog/CatalogPlugin/","title":"CatalogPlugin","text":"<p><code>CatalogPlugin</code> is an abstraction of table catalogs.</p> <p>CatalogHelper</p> <p>CatalogHelper is a Scala implicit class of <code>CatalogPlugin</code> with extensions methods.</p>"},{"location":"connector/catalog/CatalogPlugin/#contract","title":"Contract","text":""},{"location":"connector/catalog/CatalogPlugin/#default-namespace","title":"Default Namespace <pre><code>String[] defaultNamespace()\n</code></pre> <p>Default namespace</p> <p>Default: (empty)</p> <p>Used when:</p> <ul> <li><code>CatalogManager</code> is requested for the current namespace</li> </ul>","text":""},{"location":"connector/catalog/CatalogPlugin/#initializing-catalogplugin","title":"Initializing CatalogPlugin <pre><code>void initialize(\n  String name,\n  CaseInsensitiveStringMap options)\n</code></pre> <p>Initializes this <code>CatalogPlugin</code> with the following:</p> <ul> <li>Name that was used in <code>spark.sql.catalog.[name]</code> configuration property</li> <li><code>spark.sql.catalog.[name].</code>-prefixed case-insensitive options</li> </ul> <p>Used when:</p> <ul> <li><code>Catalogs</code> utility is used to load a catalog by name</li> </ul>","text":""},{"location":"connector/catalog/CatalogPlugin/#name","title":"Name <pre><code>String name()\n</code></pre>  <p>SHOW CURRENT NAMESPACE</p> <p>Use <code>SHOW CURRENT NAMESPACE</code> command to display the name.</p>","text":""},{"location":"connector/catalog/CatalogPlugin/#implementations","title":"Implementations","text":"<ul> <li>FunctionCatalog</li> <li>SupportsNamespaces</li> <li>TableCatalog</li> </ul>"},{"location":"connector/catalog/CatalogPlugin/#demo","title":"Demo","text":"<p>Learn more in Demo: Developing CatalogPlugin.</p>"},{"location":"connector/catalog/CatalogV2Util/","title":"CatalogV2Util Utility","text":""},{"location":"connector/catalog/CatalogV2Util/#loading-table","title":"Loading Table <pre><code>loadTable(\n  catalog: CatalogPlugin,\n  ident: Identifier,\n  timeTravelSpec: Option[TimeTravelSpec] = None): Option[Table]\n</code></pre> <p><code>loadTable</code> getTable.</p>  <p><code>loadTable</code> is used when:</p> <ul> <li>ResolveRelations logical analysis rule is executed (and lookupTableOrView and resolveRelation)</li> <li><code>CatalogV2Util</code> is requested to loadRelation</li> <li><code>CatalogImpl</code> is requested to load a table</li> </ul>","text":""},{"location":"connector/catalog/CatalogV2Util/#gettable","title":"getTable <pre><code>getTable(\n  catalog: CatalogPlugin,\n  ident: Identifier,\n  timeTravelSpec: Option[TimeTravelSpec] = None): Table\n</code></pre> <p><code>getTable</code> requests the given CatalogPlugin for the TableCatalog to loadTable.</p>  <p><code>getTable</code> is used when:</p> <ul> <li><code>CatalogV2Util</code> is requested to loadTable</li> <li><code>DataSourceV2Utils</code> is requested to loadV2Source</li> </ul>","text":""},{"location":"connector/catalog/CatalogV2Util/#gettableprovidercatalog","title":"getTableProviderCatalog <pre><code>getTableProviderCatalog(\n  provider: SupportsCatalogOptions,\n  catalogManager: CatalogManager,\n  options: CaseInsensitiveStringMap): TableCatalog\n</code></pre> <p><code>getTableProviderCatalog</code>...FIXME</p> <p><code>getTableProviderCatalog</code> is used when:</p> <ul> <li> <p><code>DataFrameReader</code> is requested to load (for a data source that is a [SupportsCatalogOptions])</p> </li> <li> <p><code>DataFrameWriter</code> is requested to save (for a data source that is a [SupportsCatalogOptions])</p> </li> </ul>","text":""},{"location":"connector/catalog/CatalogV2Util/#creating-altertable-logical-command","title":"Creating AlterTable Logical Command <pre><code>createAlterTable(\n  originalNameParts: Seq[String],\n  catalog: CatalogPlugin,\n  tableName: Seq[String],\n  changes: Seq[TableChange]): AlterTable\n</code></pre> <p><code>createAlterTable</code> converts the CatalogPlugin to a TableCatalog.</p> <p><code>createAlterTable</code> creates an AlterTable (with an <code>UnresolvedV2Relation</code>).</p> <p><code>createAlterTable</code> is used when:</p> <ul> <li>ResolveCatalogs and ResolveSessionCatalog logical resolution rules are executed (and resolve <code>AlterTableAddColumnsStatement</code>, <code>AlterTableReplaceColumnsStatement</code>, <code>AlterTableAlterColumnStatement</code>, <code>AlterTableRenameColumnStatement</code>, <code>AlterTableDropColumnsStatement</code>, <code>AlterTableSetPropertiesStatement</code>, <code>AlterTableUnsetPropertiesStatement</code>, <code>AlterTableSetLocationStatement</code> operators)</li> </ul>","text":""},{"location":"connector/catalog/Catalogs/","title":"Catalogs","text":"<p><code>Catalogs</code> is a utility to load (and initialize) a CatalogPlugin by a given name.</p>"},{"location":"connector/catalog/Catalogs/#loading-catalog-by-name","title":"Loading Catalog by Name <pre><code>load(\n  name: String,\n  conf: SQLConf): CatalogPlugin\n</code></pre> <p><code>load</code> finds the class name of the CatalogPlugin in the given SQLConf by <code>spark.sql.catalog.[name]</code> key (or throws an CatalogNotFoundException).</p> <p><code>load</code> loads the class and makes sure that it is a CatalogPlugin (or throws an SparkException).</p> <p>In the end, <code>load</code> creates a new instance (using a public no-arg constructor) and requests the <code>CatalogPlugin</code> to initialize (with the given <code>name</code> and all the catalog options that use <code>spark.sql.catalog.[name]</code> prefix).</p>  <p><code>load</code> is used when:</p> <ul> <li><code>CatalogManager</code> is requested to look up a catalog by name and loadV2SessionCatalog</li> </ul>","text":""},{"location":"connector/catalog/Catalogs/#catalognotfoundexception","title":"CatalogNotFoundException <p><code>load</code> throws a <code>CatalogNotFoundException</code> when the <code>spark.sql.catalog.[name]</code> key could not be found:</p> <pre><code>Catalog '[name]' plugin class not found: spark.sql.catalog.[name] is not defined\n</code></pre>","text":""},{"location":"connector/catalog/Catalogs/#sparkexception","title":"SparkException <p><code>load</code> throws a <code>SparkException</code> when the class name is not of the <code>CatalogPlugin</code> type:</p> <pre><code>Plugin class for catalog '[name]' does not implement CatalogPlugin: [pluginClassName]\n</code></pre>","text":""},{"location":"connector/catalog/Catalogs/#collecting-catalog-options","title":"Collecting Catalog Options <pre><code>catalogOptions(\n  name: String,\n  conf: SQLConf): CaseInsensitiveStringMap\n</code></pre> <p><code>catalogOptions</code> collects all options with <code>spark.sql.catalog.[name].</code> prefix (in the given SQLConf).</p>","text":""},{"location":"connector/catalog/Column/","title":"Column","text":"<p><code>Column</code> is...FIXME</p>"},{"location":"connector/catalog/DelegatingCatalogExtension/","title":"DelegatingCatalogExtension","text":"<p><code>DelegatingCatalogExtension</code> is an extension of the CatalogExtension abstraction for catalogs that delegate unsupported catalog functions to the built-in session catalog.</p>"},{"location":"connector/catalog/FunctionCatalog/","title":"FunctionCatalog","text":"<p><code>FunctionCatalog</code> is an extension of the CatalogPlugin abstraction for function catalogs.</p>"},{"location":"connector/catalog/FunctionCatalog/#contract-subset","title":"Contract (Subset)","text":""},{"location":"connector/catalog/FunctionCatalog/#loading-function","title":"Loading Function <pre><code>UnboundFunction loadFunction(\n  Identifier ident)\n</code></pre> <p>See V2SessionCatalog</p> <p>Used when:</p> <ul> <li><code>DelegatingCatalogExtension</code> is requested to loadFunction</li> <li><code>FunctionCatalog</code> is requested to functionExists</li> <li><code>ResolveFunctions</code> logical analysis rule is requested to resolveV2Function</li> <li><code>V2ExpressionUtils</code> is requested to <code>loadV2FunctionOpt</code></li> <li><code>CatalogV2Util</code> is requested to load a function</li> </ul>","text":""},{"location":"connector/catalog/FunctionCatalog/#implementations","title":"Implementations","text":"<ul> <li>CatalogExtension</li> <li><code>FakeV2SessionCatalog</code></li> <li>V2SessionCatalog</li> </ul>"},{"location":"connector/catalog/StagingTableCatalog/","title":"StagingTableCatalog","text":"<p><code>StagingTableCatalog</code> is an extension of the TableCatalog abstraction for table catalogs that can stage Create, CreateOrReplace and Replace operations (for atomic <code>CREATE TABLE AS SELECT</code> and <code>REPLACE TABLE</code> and <code>REPLACE TABLE AS SELECT</code> queries).</p> <ol> <li><code>AtomicCreateTableAsSelectExec</code> is created for CreateTableAsSelects on a <code>StagingTableCatalog</code> (otherwise, it is a CreateTableAsSelectExec)</li> <li><code>AtomicReplaceTableExec</code> is created for <code>ReplaceTable</code>s on a <code>StagingTableCatalog</code> (otherwise, it is a <code>ReplaceTableExec</code>)</li> <li><code>AtomicReplaceTableAsSelectExec</code> is created for <code>ReplaceTableAsSelect</code>s on a <code>StagingTableCatalog</code> (otherwise, it is a <code>ReplaceTableAsSelectExec</code>)</li> </ol>"},{"location":"connector/catalog/StagingTableCatalog/#contract","title":"Contract","text":""},{"location":"connector/catalog/StagingTableCatalog/#stagecreate","title":"stageCreate <pre><code>StagedTable stageCreate(\n  Identifier ident,\n  StructType schema,\n  Transform[] partitions,\n  Map&lt;String, String&gt; properties)\n</code></pre> <p>Creates a StagedTable</p> <p>Used when:</p> <ul> <li><code>AtomicCreateTableAsSelectExec</code> unary physical command is executed</li> </ul>","text":""},{"location":"connector/catalog/StagingTableCatalog/#stagecreateorreplace","title":"stageCreateOrReplace <pre><code>StagedTable stageCreateOrReplace(\n  Identifier ident,\n  StructType schema,\n  Transform[] partitions,\n  Map&lt;String, String&gt; properties)\n</code></pre> <p>Creates a StagedTable</p> <p>Used when:</p> <ul> <li><code>AtomicReplaceTableExec</code> leaf physical command is executed</li> <li><code>AtomicReplaceTableAsSelectExec</code> unary physical command is executed</li> </ul>","text":""},{"location":"connector/catalog/StagingTableCatalog/#stagereplace","title":"stageReplace <pre><code>StagedTable stageReplace(\n  Identifier ident,\n  StructType schema,\n  Transform[] partitions,\n  Map&lt;String, String&gt; properties)\n</code></pre> <p>Creates a StagedTable</p> <p>Used when:</p> <ul> <li><code>AtomicReplaceTableExec</code> leaf physical command is executed</li> <li><code>AtomicReplaceTableAsSelectExec</code> unary physical command is executed</li> </ul>","text":""},{"location":"connector/catalog/StagingTableCatalog/#implementations","title":"Implementations","text":"<p>Note</p> <p>No known native Spark SQL implementations.</p>"},{"location":"connector/catalog/SupportsCatalogOptions/","title":"SupportsCatalogOptions","text":"<p><code>SupportsCatalogOptions</code> is...FIXME</p>"},{"location":"connector/catalog/SupportsNamespaces/","title":"SupportsNamespaces","text":"<p><code>SupportsNamespaces</code> is...FIXME</p>"},{"location":"connector/catalog/TableCatalog/","title":"TableCatalog","text":"<p><code>TableCatalog</code> is an extension of the CatalogPlugin abstraction for table catalogs.</p>"},{"location":"connector/catalog/TableCatalog/#contract","title":"Contract","text":""},{"location":"connector/catalog/TableCatalog/#altering-table","title":"Altering Table <pre><code>Table alterTable(\n  Identifier ident,\n  TableChange... changes)\n</code></pre> <p>Changes (alters) a table based on the given TableChanges</p> <p>Used when:</p> <ul> <li>AlterTableExec physical command is executed</li> <li><code>DelegatingCatalogExtension</code> is requested to alterTable</li> </ul>","text":""},{"location":"connector/catalog/TableCatalog/#createtable","title":"createTable <pre><code>Table createTable(\n  Identifier ident,\n  Column[] columns,\n  Transform[] partitions,\n  Map&lt;String, String&gt; properties)\n</code></pre> <p>See:</p> <ul> <li>V2SessionCatalog</li> </ul> <p>Used when the following commands are executed:</p> <ul> <li><code>CreateTableExec</code></li> <li><code>ReplaceTableExec</code></li> <li>CreateTableAsSelectExec</li> <li><code>ReplaceTableAsSelectExec</code></li> </ul>","text":""},{"location":"connector/catalog/TableCatalog/#droptable","title":"dropTable <pre><code>boolean dropTable(\n  Identifier ident)\n</code></pre> <p>Used when the following commands are executed:</p> <ul> <li><code>DropTableExec</code></li> <li><code>ReplaceTableExec</code></li> <li>CreateTableAsSelectExec</li> <li><code>ReplaceTableAsSelectExec</code></li> </ul>","text":""},{"location":"connector/catalog/TableCatalog/#listing-tables","title":"Listing Tables <pre><code>Identifier[] listTables(\n  String[] namespace)\n</code></pre> <p>Used when the following commands are executed:</p> <ul> <li>DropNamespaceExec</li> <li>ShowTablesExec</li> </ul>","text":""},{"location":"connector/catalog/TableCatalog/#loading-table","title":"Loading Table <pre><code>Table loadTable(\n  Identifier ident)\nTable loadTable(\n  Identifier ident,\n  long timestamp)\nTable loadTable(\n  Identifier ident,\n  String version)\n</code></pre> <p>Used when:</p> <ul> <li><code>CatalogV2Util</code> is requested to load a table</li> <li><code>DataFrameReader</code> is requested to load (for SupportsCatalogOptions providers)</li> <li><code>DataFrameWriter</code> is requested to save, insertInto and saveAsTable</li> <li><code>DelegatingCatalogExtension</code> is requested to loadTable</li> <li><code>TableCatalog</code> is requested to tableExists</li> <li><code>V2SessionCatalog</code> is requested to createTable, alterTable, dropTable, renameTable</li> </ul>","text":""},{"location":"connector/catalog/TableCatalog/#renametable","title":"renameTable <pre><code>void renameTable(\n  Identifier oldIdent,\n  Identifier newIdent)\n</code></pre> <p>Used when the following commands are executed:</p> <ul> <li><code>RenameTableExec</code></li> </ul>","text":""},{"location":"connector/catalog/TableCatalog/#tableexists","title":"tableExists <pre><code>boolean tableExists(\n  Identifier ident)\n</code></pre> <p>Used when:</p> <ul> <li>The following commands are executed:</li> <li><code>AtomicCreateTableAsSelectExec</code></li> <li><code>AtomicReplaceTableAsSelectExec</code></li> <li><code>AtomicReplaceTableExec</code></li> <li>CreateTableAsSelectExec</li> <li><code>CreateTableExec</code></li> <li><code>DropTableExec</code></li> <li><code>ReplaceTableAsSelectExec</code></li> <li> <p><code>ReplaceTableExec</code></p> </li> <li> <p><code>V2SessionCatalog</code> is requested to renameTable</p> </li> </ul>","text":""},{"location":"connector/catalog/TableCatalog/#implementations","title":"Implementations","text":"<ul> <li>CatalogExtension</li> <li>StagingTableCatalog</li> <li>V2SessionCatalog</li> </ul>"},{"location":"connector/catalog/TableChange/","title":"TableChange","text":"<p><code>TableChange</code> is an abstraction of changes to a table (when <code>TableCatalog</code> is requested to alter one).</p>"},{"location":"connector/catalog/TableChange/#implementations","title":"Implementations","text":""},{"location":"connector/catalog/TableChange/#columnchange","title":"ColumnChange <p><code>ColumnChange</code> is an extension of the <code>TableChange</code> abstraction for the column changes:</p> <ul> <li><code>AddColumn</code></li> <li><code>DeleteColumn</code></li> <li>RenameColumn</li> <li><code>UpdateColumnComment</code></li> <li><code>UpdateColumnNullability</code></li> <li><code>UpdateColumnPosition</code></li> <li><code>UpdateColumnType</code></li> </ul>","text":""},{"location":"connector/catalog/TableChange/#renamecolumn","title":"RenameColumn <p><code>RenameColumn</code> is a <code>TableChange</code> to rename a field.</p> <p><code>RenameColumn</code> is created when <code>TableChange</code> is requested for one</p>","text":""},{"location":"connector/catalog/TableChange/#removeproperty","title":"RemoveProperty","text":""},{"location":"connector/catalog/TableChange/#setproperty","title":"SetProperty","text":""},{"location":"connector/catalog/TableChange/#creating-renamecolumn","title":"Creating RenameColumn <pre><code>TableChange renameColumn(\n  String[] fieldNames,\n  String newName)\n</code></pre>  Static Method <p><code>renameColumn</code> is declared as static that is invoked without a reference to a particular object.</p> <p>Learn more in the Java Language Specification.</p>  <p><code>renameColumn</code> creates a RenameColumn table change.</p> <p><code>renameColumn</code> is used when:</p> <ul> <li><code>RenameColumn</code> logical command is requested for the table changes</li> </ul>","text":""},{"location":"connector/catalog/V2TableWithV1Fallback/","title":"V2TableWithV1Fallback Tables","text":"<p><code>V2TableWithV1Fallback</code> is an extension of the Table abstraction for tables with V1 fallback support (using CatalogTable).</p>"},{"location":"connector/catalog/V2TableWithV1Fallback/#contract","title":"Contract","text":""},{"location":"connector/catalog/V2TableWithV1Fallback/#v1table","title":"v1Table <pre><code>v1Table: CatalogTable\n</code></pre> <p>CatalogTable to fall back to for unsupported V2 capabilities (that are supported in V1)</p> <p>Used when:</p> <ul> <li><code>ResolveRelations</code> logical resolution rule is requested to createRelation (for a streaming table)</li> <li><code>DataStreamWriter</code> (Spark Structured Streaming) is requested to <code>toTable</code></li> </ul>","text":""},{"location":"connector/catalog/V2TableWithV1Fallback/#implementations","title":"Implementations","text":"<p>Note</p> <p>No known native Spark SQL implementations.</p>"},{"location":"connector/expressions/","title":"Connector Expressions","text":""},{"location":"connector/expressions/Aggregation/","title":"Aggregation Expression","text":"<p><code>Aggregation</code> is...FIXME</p>"},{"location":"connector/expressions/SortOrder/","title":"SortOrder Expression","text":"<p><code>SortOrder</code> is...FIXME</p>"},{"location":"connectors/","title":"Connectors","text":"<p>Spark SQL comes with the following built-in connectors (data sources) based on Connector API for the following data \"formats\":</p> <ul> <li>kafka</li> <li>parquet</li> <li>others (see the menu on the left)</li> </ul>"},{"location":"connectors/AggregatePushDownUtils/","title":"AggregatePushDownUtils","text":""},{"location":"connectors/AggregatePushDownUtils/#getschemaforpushedaggregation","title":"getSchemaForPushedAggregation <pre><code>getSchemaForPushedAggregation(\n  aggregation: Aggregation,\n  schema: StructType,\n  partitionNames: Set[String],\n  dataFilters: Seq[Expression]): Option[StructType]\n</code></pre> <p><code>getSchemaForPushedAggregation</code>...FIXME</p>  <p><code>getSchemaForPushedAggregation</code> is used when:</p> <ul> <li><code>OrcScanBuilder</code> is requested to <code>pushAggregation</code></li> <li><code>ParquetScanBuilder</code> is requested to pushAggregation</li> </ul>","text":""},{"location":"connectors/BaseDynamicPartitionDataWriter/","title":"BaseDynamicPartitionDataWriter","text":"<p><code>BaseDynamicPartitionDataWriter</code> is an extension of the FileFormatDataWriter abstraction for dynamic partition writers.</p>"},{"location":"connectors/BaseDynamicPartitionDataWriter/#implementations","title":"Implementations","text":"<ul> <li>DynamicPartitionDataConcurrentWriter</li> <li>DynamicPartitionDataSingleWriter</li> </ul>"},{"location":"connectors/BaseDynamicPartitionDataWriter/#creating-instance","title":"Creating Instance","text":"<p><code>BaseDynamicPartitionDataWriter</code> takes the following to be created:</p> <ul> <li> <code>WriteJobDescription</code> <li> <code>TaskAttemptContext</code> (Apache Hadoop) <li> <code>FileCommitProtocol</code> (Spark Core) <li> Custom SQLMetrics <p>Abstract Class</p> <p><code>BaseDynamicPartitionDataWriter</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete BaseDynamicPartitionDataWriters.</p>"},{"location":"connectors/BaseDynamicPartitionDataWriter/#renewcurrentwriter","title":"renewCurrentWriter <pre><code>renewCurrentWriter(\n  partitionValues: Option[InternalRow],\n  bucketId: Option[Int],\n  closeCurrentWriter: Boolean): Unit\n</code></pre> <p><code>renewCurrentWriter</code>...FIXME</p>  <p><code>renewCurrentWriter</code> is used when:</p> <ul> <li><code>BaseDynamicPartitionDataWriter</code> is requested to renewCurrentWriterIfTooManyRecords</li> <li><code>DynamicPartitionDataSingleWriter</code> is requested to write a record</li> <li><code>DynamicPartitionDataConcurrentWriter</code> is requested to setupCurrentWriterUsingMap</li> </ul>","text":""},{"location":"connectors/BaseDynamicPartitionDataWriter/#getpartitionpath","title":"getPartitionPath <pre><code>getPartitionPath: InternalRow =&gt; String\n</code></pre>  Lazy Value <p><code>getPartitionPath</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>getPartitionPath</code>...FIXME</p>","text":""},{"location":"connectors/BaseDynamicPartitionDataWriter/#partitionpathexpression","title":"partitionPathExpression <pre><code>partitionPathExpression: Expression\n</code></pre>  Lazy Value <p><code>partitionPathExpression</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>partitionPathExpression</code>...FIXME</p>","text":""},{"location":"connectors/BasicWriteJobStatsTracker/","title":"BasicWriteJobStatsTracker","text":"<p><code>BasicWriteJobStatsTracker</code> is a WriteJobStatsTracker.</p>"},{"location":"connectors/BasicWriteJobStatsTracker/#creating-instance","title":"Creating Instance","text":"<p><code>BasicWriteJobStatsTracker</code> takes the following to be created:</p> <ul> <li> Serializable Hadoop <code>Configuration</code> (Hadoop) <li> Driver-side metrics (<code>Map[String, SQLMetric]</code>) <li> Task commit time SQLMetric <p><code>BasicWriteJobStatsTracker</code> is created when:</p> <ul> <li><code>DataWritingCommand</code> is requested for a BasicWriteJobStatsTracker</li> <li><code>FileWrite</code> is requested to createWriteJobDescription</li> <li><code>FileStreamSink</code> (Spark Structured Streaming) is requested for a <code>BasicWriteJobStatsTracker</code></li> </ul>"},{"location":"connectors/BasicWriteJobStatsTracker/#creating-writetaskstatstracker","title":"Creating WriteTaskStatsTracker  WriteJobStatsTracker <pre><code>newTaskInstance(): WriteTaskStatsTracker\n</code></pre> <p><code>newTaskInstance</code> is part of the WriteJobStatsTracker abstraction.</p>  <p><code>newTaskInstance</code> creates a new BasicWriteTaskStatsTracker (with the serializable Hadoop Configuration and the taskCommitTimeMetric).</p>","text":""},{"location":"connectors/BasicWriteJobStatsTracker/#processing-write-job-statistics","title":"Processing Write Job Statistics  WriteJobStatsTracker <pre><code>processStats(\n  stats: Seq[WriteTaskStats],\n  jobCommitTime: Long): Unit\n</code></pre> <p><code>processStats</code> is part of the WriteJobStatsTracker abstraction.</p>  <p><code>processStats</code> uses the given BasicWriteTaskStatses to set the following driverSideMetrics:</p> <ul> <li><code>jobCommitTime</code></li> <li><code>numFiles</code></li> <li><code>numOutputBytes</code></li> <li><code>numOutputRows</code></li> <li><code>numParts</code></li> </ul> <p><code>processStats</code> requests the active <code>SparkContext</code> for the spark.sql.execution.id.</p> <p>In the end, <code>processStats</code> posts the metric updates.</p>","text":""},{"location":"connectors/BasicWriteTaskStats/","title":"BasicWriteTaskStats","text":"<p><code>BasicWriteTaskStats</code> is a WriteTaskStats.</p>"},{"location":"connectors/BasicWriteTaskStats/#creating-instance","title":"Creating Instance","text":"<p><code>BasicWriteTaskStats</code> takes the following to be created:</p> <ul> <li> Partitions (<code>Seq[InternalRow]</code>) <li> Number of files <li> Number of bytes <li> Number of rows <p><code>BasicWriteTaskStats</code> is created when:</p> <ul> <li><code>BasicWriteTaskStatsTracker</code> is requested for getFinalStats</li> </ul>"},{"location":"connectors/BasicWriteTaskStatsTracker/","title":"BasicWriteTaskStatsTracker","text":"<p><code>BasicWriteTaskStatsTracker</code> is a WriteTaskStatsTracker.</p>"},{"location":"connectors/BasicWriteTaskStatsTracker/#creating-instance","title":"Creating Instance","text":"<p><code>BasicWriteTaskStatsTracker</code> takes the following to be created:</p> <ul> <li> Hadoop Configuration <li> Task Commit Time SQLMetric <p><code>BasicWriteTaskStatsTracker</code> is created when:</p> <ul> <li><code>BasicWriteJobStatsTracker</code> is requested for a new WriteTaskStatsTracker instance</li> </ul>"},{"location":"connectors/BasicWriteTaskStatsTracker/#submittedfiles","title":"submittedFiles <p><code>BasicWriteTaskStatsTracker</code> uses <code>submittedFiles</code> registry of the file paths added (written out to).</p> <p>The <code>submittedFiles</code> registry is used to updateFileStats when getFinalStats.</p> <p>A file path is removed when <code>BasicWriteTaskStatsTracker</code> is requested to closeFile.</p> <p>All the file paths are removed in getFinalStats.</p>","text":""},{"location":"connectors/BasicWriteTaskStatsTracker/#updatefilestats","title":"updateFileStats <pre><code>updateFileStats(\n  filePath: String): Unit\n</code></pre> <p><code>updateFileStats</code> gets the size of the given <code>filePath</code>.</p> <p>If the file length is found, it is added to the numBytes registry with the numFiles incremented.</p>","text":""},{"location":"connectors/BasicWriteTaskStatsTracker/#processing-new-file-notification","title":"Processing New File Notification <pre><code>newFile(\n  filePath: String): Unit\n</code></pre> <p><code>newFile</code> adds the given <code>filePath</code> to the submittedFiles registry and increments the numSubmittedFiles counter.</p> <p><code>newFile</code> is part of the WriteTaskStatsTracker abstraction.</p>","text":""},{"location":"connectors/BasicWriteTaskStatsTracker/#final-writetaskstats","title":"Final WriteTaskStats <pre><code>getFinalStats(\n  taskCommitTime: Long): WriteTaskStats\n</code></pre> <p><code>getFinalStats</code> updateFileStats for every submittedFiles that are then cleared up.</p> <p><code>getFinalStats</code> sets the output metrics (of the current Spark task) as follows:</p> <ul> <li><code>bytesWritten</code> to be numBytes</li> <li><code>recordsWritten</code> to be numRows</li> </ul> <p><code>getFinalStats</code> prints out the following INFO message when the numSubmittedFiles is different from the numFiles:</p> <pre><code>Expected [numSubmittedFiles] files, but only saw $numFiles.\nThis could be due to the output format not writing empty files,\nor files being not immediately visible in the filesystem.\n</code></pre> <p><code>getFinalStats</code> adds the given <code>taskCommitTime</code> to the taskCommitTimeMetric if defined.</p> <p>In the end, creates a new BasicWriteTaskStats with the partitions, numFiles, numBytes, and numRows.</p>  <p><code>getFinalStats</code> is part of the WriteTaskStatsTracker abstraction.</p>","text":""},{"location":"connectors/BasicWriteTaskStatsTracker/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTracker</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTracker=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"connectors/CatalogFileIndex/","title":"CatalogFileIndex","text":"<p><code>CatalogFileIndex</code> is a FileIndex.</p>"},{"location":"connectors/CatalogFileIndex/#creating-instance","title":"Creating Instance","text":"<p><code>CatalogFileIndex</code> takes the following to be created:</p> <ul> <li> SparkSession <li> CatalogTable <li> Estimated Size <p><code>CatalogFileIndex</code> is created when:</p> <ul> <li><code>HiveMetastoreCatalog</code> is requested to convert a HiveTableRelation to a LogicalRelation</li> <li><code>DataSource</code> is requested to create a BaseRelation for a FileFormat</li> </ul>"},{"location":"connectors/CatalogFileIndex/#filestatuscache","title":"FileStatusCache <p><code>CatalogFileIndex</code> creates a FileStatusCache when created.</p> <p>The <code>FileStatusCache</code> is used when:</p> <ul> <li>filterPartitions (and create a InMemoryFileIndex)</li> <li>refresh (and invalidateAll)</li> </ul>","text":""},{"location":"connectors/CatalogFileIndex/#listing-files","title":"Listing Files <pre><code>listFiles(\n  partitionFilters: Seq[Expression],\n  dataFilters: Seq[Expression]): Seq[PartitionDirectory]\n</code></pre> <p><code>listFiles</code> lists the partitions for the input partition filters and then requests them for the underlying partition files.</p> <p><code>listFiles</code> is part of the FileIndex abstraction.</p>","text":""},{"location":"connectors/CatalogFileIndex/#input-files","title":"Input Files <pre><code>inputFiles: Array[String]\n</code></pre> <p><code>inputFiles</code> lists all the partitions and then requests them for the input files.</p> <p><code>inputFiles</code> is part of the FileIndex abstraction.</p>","text":""},{"location":"connectors/CatalogFileIndex/#root-paths","title":"Root Paths <pre><code>rootPaths: Seq[Path]\n</code></pre> <p><code>rootPaths</code> returns the base location converted to a Hadoop Path.</p> <p><code>rootPaths</code> is part of the FileIndex abstraction.</p>","text":""},{"location":"connectors/CatalogFileIndex/#listing-partitions-by-given-predicate-expressions","title":"Listing Partitions By Given Predicate Expressions <pre><code>filterPartitions(\n  filters: Seq[Expression]): InMemoryFileIndex\n</code></pre> <p><code>filterPartitions</code> requests the CatalogTable for the partition columns.</p> <p>For a partitioned table, <code>filterPartitions</code> starts tracking time. <code>filterPartitions</code> requests the SessionCatalog for the partitions by filter and creates a PrunedInMemoryFileIndex (with the partition listing time).</p> <p>For an unpartitioned table (no partition columns defined), <code>filterPartitions</code> simply returns a InMemoryFileIndex (with the base location and no user-specified schema).</p> <p><code>filterPartitions</code> is used when:</p> <ul> <li><code>HiveMetastoreCatalog</code> is requested to convert a HiveTableRelation to a LogicalRelation</li> <li><code>CatalogFileIndex</code> is requested to listFiles and inputFiles</li> <li>PruneFileSourcePartitions logical optimization is executed</li> </ul>","text":""},{"location":"connectors/CatalogFileIndex/#internal-properties","title":"Internal Properties","text":""},{"location":"connectors/CatalogFileIndex/#base-location","title":"Base Location <p>Base location (as a Java URI) as defined in the CatalogTable metadata (under the locationUri of the storage)</p> <p>Used when <code>CatalogFileIndex</code> is requested to filter the partitions and for the root paths</p>","text":""},{"location":"connectors/CatalogFileIndex/#hadoop-configuration","title":"Hadoop Configuration <p>Hadoop Configuration</p> <p>Used when <code>CatalogFileIndex</code> is requested to filter the partitions</p>","text":""},{"location":"connectors/DataSourceV2Utils/","title":"DataSourceV2Utils Utility","text":"<p><code>DataSourceV2Utils</code> is an utility to extractSessionConfigs and getTableFromProvider for batch and streaming reads and writes.</p>"},{"location":"connectors/DataSourceV2Utils/#extractsessionconfigs","title":"extractSessionConfigs <pre><code>extractSessionConfigs(\n  source: TableProvider,\n  conf: SQLConf): Map[String, String]\n</code></pre>  <p>Note</p> <p><code>extractSessionConfigs</code> supports data sources with SessionConfigSupport only.</p>  <p><code>extractSessionConfigs</code> requests the <code>SessionConfigSupport</code> data source for the custom key prefix for configuration options that is used to find all configuration options with the keys in the format of spark.datasource.[keyPrefix] in the given SQLConf.</p> <p><code>extractSessionConfigs</code> returns the matching keys with the spark.datasource.[keyPrefix] prefix removed (i.e. <code>spark.datasource.keyPrefix.k1</code> becomes <code>k1</code>).</p> <p><code>extractSessionConfigs</code> is used when:</p> <ul> <li><code>DataFrameReader</code> is requested to load data</li> <li><code>DataFrameWriter</code> is requested to save data</li> <li>(Spark Structured Streaming) <code>DataStreamReader</code> is requested to load data from a streaming data source</li> <li>(Spark Structured Streaming) <code>DataStreamWriter</code> is requested to start a streaming query</li> </ul>","text":""},{"location":"connectors/DataSourceV2Utils/#creating-table-using-tableprovider","title":"Creating Table (using TableProvider) <pre><code>getTableFromProvider(\n  provider: TableProvider,\n  options: CaseInsensitiveStringMap,\n  userSpecifiedSchema: Option[StructType]): Table\n</code></pre> <p><code>getTableFromProvider</code> creates a Table for the given TableProvider, options and user-defined schema.</p>  <p><code>getTableFromProvider</code> is used when:</p> <ul> <li><code>DataFrameWriter</code> is requested to save data</li> <li><code>DataSourceV2Utils</code> is requested to loadV2Source</li> <li><code>DataStreamReader</code> (Spark Structured Streaming) is requested to load data from a streaming data source</li> <li><code>DataStreamWriter</code> (Spark Structured Streaming) is requested to start a streaming query</li> </ul>","text":""},{"location":"connectors/DataSourceV2Utils/#loadv2source","title":"loadV2Source <pre><code>loadV2Source(\n  sparkSession: SparkSession,\n  provider: TableProvider,\n  userSpecifiedSchema: Option[StructType],\n  extraOptions: CaseInsensitiveMap[String],\n  source: String,\n  paths: String*): Option[DataFrame]\n</code></pre> <p><code>loadV2Source</code> creates a DataFrame.</p>  <p><code>loadV2Source</code> is used when:</p> <ul> <li><code>DataFrameReader</code> is requested to load data</li> <li>CreateTempViewUsing logical operator is executed</li> </ul>","text":""},{"location":"connectors/DataWritingSparkTask/","title":"DataWritingSparkTask Utility","text":"<p><code>DataWritingSparkTask</code> utility defines a partition processing function that <code>V2TableWriteExec</code> unary physical commands use to schedule a Spark job for writing data out.</p> <p><code>DataWritingSparkTask</code> is executed on executors.</p>"},{"location":"connectors/DataWritingSparkTask/#partition-processing-function","title":"Partition Processing Function <pre><code>run(\n  writerFactory: DataWriterFactory,\n  context: TaskContext,\n  iter: Iterator[InternalRow],\n  useCommitCoordinator: Boolean,\n  customMetrics: Map[String, SQLMetric]): DataWritingSparkTaskResult\n</code></pre> <p><code>run</code> requests the DataWriterFactory for a DataWriter (for the partition and task of the <code>TaskContext</code>).</p> <p>For every InternalRow (in the given <code>iter</code> collection), <code>run</code> requests the <code>DataWriter</code> to write out the InternalRow. <code>run</code> counts all the <code>InternalRow</code>s.</p> <p>After all the rows have been written out successfully, <code>run</code> requests the <code>DataWriter</code> to commit (with or without requesting the <code>OutputCommitCoordinator</code> for authorization) that gives the final <code>WriterCommitMessage</code>.</p> <p>With <code>useCommitCoordinator</code> flag enabled, <code>run</code>...FIXME</p> <p>With <code>useCommitCoordinator</code> flag disabled, <code>run</code> prints out the following INFO message to the logs and requests the <code>DataWriter</code> to commit.</p> <p><code>run</code> prints out the following INFO message to the logs:</p> <pre><code>Committed partition [partId] (task [taskId], attempt [attemptId], stage [stageId].[stageAttempt])\n</code></pre> <p>In the end, <code>run</code> returns a <code>DataWritingSparkTaskResult</code> with the count (of the rows written out) and the final <code>WriterCommitMessage</code>.</p>","text":""},{"location":"connectors/DataWritingSparkTask/#usage","title":"Usage","text":"<p><code>run</code>\u00a0is used when:</p> <ul> <li><code>V2TableWriteExec</code> unary physical command is requested to writeWithV2</li> </ul>"},{"location":"connectors/DataWritingSparkTask/#using-commitcoordinator","title":"Using CommitCoordinator <p>With the given <code>useCommitCoordinator</code> flag enabled, <code>run</code> requests the <code>SparkEnv</code> for the <code>OutputCommitCoordinator</code> (Spark Core) that is asked whether to commit the write task output or not (<code>canCommit</code>).</p>","text":""},{"location":"connectors/DataWritingSparkTask/#commit-authorized","title":"Commit Authorized","text":"<p>If authorized, <code>run</code> prints out the following INFO message to the logs:</p> <pre><code>Commit authorized for partition [partId] (task [taskId], attempt [attemptId], stage [stageId].[stageAttempt])\n</code></pre>"},{"location":"connectors/DataWritingSparkTask/#commit-denied","title":"Commit Denied","text":"<p>If not authorized, <code>run</code> prints out the following INFO message to the logs and throws a <code>CommitDeniedException</code>.</p> <pre><code>Commit denied for partition [partId] (task [taskId], attempt [attemptId], stage [stageId].[stageAttempt])\n</code></pre>"},{"location":"connectors/DataWritingSparkTask/#no-commitcoordinator","title":"No CommitCoordinator <p>With the given <code>useCommitCoordinator</code> flag disabled, <code>run</code> prints out the following INFO message to the logs:</p> <pre><code>Writer for partition [partitionId] is committing.\n</code></pre>","text":""},{"location":"connectors/DataWritingSparkTask/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"connectors/DynamicPartitionDataConcurrentWriter/","title":"DynamicPartitionDataConcurrentWriter","text":"<p><code>DynamicPartitionDataConcurrentWriter</code> is...FIXME</p>"},{"location":"connectors/DynamicPartitionDataSingleWriter/","title":"DynamicPartitionDataSingleWriter","text":"<p><code>DynamicPartitionDataSingleWriter</code> is...FIXME</p>"},{"location":"connectors/FileBatchWrite/","title":"FileBatchWrite","text":"<p><code>FileBatchWrite</code> is a BatchWrite that uses the given FileCommitProtocol to coordinate a writing job (abort or commit).</p>"},{"location":"connectors/FileBatchWrite/#creating-instance","title":"Creating Instance","text":"<p><code>FileBatchWrite</code> takes the following to be created:</p> <ul> <li> Hadoop Job <li> <code>WriteJobDescription</code> <li> <code>FileCommitProtocol</code> (Spark Core) <p><code>FileBatchWrite</code> is created when:</p> <ul> <li><code>FileWrite</code> is requested for a BatchWrite</li> </ul>"},{"location":"connectors/FileBatchWrite/#aborting-write-job","title":"Aborting Write Job <pre><code>abort(\n  messages: Array[WriterCommitMessage]): Unit\n</code></pre> <p><code>abort</code> requests the FileCommitProtocol to abort the Job.</p> <p><code>abort</code> is part of the BatchWrite abstraction.</p>","text":""},{"location":"connectors/FileBatchWrite/#committing-write-job","title":"Committing Write Job <pre><code>commit(\n  messages: Array[WriterCommitMessage]): Unit\n</code></pre> <p><code>commit</code> prints out the following INFO message to the logs:</p> <pre><code>Start to commit write Job [uuid].\n</code></pre> <p><code>commit</code> requests the FileCommitProtocol to commit the Job (with the <code>WriteTaskResult</code> extracted from the given <code>WriterCommitMessage</code>s). <code>commit</code> measures the commit duration.</p> <p><code>commit</code> prints out the following INFO message to the logs:</p> <pre><code>Write Job [uuid] committed. Elapsed time: [duration] ms.\n</code></pre> <p><code>commit</code> handles the statistics of this write job.</p> <p>In the end, <code>commit</code> prints out the following INFO message to the logs:</p> <pre><code>Finished processing stats for write job [uuid].\n</code></pre>  <p><code>commit</code> is part of the BatchWrite abstraction.</p>","text":""},{"location":"connectors/FileBatchWrite/#creating-batch-datawriterfactory","title":"Creating Batch DataWriterFactory <pre><code>createBatchWriterFactory(\n  info: PhysicalWriteInfo): DataWriterFactory\n</code></pre> <p><code>createBatchWriterFactory</code> creates a new FileWriterFactory.</p> <p><code>createBatchWriterFactory</code> is part of the BatchWrite abstraction.</p>","text":""},{"location":"connectors/FileBatchWrite/#usecommitcoordinator","title":"useCommitCoordinator <pre><code>useCommitCoordinator(): Boolean\n</code></pre> <p><code>FileBatchWrite</code> does not require a Commit Coordinator (and returns <code>false</code>).</p> <p><code>useCommitCoordinator</code> is part of the BatchWrite abstraction.</p>","text":""},{"location":"connectors/FileBatchWrite/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.v2.FileBatchWrite</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.datasources.v2.FileBatchWrite=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"connectors/FileDataSourceV2/","title":"FileDataSourceV2 Table Providers","text":"<p><code>FileDataSourceV2</code> is an extension of the TableProvider abstraction for file-based table providers.</p>"},{"location":"connectors/FileDataSourceV2/#contract","title":"Contract","text":""},{"location":"connectors/FileDataSourceV2/#fallbackfileformat","title":"fallbackFileFormat <pre><code>fallbackFileFormat: Class[_ &lt;: FileFormat]\n</code></pre> <p>A V1 FileFormat class of this file-based data source</p> <p>See:</p> <ul> <li>ParquetDataSourceV2</li> </ul> <p>Used when:</p> <ul> <li><code>DDLUtils</code> is requested to <code>checkDataColNames</code></li> <li><code>DataSource</code> is requested for the providingClass (for resolving data source relation for catalog tables)</li> <li><code>PreprocessTableCreation</code> logical analysis rule is executed</li> </ul>","text":""},{"location":"connectors/FileDataSourceV2/#table","title":"Table <pre><code>getTable(\n  options: CaseInsensitiveStringMap): Table\ngetTable(\n  options: CaseInsensitiveStringMap,\n  schema: StructType): Table\ngetTable(\n  schema: StructType,\n  partitioning: Array[Transform],\n  properties: Map[String, String]): Table // (1)!\n</code></pre> <ol> <li>Part of the TableProvider abstraction</li> </ol> <p>A Table of this table provider</p> <p>See:</p> <ul> <li>ParquetDataSourceV2</li> </ul> <p>Used when:</p> <ul> <li><code>FileDataSourceV2</code> is requested for a table (as a TableProvider) and inferSchema</li> </ul>","text":""},{"location":"connectors/FileDataSourceV2/#implementations","title":"Implementations","text":"<ul> <li><code>AvroDataSourceV2</code></li> <li><code>CSVDataSourceV2</code></li> <li><code>JsonDataSourceV2</code></li> <li><code>OrcDataSourceV2</code></li> <li>ParquetDataSourceV2</li> <li><code>TextDataSourceV2</code></li> </ul>"},{"location":"connectors/FileDataSourceV2/#datasourceregister","title":"DataSourceRegister <p><code>FileDataSourceV2</code> is a DataSourceRegister.</p>","text":""},{"location":"connectors/FileDataSourceV2/#schema-inference","title":"Schema Inference <pre><code>inferSchema(\n  options: CaseInsensitiveStringMap): StructType\n</code></pre> <p><code>inferSchema</code> is part of the TableProvider abstraction.</p>  <p><code>inferSchema</code> requests the Table for the schema.</p> <p>If not available, <code>inferSchema</code> creates a Table and \"saves\" it for later (in t registry).</p>","text":""},{"location":"connectors/FileDataSourceV2/#table-name","title":"Table Name <pre><code>getTableName(\n  map: CaseInsensitiveStringMap,\n  paths: Seq[String]): String\n</code></pre> <p><code>getTableName</code> uses short name and the given <code>paths</code> to create the following table name (possibly redacting sensitive parts per spark.sql.redaction.string.regex):</p> <pre><code>[short name] [comma-separated paths]\n</code></pre>","text":""},{"location":"connectors/FileDataSourceV2/#paths","title":"Paths <pre><code>getPaths(\n  map: CaseInsensitiveStringMap): Seq[String]\n</code></pre> <p><code>getPaths</code> concatenates the values of the <code>paths</code> and <code>path</code> keys (from the given <code>map</code>).</p>","text":""},{"location":"connectors/FileFormat/","title":"FileFormat","text":"<p><code>FileFormat</code> is an abstraction of data sources that can read and write data stored in files.</p>"},{"location":"connectors/FileFormat/#contract","title":"Contract","text":""},{"location":"connectors/FileFormat/#building-data-reader","title":"Building Data Reader <pre><code>buildReader(\n  sparkSession: SparkSession,\n  dataSchema: StructType,\n  partitionSchema: StructType,\n  requiredSchema: StructType,\n  filters: Seq[Filter],\n  options: Map[String, String],\n  hadoopConf: Configuration): PartitionedFile =&gt; Iterator[InternalRow]\n</code></pre> <p>Builds a Catalyst data reader (a function that reads a single PartitionedFile file in to produce InternalRows).</p> <p><code>buildReader</code> throws an <code>UnsupportedOperationException</code> by default (and should therefore be overriden to work):</p> <pre><code>buildReader is not supported for [this]\n</code></pre> <p>Used when <code>FileFormat</code> is requested to buildReaderWithPartitionValues.</p>","text":""},{"location":"connectors/FileFormat/#schema-inference","title":"Schema Inference <pre><code>inferSchema(\n  sparkSession: SparkSession,\n  options: Map[String, String],\n  files: Seq[FileStatus]): Option[StructType]\n</code></pre> <p>Infers the schema of the given files (as Hadoop FileStatuses) if supported. Otherwise, <code>None</code> should be returned.</p> <p>Used when:</p> <ul> <li><code>HiveMetastoreCatalog</code> is requested to inferIfNeeded</li> <li><code>DataSource</code> is requested to getOrInferFileFormatSchema and resolveRelation</li> </ul>","text":""},{"location":"connectors/FileFormat/#issplitable","title":"isSplitable <pre><code>isSplitable(\n  sparkSession: SparkSession,\n  options: Map[String, String],\n  path: Path): Boolean\n</code></pre> <p>Controls whether this format (under the given Hadoop Path and the <code>options</code>) is splittable or not</p> <p>Default: <code>false</code></p> <p>Always splitable:</p> <ul> <li>AvroFileFormat</li> <li><code>OrcFileFormat</code></li> <li>ParquetFileFormat</li> </ul> <p>Never splitable:</p> <ul> <li><code>BinaryFileFormat</code></li> </ul> <p>Used when:</p> <ul> <li><code>FileSourceScanExec</code> physical operator is requested to create an RDD for a non-bucketed read (when requested for the inputRDD)</li> </ul>","text":""},{"location":"connectors/FileFormat/#preparing-write","title":"Preparing Write <pre><code>prepareWrite(\n  sparkSession: SparkSession,\n  job: Job,\n  options: Map[String, String],\n  dataSchema: StructType): OutputWriterFactory\n</code></pre> <p>Prepares a write job and returns an <code>OutputWriterFactory</code></p> <p>Used when <code>FileFormatWriter</code> utility is used to write out a query result</p>","text":""},{"location":"connectors/FileFormat/#supportbatch","title":"supportBatch <pre><code>supportBatch(\n  sparkSession: SparkSession,\n  dataSchema: StructType): Boolean\n</code></pre> <p>Whether this format supports vectorized decoding or not</p> <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li><code>FileSourceScanExec</code> physical operator is requested for the supportsBatch flag</li> <li><code>OrcFileFormat</code> is requested to <code>buildReaderWithPartitionValues</code></li> <li><code>ParquetFileFormat</code> is requested to buildReaderWithPartitionValues</li> </ul>","text":""},{"location":"connectors/FileFormat/#supportdatatype","title":"supportDataType <pre><code>supportDataType(\n  dataType: DataType): Boolean\n</code></pre> <p>Controls whether this format supports the given DataType in read or write paths</p> <p>Default: <code>true</code> (all data types are supported)</p> <p>Used when <code>DataSourceUtils</code> is used to <code>verifySchema</code></p>","text":""},{"location":"connectors/FileFormat/#vector-types","title":"Vector Types <pre><code>vectorTypes(\n  requiredSchema: StructType,\n  partitionSchema: StructType,\n  sqlConf: SQLConf): Option[Seq[String]]\n</code></pre> <p>Defines the fully-qualified class names (types) of the concrete ColumnVectors for every column in the input <code>requiredSchema</code> and <code>partitionSchema</code> schemas (to use in columnar processing mode)</p> <p>Default: <code>None</code> (undefined)</p> <p>Used when:</p> <ul> <li><code>FileSourceScanExec</code> physical operator is requested for the vectorTypes</li> </ul>","text":""},{"location":"connectors/FileFormat/#implementations","title":"Implementations","text":"<ul> <li>AvroFileFormat</li> <li><code>BinaryFileFormat</code></li> <li>HiveFileFormat</li> <li><code>ImageFileFormat</code></li> <li><code>OrcFileFormat</code></li> <li>ParquetFileFormat</li> <li><code>TextBasedFileFormat</code></li> </ul>"},{"location":"connectors/FileFormat/#building-data-reader-with-partition-values","title":"Building Data Reader With Partition Values <pre><code>buildReaderWithPartitionValues(\n  sparkSession: SparkSession,\n  dataSchema: StructType,\n  partitionSchema: StructType,\n  requiredSchema: StructType,\n  filters: Seq[Filter],\n  options: Map[String, String],\n  hadoopConf: Configuration): PartitionedFile =&gt; Iterator[InternalRow]\n</code></pre> <p><code>buildReaderWithPartitionValues</code> builds a data reader with partition column values appended.</p>  <p>Note</p> <p><code>buildReaderWithPartitionValues</code> is simply an enhanced buildReader that appends partition column values to the internal rows produced by the reader function.</p>  <p><code>buildReaderWithPartitionValues</code> builds a data reader with the input parameters and gives a data reader function (of a PartitionedFile to an <code>Iterator[InternalRow]</code>) that does the following:</p> <ol> <li> <p>Creates a converter by requesting <code>GenerateUnsafeProjection</code> to generate an UnsafeProjection for the attributes of the input <code>requiredSchema</code> and <code>partitionSchema</code></p> </li> <li> <p>Applies the data reader to a <code>PartitionedFile</code> and converts the result using the converter on the joined row with the partition column values appended.</p> </li> </ol> <p><code>buildReaderWithPartitionValues</code> is used when <code>FileSourceScanExec</code> physical operator is requested for the inputRDD.</p>","text":""},{"location":"connectors/FileFormatDataWriter/","title":"FileFormatDataWriter","text":"<p><code>FileFormatDataWriter</code> is an extension of the DataWriter abstraction for data writers (of InternalRows).</p>"},{"location":"connectors/FileFormatDataWriter/#contract","title":"Contract","text":""},{"location":"connectors/FileFormatDataWriter/#writing-record-out","title":"Writing Record Out <pre><code>write(\n  record: InternalRow): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>FileFormatDataWriter</code> is requested to writeWithMetrics</li> </ul>","text":""},{"location":"connectors/FileFormatDataWriter/#implementations","title":"Implementations","text":"<ul> <li>BaseDynamicPartitionDataWriter</li> <li><code>EmptyDirectoryDataWriter</code></li> <li>SingleDirectoryDataWriter</li> </ul>"},{"location":"connectors/FileFormatDataWriter/#creating-instance","title":"Creating Instance","text":"<p><code>FileFormatDataWriter</code> takes the following to be created:</p> <ul> <li> <code>WriteJobDescription</code> <li> <code>TaskAttemptContext</code> (Apache Hadoop) <li> <code>FileCommitProtocol</code> (Spark Core) <li> Custom SQLMetrics by name (<code>Map[String, SQLMetric]</code>) <p>Abstract Class</p> <p><code>FileFormatDataWriter</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete FileFormatDataWriters.</p>"},{"location":"connectors/FileFormatDataWriter/#writewithmetrics","title":"writeWithMetrics <pre><code>writeWithMetrics(\n  record: InternalRow,\n  count: Long): Unit\n</code></pre> <p><code>writeWithMetrics</code> updates the CustomTaskMetrics with the customMetrics and writes out the given InternalRow.</p> <p><code>writeWithMetrics</code> is used when:</p> <ul> <li><code>FileFormatDataWriter</code> is requested to write out (a collection of) records</li> </ul>","text":""},{"location":"connectors/FileFormatDataWriter/#writing-out-collection-of-records","title":"Writing Out (Collection of) Records <pre><code>writeWithIterator(\n  iterator: Iterator[InternalRow]): Unit\n</code></pre> <p><code>writeWithIterator</code>...FIXME</p> <p><code>writeWithIterator</code> is used when:</p> <ul> <li><code>FileFormatWriter</code> utility is used to write data out in a single Spark task</li> </ul>","text":""},{"location":"connectors/FileFormatDataWriter/#committing-successful-write","title":"Committing Successful Write <pre><code>commit(): WriteTaskResult\n</code></pre> <p><code>commit</code> releaseResources.</p> <p><code>commit</code> requests the FileCommitProtocol to <code>commitTask</code> (that gives a <code>TaskCommitMessage</code>).</p> <p><code>commit</code> creates a new <code>ExecutedWriteSummary</code> with the updatedPartitions and the WriteTaskStats of the WriteTaskStatsTrackers.</p> <p>In the end, <code>commit</code> creates a <code>WriteTaskResult</code> (for the <code>TaskCommitMessage</code> and the <code>ExecutedWriteSummary</code>).</p>  <p><code>commit</code> is part of the DataWriter abstraction.</p>","text":""},{"location":"connectors/FileFormatWriter/","title":"FileFormatWriter","text":"<p><code>FileFormatWriter</code> utility is used to write out query result for the following:</p> <ul> <li>Hive-related InsertIntoHiveDirCommand and InsertIntoHiveTable logical commands (via SaveAsHiveFile.saveAsHiveFile)</li> <li>InsertIntoHadoopFsRelationCommand logical command</li> <li><code>FileStreamSink</code> (Spark Structured Streaming) is requested to write out a micro-batch</li> </ul>"},{"location":"connectors/FileFormatWriter/#write","title":"Writing Out Query Result","text":"<pre><code>write(\nsparkSession: SparkSession,\nplan: SparkPlan,\nfileFormat: FileFormat,\ncommitter: FileCommitProtocol,\noutputSpec: OutputSpec,\nhadoopConf: Configuration,\npartitionColumns: Seq[Attribute],\nbucketSpec: Option[BucketSpec],\nstatsTrackers: Seq[WriteJobStatsTracker],\noptions: Map[String, String],\nnumStaticPartitionCols: Int = 0): Set[String]\n</code></pre> <p><code>write</code> creates a Hadoop Job instance (with the given Hadoop Configuration) and uses the following job output classes:</p> Job Output Property Class Key <code>Void</code> Value <code>InternalRow</code> <p><code>write</code> sets the output directory (<code>mapreduce.output.fileoutputformat.outputdir</code> property of the map-reduce job) to be the <code>outputPath</code> of the given <code>OutputSpec</code>.</p> <p> <code>write</code> requests the given <code>FileFormat</code> to prepareWrite. <p> <code>write</code> creates a <code>WriteJobDescription</code> with the following options (if available): Option Fallback Configuration Property <code>maxRecordsPerFile</code> spark.sql.files.maxRecordsPerFile <code>timeZone</code> spark.sql.session.timeZone <p><code>write</code> sets <code>spark.sql.sources.writeJobUUID</code> configuration in the map-reduce Job instance.</p> <p>In the end, <code>write</code> executeWrite.</p>"},{"location":"connectors/FileFormatWriter/#executeWrite","title":"executeWrite","text":"<pre><code>executeWrite(\nsession: SparkSession,\nplanForWrites: SparkPlan,\nwriteFilesSpec: WriteFilesSpec,\njob: Job): Set[String]\nexecuteWrite(\nsparkSession: SparkSession,\nplan: SparkPlan,\njob: Job,\ndescription: WriteJobDescription,\ncommitter: FileCommitProtocol,\noutputSpec: OutputSpec,\nrequiredOrdering: Seq[Expression],\npartitionColumns: Seq[Attribute],\nsortColumns: Seq[Attribute],\norderingMatched: Boolean): Set[String]\n</code></pre> <p><code>executeWrite</code> writeAndCommit (with the given Hadoop <code>Job</code>, <code>WriteJobDescription</code>, and <code>FileCommitProtocol</code>) and a function that does the following:</p> <ol> <li>Prepares an <code>RDD[WriterCommitMessage]</code> (by executing the given <code>WriteFilesSpec</code> or the <code>SparkPlan</code>)</li> <li>Runs a Spark job for the <code>RDD[WriterCommitMessage]</code> that \"collects\" <code>WriteTaskResult</code>s (from executing write tasks)</li> </ol>"},{"location":"connectors/FileFormatWriter/#writeAndCommit","title":"writeAndCommit","text":"<pre><code>writeAndCommit(\njob: Job,\ndescription: WriteJobDescription,\ncommitter: FileCommitProtocol)(\nf: =&gt; Array[WriteTaskResult]): Set[String]\n</code></pre> <p><code>writeAndCommit</code>...FIXME</p> <p><code>writeAndCommit</code> prints out the following INFO message to the logs:</p> <pre><code>Start to commit write Job [uuid].\n</code></pre> <p><code>writeAndCommit</code> requests the given <code>FileCommitProtocol</code> to <code>commitJob</code>.</p> <p><code>writeAndCommit</code> prints out the following INFO message to the logs:</p> <pre><code>Write Job [uuid] committed. Elapsed time: [duration] ms.\n</code></pre> <p><code>writeAndCommit</code> processStats.</p> <p><code>writeAndCommit</code> prints out the following INFO message to the logs:</p> <pre><code>Finished processing stats for write job [uuid].\n</code></pre> <p><code>writeAndCommit</code> returns the updated partitions.</p> <p>In case of any <code>Throwable</code>, <code>writeAndCommit</code> prints out the following ERROR message to the logs:</p> <pre><code>Aborting job [uuid].\n</code></pre> <p><code>writeAndCommit</code> requests the given <code>FileCommitProtocol</code> to <code>abortJob</code>.</p>"},{"location":"connectors/FileFormatWriter/#write-usage","title":"Usage","text":"<p><code>write</code> is used when:</p> <ul> <li><code>SaveAsHiveFile</code> is requested to saveAsHiveFile</li> <li>InsertIntoHadoopFsRelationCommand logical command is executed</li> <li><code>FileStreamSink</code> (Spark Structured Streaming) is requested to <code>addBatch</code></li> </ul>"},{"location":"connectors/FileFormatWriter/#review-me","title":"Review Me","text":"<p><code>write</code> requests the given <code>FileCommitProtocol</code> committer to <code>setupJob</code>.</p> <p> <code>write</code> executes the given SparkPlan (and generates an RDD). The execution can be directly on the given physical operator if ordering matches the requirements or uses SortExec physical operator (with <code>global</code> flag off). <p> <code>write</code> runs a Spark job (action) on the RDD with executeTask as the partition function. The result task handler simply requests the given <code>FileCommitProtocol</code> committer to <code>onTaskCommit</code> (with the <code>TaskCommitMessage</code> of a <code>WriteTaskResult</code>) and saves the <code>WriteTaskResult</code>. <p> <code>write</code> requests the given <code>FileCommitProtocol</code> committer to <code>commitJob</code> (with the Hadoop <code>Job</code> instance and the <code>TaskCommitMessage</code> of all write tasks). <p><code>write</code> prints out the following INFO message to the logs:</p> <pre><code>Write Job [uuid] committed.\n</code></pre> <p> <code>write</code> processStats. <p><code>write</code> prints out the following INFO message to the logs:</p> <pre><code>Finished processing stats for write job [uuid].\n</code></pre> <p>In the end, <code>write</code> returns all the partition paths that were updated during this write job.</p>"},{"location":"connectors/FileFormatWriter/#write-and-throwables","title":"write And Throwables <p>In case of any <code>Throwable</code>, <code>write</code> prints out the following ERROR message to the logs:</p> <pre><code>Aborting job [uuid].\n</code></pre> <p> <code>write</code> requests the given <code>FileCommitProtocol</code> committer to <code>abortJob</code> (with the Hadoop <code>Job</code> instance). <p>In the end, <code>write</code> throws a <code>SparkException</code>.</p>","text":""},{"location":"connectors/FileFormatWriter/#writing-data-out-in-single-spark-task","title":"Writing Data Out In Single Spark Task <pre><code>executeTask(\n  description: WriteJobDescription,\n  jobIdInstant: Long,\n  sparkStageId: Int,\n  sparkPartitionId: Int,\n  sparkAttemptNumber: Int,\n  committer: FileCommitProtocol,\n  iterator: Iterator[InternalRow]): WriteTaskResult\n</code></pre> <p><code>executeTask</code>...FIXME</p>","text":""},{"location":"connectors/FileFormatWriter/#processing-write-job-statistics","title":"Processing Write Job Statistics <pre><code>processStats(\n  statsTrackers: Seq[WriteJobStatsTracker],\n  statsPerTask: Seq[Seq[WriteTaskStats]],\n  jobCommitDuration: Long): Unit\n</code></pre> <p><code>processStats</code> requests every WriteJobStatsTracker to processStats (for respective WriteTaskStats in the given <code>statsPerTask</code>).</p>","text":""},{"location":"connectors/FileFormatWriter/#logging","title":"Logging","text":"<p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.FileFormatWriter</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.FileFormatWriter.name = org.apache.spark.sql.execution.datasources.FileFormatWriter\nlogger.FileFormatWriter.level = all\n</code></pre> <p>Refer to Logging.</p>"},{"location":"connectors/FileIndex/","title":"FileIndex","text":"<p><code>FileIndex</code> is an abstraction of file indices for root paths and partition schema that make up a relation.</p> <p><code>FileIndex</code> is an optimization technique that is used with a HadoopFsRelation to avoid expensive file listings (esp. on object storages like Amazon S3 or Google Cloud Storage)</p>"},{"location":"connectors/FileIndex/#contract","title":"Contract","text":""},{"location":"connectors/FileIndex/#input-files","title":"Input Files <pre><code>inputFiles: Array[String]\n</code></pre> <p>File names to read when scanning this relation</p> <p>Used when:</p> <ul> <li><code>Dataset</code> is requested for inputFiles</li> <li><code>HadoopFsRelation</code> is requested for input files</li> </ul>","text":""},{"location":"connectors/FileIndex/#listing-files","title":"Listing Files <pre><code>listFiles(\n  partitionFilters: Seq[Expression],\n  dataFilters: Seq[Expression]): Seq[PartitionDirectory]\n</code></pre> <p>File names (grouped into partitions when the data is partitioned)</p> <p>Used when:</p> <ul> <li><code>HiveMetastoreCatalog</code> is requested to convert a HiveTableRelation to a LogicalRelation</li> <li><code>FileSourceScanExec</code> physical operator is requested for selectedPartitions</li> <li>OptimizeMetadataOnlyQuery logical optimization is executed</li> <li><code>FileScan</code> is requested for partitions</li> </ul>","text":""},{"location":"connectors/FileIndex/#metadata-duration","title":"Metadata Duration <pre><code>metadataOpsTimeNs: Option[Long] = None\n</code></pre> <p>Metadata operation time for listing files (in nanoseconds)</p> <p>Used when <code>FileSourceScanExec</code> physical operator is requested for partitions</p>","text":""},{"location":"connectors/FileIndex/#partitions","title":"Partitions <pre><code>partitionSchema: StructType\n</code></pre> <p>Partition schema (StructType)</p> <p>Used when:</p> <ul> <li><code>DataSource</code> is requested to getOrInferFileFormatSchema and resolve a FileFormat-based relation</li> <li><code>FallBackFileSourceV2</code> logical resolution rule is executed</li> <li>FileScanBuilder is created</li> <li><code>FileTable</code> is requested for dataSchema and partitioning</li> </ul>","text":""},{"location":"connectors/FileIndex/#refreshing-cached-file-listings","title":"Refreshing Cached File Listings <pre><code>refresh(): Unit\n</code></pre> <p>Refreshes the file listings that may have been cached</p> <p>Used when:</p> <ul> <li><code>CacheManager</code> is requested to recacheByPath</li> <li>InsertIntoHadoopFsRelationCommand is executed</li> <li><code>LogicalRelation</code> logical operator is requested to refresh (for a HadoopFsRelation)</li> </ul>","text":""},{"location":"connectors/FileIndex/#root-paths","title":"Root Paths <pre><code>rootPaths: Seq[Path]\n</code></pre> <p>Root paths from which the catalog gets the files (as Hadoop <code>Path</code>s). There could be a single root path of the entire table (with partition directories) or individual partitions.</p> <p>Used when:</p> <ul> <li><code>HiveMetastoreCatalog</code> is requested for a cached LogicalRelation (when requested to convert a HiveTableRelation)</li> <li><code>OptimizedCreateHiveTableAsSelectCommand</code> is executed</li> <li><code>CacheManager</code> is requested to recache by path</li> <li><code>FileSourceScanExec</code> physical operator is requested for the metadata and verboseStringWithOperatorId</li> <li><code>DDLUtils</code> utility is used to <code>verifyNotReadPath</code></li> <li>DataSourceAnalysis logical resolution rule is executed (for an <code>InsertIntoStatement</code> over a HadoopFsRelation)</li> <li><code>FileScan</code> is requested for a description</li> </ul>","text":""},{"location":"connectors/FileIndex/#estimated-size","title":"Estimated Size <pre><code>sizeInBytes: Long\n</code></pre> <p>Estimated size of the data of the relation (in bytes)</p> <p>Used when:</p> <ul> <li><code>HadoopFsRelation</code> is requested for the estimated size</li> <li><code>FileScan</code> is requested for statistics</li> </ul>","text":""},{"location":"connectors/FileIndex/#implementations","title":"Implementations","text":"<ul> <li>CatalogFileIndex</li> <li>PartitioningAwareFileIndex</li> </ul>"},{"location":"connectors/FilePartition/","title":"FilePartition","text":"<p><code>FilePartition</code> is a <code>Partition</code> (Apache Spark).</p> <p><code>FilePartition</code> is an InputPartition with a collection of file blocks (that should be read by a single task).</p> <p><code>FilePartition</code> is used in the following:</p> <ul> <li><code>FileSourceScanExec</code> physical operator is requested to createBucketedReadRDD and createReadRDD</li> <li><code>FileScan</code> is requested to plan input partitions (for BatchScanExec physical operator)</li> </ul>"},{"location":"connectors/FilePartition/#creating-instance","title":"Creating Instance","text":"<p><code>FilePartition</code> takes the following to be created:</p> <ul> <li> Partition Index <li> PartitionedFiles <p><code>FilePartition</code> is created when:</p> <ul> <li><code>FileSourceScanExec</code> physical operator is requested to createBucketedReadRDD</li> <li><code>FilePartition</code> is requested to getFilePartitions</li> </ul>"},{"location":"connectors/FilePartition/#getfilepartitions","title":"getFilePartitions <pre><code>getFilePartitions(\n  sparkSession: SparkSession,\n  partitionedFiles: Seq[PartitionedFile],\n  maxSplitBytes: Long): Seq[FilePartition]\n</code></pre> <p><code>getFilePartitions</code>...FIXME</p>  <p><code>getFilePartitions</code> is used when:</p> <ul> <li><code>FileSourceScanExec</code> physical operator is requested to createReadRDD</li> <li><code>FileScan</code> is requested for the partitions</li> </ul>","text":""},{"location":"connectors/FilePartition/#preferredlocations","title":"preferredLocations  Signature <pre><code>preferredLocations(): Array[String]\n</code></pre> <p><code>preferredLocations</code> is part of the InputPartition abstraction.</p>  <p><code>preferredLocations</code>...FIXME</p>","text":""},{"location":"connectors/FilePartition/#maxsplitbytes","title":"maxSplitBytes <pre><code>maxSplitBytes(\n  sparkSession: SparkSession,\n  selectedPartitions: Seq[PartitionDirectory]): Long\n</code></pre>  <p><code>maxSplitBytes</code> can be adjusted based on the following configuration properties:</p> <ul> <li>spark.sql.files.maxPartitionBytes</li> <li>spark.sql.files.openCostInBytes</li> <li>spark.sql.files.minPartitionNum (default: Default Parallelism of Leaf Nodes)</li> </ul>  <p><code>maxSplitBytes</code> calculates the total size of all the files (in the given <code>PartitionDirectory</code>ies) with spark.sql.files.openCostInBytes overhead added (to the size of every file).</p>  PartitionDirectory <p><code>PartitionDirectory</code> is a collection of <code>FileStatus</code>es (Apache Hadoop) along with partition values (if there are any).</p>  <p><code>maxSplitBytes</code> calculates how many bytes to allow per partition (<code>bytesPerCore</code>) that is the total size of all the files divided by spark.sql.files.minPartitionNum configuration property.</p> <p>In the end, <code>maxSplitBytes</code> is spark.sql.files.maxPartitionBytes unless the maximum of spark.sql.files.openCostInBytes and <code>bytesPerCore</code> is even smaller.</p>  <p><code>maxSplitBytes</code> is used when:</p> <ul> <li><code>FileSourceScanExec</code> physical operator is requested to create an RDD for scanning (and creates a FileScanRDD)</li> <li><code>FileScan</code> is requested for partitions</li> </ul>","text":""},{"location":"connectors/FilePartitionReaderFactory/","title":"FilePartitionReaderFactory","text":"<p><code>FilePartitionReaderFactory</code> is an extension of the PartitionReaderFactory abstraction for PartitionReader factories of file-based connectors.</p>"},{"location":"connectors/FilePartitionReaderFactory/#contract","title":"Contract","text":""},{"location":"connectors/FilePartitionReaderFactory/#building-partitionreader","title":"Building PartitionReader <pre><code>buildReader(\n  partitionedFile: PartitionedFile): PartitionReader[InternalRow]\n</code></pre> <p>PartitionReader (of InternalRows)</p> <p>See:</p> <ul> <li>ParquetPartitionReaderFactory</li> </ul> <p>Used when:</p> <ul> <li><code>FilePartitionReaderFactory</code> is requested to create a reader</li> </ul>","text":""},{"location":"connectors/FilePartitionReaderFactory/#building-columnar-partitionreader","title":"Building Columnar PartitionReader <pre><code>buildColumnarReader(\n  partitionedFile: PartitionedFile): PartitionReader[ColumnarBatch]\n</code></pre> <p>PartitionReader (of ColumnarBatchs)</p> <p>See:</p> <ul> <li>ParquetPartitionReaderFactory</li> </ul> <p>Used when:</p> <ul> <li><code>FilePartitionReaderFactory</code> is requested to create a columnar reader</li> </ul>","text":""},{"location":"connectors/FilePartitionReaderFactory/#options","title":"Options <pre><code>options: FileSourceOptions\n</code></pre> <p>See:</p> <ul> <li>ParquetPartitionReaderFactory</li> </ul> <p>Used when:</p> <ul> <li><code>FilePartitionReaderFactory</code> is requested to create a reader and columnar reader</li> </ul>","text":""},{"location":"connectors/FilePartitionReaderFactory/#implementations","title":"Implementations","text":"<ul> <li><code>AvroPartitionReaderFactory</code></li> <li><code>CSVPartitionReaderFactory</code></li> <li><code>JsonPartitionReaderFactory</code></li> <li><code>OrcPartitionReaderFactory</code></li> <li>ParquetPartitionReaderFactory</li> <li><code>TextPartitionReaderFactory</code></li> </ul>"},{"location":"connectors/FilePartitionReaderFactory/#creating-partitionreader","title":"Creating PartitionReader  Signature <pre><code>createReader(\n  partition: InputPartition): PartitionReader[InternalRow]\n</code></pre> <p><code>createReader</code> is part of the PartitionReaderFactory abstraction.</p>  <p><code>createReader</code>...FIXME</p>","text":""},{"location":"connectors/FilePartitionReaderFactory/#creating-columnar-partitionreader","title":"Creating Columnar PartitionReader  Signature <pre><code>createColumnarReader(\n  partition: InputPartition): PartitionReader[ColumnarBatch]\n</code></pre> <p><code>createColumnarReader</code> is part of the PartitionReaderFactory abstraction.</p>  <p><code>createColumnarReader</code> makes sure that the given InputPartition is a FilePartition (or throws an <code>AssertionError</code>).</p> <p><code>createColumnarReader</code> creates a new columnar PartitionReader for every PartitionedFile (of the <code>FilePartition</code>).</p> <p>In the end, <code>createColumnarReader</code> creates a <code>FilePartitionReader</code> for the files.</p>","text":""},{"location":"connectors/FileScan/","title":"FileScan","text":"<p><code>FileScan</code> is an extension of the Scan abstraction for scans in Batch queries.</p>"},{"location":"connectors/FileScan/#supportsreportstatistics","title":"SupportsReportStatistics <p><code>FileScan</code> is a SupportsReportStatistics.</p>","text":""},{"location":"connectors/FileScan/#contract","title":"Contract","text":""},{"location":"connectors/FileScan/#datafilters","title":"DataFilters <pre><code>dataFilters: Seq[Expression]\n</code></pre> <p>Expressions</p> <p>Used when:</p> <ul> <li><code>FileScan</code> is requested for normalized DataFilters, metadata, partitions</li> </ul>","text":""},{"location":"connectors/FileScan/#fileindex","title":"FileIndex <pre><code>fileIndex: PartitioningAwareFileIndex\n</code></pre> <p>PartitioningAwareFileIndex</p>","text":""},{"location":"connectors/FileScan/#getfileunsplittablereason","title":"getFileUnSplittableReason <pre><code>getFileUnSplittableReason(\n  path: Path): String\n</code></pre>","text":""},{"location":"connectors/FileScan/#partition-filters","title":"Partition Filters <pre><code>partitionFilters: Seq[Expression]\n</code></pre> <p>Expressions</p>","text":""},{"location":"connectors/FileScan/#read-data-schema","title":"Read Data Schema <pre><code>readDataSchema: StructType\n</code></pre> <p>StructType</p>  <p>Three Schemas</p> <p>Beside the read data schema of a <code>FileScan</code>, there are two others:</p> <ol> <li>readPartitionSchema</li> <li>readSchema</li> </ol>","text":""},{"location":"connectors/FileScan/#read-partition-schema","title":"Read Partition Schema <pre><code>readPartitionSchema: StructType\n</code></pre>","text":""},{"location":"connectors/FileScan/#seqtostring","title":"seqToString <pre><code>seqToString(\n  seq: Seq[Any]): String\n</code></pre>","text":""},{"location":"connectors/FileScan/#sparksession","title":"sparkSession <pre><code>sparkSession: SparkSession\n</code></pre> <p>SparkSession associated with this <code>FileScan</code></p>","text":""},{"location":"connectors/FileScan/#withfilters","title":"withFilters <pre><code>withFilters(\n  partitionFilters: Seq[Expression],\n  dataFilters: Seq[Expression]): FileScan\n</code></pre>","text":""},{"location":"connectors/FileScan/#implementations","title":"Implementations <ul> <li>ParquetScan</li> <li>others</li> </ul>","text":""},{"location":"connectors/FileScan/#description","title":"description <pre><code>description(): String\n</code></pre> <p><code>description</code> is part of the Scan abstraction.</p>  <p><code>description</code>...FIXME</p>","text":""},{"location":"connectors/FileScan/#planInputPartitions","title":"Planning Input Partitions  Signature <pre><code>planInputPartitions(): Array[InputPartition]\n</code></pre> <p><code>planInputPartitions</code> is part of the Batch abstraction.</p>  <p><code>planInputPartitions</code> is the file partitions.</p>","text":""},{"location":"connectors/FileScan/#partitions","title":"File Partitions <pre><code>partitions: Seq[FilePartition]\n</code></pre> <p><code>partitions</code> requests the PartitioningAwareFileIndex for the partition directories (selectedPartitions).</p> <p>For every selected partition directory, <code>partitions</code> requests the Hadoop FileStatuses that are split (if isSplitable) to maxSplitBytes and sorted by size (in reversed order).</p> <p>In the end, <code>partitions</code> returns the FilePartitions.</p>","text":""},{"location":"connectors/FileScan/#estimatestatistics","title":"estimateStatistics <pre><code>estimateStatistics(): Statistics\n</code></pre> <p><code>estimateStatistics</code> is part of the SupportsReportStatistics abstraction.</p>  <p><code>estimateStatistics</code>...FIXME</p>","text":""},{"location":"connectors/FileScan/#converting-to-batch","title":"Converting to Batch <pre><code>toBatch: Batch\n</code></pre> <p><code>toBatch</code> is part of the Scan abstraction.</p>  <p><code>toBatch</code> is this FileScan.</p>","text":""},{"location":"connectors/FileScan/#read-schema","title":"Read Schema <pre><code>readSchema(): StructType\n</code></pre> <p><code>readSchema</code> is part of the Scan abstraction.</p>  <p><code>readSchema</code> is the readDataSchema with the readPartitionSchema.</p>","text":""},{"location":"connectors/FileScan/#issplitable","title":"isSplitable <pre><code>isSplitable(\n  path: Path): Boolean\n</code></pre> <p><code>isSplitable</code> is disabled by default (<code>false</code>).</p>    FileScan isSplitable     <code>AvroScan</code> <code>true</code>   ParquetScan isSplitable     <p>Used when:</p> <ul> <li><code>FileScan</code> is requested to getFileUnSplittableReason and partitions</li> </ul>","text":""},{"location":"connectors/FileScan/#supportsmetadata","title":"SupportsMetadata <p><code>FileScan</code> is a SupportsMetadata.</p>","text":""},{"location":"connectors/FileScan/#metadata","title":"Metadata <pre><code>getMetaData(): Map[String, String]\n</code></pre> <p><code>getMetaData</code> is part of the SupportsMetadata abstraction.</p>  <p><code>getMetaData</code> returns the following metadata:</p>    Name Description     <code>Format</code> The lower-case name of this FileScan (with <code>Scan</code> removed)   <code>ReadSchema</code> catalogString of the Read Data Schema   <code>PartitionFilters</code> Partition Filters   <code>DataFilters</code> Data Filters   <code>Location</code> PartitioningAwareFileIndex followed by root paths (with their number in the file listing up to spark.sql.maxMetadataStringLength)","text":""},{"location":"connectors/FileScanBuilder/","title":"FileScanBuilder","text":"<p><code>FileScanBuilder</code> is...FIXME</p>"},{"location":"connectors/FileStatusCache/","title":"FileStatusCache","text":"<p><code>FileStatusCache</code> is an abstraction of Spark application-wide FileStatus Caches for Partition File Metadata Caching.</p> <p><code>FileStatusCache</code> is created using FileStatusCache.getOrCreate factory.</p> <p><code>FileStatusCache</code> is used to create an InMemoryFileIndex.</p>"},{"location":"connectors/FileStatusCache/#contract","title":"Contract","text":""},{"location":"connectors/FileStatusCache/#getleaffiles","title":"getLeafFiles <pre><code>getLeafFiles(\n  path: Path): Option[Array[FileStatus]]\n</code></pre> <p>Default: <code>None</code> (undefined)</p> <p>See:</p> <ul> <li>SharedInMemoryCache</li> </ul> <p>Used when:</p> <ul> <li><code>InMemoryFileIndex</code> is requested to listLeafFiles</li> </ul>","text":""},{"location":"connectors/FileStatusCache/#invalidateall","title":"invalidateAll <pre><code>invalidateAll(): Unit\n</code></pre> <p>See:</p> <ul> <li>SharedInMemoryCache</li> </ul> <p>Used when:</p> <ul> <li><code>CatalogFileIndex</code> is requested to refresh</li> <li><code>InMemoryFileIndex</code> is requested to refresh</li> </ul>","text":""},{"location":"connectors/FileStatusCache/#putleaffiles","title":"putLeafFiles <pre><code>putLeafFiles(\n  path: Path,\n  leafFiles: Array[FileStatus]): Unit\n</code></pre> <p>See:</p> <ul> <li>SharedInMemoryCache</li> </ul> <p>Used when:</p> <ul> <li><code>InMemoryFileIndex</code> is requested to listLeafFiles</li> </ul>","text":""},{"location":"connectors/FileStatusCache/#implementations","title":"Implementations","text":"<ul> <li><code>NoopCache</code></li> <li>SharedInMemoryCache</li> </ul>"},{"location":"connectors/FileStatusCache/#looking-up-filestatuscache","title":"Looking Up FileStatusCache <pre><code>getOrCreate(\n  session: SparkSession): FileStatusCache\n</code></pre> <p><code>getOrCreate</code> creates a SharedInMemoryCache when all the following hold:</p> <ul> <li>spark.sql.hive.manageFilesourcePartitions is enabled</li> <li>spark.sql.hive.filesourcePartitionFileCacheSize is greater than <code>0</code></li> </ul> <p><code>getOrCreate</code> requests the <code>SharedInMemoryCache</code> to createForNewClient.</p> <p>Otherwise, <code>getOrCreate</code> returns the <code>NoopCache</code> (that does no caching).</p>  <p><code>getOrCreate</code> is used when:</p> <ul> <li><code>CatalogFileIndex</code> is requested for the FileStatusCache</li> <li><code>DataSource</code> is requested to create an InMemoryFileIndex</li> <li><code>FileTable</code> is requested for the PartitioningAwareFileIndex (for a non-streaming file-based datasource)</li> </ul>","text":""},{"location":"connectors/FileTable/","title":"FileTable","text":"<p><code>FileTable</code> is an extension of the Table abstraction for file-based tables with support for read and write.</p>"},{"location":"connectors/FileTable/#contract","title":"Contract","text":""},{"location":"connectors/FileTable/#fallback-fileformat","title":"Fallback FileFormat <pre><code>fallbackFileFormat: Class[_ &lt;: FileFormat]\n</code></pre> <p>Fallback V1 FileFormat</p> <p>Used when <code>FallBackFileSourceV2</code> extended resolution rule is executed (to resolve an <code>InsertIntoStatement</code> with a DataSourceV2Relation with a <code>FileTable</code>)</p>","text":""},{"location":"connectors/FileTable/#format-name","title":"Format Name <pre><code>formatName: String\n</code></pre> <p>Name of the file table (format)</p>    FileTable Format Name     <code>AvroTable</code> <code>AVRO</code>   <code>CSVTable</code> <code>CSV</code>   <code>JsonTable</code> <code>JSON</code>   <code>OrcTable</code> <code>ORC</code>   ParquetTable Parquet   <code>TextTable</code> <code>Text</code>","text":""},{"location":"connectors/FileTable/#schema-inference","title":"Schema Inference <pre><code>inferSchema(\n    files: Seq[FileStatus]): Option[StructType]\n</code></pre> <p>Infers schema of the given <code>files</code> (as Hadoop FileStatuses)</p> <p>See:</p> <ul> <li>ParquetTable</li> </ul> <p>Used when:</p> <ul> <li><code>FileTable</code> is requested for the data schema</li> </ul>","text":""},{"location":"connectors/FileTable/#supportsdatatype","title":"supportsDataType <pre><code>supportsDataType(\n    dataType: DataType): Boolean = true\n</code></pre> <p>Controls whether the given DataType is supported by the file-backed table</p> <p>Default: All DataTypes are supported</p> <p>See:</p> <ul> <li>ParquetTable</li> </ul> <p>Used when:</p> <ul> <li><code>FileTable</code> is requested for the schema</li> </ul>","text":""},{"location":"connectors/FileTable/#implementations","title":"Implementations","text":"<ul> <li><code>AvroTable</code></li> <li><code>CSVTable</code></li> <li><code>JsonTable</code></li> <li><code>OrcTable</code></li> <li>ParquetTable</li> <li><code>TextTable</code></li> </ul>"},{"location":"connectors/FileTable/#creating-instance","title":"Creating Instance","text":"<p><code>FileTable</code> takes the following to be created:</p> <ul> <li> SparkSession <li> Options <li> Paths <li> Optional user-defined schema (<code>Option[StructType]</code>) <p><code>FileTable</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete FileTables.</p>"},{"location":"connectors/FileTable/#table-capabilities","title":"Table Capabilities <pre><code>capabilities: java.util.Set[TableCapability]\n</code></pre> <p><code>capabilities</code> is part of the Table abstraction.</p>  <p><code>capabilities</code> are the following TableCapabilities:</p> <ul> <li>BATCH_READ</li> <li>BATCH_WRITE</li> <li>TRUNCATE</li> </ul>","text":""},{"location":"connectors/FileTable/#data-schema","title":"Data Schema <pre><code>dataSchema: StructType\n</code></pre> <p><code>dataSchema</code> is the schema of the data of the file-backed table</p>  Lazy Value <p><code>dataSchema</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>   <p><code>dataSchema</code> is used when:</p> <ul> <li><code>FileTable</code> is requested for the schema</li> </ul>","text":""},{"location":"connectors/FileTable/#partitioning","title":"Partitioning <pre><code>partitioning: Array[Transform]\n</code></pre> <p><code>partitioning</code> is part of the Table abstraction.</p>  <p><code>partitioning</code>...FIXME</p>","text":""},{"location":"connectors/FileTable/#properties","title":"Properties <pre><code>properties: util.Map[String, String]\n</code></pre> <p><code>properties</code> is part of the Table abstraction.</p>  <p><code>properties</code> returns the options.</p>","text":""},{"location":"connectors/FileTable/#table-schema","title":"Table Schema  Signature <pre><code>schema: StructType\n</code></pre> <p><code>schema</code> is part of the Table abstraction.</p>  <p><code>schema</code> checks the dataSchema for column name duplication.</p> <p><code>schema</code> makes sure that all field types in the dataSchema are supported.</p> <p><code>schema</code> requests the PartitioningAwareFileIndex for the partitionSchema to checks for column name duplication.</p> <p>In the end, <code>schema</code> is the dataSchema followed by (the fields of) the partitionSchema.</p>","text":""},{"location":"connectors/FileTable/#partitioningawarefileindex","title":"PartitioningAwareFileIndex <pre><code>fileIndex: PartitioningAwareFileIndex\n</code></pre>  Lazy Value <p><code>fileIndex</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>fileIndex</code> creates one of the following PartitioningAwareFileIndexs:</p> <ul> <li><code>MetadataLogFileIndex</code> when reading from the results of a streaming query (and loading files from the metadata log instead of listing them using HDFS APIs)</li> <li>InMemoryFileIndex</li> </ul>  <p><code>fileIndex</code> is used when:</p> <ul> <li>FileTables are requested for FileScanBuilders</li> <li><code>Dataset</code> is requested for the inputFiles</li> <li><code>CacheManager</code> is requested to lookupAndRefresh</li> <li><code>FallBackFileSourceV2</code> is created</li> <li><code>FileTable</code> is requested to dataSchema, schema, partitioning</li> </ul>","text":""},{"location":"connectors/FileWrite/","title":"FileWrite","text":"<p><code>FileWrite</code> is an extension of the Write abstraction for file writers.</p>"},{"location":"connectors/FileWrite/#contract","title":"Contract","text":""},{"location":"connectors/FileWrite/#formatName","title":"Format Name","text":"<pre><code>formatName: String\n</code></pre> <p>See:</p> <ul> <li>ParquetWrite</li> </ul> <p>Used when:</p> <ul> <li><code>FileWrite</code> is requested for the description and validateInputs</li> </ul>"},{"location":"connectors/FileWrite/#info","title":"LogicalWriteInfo","text":"<pre><code>info: LogicalWriteInfo\n</code></pre> <p>See:</p> <ul> <li>ParquetWrite</li> </ul> <p>Used when:</p> <ul> <li><code>FileWrite</code> is requested for the schema, the queryId and the options</li> </ul>"},{"location":"connectors/FileWrite/#paths","title":"paths","text":"<pre><code>paths: Seq[String]\n</code></pre> <p>See:</p> <ul> <li>ParquetWrite</li> </ul> <p>Used when:</p> <ul> <li><code>FileWrite</code> is requested for a BatchWrite and to validateInputs</li> </ul>"},{"location":"connectors/FileWrite/#prepareWrite","title":"Preparing Write Job","text":"<pre><code>prepareWrite(\nsqlConf: SQLConf,\njob: Job,\noptions: Map[String, String],\ndataSchema: StructType): OutputWriterFactory\n</code></pre> <p>Prepares a write job and returns an <code>OutputWriterFactory</code></p> <p>See:</p> <ul> <li>ParquetWrite</li> </ul> <p>Used when:</p> <ul> <li><code>FileWrite</code> is requested for a BatchWrite (and creates a WriteJobDescription)</li> </ul>"},{"location":"connectors/FileWrite/#supportsDataType","title":"supportsDataType","text":"<pre><code>supportsDataType: DataType =&gt; Boolean\n</code></pre> <p>See:</p> <ul> <li>ParquetWrite</li> </ul> <p>Used when:</p> <ul> <li><code>FileWrite</code> is requested to validateInputs</li> </ul>"},{"location":"connectors/FileWrite/#implementations","title":"Implementations","text":"<ul> <li><code>AvroWrite</code></li> <li><code>CSVWrite</code></li> <li><code>JsonWrite</code></li> <li><code>OrcWrite</code></li> <li>ParquetWrite</li> <li><code>TextWrite</code></li> </ul>"},{"location":"connectors/FileWrite/#toBatch","title":"Creating BatchWrite","text":"Write <pre><code>toBatch: BatchWrite\n</code></pre> <p><code>toBatch</code> is part of the Write abstraction.</p> <p><code>toBatch</code> validateInputs.</p> <p><code>toBatch</code> creates a new Hadoop Job for just a single path out of the paths.</p> <p><code>toBatch</code> creates a <code>FileCommitProtocol</code> (Spark Core) with the following:</p> <ol> <li>spark.sql.sources.commitProtocolClass</li> <li>A random job ID</li> <li>The first of the paths</li> </ol> <p><code>toBatch</code> creates a WriteJobDescription.</p> <p><code>toBatch</code> requests the <code>FileCommitProtocol</code> to <code>setupJob</code> (with the Hadoop <code>Job</code> instance).</p> <p>In the end, <code>toBatch</code> creates a FileBatchWrite (for the Hadoop <code>Job</code>, the <code>WriteJobDescription</code> and the <code>FileCommitProtocol</code>).</p>"},{"location":"connectors/FileWrite/#createWriteJobDescription","title":"Creating WriteJobDescription","text":"<pre><code>createWriteJobDescription(\nsparkSession: SparkSession,\nhadoopConf: Configuration,\njob: Job,\npathName: String,\noptions: Map[String, String]): WriteJobDescription\n</code></pre> <p><code>createWriteJobDescription</code>...FIXME</p>"},{"location":"connectors/FileWriterFactory/","title":"FileWriterFactory","text":"<p><code>FileWriterFactory</code> is a DataWriterFactory of FileBatchWrites.</p>"},{"location":"connectors/FileWriterFactory/#creating-instance","title":"Creating Instance","text":"<p><code>FileWriterFactory</code> takes the following to be created:</p> <ul> <li> <code>WriteJobDescription</code> <li> <code>FileCommitProtocol</code> (Spark Core) <p><code>FileWriterFactory</code> is created when:</p> <ul> <li><code>FileBatchWrite</code> is requested for a DataWriterFactory</li> </ul>"},{"location":"connectors/FileWriterFactory/#creating-datawriter","title":"Creating DataWriter <pre><code>createWriter(\n  partitionId: Int,\n  realTaskId: Long): DataWriter[InternalRow]\n</code></pre> <p><code>createWriter</code> creates a TaskAttemptContext.</p> <p><code>createWriter</code> requests the FileCommitProtocol to <code>setupTask</code> (with the <code>TaskAttemptContext</code>).</p> <p>For a non-partitioned write job (i.e., no partition columns in the WriteJobDescription), <code>createWriter</code> creates a SingleDirectoryDataWriter. Otherwise, <code>createWriter</code> creates a <code>DynamicPartitionDataSingleWriter</code>.</p>  <p><code>createWriter</code> is part of the DataWriterFactory abstraction.</p>","text":""},{"location":"connectors/FileWriterFactory/#creating-hadoop-taskattemptcontext","title":"Creating Hadoop TaskAttemptContext <pre><code>createTaskAttemptContext(\n  partitionId: Int): TaskAttemptContextImpl\n</code></pre> <p><code>createTaskAttemptContext</code> creates a Hadoop JobID.</p> <p><code>createTaskAttemptContext</code> creates a Hadoop TaskID (for the <code>JobID</code> and the given <code>partitionId</code> as <code>TaskType.MAP</code> type).</p> <p><code>createTaskAttemptContext</code> creates a Hadoop TaskAttemptID (for the <code>TaskID</code>).</p> <p><code>createTaskAttemptContext</code> uses the Hadoop Configuration (from the WriteJobDescription) to set the following properties:</p>    Name Value     mapreduce.job.id the <code>JobID</code>   mapreduce.task.id the <code>TaskID</code>   mapreduce.task.attempt.id the <code>TaskAttemptID</code>   mapreduce.task.ismap <code>true</code>   mapreduce.task.partition <code>0</code>    <p>In the end, <code>createTaskAttemptContext</code> creates a new Hadoop TaskAttemptContextImpl (with the <code>Configuration</code> and the <code>TaskAttemptID</code>).</p>","text":""},{"location":"connectors/HadoopFileLinesReader/","title":"HadoopFileLinesReader","text":"<p><code>HadoopFileLinesReader</code> is a Scala Iterator of Apache Hadoop's org.apache.hadoop.io.Text.</p> <p><code>HadoopFileLinesReader</code> is &lt;&gt; to access datasets in the following data sources: <ul> <li><code>SimpleTextSource</code></li> <li><code>LibSVMFileFormat</code></li> <li><code>TextInputCSVDataSource</code></li> <li><code>TextInputJsonDataSource</code></li> <li><code>TextFileFormat</code></li> </ul> <p><code>HadoopFileLinesReader</code> uses the internal &lt;&gt; that handles accessing files using Hadoop's FileSystem API."},{"location":"connectors/HadoopFileLinesReader/#creating-instance","title":"Creating Instance","text":"<p><code>HadoopFileLinesReader</code> takes the following when created:</p> <ul> <li>[[file]] PartitionedFile</li> <li>[[conf]] Hadoop's <code>Configuration</code></li> </ul> <p>=== [[iterator]] <code>iterator</code> Internal Property</p>"},{"location":"connectors/HadoopFileLinesReader/#source-scala","title":"[source, scala]","text":""},{"location":"connectors/HadoopFileLinesReader/#iterator-recordreaderiteratortext","title":"iterator: RecordReaderIterator[Text]","text":"<p>When &lt;&gt;, <code>HadoopFileLinesReader</code> creates an internal <code>iterator</code> that uses Hadoop's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/lib/input/FileSplit.html[org.apache.hadoop.mapreduce.lib.input.FileSplit] with Hadoop's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[org.apache.hadoop.fs.Path] and &lt;&gt;. <p><code>iterator</code> creates Hadoop's <code>TaskAttemptID</code>, <code>TaskAttemptContextImpl</code> and <code>LineRecordReader</code>.</p> <p><code>iterator</code> initializes <code>LineRecordReader</code> and passes it on to a RecordReaderIterator.</p> <p>NOTE: <code>iterator</code> is used for <code>Iterator</code>-specific methods, i.e. <code>hasNext</code>, <code>next</code> and <code>close</code>.</p>"},{"location":"connectors/HadoopFsRelation/","title":"HadoopFsRelation","text":"<p><code>HadoopFsRelation</code> is a BaseRelation and FileRelation.</p>"},{"location":"connectors/HadoopFsRelation/#creating-instance","title":"Creating Instance","text":"<p><code>HadoopFsRelation</code> takes the following to be created:</p> <ul> <li> FileIndex <li> Partition Schema (StructType) <li> Data Schema (StructType) <li>Optional bucketing specification</li> <li> FileFormat <li> Options (<code>Map[String, String]</code>) <li> SparkSession <p><code>HadoopFsRelation</code> is created when:</p> <ul> <li><code>DataSource</code> is requested to resolve a relation for file-based data sources</li> <li><code>HiveMetastoreCatalog</code> is requested to convert a HiveTableRelation to a LogicalRelation over a HadoopFsRelation (for RelationConversions logical post-hoc evaluation rule for <code>parquet</code> or <code>native</code> and <code>hive</code> ORC formats)</li> </ul>"},{"location":"connectors/HadoopFsRelation/#bucketing-specification","title":"Bucketing Specification <p><code>HadoopFsRelation</code> can be given a bucketing specification when created.</p> <p>The bucketing specification is defined for non-streaming file-based data sources and used for the following:</p> <ul> <li> <p>Output partitioning scheme and output data ordering of the corresponding FileSourceScanExec physical operator</p> </li> <li> <p>DataSourceAnalysis post-hoc logical resolution rule (when executed on a InsertIntoTable logical operator over a LogicalRelation with <code>HadoopFsRelation</code> relation)</p> </li> </ul>","text":""},{"location":"connectors/HadoopFsRelation/#files-to-scan-input-files","title":"Files to Scan (Input Files) <pre><code>inputFiles: Array[String]\n</code></pre> <p><code>inputFiles</code> requests the FileIndex for the inputFiles.</p> <p><code>inputFiles</code> is part of the FileRelation abstraction.</p>","text":""},{"location":"connectors/HadoopFsRelation/#estimated-size","title":"Estimated Size <pre><code>sizeInBytes: Long\n</code></pre> <p><code>sizeInBytes</code> requests the FileIndex for the size and multiplies it by the value of spark.sql.sources.fileCompressionFactor configuration property.</p> <p><code>sizeInBytes</code> is part of the BaseRelation abstraction.</p>","text":""},{"location":"connectors/HadoopFsRelation/#human-friendly-textual-representation","title":"Human-Friendly Textual Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code> is the following text based on the FileFormat:</p> <ul> <li> <p>shortName for DataSourceRegister data sources</p> </li> <li> <p>HadoopFiles otherwise</p> </li> </ul>","text":""},{"location":"connectors/HadoopFsRelation/#demo","title":"Demo <pre><code>// Demo the different cases when `HadoopFsRelation` is created\n\nimport org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}\n\n// Example 1: spark.table for DataSource tables (provider != hive)\nimport org.apache.spark.sql.catalyst.TableIdentifier\nval t1ID = TableIdentifier(tableName = \"t1\")\nspark.sessionState.catalog.dropTable(name = t1ID, ignoreIfNotExists = true, purge = true)\nspark.range(5).write.saveAsTable(\"t1\")\n\nval metadata = spark.sessionState.catalog.getTableMetadata(t1ID)\nscala&gt; println(metadata.provider.get)\nparquet\n\nassert(metadata.provider.get != \"hive\")\n\nval q = spark.table(\"t1\")\n// Avoid dealing with UnresolvedRelations and SubqueryAliases\n// Hence going stright for optimizedPlan\nval plan1 = q.queryExecution.optimizedPlan\n\nscala&gt; println(plan1.numberedTreeString)\n00 Relation[id#7L] parquet\n\nval LogicalRelation(rel1, _, _, _) = plan1.asInstanceOf[LogicalRelation]\nval hadoopFsRel = rel1.asInstanceOf[HadoopFsRelation]\n\n// Example 2: spark.read with format as a `FileFormat`\nval q = spark.read.text(\"README.md\")\nval plan2 = q.queryExecution.logical\n\nscala&gt; println(plan2.numberedTreeString)\n00 Relation[value#2] text\n\nval LogicalRelation(relation, _, _, _) = plan2.asInstanceOf[LogicalRelation]\nval hadoopFsRel = relation.asInstanceOf[HadoopFsRelation]\n\n// Example 3: Bucketing specified\nval tableName = \"bucketed_4_id\"\nspark\n  .range(100000000)\n  .write\n  .bucketBy(4, \"id\")\n  .sortBy(\"id\")\n  .mode(\"overwrite\")\n  .saveAsTable(tableName)\n\nval q = spark.table(tableName)\n// Avoid dealing with UnresolvedRelations and SubqueryAliases\n// Hence going stright for optimizedPlan\nval plan3 = q.queryExecution.optimizedPlan\n\nscala&gt; println(plan3.numberedTreeString)\n00 Relation[id#52L] parquet\n\nval LogicalRelation(rel3, _, _, _) = plan3.asInstanceOf[LogicalRelation]\nval hadoopFsRel = rel3.asInstanceOf[HadoopFsRelation]\nval bucketSpec = hadoopFsRel.bucketSpec.get\n\n// Exercise 3: spark.table for Hive tables (provider == hive)\n</code></pre>","text":""},{"location":"connectors/InMemoryFileIndex/","title":"InMemoryFileIndex","text":"<p><code>InMemoryFileIndex</code> is a PartitioningAwareFileIndex.</p>"},{"location":"connectors/InMemoryFileIndex/#creating-instance","title":"Creating Instance","text":"<p><code>InMemoryFileIndex</code> takes the following to be created:</p> <ul> <li> SparkSession <li> Root Paths (as Hadoop Paths) <li> Parameters (<code>Map[String, String]</code>) <li> User-Defined Schema (<code>Option[StructType]</code>) <li>FileStatusCache</li> <li> User-Defined Partition Spec (default: <code>undefined</code>) <li> <code>metadataOpsTimeNs</code> (<code>Option[Long]</code>, default: <code>undefined</code>) <p>While being created, <code>InMemoryFileIndex</code> refresh0.</p> <p><code>InMemoryFileIndex</code> is created when:</p> <ul> <li><code>HiveMetastoreCatalog</code> is requested to inferIfNeeded</li> <li><code>CatalogFileIndex</code> is requested for the partitions by the given predicate expressions for a non-partitioned Hive table</li> <li><code>DataSource</code> is requested to createInMemoryFileIndex</li> <li><code>FileTable</code> is requested for a PartitioningAwareFileIndex</li> </ul>"},{"location":"connectors/InMemoryFileIndex/#filestatuscache","title":"FileStatusCache <p><code>InMemoryFileIndex</code> can be given a FileStatusCache. Unless given, <code>InMemoryFileIndex</code> uses the <code>NoopCache</code>.</p> <p><code>FileStatusCache</code> is given (based on the configuration properties) when:</p> <ul> <li><code>CatalogFileIndex</code> is requested to filter the partitions</li> <li><code>DataSource</code> is requested to create an InMemoryFileIndex</li> <li><code>FileTable</code> is requested for the PartitioningAwareFileIndex</li> </ul>","text":""},{"location":"connectors/InMemoryFileIndex/#refreshing-cached-file-listings","title":"Refreshing Cached File Listings <pre><code>refresh(): Unit\n</code></pre> <p><code>refresh</code> requests the FileStatusCache to <code>invalidateAll</code> and then refresh0.</p> <p><code>refresh</code> is part of the FileIndex abstraction.</p>","text":""},{"location":"connectors/InMemoryFileIndex/#refreshing-cached-file-listings-internal","title":"Refreshing Cached File Listings (Internal) <pre><code>refresh0(): Unit\n</code></pre> <p><code>refresh0</code>...FIXME</p> <p><code>refresh0</code> is used when <code>InMemoryFileIndex</code> is created and requested to refresh.</p>","text":""},{"location":"connectors/InMemoryFileIndex/#root-paths","title":"Root Paths <pre><code>rootPaths: Seq[Path]\n</code></pre> <p>The root paths with streaming metadata directories and files filtered out (e.g. <code>_spark_metadata</code> streaming metadata directories).</p> <p><code>rootPaths</code> is part of the FileIndex abstraction.</p>","text":""},{"location":"connectors/OutputWriter/","title":"OutputWriter","text":"<p><code>OutputWriter</code> is an abstraction of output writers that write rows to a file system.</p>"},{"location":"connectors/OutputWriter/#contract","title":"Contract","text":""},{"location":"connectors/OutputWriter/#closing","title":"Closing <pre><code>close(): Unit\n</code></pre> <p>Closes this <code>OutputWriter</code></p> <p>Used when:</p> <ul> <li><code>FileFormatDataWriter</code> is requested to releaseCurrentWriter</li> <li><code>DynamicPartitionDataConcurrentWriter</code> is requested to <code>releaseResources</code></li> </ul>","text":""},{"location":"connectors/OutputWriter/#path","title":"Path <pre><code>path(): String\n</code></pre> <p>The file path to write records to</p> <p>Used when:</p> <ul> <li><code>FileFormatDataWriter</code> is requested to releaseCurrentWriter</li> <li><code>SingleDirectoryDataWriter</code> is requested to write</li> <li><code>BaseDynamicPartitionDataWriter</code> is requested to writeRecord</li> </ul>","text":""},{"location":"connectors/OutputWriter/#writing-row-out","title":"Writing Row Out <pre><code>write(\n  row: InternalRow): Unit\n</code></pre> <p>Writes out a single InternalRow</p> <p>Used when:</p> <ul> <li><code>SingleDirectoryDataWriter</code> is requested to write</li> <li><code>BaseDynamicPartitionDataWriter</code> is requested to writeRecord</li> </ul>","text":""},{"location":"connectors/OutputWriter/#implementations","title":"Implementations","text":"<ul> <li><code>AvroOutputWriter</code></li> <li><code>CsvOutputWriter</code></li> <li><code>HiveOutputWriter</code></li> <li><code>JsonOutputWriter</code></li> <li><code>LibSVMOutputWriter</code></li> <li><code>OrcOutputWriter</code></li> <li><code>OrcOutputWriter</code></li> <li><code>ParquetOutputWriter</code></li> <li><code>TextOutputWriter</code></li> </ul>"},{"location":"connectors/PartitionedFile/","title":"PartitionedFile","text":"<p><code>PartitionedFile</code> is a part (block) of a file (similarly to a Parquet block or a HDFS split).</p> <p><code>PartitionedFile</code> represents a chunk of a file that will be read, along with partition column values appended to each row, in a partition.</p>"},{"location":"connectors/PartitionedFile/#creating-instance","title":"Creating Instance","text":"<p><code>PartitionedFile</code> takes the following to be created:</p> <ul> <li>Partition Column Values</li> <li> Path of the file to read <li> Beginning offset (in bytes) <li> Length of this file part (number of bytes to read) <li>Block Hosts</li> <li> Modification time <li> File size <p><code>PartitionedFile</code> is created when:</p> <ul> <li><code>PartitionedFileUtil</code> is requested for split files and getPartitionedFile</li> </ul>"},{"location":"connectors/PartitionedFile/#partition-column-values","title":"Partition Column Values <pre><code>partitionValues: InternalRow\n</code></pre> <p><code>PartitionedFile</code> is given an InternalRow with the partition column values to be appended to each row.</p> <p>The partition column values are the values of the partition columns and therefore part of the directory structure not the partitioned files themselves (that together are the partitioned dataset).</p>","text":""},{"location":"connectors/PartitionedFile/#block-hosts","title":"Block Hosts <pre><code>locations: Array[String]\n</code></pre> <p><code>PartitionedFile</code> is given a collection of nodes (host names) with data blocks.</p> <p>Default: (empty)</p>","text":""},{"location":"connectors/PartitionedFile/#string-representation","title":"String Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code> is part of the <code>Object</code> (Java) abstraction.</p>  <p><code>toString</code> is the following text:</p> <pre><code>path: [filePath], range: [start]-[start+length], partition values: [partitionValues]\n</code></pre>","text":""},{"location":"connectors/PartitionedFile/#demo","title":"Demo <pre><code>import org.apache.spark.sql.execution.datasources.PartitionedFile\nimport org.apache.spark.sql.catalyst.InternalRow\n\nval partFile = PartitionedFile(InternalRow.empty, \"fakePath0\", 0, 10, Array(\"host0\", \"host1\"))\n\nprintln(partFile)\n</code></pre> <pre><code>path: fakePath0, range: 0-10, partition values: [empty row]\n</code></pre>","text":""},{"location":"connectors/PartitionedFileUtil/","title":"PartitionedFileUtil","text":"<p>When requested for split files of a file (Apache Hadoop), <code>PartitionedFileUtil</code> uses isSplitable property of a FileFormat and creates one or more PartitionedFiles.</p> <p>Only when splitable, a file will have as many PartitionedFiles as the number of parts of maxSplitBytes size.</p>"},{"location":"connectors/PartitionedFileUtil/#split-files","title":"Split Files <pre><code>splitFiles(\n  sparkSession: SparkSession,\n  file: FileStatus,\n  filePath: Path,\n  isSplitable: Boolean,\n  maxSplitBytes: Long,\n  partitionValues: InternalRow): Seq[PartitionedFile]\n</code></pre> <p><code>splitFiles</code> branches off based on the given <code>isSplitable</code> flag.</p> <p>If splitable, <code>splitFiles</code> uses the given <code>maxSplitBytes</code> to split the given <code>file</code> into PartitionedFiles for every part file.</p> <p>Otherwise, <code>splitFiles</code> creates a single PartitionedFile for the given <code>file</code> (with the given <code>filePath</code> and <code>partitionValues</code>).</p>  <p><code>splitFiles</code> is used when:</p> <ul> <li><code>FileSourceScanExec</code> is requested to createReadRDD</li> <li><code>FileScan</code> is requested for the partitions</li> </ul>","text":""},{"location":"connectors/PartitionedFileUtil/#getpartitionedfile","title":"getPartitionedFile <pre><code>getPartitionedFile(\n  file: FileStatus,\n  filePath: Path,\n  partitionValues: InternalRow): PartitionedFile\n</code></pre> <p><code>getPartitionedFile</code> finds the BlockLocations of the given <code>FileStatus</code> (Apache Hadoop).</p> <p><code>getPartitionedFile</code> finds the BlockHosts with the <code>BlockLocation</code>s.</p> <p>In the end, <code>getPartitionedFile</code> creates a PartitionedFile with the following:</p>    Argument Value     partitionValues The given <code>partitionValues</code>   filePath The URI of the given <code>filePath</code>   start 0   length The lenght of the <code>file</code>   locations Block hosts   modificationTime The modification time of the <code>file</code>   fileSize The size of the <code>file</code>     <p><code>getPartitionedFile</code> is used when:</p> <ul> <li><code>FileSourceScanExec</code> is requested to create a FileScanRDD with Bucketing Support</li> <li><code>PartitionedFileUtil</code> is requested to splitFiles</li> </ul>","text":""},{"location":"connectors/PartitioningAwareFileIndex/","title":"PartitioningAwareFileIndex","text":"<p><code>PartitioningAwareFileIndex</code>\u00a0is an extension of the FileIndex abstraction for indices that are aware of partitioned tables.</p>"},{"location":"connectors/PartitioningAwareFileIndex/#contract","title":"Contract","text":""},{"location":"connectors/PartitioningAwareFileIndex/#leafdirtochildrenfiles","title":"leafDirToChildrenFiles <pre><code>leafDirToChildrenFiles: Map[Path, Array[FileStatus]]\n</code></pre> <p>Used for files matching filters, all files and infer partitioning</p>","text":""},{"location":"connectors/PartitioningAwareFileIndex/#leaf-files","title":"Leaf Files <pre><code>leafFiles: mutable.LinkedHashMap[Path, FileStatus]\n</code></pre> <p>Used for all files and base locations</p>","text":""},{"location":"connectors/PartitioningAwareFileIndex/#partitionspec","title":"PartitionSpec <pre><code>partitionSpec(): PartitionSpec\n</code></pre> <p>Partition specification with partition columns and values, and directories (as Hadoop Paths)</p> <p>Used for a partition schema, to list the files matching filters and all files</p>","text":""},{"location":"connectors/PartitioningAwareFileIndex/#implementations","title":"Implementations","text":"<ul> <li>InMemoryFileIndex</li> <li><code>MetadataLogFileIndex</code> (Spark Structured Streaming)</li> </ul>"},{"location":"connectors/PartitioningAwareFileIndex/#creating-instance","title":"Creating Instance","text":"<p><code>PartitioningAwareFileIndex</code> takes the following to be created:</p> <ul> <li> SparkSession <li> Options for partition discovery (<code>Map[String, String]</code>) <li> Optional User-Defined Schema <li> <code>FileStatusCache</code> (default: <code>NoopCache</code>) Abstract Class <p><code>PartitioningAwareFileIndex</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete PartitioningAwareFileIndexes.</p>"},{"location":"connectors/PartitioningAwareFileIndex/#all-files","title":"All Files <pre><code>allFiles(): Seq[FileStatus]\n</code></pre> <p><code>allFiles</code>...FIXME</p>  <p><code>allFiles</code> is used when:</p> <ul> <li><code>DataSource</code> is requested to getOrInferFileFormatSchema and resolveRelation</li> <li><code>PartitioningAwareFileIndex</code> is requested for files matching filters, input files, and size</li> <li><code>FileTable</code> is requested for a data schema</li> </ul>","text":""},{"location":"connectors/PartitioningAwareFileIndex/#files-matching-filters","title":"Files Matching Filters <pre><code>listFiles(\n  partitionFilters: Seq[Expression],\n  dataFilters: Seq[Expression]): Seq[PartitionDirectory]\n</code></pre> <p><code>listFiles</code>\u00a0is part of the FileIndex abstraction.</p>  <p><code>listFiles</code>...FIXME</p>","text":""},{"location":"connectors/PartitioningAwareFileIndex/#partition-schema","title":"Partition Schema <pre><code>partitionSchema: StructType\n</code></pre> <p><code>partitionSchema</code>\u00a0is part of the FileIndex abstraction.</p>  <p><code>partitionSchema</code> gives the <code>partitionColumns</code> of the partition specification.</p>","text":""},{"location":"connectors/PartitioningAwareFileIndex/#input-files","title":"Input Files <pre><code>inputFiles: Array[String]\n</code></pre> <p><code>inputFiles</code>\u00a0is part of the FileIndex abstraction.</p>  <p><code>inputFiles</code> requests all the files for their location (as Hadoop Paths converted to <code>String</code>s).</p>","text":""},{"location":"connectors/PartitioningAwareFileIndex/#size","title":"Size <pre><code>sizeInBytes: Long\n</code></pre> <p><code>sizeInBytes</code>\u00a0is part of the FileIndex abstraction.</p>  <p><code>sizeInBytes</code> sums up the length (in bytes) of all the files.</p>","text":""},{"location":"connectors/PartitioningAwareFileIndex/#inferring-partitioning","title":"Inferring Partitioning <pre><code>inferPartitioning(): PartitionSpec\n</code></pre> <p><code>inferPartitioning</code>...FIXME</p>  <p><code>inferPartitioning</code>\u00a0is used by the PartitioningAwareFileIndices.</p>","text":""},{"location":"connectors/PartitioningAwareFileIndex/#base-locations","title":"Base Locations <pre><code>basePaths: Set[Path]\n</code></pre> <p><code>basePaths</code>\u00a0is used to infer partitioning.</p> <p><code>basePaths</code>...FIXME</p>","text":""},{"location":"connectors/PrunedInMemoryFileIndex/","title":"PrunedInMemoryFileIndex","text":"<p>:hadoop-version: 2.10.0 :url-hadoop-javadoc: https://hadoop.apache.org/docs/r{hadoop-version}/api</p> <p><code>PrunedInMemoryFileIndex</code> is a InMemoryFileIndex.md[InMemoryFileIndex] for a &lt;&gt; at an &lt;&gt;. <p><code>PrunedInMemoryFileIndex</code> may be given the &lt;&gt;. <p><code>PrunedInMemoryFileIndex</code> is &lt;&gt; when <code>CatalogFileIndex</code> is requested to CatalogFileIndex.md#filterPartitions[filter the partitions of a partitioned table]. <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex=ALL\n</code></pre>"},{"location":"connectors/PrunedInMemoryFileIndex/#refer-to-spark-loggingmdlogging","title":"Refer to spark-logging.md[Logging].","text":"<p>=== [[creating-instance]] Creating PrunedInMemoryFileIndex Instance</p> <p><code>PrunedInMemoryFileIndex</code> takes the following to be created:</p> <ul> <li>[[sparkSession]] SparkSession.md[SparkSession]</li> <li>[[tableBasePath]] Location of the Hive metastore table (as a Hadoop {url-hadoop-javadoc}/org/apache/hadoop/fs/Path.html[Path])</li> <li>[[fileStatusCache]] <code>FileStatusCache</code></li> <li>[[partitionSpec]] <code>PartitionSpec</code> (from a Hive metastore)</li> <li>[[metadataOpsTimeNs]] Optional time of the partition metadata listing</li> </ul>"},{"location":"connectors/RecordReaderIterator/","title":"RecordReaderIterator","text":"<p>[[creating-instance]] [[rowReader]] <code>RecordReaderIterator</code> is a Scala https://www.scala-lang.org/api/2.12.x/scala/collection/Iterator.html[scala.collection.Iterator] over the values of a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/RecordReader.html[RecordReader].</p> <p><code>RecordReaderIterator</code> is &lt;&gt; when: <ul> <li> <p>FIXME</p> </li> <li> <p>HadoopFileLinesReader and <code>HadoopFileWholeTextReader</code> are requested for an value iterator</p> </li> <li> <p>Legacy <code>OrcFileFormat</code> is requested to <code>buildReader</code></p> </li> </ul> <p>[[close]] When requested to close, <code>RecordReaderIterator</code> simply requests the underlying &lt;&gt; to close. <p>[[hasNext]] When requested to check whether or not there more internal rows, <code>RecordReaderIterator</code> simply requests the underlying &lt;&gt; for <code>nextKeyValue</code>. <p>[[next]] When requested for the next internal row, <code>RecordReaderIterator</code> simply requests the underlying &lt;&gt; for <code>getCurrentValue</code>."},{"location":"connectors/SchemaMergeUtils/","title":"SchemaMergeUtils","text":""},{"location":"connectors/SchemaMergeUtils/#mergeschemasinparallel","title":"mergeSchemasInParallel <pre><code>mergeSchemasInParallel(\n  sparkSession: SparkSession,\n  parameters: Map[String, String],\n  files: Seq[FileStatus],\n  schemaReader: (Seq[FileStatus], Configuration, Boolean) =&gt; Seq[StructType]): Option[StructType]\n</code></pre> <p><code>mergeSchemasInParallel</code> determines a merged schema with a distributed Spark job.</p>  <p><code>mergeSchemasInParallel</code> creates an RDD with file paths and their lenght with the number of partitions up to the default parallelism (number of CPU cores in a cluster).</p> <p>In the end, <code>mergeSchemasInParallel</code> collects the RDD result that are merged schemas for files (per partition) that <code>mergeSchemasInParallel</code> merge all together to give the final merge schema.</p>  <p><code>mergeSchemasInParallel</code> is used when:</p> <ul> <li><code>OrcFileFormat</code> is requested to <code>inferSchema</code></li> <li><code>OrcUtils</code> is requested to infer schema</li> <li><code>ParquetFileFormat</code> is requested to infer schema</li> </ul>","text":""},{"location":"connectors/SharedInMemoryCache/","title":"SharedInMemoryCache","text":"<p><code>SharedInMemoryCache</code> is...FIXME</p>"},{"location":"connectors/SingleDirectoryDataWriter/","title":"SingleDirectoryDataWriter","text":"<p><code>SingleDirectoryDataWriter</code> is a FileFormatDataWriter for FileFormatWriter and FileWriterFactory.</p>"},{"location":"connectors/SingleDirectoryDataWriter/#creating-instance","title":"Creating Instance","text":"<p><code>SingleDirectoryDataWriter</code> takes the following to be created:</p> <ul> <li> <code>WriteJobDescription</code> <li> Hadoop TaskAttemptContext <li> <code>FileCommitProtocol</code> (Spark Core) <li> Custom SQLMetrics by name (<code>Map[String, SQLMetric]</code>) <p>While being created, <code>SingleDirectoryDataWriter</code> creates a new OutputWriter.</p> <p><code>SingleDirectoryDataWriter</code> is created when:</p> <ul> <li><code>FileFormatWriter</code> is requested to write data out (in a single Spark task) (of a non-partitioned non-bucketed write job)</li> <li><code>FileWriterFactory</code> is requested for a DataWriter (of a non-partitioned write job)</li> </ul>"},{"location":"connectors/SingleDirectoryDataWriter/#recordsinfile-counter","title":"recordsInFile Counter <p><code>SingleDirectoryDataWriter</code> uses <code>recordsInFile</code> counter to track how many records have been written out.</p> <p><code>recordsInFile</code> counter is <code>0</code> when <code>SingleDirectoryDataWriter</code> creates a new OutputWriter (and increments until <code>maxRecordsPerFile</code> threshold if defined).</p>","text":""},{"location":"connectors/SingleDirectoryDataWriter/#writing-record-out","title":"Writing Record Out <pre><code>write(\n  record: InternalRow): Unit\n</code></pre> <p><code>write</code> creates a new OutputWriter for a positive <code>maxRecordsPerFile</code> (of the WriteJobDescription) and the recordsInFile counter above the threshold.</p> <p><code>write</code> requests the current OutputWriter to write the record and informs the WriteTaskStatsTrackers that there was a new row.</p> <p><code>write</code> increments the recordsInFile.</p>  <p><code>write</code> is part of the FileFormatDataWriter abstraction.</p>","text":""},{"location":"connectors/SingleDirectoryDataWriter/#creating-new-outputwriter","title":"Creating New OutputWriter <pre><code>newOutputWriter(): Unit\n</code></pre> <p><code>newOutputWriter</code> sets the recordsInFile counter to <code>0</code>.</p> <p><code>newOutputWriter</code> releaseResources.</p> <p><code>newOutputWriter</code> uses the given WriteJobDescription to access the <code>OutputWriterFactory</code> for a file extension (<code>ext</code>).</p> <p><code>newOutputWriter</code> requests the given FileCommitProtocol for a path of a new data file (with <code>-c[fileCounter][nnn][ext]</code> suffix).</p> <p><code>newOutputWriter</code> uses the given WriteJobDescription to access the <code>OutputWriterFactory</code> for a new OutputWriter.</p> <p><code>newOutputWriter</code> informs the WriteTaskStatsTrackers that a new file is about to be written.</p> <p><code>newOutputWriter</code> is used when:</p> <ul> <li><code>SingleDirectoryDataWriter</code> is created and requested to write (every <code>maxRecordsPerFile</code> threshold)</li> </ul>","text":""},{"location":"connectors/WriteJobStatsTracker/","title":"WriteJobStatsTracker","text":"<p><code>WriteJobStatsTracker</code> is an abstraction of write job statistics trackers.</p> <p><code>WriteJobStatsTracker</code> is a <code>Serializable</code>.</p>"},{"location":"connectors/WriteJobStatsTracker/#contract","title":"Contract","text":""},{"location":"connectors/WriteJobStatsTracker/#newTaskInstance","title":"Creating WriteTaskStatsTracker","text":"<pre><code>newTaskInstance(): WriteTaskStatsTracker\n</code></pre> <p>Creates a new WriteTaskStatsTracker</p> <p>See:</p> <ul> <li>BasicWriteJobStatsTracker</li> </ul> <p>Used when:</p> <ul> <li><code>FileFormatDataWriter</code> is created</li> </ul>"},{"location":"connectors/WriteJobStatsTracker/#processStats","title":"Processing Write Job Statistics","text":"<pre><code>processStats(\nstats: Seq[WriteTaskStats],\njobCommitTime: Long): Unit\n</code></pre> <p>See:</p> <ul> <li>BasicWriteJobStatsTracker</li> </ul> <p>Used when:</p> <ul> <li><code>FileFormatWriter</code> utility is used to process the statistics (of a write job)</li> </ul>"},{"location":"connectors/WriteJobStatsTracker/#implementations","title":"Implementations","text":"<ul> <li>BasicWriteJobStatsTracker</li> </ul>"},{"location":"connectors/WriteTaskStats/","title":"WriteTaskStats","text":"<p><code>WriteTaskStats</code> is the no-method contract of &lt;&gt; collected during a Write Task. <p>[[implementations]] NOTE: BasicWriteTaskStats is the one and only known implementation of the &lt;&gt; in Apache Spark."},{"location":"connectors/WriteTaskStatsTracker/","title":"WriteTaskStatsTracker","text":"<p><code>WriteTaskStatsTracker</code> is an abstraction of WriteTaskStatsTrackers that are notified about and can collect the WriteTaskStats about files, partitions and rows processed.</p>"},{"location":"connectors/WriteTaskStatsTracker/#contract","title":"Contract","text":""},{"location":"connectors/WriteTaskStatsTracker/#closing-file","title":"Closing File <pre><code>closeFile(\n  filePath: String): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>FileFormatDataWriter</code> is requested to releaseCurrentWriter</li> </ul>","text":""},{"location":"connectors/WriteTaskStatsTracker/#final-writetaskstats","title":"Final WriteTaskStats <pre><code>getFinalStats(\n  taskCommitTime: Long): WriteTaskStats\n</code></pre> <p>Creates a WriteTaskStats</p> <p>Used when:</p> <ul> <li><code>FileFormatDataWriter</code> is requested to commit a successful write</li> </ul>","text":""},{"location":"connectors/WriteTaskStatsTracker/#new-file","title":"New File <pre><code>newFile(\n  filePath: String): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>SingleDirectoryDataWriter</code> is requested to newOutputWriter</li> <li><code>BaseDynamicPartitionDataWriter</code> is requested to renewCurrentWriter</li> </ul>","text":""},{"location":"connectors/WriteTaskStatsTracker/#new-partition","title":"New Partition <pre><code>newPartition(\n  partitionValues: InternalRow): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>DynamicPartitionDataSingleWriter</code> is requested to write out a record</li> <li><code>DynamicPartitionDataConcurrentWriter</code> is requested to write out a record</li> </ul>","text":""},{"location":"connectors/WriteTaskStatsTracker/#new-row","title":"New Row <pre><code>newRow(\n  filePath: String,\n  row: InternalRow): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>SingleDirectoryDataWriter</code> is requested to write out a record</li> <li><code>BaseDynamicPartitionDataWriter</code> is requested to write a record out</li> </ul>","text":""},{"location":"connectors/WriteTaskStatsTracker/#implementations","title":"Implementations","text":"<ul> <li>BasicWriteTaskStatsTracker</li> </ul>"},{"location":"cost-based-optimization/","title":"Cost-Based Optimization (CBO)","text":"<p>Cost-Based Optimization (Cost-Based Query Optimization, CBO Optimizer, CBO) is an optimization technique in Spark SQL that uses table statistics to determine the most efficient query execution plan of a structured query (given the logical query plan).</p> <p>CBO is configured using spark.sql.cbo.enabled configuration property.</p> <p>CBO uses logical optimization rules to optimize the logical plan of a structured query based on statistics.</p> <p>You first use ANALYZE TABLE COMPUTE STATISTICS SQL command to compute table statistics. Use DESCRIBE EXTENDED SQL command to inspect the statistics.</p> <p>Logical operators have statistics support that is used for query planning.</p> <p>There is also support for equi-height column histograms.</p>"},{"location":"cost-based-optimization/#table-statistics","title":"Table Statistics <p>The table statistics can be computed for tables, partitions and columns and are as follows:</p> <ol> <li> <p> Total size (in bytes) of an table or table partitions  <li> <p> Row count of an table or table partitions  <li> <p> Column statistics: <code>min</code>, <code>max</code>, <code>num_nulls</code>, <code>distinct_count</code>, <code>avg_col_len</code>, <code>max_col_len</code>, <code>histogram</code>","text":""},{"location":"cost-based-optimization/#sparksqlcboenabled-configuration-property","title":"spark.sql.cbo.enabled Configuration Property <p>Cost-based optimization is enabled when spark.sql.cbo.enabled configuration property is enabled.</p> <pre><code>val sqlConf = spark.sessionState.conf\nassert(sqlConf.cboEnabled == false, \"CBO is disabled by default)\n</code></pre> <p>Create a new SparkSession with CBO enabled.</p> <pre><code>// You could spark-submit -c spark.sql.cbo.enabled=true\nval sparkCboEnabled = spark.newSession\nimport org.apache.spark.sql.internal.SQLConf.CBO_ENABLED\nsparkCboEnabled.conf.set(CBO_ENABLED.key, true)\nassert(sparkCboEnabled.conf.get(CBO_ENABLED.key))\n</code></pre>","text":""},{"location":"cost-based-optimization/#analyze-table-compute-statistics-sql-command","title":"ANALYZE TABLE COMPUTE STATISTICS SQL Command <p>Cost-Based Optimization uses the statistics stored in a metastore (external catalog) using ANALYZE TABLE SQL command.</p> <pre><code>ANALYZE TABLE tableIdentifier partitionSpec?\nCOMPUTE STATISTICS (NOSCAN | FOR COLUMNS identifierSeq)?\n</code></pre> <p>Depending on the variant, <code>ANALYZE TABLE</code> computes different statistics (for a table, partitions or columns).</p> <ol> <li> <p><code>ANALYZE TABLE</code> with neither <code>PARTITION</code> specification nor <code>FOR COLUMNS</code> clause</p> </li> <li> <p><code>ANALYZE TABLE</code> with <code>PARTITION</code> specification (but no <code>FOR COLUMNS</code> clause)</p> </li> <li> <p><code>ANALYZE TABLE</code> with <code>FOR COLUMNS</code> clause (but no <code>PARTITION</code> specification)</p> </li> </ol>  <p>spark.sql.statistics.histogram.enabled</p> <p>Use spark.sql.statistics.histogram.enabled configuration property to enable column (equi-height) histograms that can provide better estimation accuracy but cause an extra table scan).</p>  <p>When executed, the above <code>ANALYZE TABLE</code> variants are translated to the following logical commands (in a logical query plan), respectively:</p> <ol> <li> <p>AnalyzeTableCommand</p> </li> <li> <p>AnalyzePartitionCommand</p> </li> <li> <p>AnalyzeColumnCommand</p> </li> </ol>","text":""},{"location":"cost-based-optimization/#analyze-table-partition-for-columns-incorrect","title":"ANALYZE TABLE PARTITION FOR COLUMNS Incorrect","text":"<p><code>ANALYZE TABLE</code> with <code>PARTITION</code> specification and <code>FOR COLUMNS</code> clause is incorrect.</p> <pre><code>// !!! INCORRECT !!!\nANALYZE TABLE t1 PARTITION (p1, p2) COMPUTE STATISTICS FOR COLUMNS id, p1\n</code></pre> <p>In such a case, <code>SparkSqlAstBuilder</code> reports a WARN message to the logs and simply ignores the partition specification.</p> <pre><code>WARN Partition specification is ignored when collecting column statistics: [partitionSpec]\n</code></pre>"},{"location":"cost-based-optimization/#describe-extended-sql-command","title":"DESCRIBE EXTENDED SQL Command <p>You can view the statistics of a table, partitions or a column (stored in a metastore) using DESCRIBE EXTENDED SQL command.</p> <pre><code>(DESC | DESCRIBE) TABLE? (EXTENDED | FORMATTED)?\ntableIdentifier partitionSpec? describeColName?\n</code></pre> <p>Table-level statistics are in <code>Statistics</code> row while partition-level statistics are in <code>Partition Statistics</code> row.</p>  <p>Tip</p> <p>Use <code>DESC EXTENDED tableName</code> for table-level statistics and <code>DESC EXTENDED tableName PARTITION (p1, p2, ...)</code> for partition-level statistics only.</p>  <pre><code>// table-level statistics are in Statistics row\nscala&gt; sql(\"DESC EXTENDED t1\").show(numRows = 30, truncate = false)\n+----------------------------+--------------------------------------------------------------+-------+\n|col_name                    |data_type                                                     |comment|\n+----------------------------+--------------------------------------------------------------+-------+\n|id                          |int                                                           |null   |\n|p1                          |int                                                           |null   |\n|p2                          |string                                                        |null   |\n|# Partition Information     |                                                              |       |\n|# col_name                  |data_type                                                     |comment|\n|p1                          |int                                                           |null   |\n|p2                          |string                                                        |null   |\n|                            |                                                              |       |\n|# Detailed Table Information|                                                              |       |\n|Database                    |default                                                       |       |\n|Table                       |t1                                                            |       |\n|Owner                       |jacek                                                         |       |\n|Created Time                |Wed Dec 27 14:10:44 CET 2017                                  |       |\n|Last Access                 |Thu Jan 01 01:00:00 CET 1970                                  |       |\n|Created By                  |Spark 2.3.0                                                   |       |\n|Type                        |MANAGED                                                       |       |\n|Provider                    |parquet                                                       |       |\n|Table Properties            |[transient_lastDdlTime=1514453141]                            |       |\n|Statistics                  |714 bytes, 2 rows                                             |       |\n|Location                    |file:/Users/jacek/dev/oss/spark/spark-warehouse/t1            |       |\n|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n|Storage Properties          |[serialization.format=1]                                      |       |\n|Partition Provider          |Catalog                                                       |       |\n+----------------------------+--------------------------------------------------------------+-------+\n\nscala&gt; spark.table(\"t1\").show\n+---+---+----+\n| id| p1|  p2|\n+---+---+----+\n|  0|  0|zero|\n|  1|  1| one|\n+---+---+----+\n\n// partition-level statistics are in Partition Statistics row\nscala&gt; sql(\"DESC EXTENDED t1 PARTITION (p1=0, p2='zero')\").show(numRows = 30, truncate = false)\n+--------------------------------+---------------------------------------------------------------------------------+-------+\n|col_name                        |data_type                                                                        |comment|\n+--------------------------------+---------------------------------------------------------------------------------+-------+\n|id                              |int                                                                              |null   |\n|p1                              |int                                                                              |null   |\n|p2                              |string                                                                           |null   |\n|# Partition Information         |                                                                                 |       |\n|# col_name                      |data_type                                                                        |comment|\n|p1                              |int                                                                              |null   |\n|p2                              |string                                                                           |null   |\n|                                |                                                                                 |       |\n|# Detailed Partition Information|                                                                                 |       |\n|Database                        |default                                                                          |       |\n|Table                           |t1                                                                               |       |\n|Partition Values                |[p1=0, p2=zero]                                                                  |       |\n|Location                        |file:/Users/jacek/dev/oss/spark/spark-warehouse/t1/p1=0/p2=zero                  |       |\n|Serde Library                   |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                      |       |\n|InputFormat                     |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                    |       |\n|OutputFormat                    |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                   |       |\n|Storage Properties              |[path=file:/Users/jacek/dev/oss/spark/spark-warehouse/t1, serialization.format=1]|       |\n|Partition Parameters            |{numFiles=1, transient_lastDdlTime=1514469540, totalSize=357}                    |       |\n|Partition Statistics            |357 bytes, 1 rows                                                                |       |\n|                                |                                                                                 |       |\n|# Storage Information           |                                                                                 |       |\n|Location                        |file:/Users/jacek/dev/oss/spark/spark-warehouse/t1                               |       |\n|Serde Library                   |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                      |       |\n|InputFormat                     |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                    |       |\n|OutputFormat                    |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                   |       |\n|Storage Properties              |[serialization.format=1]                                                         |       |\n+--------------------------------+---------------------------------------------------------------------------------+-------+\n</code></pre> <p>You can view the statistics of a single column using <code>DESC EXTENDED tableName columnName</code> that are in a Dataset with two columns, i.e. <code>info_name</code> and <code>info_value</code>.</p> <pre><code>scala&gt; sql(\"DESC EXTENDED t1 id\").show\n+--------------+----------+\n|info_name     |info_value|\n+--------------+----------+\n|col_name      |id        |\n|data_type     |int       |\n|comment       |NULL      |\n|min           |0         |\n|max           |1         |\n|num_nulls     |0         |\n|distinct_count|2         |\n|avg_col_len   |4         |\n|max_col_len   |4         |\n|histogram     |NULL      |\n+--------------+----------+\n\nscala&gt; sql(\"DESC EXTENDED t1 p1\").show\n+--------------+----------+\n|info_name     |info_value|\n+--------------+----------+\n|col_name      |p1        |\n|data_type     |int       |\n|comment       |NULL      |\n|min           |0         |\n|max           |1         |\n|num_nulls     |0         |\n|distinct_count|2         |\n|avg_col_len   |4         |\n|max_col_len   |4         |\n|histogram     |NULL      |\n+--------------+----------+\n\nscala&gt; sql(\"DESC EXTENDED t1 p2\").show\n+--------------+----------+\n|info_name     |info_value|\n+--------------+----------+\n|col_name      |p2        |\n|data_type     |string    |\n|comment       |NULL      |\n|min           |NULL      |\n|max           |NULL      |\n|num_nulls     |0         |\n|distinct_count|2         |\n|avg_col_len   |4         |\n|max_col_len   |4         |\n|histogram     |NULL      |\n+--------------+----------+\n</code></pre>","text":""},{"location":"cost-based-optimization/#cost-based-optimizations","title":"Cost-Based Optimizations <p>The Catalyst Optimizer uses heuristics (rules) that are applied to a logical query plan for cost-based optimization.</p> <p>Among the optimization rules are the following:</p> <ol> <li>CostBasedJoinReorder logical optimization rule for join reordering with two or more consecutive inner or cross joins (possibly separated by <code>Project</code> operators) when spark.sql.cbo.enabled and spark.sql.cbo.joinReorder.enabled configuration properties are both enabled.</li> </ol>","text":""},{"location":"cost-based-optimization/#logical-commands-for-altering-table-statistics","title":"Logical Commands for Altering Table Statistics <p>The following are the logical commands that alter table statistics in a metastore (external catalog):</p> <ol> <li> <p>AnalyzeTableCommand</p> </li> <li> <p>AnalyzeColumnCommand</p> </li> <li> <p><code>AlterTableAddPartitionCommand</code></p> </li> <li> <p><code>AlterTableDropPartitionCommand</code></p> </li> <li> <p><code>AlterTableSetLocationCommand</code></p> </li> <li> <p><code>TruncateTableCommand</code></p> </li> <li> <p>InsertIntoHiveTable</p> </li> <li> <p>InsertIntoHadoopFsRelationCommand</p> </li> <li> <p><code>LoadDataCommand</code></p> </li> </ol>","text":""},{"location":"cost-based-optimization/#explain-cost-sql-command","title":"EXPLAIN COST SQL Command  <p>Fixme</p> <p>See LogicalPlanStats</p>","text":""},{"location":"cost-based-optimization/#logicalplanstats-statistics-estimates-of-logical-operator","title":"LogicalPlanStats \u2014 Statistics Estimates of Logical Operator <p>LogicalPlanStats adds statistics support to logical operators and is used for query planning (with or without cost-based optimization, e.g. CostBasedJoinReorder or JoinSelection, respectively).</p>","text":""},{"location":"cost-based-optimization/#equi-height-histograms-for-columns","title":"Equi-Height Histograms for Columns <p>From SPARK-17074 generate equi-height histogram for column:</p>  <p>Equi-height histogram is effective in handling skewed data distribution.</p> <p>For equi-height histogram, the heights of all bins(intervals) are the same. The default number of bins we use &gt; is 254.</p> <p>Now we use a two-step method to generate an equi-height histogram: 1. use percentile_approx to get percentiles (end points of the equi-height bin intervals); 2. use a new aggregate function to get distinct counts in each of these bins.</p> <p>Note that this method takes two table scans. In the future we may provide other algorithms which need only one table scan.</p>  <p>From [SPARK-17074][SQL] Generate equi-height histogram in column statistics #19479:</p>  <p>Equi-height histogram is effective in cardinality estimation, and more accurate than basic column stats (min, &gt; max, ndv, etc) especially in skew distribution.</p> <p>For equi-height histogram, all buckets (intervals) have the same height (frequency).</p> <p>we use a two-step method to generate an equi-height histogram:</p> <ol> <li> <p>use ApproximatePercentile to get percentiles p(0), p(1/n), p(2/n) ... p((n-1)/n), p(1);</p> </li> <li> <p>construct range values of buckets, e.g. [p(0), p(1/n)], [p(1/n), p(2/n)] ... [p((n-1)/n), p(1)], and use ApproxCountDistinctForIntervals to count ndv in each bucket. Each bucket is of the form: (lowerBound, higherBound, ndv).</p> </li> </ol>  <p>Spark SQL uses column statistics that may optionally hold the histogram of values (which is empty by default). With spark.sql.statistics.histogram.enabled configuration property turned on ANALYZE TABLE COMPUTE STATISTICS FOR COLUMNS SQL command generates column (equi-height) histograms.</p> <pre><code>// Computing column statistics with histogram\n// ./bin/spark-shell --conf spark.sql.statistics.histogram.enabled=true\nscala&gt; spark.sessionState.conf.histogramEnabled\nres1: Boolean = true\n\nval tableName = \"t1\"\n\n// Make the example reproducible\nimport org.apache.spark.sql.catalyst.TableIdentifier\nval tid = TableIdentifier(tableName)\nval sessionCatalog = spark.sessionState.catalog\nsessionCatalog.dropTable(tid, ignoreIfNotExists = true, purge = true)\n\n// CREATE TABLE t1\nSeq((0, 0, \"zero\"), (1, 1, \"one\")).\n  toDF(\"id\", \"p1\", \"p2\").\n  write.\n  saveAsTable(tableName)\n\n// As we drop and create immediately we may face problems with unavailable partition files\n// Invalidate cache\nspark.sql(s\"REFRESH TABLE $tableName\")\n\n// Use ANALYZE TABLE...FOR COLUMNS to compute column statistics\n// that saves them in a metastore (aka an external catalog)\nval df = spark.table(tableName)\nval allCols = df.columns.mkString(\",\")\nval analyzeTableSQL = s\"ANALYZE TABLE t1 COMPUTE STATISTICS FOR COLUMNS $allCols\"\nspark.sql(analyzeTableSQL)\n\n// Column statistics with histogram should be in the external catalog (metastore)\n</code></pre> <p>You can inspect the column statistics using DESCRIBE EXTENDED SQL command.</p> <pre><code>// Inspecting column statistics with column histogram\n// See the above example for how to compute the stats\nval colName = \"id\"\nval descExtSQL = s\"DESC EXTENDED $tableName $colName\"\n\n// 254 bins by default --&gt; num_of_bins in histogram row below\nscala&gt; sql(descExtSQL).show(truncate = false)\n+--------------+-----------------------------------------------------+\n|info_name     |info_value                                           |\n+--------------+-----------------------------------------------------+\n|col_name      |id                                                   |\n|data_type     |int                                                  |\n|comment       |NULL                                                 |\n|min           |0                                                    |\n|max           |1                                                    |\n|num_nulls     |0                                                    |\n|distinct_count|2                                                    |\n|avg_col_len   |4                                                    |\n|max_col_len   |4                                                    |\n|histogram     |height: 0.007874015748031496, num_of_bins: 254       |\n|bin_0         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_1         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_2         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_3         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_4         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_5         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_6         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_7         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_8         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_9         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n+--------------+-----------------------------------------------------+\nonly showing top 20 rows\n</code></pre>","text":""},{"location":"cost-based-optimization/BasicStatsPlanVisitor/","title":"BasicStatsPlanVisitor \u2014 Computing Statistics for Cost-Based Optimization","text":"<p><code>BasicStatsPlanVisitor</code> is a LogicalPlanVisitor that computes the statistics of a logical query plan for cost-based optimization.</p> <p><code>BasicStatsPlanVisitor</code> is used exclusively when a logical operator is requested for the statistics with cost-based optimization enabled.</p> <p><code>BasicStatsPlanVisitor</code> comes with custom &lt;&gt; for a few logical operators and falls back to SizeInBytesOnlyStatsPlanVisitor for the others. <p>[[handlers]] .BasicStatsPlanVisitor's Visitor Handlers [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Logical Operator | Handler | Behaviour</p> <p>| [[Aggregate]] Aggregate.md[Aggregate] | [[visitAggregate]] visitAggregate | Requests <code>AggregateEstimation</code> for statistics estimates and query hints or falls back to SizeInBytesOnlyStatsPlanVisitor</p> <p>| [[Filter]] <code>Filter</code> | [[visitFilter]] visitFilter | Requests <code>FilterEstimation</code> for statistics estimates and query hints or falls back to SizeInBytesOnlyStatsPlanVisitor</p> <p>| [[Join]] Join.md[Join] | [[visitJoin]] visitJoin | Requests <code>JoinEstimation</code> for statistics estimates and query hints or falls back to SizeInBytesOnlyStatsPlanVisitor</p> <p>| [[Project]] Project.md[Project] | [[visitProject]] visitProject | Requests <code>ProjectEstimation</code> for statistics estimates and query hints or falls back to SizeInBytesOnlyStatsPlanVisitor |===</p>"},{"location":"cost-based-optimization/CatalogColumnStat/","title":"CatalogColumnStat","text":""},{"location":"cost-based-optimization/CatalogColumnStat/#creating-instance","title":"Creating Instance","text":"<p><code>CatalogColumnStat</code> takes the following to be created:</p> <ul> <li> Distinct Count (number of distinct values) <li> Minimum Value <li> Maximum Value <li> Null Count (number of <code>null</code> values) <li> Average length of the values (for fixed-length types, this should be a constant) <li> Maximum length of the values (for fixed-length types, this should be a constant) <li> <code>Histogram</code> <li> 2 <p>Note</p> <p><code>CatalogColumnStat</code> uses the same input parameters as ColumnStat.</p> <p><code>CatalogColumnStat</code> is created when:</p> <ul> <li><code>CatalogColumnStat</code> is requested to fromMap</li> <li><code>ColumnStat</code> is requested to toCatalogColumnStat</li> </ul>"},{"location":"cost-based-optimization/CatalogColumnStat/#frommap","title":"fromMap <pre><code>fromMap(\n  table: String,\n  colName: String,\n  map: Map[String, String]): Option[CatalogColumnStat]\n</code></pre> <p><code>fromMap</code> creates a CatalogColumnStat using the given <code>map</code> (with the <code>colName</code>-prefixed keys).</p>  <p><code>fromMap</code> is used when:</p> <ul> <li><code>HiveExternalCatalog</code> is requested to statsFromProperties</li> </ul>","text":""},{"location":"cost-based-optimization/CatalogColumnStat/#toplanstat","title":"toPlanStat <pre><code>toPlanStat(\n  colName: String,\n  dataType: DataType): ColumnStat\n</code></pre> <p><code>toPlanStat</code> converts this <code>CatalogColumnStat</code> to a ColumnStat.</p>  <p><code>toPlanStat</code> is used when:</p> <ul> <li><code>CatalogStatistics</code> is requested to toPlanStats</li> </ul>","text":""},{"location":"cost-based-optimization/ColumnStat/","title":"ColumnStat","text":""},{"location":"cost-based-optimization/ColumnStat/#creating-instance","title":"Creating Instance","text":"<p><code>ColumnStat</code> takes the following to be created:</p> <ul> <li> Distinct Count (number of distinct values) <li> Minimum Value <li> Maximum Value <li> Null Count (number of <code>null</code> values) <li> Average length of the values (for fixed-length types, this should be a constant) <li> Maximum length of the values (for fixed-length types, this should be a constant) <li> <code>Histogram</code> <li> 2 <p><code>ColumnStat</code> is created when:</p> <ul> <li><code>CatalogColumnStat</code> is requested to toPlanStat</li> <li><code>Range</code> logical operator is requested to <code>computeStats</code></li> <li><code>EstimationUtils</code> is requested to <code>nullColumnStat</code></li> <li><code>JoinEstimation</code> is requested to computeByNdv, computeByHistogram</li> <li><code>UnionEstimation</code> is requested to <code>computeMinMaxStats</code>, <code>computeNullCountStats</code></li> <li><code>CommandUtils</code> is requested to rowToColumnStat</li> </ul>"},{"location":"cost-based-optimization/ColumnStat/#converting-to-catalogcolumnstat","title":"Converting to CatalogColumnStat <pre><code>toCatalogColumnStat(\n  colName: String,\n  dataType: DataType): CatalogColumnStat\n</code></pre> <p><code>toCatalogColumnStat</code> converts this <code>ColumnStat</code> to a CatalogColumnStat.</p>  <p><code>toCatalogColumnStat</code> is used when:</p> <ul> <li><code>PruneHiveTablePartitions</code> logical optimization is requested to <code>updateTableMeta</code></li> <li><code>AnalyzeColumnCommand</code> logical command is requested to analyzeColumnInCatalog</li> <li><code>PruneFileSourcePartitions</code> logical optimization is executed</li> </ul>","text":""},{"location":"cost-based-optimization/EstimationUtils/","title":"EstimationUtils","text":""},{"location":"cost-based-optimization/JoinEstimation/","title":"JoinEstimation","text":"<p><code>JoinEstimation</code> is a utility that &lt;&gt;. <p><code>JoinEstimation</code> is &lt;&gt; exclusively for <code>BasicStatsPlanVisitor</code> to estimate statistics of a Join logical operator. <p>[[creating-instance]] [[join]] <code>JoinEstimation</code> takes a Join.md[Join] logical operator when created.</p> <p>[[leftStats]] [[rightStats]] When &lt;&gt;, <code>JoinEstimation</code> immediately takes the estimated statistics and query hints of the Join.md#left[left] and Join.md#right[right] sides of the &lt;&gt; logical operator."},{"location":"cost-based-optimization/JoinEstimation/#source-scala","title":"[source, scala]","text":"<p>// JoinEstimation requires row count stats for join statistics estimates // With cost-based optimization off, size in bytes is available only // That would give no join estimates whatsoever (except size in bytes) // Make sure that you <code>--conf spark.sql.cbo.enabled=true</code> scala&gt; println(spark.sessionState.conf.cboEnabled) true</p> <p>// Build a query with join operator // From the available data sources tables seem the best...so far val r1 = spark.range(5) scala&gt; println(r1.queryExecution.analyzed.stats.simpleString) sizeInBytes=40.0 B, hints=none</p> <p>// Make the demo reproducible val db = spark.catalog.currentDatabase spark.sharedState.externalCatalog.dropTable(db, table = \"t1\", ignoreIfNotExists = true, purge = true) spark.sharedState.externalCatalog.dropTable(db, table = \"t2\", ignoreIfNotExists = true, purge = true)</p> <p>// FIXME What relations give row count stats?</p> <p>// Register tables spark.range(5).write.saveAsTable(\"t1\") spark.range(10).write.saveAsTable(\"t2\")</p> <p>// Refresh internal registries sql(\"REFRESH TABLE t1\") sql(\"REFRESH TABLE t2\")</p> <p>// Calculate row count stats val tables = Seq(\"t1\", \"t2\") tables.map(t =&gt; s\"ANALYZE TABLE $t COMPUTE STATISTICS\").foreach(sql)</p> <p>val t1 = spark.table(\"t1\") val t2 = spark.table(\"t2\")</p> <p>// analyzed plan is just before withCachedData and optimizedPlan plans // where CostBasedJoinReorder kicks in and optimizes a query using statistics</p> <p>val t1plan = t1.queryExecution.analyzed scala&gt; println(t1plan.numberedTreeString) 00 SubqueryAlias t1 01 +- Relation[id#45L] parquet</p> <p>// Show the stats of every node in the analyzed query plan</p> <p>val p0 = t1plan.p(0) scala&gt; println(s\"Statistics of ${p0.simpleString}: ${p0.stats.simpleString}\") Statistics of SubqueryAlias t1: sizeInBytes=80.0 B, hints=none</p> <p>val p1 = t1plan.p(1) scala&gt; println(s\"Statistics of ${p1.simpleString}: ${p1.stats.simpleString}\") Statistics of Relation[id#45L] parquet: sizeInBytes=80.0 B, rowCount=5, hints=none</p> <p>val t2plan = t2.queryExecution.analyzed</p> <p>// let's get rid of the SubqueryAlias operator</p> <p>import org.apache.spark.sql.catalyst.analysis.EliminateSubqueryAliases val t1NoAliasesPlan = EliminateSubqueryAliases(t1plan) val t2NoAliasesPlan = EliminateSubqueryAliases(t2plan)</p> <p>// Using Catalyst DSL import org.apache.spark.sql.catalyst.dsl.plans._ import org.apache.spark.sql.catalyst.plans._ val plan = t1NoAliasesPlan.join(   otherPlan = t2NoAliasesPlan,   joinType = Inner,   condition = Some($\"id\".expr)) scala&gt; println(plan.numberedTreeString) 00 'Join Inner, 'id 01 :- Relation[id#45L] parquet 02 +- Relation[id#57L] parquet</p> <p>// Take Join operator off the logical plan // JoinEstimation works with Joins only import org.apache.spark.sql.catalyst.plans.logical.Join val join = plan.collect { case j: Join =&gt; j }.head</p> <p>// Make sure that row count stats are defined per join side scala&gt; join.left.stats.rowCount.isDefined res1: Boolean = true</p> <p>scala&gt; join.right.stats.rowCount.isDefined res2: Boolean = true</p> <p>// Make the example reproducible // Computing stats is once-only process and the estimates are cached join.invalidateStatsCache</p> <p>import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.JoinEstimation val stats = JoinEstimation(join).estimate scala&gt; :type stats Option[org.apache.spark.sql.catalyst.plans.logical.Statistics]</p> <p>// Stats have to be available so Option.get should just work scala&gt; println(stats.get.simpleString) Some(sizeInBytes=1200.0 B, rowCount=50, hints=none)</p> <p><code>JoinEstimation</code> can &lt;&gt; with the following Join.md#joinType[join types]: <ul> <li><code>Inner</code>, <code>Cross</code>, <code>LeftOuter</code>, <code>RightOuter</code>, <code>FullOuter</code>, <code>LeftSemi</code> and <code>LeftAnti</code></li> </ul> <p>For the other join types (e.g. <code>ExistenceJoin</code>), <code>JoinEstimation</code> prints out a DEBUG message to the logs and returns <code>None</code> (to \"announce\" that no statistics could be computed).</p>"},{"location":"cost-based-optimization/JoinEstimation/#source-scala_1","title":"[source, scala]","text":"<p>// Demo: Unsupported join type, i.e. ExistenceJoin</p> <p>// Some parts were copied from the earlier demo // FIXME Make it self-contained</p> <p>// Using Catalyst DSL // Don't even know if such existance join could ever be possible in Spark SQL // For demo purposes it's OK, isn't it? import org.apache.spark.sql.catalyst.plans.ExistenceJoin val left = t1NoAliasesPlan val right = t2NoAliasesPlan val plan = left.join(right,   joinType = ExistenceJoin(exists = 'id.long))</p> <p>// Take Join operator off the logical plan // JoinEstimation works with Joins only import org.apache.spark.sql.catalyst.plans.logical.Join val join = plan.collect { case j: Join =&gt; j }.head</p> <p>// Enable DEBUG logging level import org.apache.log4j.{Level, Logger} Logger.getLogger(\"org.apache.spark.sql.catalyst.plans.logical.statsEstimation.JoinEstimation\").setLevel(Level.DEBUG)</p> <p>scala&gt; val stats = JoinEstimation(join).estimate 18/06/13 10:29:37 DEBUG JoinEstimation: [CBO] Unsupported join type: ExistenceJoin(id#35L) stats: Option[org.apache.spark.sql.catalyst.plans.logical.Statistics] = None</p>"},{"location":"cost-based-optimization/JoinEstimation/#source-scala_2","title":"[source, scala]","text":"<p>// FIXME Describe the purpose of the demo</p> <p>// Using Catalyst DSL import org.apache.spark.sql.catalyst.dsl.plans._</p> <p>val t1 = table(ref = \"t1\")</p> <p>// HACK: Disable symbolToColumn implicit conversion // It is imported automatically in spark-shell (and makes demos impossible) // implicit def symbolToColumn(s: Symbol): org.apache.spark.sql.ColumnName trait ThatWasABadIdea implicit def symbolToColumn(ack: ThatWasABadIdea) = ack</p> <p>import org.apache.spark.sql.catalyst.dsl.expressions._ val id = 'id.long</p> <p>val t2 = table(\"t2\") import org.apache.spark.sql.catalyst.plans.LeftSemi val plan = t1.join(t2, joinType = LeftSemi, condition = Some(id)) scala&gt; println(plan.numberedTreeString) 00 'Join LeftSemi, id#2: bigint 01 :- 'UnresolvedRelation <code>t1</code> 02 +- 'UnresolvedRelation <code>t2</code></p> <p>import org.apache.spark.sql.catalyst.plans.logical.Join val join = plan match { case j: Join =&gt; j }</p> <p>import org.apache.spark.sql.catalyst.plans.logical.statsEstimation.JoinEstimation</p> <p>// FIXME java.lang.UnsupportedOperationException val stats = JoinEstimation(join).estimate</p> <p>[[logging]] [TIP] ==== Enable <code>DEBUG</code> logging level for <code>org.apache.spark.sql.catalyst.plans.logical.statsEstimation.JoinEstimation</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.catalyst.plans.logical.statsEstimation.JoinEstimation=DEBUG\n</code></pre>"},{"location":"cost-based-optimization/JoinEstimation/#refer-to-spark-loggingmdlogging","title":"Refer to spark-logging.md[Logging].","text":"<p>=== [[estimateInnerOuterJoin]] <code>estimateInnerOuterJoin</code> Internal Method</p>"},{"location":"cost-based-optimization/JoinEstimation/#source-scala_3","title":"[source, scala]","text":""},{"location":"cost-based-optimization/JoinEstimation/#estimateinnerouterjoin-optionstatistics","title":"estimateInnerOuterJoin(): Option[Statistics]","text":"<p><code>estimateInnerOuterJoin</code> destructures &lt;&gt; into a join type with the left and right keys. <p><code>estimateInnerOuterJoin</code> simply returns <code>None</code> (i.e. nothing) when either side of the &lt;&gt; have no row count statistic. <p>NOTE: <code>estimateInnerOuterJoin</code> is used exclusively when <code>JoinEstimation</code> is requested to &lt;&gt; for <code>Inner</code>, <code>Cross</code>, <code>LeftOuter</code>, <code>RightOuter</code> and <code>FullOuter</code> joins. <p>=== [[computeByNdv]] <code>computeByNdv</code> Internal Method</p>"},{"location":"cost-based-optimization/JoinEstimation/#source-scala_4","title":"[source, scala]","text":"<p>computeByNdv(   leftKey: AttributeReference,   rightKey: AttributeReference,   newMin: Option[Any],   newMax: Option[Any]): (BigInt, ColumnStat)</p> <p><code>computeByNdv</code>...FIXME</p> <p>NOTE: <code>computeByNdv</code> is used exclusively when <code>JoinEstimation</code> is requested for &lt;&gt; <p>=== [[computeCardinalityAndStats]] <code>computeCardinalityAndStats</code> Internal Method</p>"},{"location":"cost-based-optimization/JoinEstimation/#source-scala_5","title":"[source, scala]","text":"<p>computeCardinalityAndStats(   keyPairs: Seq[(AttributeReference, AttributeReference)]): (BigInt, AttributeMap[ColumnStat])</p> <p><code>computeCardinalityAndStats</code>...FIXME</p> <p>NOTE: <code>computeCardinalityAndStats</code> is used exclusively when <code>JoinEstimation</code> is requested for &lt;&gt; <p>=== [[computeByHistogram]] Computing Join Cardinality Using Equi-Height Histograms -- <code>computeByHistogram</code> Internal Method</p>"},{"location":"cost-based-optimization/JoinEstimation/#source-scala_6","title":"[source, scala]","text":"<p>computeByHistogram(   leftKey: AttributeReference,   rightKey: AttributeReference,   leftHistogram: Histogram,   rightHistogram: Histogram,   newMin: Option[Any],   newMax: Option[Any]): (BigInt, ColumnStat)</p> <p><code>computeByHistogram</code>...FIXME</p> <p>NOTE: <code>computeByHistogram</code> is used exclusively when <code>JoinEstimation</code> is requested for &lt;&gt; (and the histograms of both column attributes used in a join are available). <p>=== [[estimateLeftSemiAntiJoin]] Estimating Statistics for Left Semi and Left Anti Joins -- <code>estimateLeftSemiAntiJoin</code> Internal Method</p>"},{"location":"cost-based-optimization/JoinEstimation/#source-scala_7","title":"[source, scala]","text":""},{"location":"cost-based-optimization/JoinEstimation/#estimateleftsemiantijoin-optionstatistics","title":"estimateLeftSemiAntiJoin(): Option[Statistics]","text":"<p><code>estimateLeftSemiAntiJoin</code> estimates statistics of the &lt;&gt; logical operator only when estimated row count statistic is available. Otherwise, <code>estimateLeftSemiAntiJoin</code> simply returns <code>None</code> (i.e. no statistics estimated). <p>NOTE: cost-based-optimization/index.md#rowCount[row count] statistic of a table is available only after cost-based-optimization/index.md#ANALYZE-TABLE[ANALYZE TABLE COMPUTE STATISTICS] SQL command.</p> <p>If available, <code>estimateLeftSemiAntiJoin</code> takes the estimated row count statistic of the Join.md#left[left side] of the &lt;&gt; operator. <p>NOTE: Use cost-based-optimization/index.md#ANALYZE-TABLE[ANALYZE TABLE COMPUTE STATISTICS] SQL command on the left logical plan to compute cost-based-optimization/index.md#rowCount[row count] statistics.</p> <p>NOTE: Use cost-based-optimization/index.md#ANALYZE-TABLE[ANALYZE TABLE COMPUTE STATISTICS FOR COLUMNS] SQL command on the left logical plan to generate column (equi-height) histograms for more accurate estimations.</p> <p>In the end, <code>estimateLeftSemiAntiJoin</code> creates a new Statistics with the following estimates:</p> <p>. Total size (in bytes) is the output size for the output schema of the join, the row count statistic (aka output rows) and column histograms.</p> <p>. Row count is exactly the row count of the left side</p> <p>. Column histograms is exactly the column histograms of the left side</p> <p><code>estimateLeftSemiAntiJoin</code> is used when <code>JoinEstimation</code> is requested to &lt;&gt; for <code>LeftSemi</code> and <code>LeftAnti</code> joins. <p>=== [[estimate]] Estimating Statistics and Query Hints of Join Logical Operator -- <code>estimate</code> Method</p>"},{"location":"cost-based-optimization/JoinEstimation/#source-scala_8","title":"[source, scala]","text":""},{"location":"cost-based-optimization/JoinEstimation/#estimate-optionstatistics","title":"estimate: Option[Statistics]","text":"<p><code>estimate</code> estimates statistics and query hints of the &lt;&gt; logical operator per Join.md#joinType[join type]: <ul> <li> <p>For <code>Inner</code>, <code>Cross</code>, <code>LeftOuter</code>, <code>RightOuter</code> and <code>FullOuter</code> join types, <code>estimate</code> &lt;&gt; <li> <p>For <code>LeftSemi</code> and <code>LeftAnti</code> join types, <code>estimate</code> &lt;&gt; <p>For other join types, <code>estimate</code> prints out the following DEBUG message to the logs and returns <code>None</code> (to \"announce\" that no statistics could be computed).</p> <pre><code>[CBO] Unsupported join type: [joinType]\n</code></pre> <p>NOTE: <code>estimate</code> is used exclusively when <code>BasicStatsPlanVisitor</code> is requested to estimate statistics and query hints of a Join logical operator.</p>"},{"location":"cost-based-optimization/LogicalPlanStats/","title":"LogicalPlanStats \u2014 Statistics Estimates and Query Hints of Logical Operators","text":"<p><code>LogicalPlanStats</code> is an extension of the LogicalPlan abstraction to add Statistics for query planning (with or without cost-based optimization, e.g. CostBasedJoinReorder or JoinSelection, respectively).</p>"},{"location":"cost-based-optimization/LogicalPlanStats/#scala-definition","title":"Scala Definition","text":"<pre><code>trait LogicalPlanStats { self: LogicalPlan =&gt;\n// body omitted\n}\n</code></pre> <p><code>LogicalPlanStats</code> is a Scala trait with <code>self: LogicalPlan</code> as part of its definition. It is a very useful feature of Scala that restricts the set of classes that the trait could be used with (as well as makes the target subtype known at compile time).</p> <p>Tip</p> <p>Read up on <code>self-type</code> in Scala in the Tour of Scala.</p>"},{"location":"cost-based-optimization/LogicalPlanStats/#computing-and-caching-statistics-and-query-hints","title":"Computing (and Caching) Statistics and Query Hints <pre><code>stats: Statistics\n</code></pre> <p><code>stats</code> gets the statistics from cache if computed already. If not, <code>stats</code> branches off per whether cost-based optimization is enabled or not and requests BasicStatsPlanVisitor or SizeInBytesOnlyStatsPlanVisitor for the statistics, respectively.</p> <p><code>statsCache</code> caches the statistics for later use.</p> <p><code>stats</code> is used when:</p> <ul> <li><code>JoinSelection</code> execution planning strategy matches a logical plan:</li> <li>that is small enough for broadcast join (using <code>BroadcastHashJoinExec</code> or <code>BroadcastNestedLoopJoinExec</code> physical operators)</li> <li>whose a single partition should be small enough to build a hash table (using <code>ShuffledHashJoinExec</code> physical operator)</li> <li>that is much smaller (3X) than the other plan (for <code>ShuffledHashJoinExec</code> physical operator)</li> <li>...</li> <li><code>QueryExecution</code> is requested for stringWithStats for <code>EXPLAIN COST</code> SQL command</li> <li><code>CacheManager</code> is requested to cache a Dataset or recacheByCondition</li> <li><code>HiveMetastoreCatalog</code> is requested for <code>convertToLogicalRelation</code></li> <li><code>StarSchemaDetection</code></li> <li>CostBasedJoinReorder logical optimization is executed</li> </ul>","text":""},{"location":"cost-based-optimization/LogicalPlanStats/#invalidating-statistics-cache","title":"Invalidating Statistics Cache <pre><code>invalidateStatsCache(): Unit\n</code></pre> <p><code>invalidateStatsCache</code> clears the cache of the current logical operator and all of the children.</p> <p><code>invalidateStatsCache</code> is used when AdaptiveSparkPlanExec physical operator is requested to reOptimize.</p>","text":""},{"location":"cost-based-optimization/LogicalPlanStats/#statistics-cache","title":"Statistics Cache <pre><code>statsCache: Option[Statistics]\n</code></pre> <p><code>statsCache</code> holds the Statistics once computed (until invalidated).</p>","text":""},{"location":"cost-based-optimization/LogicalPlanStats/#demo-accessing-statistics","title":"Demo: Accessing Statistics <p>Use <code>EXPLAIN COST</code> SQL command to explain a query with the &lt;&gt;. <pre><code>scala&gt; sql(\"EXPLAIN COST SHOW TABLES\").as[String].collect.foreach(println)\n== Optimized Logical Plan ==\nShowTablesCommand false, Statistics(sizeInBytes=1.0 B, hints=none)\n\n== Physical Plan ==\nExecute ShowTablesCommand\n   +- ShowTablesCommand false\n</code></pre> <p>The statistics of a logical plan directly using stats method or indirectly requesting <code>QueryExecution</code> for text representation with statistics.</p> <pre><code>val q = sql(\"SHOW TABLES\")\nscala&gt; println(q.queryExecution.analyzed.stats)\nStatistics(sizeInBytes=1.0 B, hints=none)\n\nscala&gt; println(q.queryExecution.stringWithStats)\n== Optimized Logical Plan ==\nShowTablesCommand false, Statistics(sizeInBytes=1.0 B, hints=none)\n\n== Physical Plan ==\nExecute ShowTablesCommand\n   +- ShowTablesCommand false\n</code></pre> <pre><code>val names = Seq((1, \"one\"), (2, \"two\")).toDF(\"id\", \"name\")\n\nassert(spark.sessionState.conf.cboEnabled == false, \"CBO should be turned off\")\n\n// CBO is disabled and so only sizeInBytes stat is available\n// FIXME Why is analyzed required (not just logical)?\nval namesStatsCboOff = names.queryExecution.analyzed.stats\nscala&gt; println(namesStatsCboOff)\nStatistics(sizeInBytes=48.0 B, hints=none)\n\n// Turn CBO on\nimport org.apache.spark.sql.internal.SQLConf\nspark.sessionState.conf.setConf(SQLConf.CBO_ENABLED, true)\n\n// Make sure that CBO is really enabled\nscala&gt; println(spark.sessionState.conf.cboEnabled)\ntrue\n\n// Invalidate the stats cache\nnames.queryExecution.analyzed.invalidateStatsCache\n\n// Check out the statistics\nval namesStatsCboOn = names.queryExecution.analyzed.stats\nscala&gt; println(namesStatsCboOn)\nStatistics(sizeInBytes=48.0 B, hints=none)\n\n// Despite CBO enabled, we can only get sizeInBytes stat\n// That's because names is a LocalRelation under the covers\nscala&gt; println(names.queryExecution.optimizedPlan.numberedTreeString)\n00 LocalRelation [id#5, name#6]\n\n// LocalRelation triggers BasicStatsPlanVisitor to execute default case\n// which is exactly as if we had CBO turned off\n\n// Let's register names as a managed table\n// That will change the rules of how stats are computed\nimport org.apache.spark.sql.SaveMode\nnames.write.mode(SaveMode.Overwrite).saveAsTable(\"names\")\n\nscala&gt; spark.catalog.tableExists(\"names\")\nres5: Boolean = true\n\nscala&gt; spark.catalog.listTables.filter($\"name\" === \"names\").show\n+-----+--------+-----------+---------+-----------+\n| name|database|description|tableType|isTemporary|\n+-----+--------+-----------+---------+-----------+\n|names| default|       null|  MANAGED|      false|\n+-----+--------+-----------+---------+-----------+\n\nval namesTable = spark.table(\"names\")\n\n// names is a managed table now\n// And Relation (not LocalRelation)\nscala&gt; println(namesTable.queryExecution.optimizedPlan.numberedTreeString)\n00 Relation[id#32,name#33] parquet\n\n// Check out the statistics\nval namesStatsCboOn = namesTable.queryExecution.analyzed.stats\nscala&gt; println(namesStatsCboOn)\nStatistics(sizeInBytes=1064.0 B, hints=none)\n\n// Nothing has really changed, hasn't it?\n// Well, sizeInBytes is bigger, but that's the only stat available\n// row count stat requires ANALYZE TABLE with no NOSCAN option\nsql(\"ANALYZE TABLE names COMPUTE STATISTICS\")\n\n// Invalidate the stats cache\nnamesTable.queryExecution.analyzed.invalidateStatsCache\n\n// No change?! How so?\nval namesStatsCboOn = namesTable.queryExecution.analyzed.stats\nscala&gt; println(namesStatsCboOn)\nStatistics(sizeInBytes=1064.0 B, hints=none)\n\n// Use optimized logical plan instead\nval namesTableStats = spark.table(\"names\").queryExecution.optimizedPlan.stats\nscala&gt; println(namesTableStats)\nStatistics(sizeInBytes=64.0 B, rowCount=2, hints=none)\n</code></pre>","text":""},{"location":"cost-based-optimization/LogicalPlanVisitor/","title":"LogicalPlanVisitor \u2014 Contract for Computing Statistic Estimates and Query Hints of Logical Plan","text":"<p><code>LogicalPlanVisitor</code> is the &lt;&gt; that uses the &lt;&gt; to scan a logical query plan and compute estimates of plan statistics and query hints. <p>TIP: Read about the visitor design pattern in https://en.wikipedia.org/wiki/Visitor_pattern[Wikipedia].</p> <p>[[visit]] <code>LogicalPlanVisitor</code> defines <code>visit</code> method that dispatches computing the statistics of a logical plan to the &lt;&gt;."},{"location":"cost-based-optimization/LogicalPlanVisitor/#source-scala","title":"[source, scala]","text":""},{"location":"cost-based-optimization/LogicalPlanVisitor/#visitp-logicalplan-t","title":"visit(p: LogicalPlan): T","text":"<p>NOTE: <code>T</code> stands for the type of a result to be computed (while visiting the query plan tree) and is currently always Statistics only.</p> <p>The &lt;&gt; <code>LogicalPlanVisitor</code> is chosen per cost-based-optimization/index.md#spark.sql.cbo.enabled[spark.sql.cbo.enabled] configuration property. When turned on (i.e. <code>true</code>), <code>LogicalPlanStats</code> uses &lt;&gt; while &lt;&gt; otherwise. <p>[[implementations]] .LogicalPlanVisitors [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | LogicalPlanVisitor | Description</p> [[BasicStatsPlanVisitor]] BasicStatsPlanVisitor [[SizeInBytesOnlyStatsPlanVisitor]] SizeInBytesOnlyStatsPlanVisitor === <p>[[contract]] [[handlers]] .LogicalPlanVisitor's Logical Operators and Their Handlers [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Logical Operator | Handler</p> <p>| [[Aggregate]] Aggregate.md[Aggregate] | [[visitAggregate]] <code>visitAggregate</code></p> <p>| [[Distinct]] <code>Distinct</code> | <code>visitDistinct</code></p> <p>| [[Except]] <code>Except</code> | <code>visitExcept</code></p> <p>| [[Expand]] Expand.md[Expand] | <code>visitExpand</code></p> <p>| [[Filter]] <code>Filter</code> | [[visitFilter]] <code>visitFilter</code></p> <p>| [[Generate]] Generate.md[Generate] | <code>visitGenerate</code></p> <p>| [[GlobalLimit]] GlobalLimit | <code>visitGlobalLimit</code></p> <p>| [[Intersect]] <code>Intersect</code> | [[visitIntersect]] <code>visitIntersect</code></p> <p>| [[Join]] Join.md[Join] | [[visitJoin]] <code>visitJoin</code></p> <p>| [[LocalLimit]] <code>LocalLimit</code> | <code>visitLocalLimit</code></p> <p>| [[Pivot]] Pivot.md[Pivot] | <code>visitPivot</code></p> <p>| [[Project]] Project.md[Project] | [[visitProject]] <code>visitProject</code></p> <p>| [[Repartition]] Repartition | <code>visitRepartition</code></p> <p>| RepartitionByExpression | <code>visitRepartitionByExpr</code></p> <p>| [[ResolvedHint]] ResolvedHint.md[ResolvedHint] | <code>visitHint</code></p> <p>| [[Sample]] <code>Sample</code> | <code>visitSample</code></p> <p>| [[ScriptTransformation]] <code>ScriptTransformation</code> | <code>visitScriptTransform</code></p> <p>| [[Union]] <code>Union</code> | <code>visitUnion</code></p> <p>| [[Window]] Window.md[Window] | <code>visitWindow</code></p> <p>| [[LogicalPlan]] Other spark-sql-LogicalPlan.md[logical operators] | <code>default</code> |===</p>"},{"location":"cost-based-optimization/SizeInBytesOnlyStatsPlanVisitor/","title":"SizeInBytesOnlyStatsPlanVisitor \u2014 LogicalPlanVisitor for Total Size (in Bytes) Statistic Only","text":"<p><code>SizeInBytesOnlyStatsPlanVisitor</code> is a LogicalPlanVisitor that computes a single dimension for plan statistics, i.e. the total size (in bytes).</p> <p>=== [[default]] <code>default</code> Method</p>"},{"location":"cost-based-optimization/SizeInBytesOnlyStatsPlanVisitor/#source-scala","title":"[source, scala]","text":""},{"location":"cost-based-optimization/SizeInBytesOnlyStatsPlanVisitor/#defaultp-logicalplan-statistics","title":"default(p: LogicalPlan): Statistics","text":"<p><code>default</code> requests a LeafNode.md[leaf logical operator] for the statistics or creates a Statistics with the product of the <code>sizeInBytes</code> statistic of every child operator.</p> <p>NOTE: <code>default</code> uses the cache of the estimated statistics of a logical operator so the statistics of an operator is computed once until it is invalidated.</p> <p><code>default</code> is part of the LogicalPlanVisitor abstraction.</p> <p>=== [[visitIntersect]] <code>visitIntersect</code> Method</p>"},{"location":"cost-based-optimization/SizeInBytesOnlyStatsPlanVisitor/#source-scala_1","title":"[source, scala]","text":""},{"location":"cost-based-optimization/SizeInBytesOnlyStatsPlanVisitor/#visitintersectp-intersect-statistics","title":"visitIntersect(p: Intersect): Statistics","text":"<p><code>visitIntersect</code>...FIXME</p> <p><code>visitIntersect</code> is part of the LogicalPlanVisitor abstraction.</p> <p>=== [[visitJoin]] <code>visitJoin</code> Method</p>"},{"location":"cost-based-optimization/SizeInBytesOnlyStatsPlanVisitor/#source-scala_2","title":"[source, scala]","text":""},{"location":"cost-based-optimization/SizeInBytesOnlyStatsPlanVisitor/#visitjoinp-join-statistics","title":"visitJoin(p: Join): Statistics","text":"<p><code>visitJoin</code>...FIXME</p> <p><code>visitJoin</code> is part of the LogicalPlanVisitor abstraction.</p>"},{"location":"cost-based-optimization/Statistics/","title":"Statistics","text":"<p><code>Statistics</code> holds the following estimates of a logical operator:</p> <ul> <li> Total (output) size (in bytes) <li>Estimated number of rows</li> <li> Column Statistics (column (equi-height) histograms) <p>Note</p> <p>Cost statistics, plan statistics or query statistics are synonyms and used interchangeably.</p> <p><code>Statistics</code> is created when:</p> <ul> <li><code>CatalogStatistics</code> is requested to convert metastore statistics</li> <li>DataSourceV2Relation, DataSourceV2ScanRelation, ExternalRDD, LocalRelation, LogicalRDD, LogicalRelation, <code>Range</code>, <code>OneRowRelation</code> logical operators are requested to <code>computeStats</code></li> <li><code>AggregateEstimation</code> and JoinEstimation are requested to <code>estimate</code></li> <li>SizeInBytesOnlyStatsPlanVisitor is executed</li> <li>QueryStageExec physical operator is requested to <code>computeStats</code></li> <li>DetermineTableStats logical resolution rule is executed</li> </ul>"},{"location":"cost-based-optimization/Statistics/#row-count","title":"Row Count <p>Row Count estimate is used in CostBasedJoinReorder logical optimization for Cost-Based Optimization.</p>","text":""},{"location":"cost-based-optimization/Statistics/#statistics-and-catalogstatistics","title":"Statistics and CatalogStatistics <p>CatalogStatistics is a \"subset\" of all possible <code>Statistics</code> (as there are no concepts of attributes in metastore).</p> <p><code>CatalogStatistics</code> are statistics stored in an external catalog (usually a Hive metastore) and are often referred as Hive statistics while <code>Statistics</code> represents the Spark statistics.</p>","text":""},{"location":"cost-based-optimization/Statistics/#accessing-statistics-of-logical-operator","title":"Accessing Statistics of Logical Operator <p>Statistics of a logical plan are available using stats property.</p> <pre><code>val q = spark.range(5).hint(\"broadcast\").join(spark.range(1), \"id\")\nval plan = q.queryExecution.optimizedPlan\nval stats = plan.stats\n\nscala&gt; :type stats\norg.apache.spark.sql.catalyst.plans.logical.Statistics\n\nscala&gt; println(stats.simpleString)\nsizeInBytes=213.0 B, hints=none\n</code></pre>  <p>Note</p> <p>Use ANALYZE TABLE COMPUTE STATISTICS SQL command to compute total size and row count statistics of a table.</p>   <p>Note</p> <p>Use ANALYZE TABLE COMPUTE STATISTICS FOR COLUMNS SQL Command to generate column (equi-height) histograms of a table.</p>","text":""},{"location":"cost-based-optimization/Statistics/#textual-representation","title":"Textual Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code> gives textual representation of the <code>Statistics</code>.</p> <pre><code>import org.apache.spark.sql.catalyst.plans.logical.Statistics\nimport org.apache.spark.sql.catalyst.plans.logical.HintInfo\nval stats = Statistics(sizeInBytes = 10, rowCount = Some(20))\n\nscala&gt; println(stats)\nStatistics(sizeInBytes=10.0 B, rowCount=20)\n</code></pre>","text":""},{"location":"default-columns/","title":"Default Columns","text":"<p>Apache Spark 3.4 introduces support for <code>DEFAULT</code> columns in the following SQL statements:</p> <ul> <li><code>ALTER TABLE ADD (COLUMN | COLUMNS)</code></li> <li><code>ALTER TABLE REPLACE COLUMNS</code></li> <li><code>CREATE TABLE</code></li> <li><code>(CREATE OR)? REPLACE TABLE</code></li> </ul> <p>Default Columns are columns defined with <code>DEFAULT</code> clause.</p> <pre><code>defaultExpression\n: DEFAULT expression\n;\n\ncolDefinitionOption\n: NOT NULL\n| defaultExpression\n| generationExpression\n| commentSpec\n;\n\ncreateOrReplaceTableColType\n: colName dataType colDefinitionOption*\n;\n\nqualifiedColTypeWithPosition\n: multipartIdentifier dataType (NOT NULL)? defaultExpression? commentSpec? colPosition?\n;\n</code></pre> <p>Default Columns are enabled using spark.sql.defaultColumn.enabled configuration property.</p> <p>With DEFAULT columns, <code>INSERT</code>, <code>UPDATE</code>, <code>MERGE</code> statements can reference the value using the <code>DEFAULT</code> keyword.</p> <pre><code>CREATE TABLE T(a INT, b INT NOT NULL);\n\n-- The default default is NULL\nINSERT INTO T VALUES (DEFAULT, 0);\nINSERT INTO T(b)  VALUES (1);\nSELECT * FROM T;\n(NULL, 0)\n(NULL, 1)\n\n-- Adding a default to a table with rows, sets the values for the\n-- existing rows (exist default) and new rows (current default).\nALTER TABLE T ADD COLUMN c INT DEFAULT 5;\nINSERT INTO T VALUES (1, 2, DEFAULT);\nSELECT * FROM T;\n(NULL, 0, 5)\n(NULL, 1, 5)\n(1, 2, 5)\n</code></pre> <p>Default Columns uses the following configuration properties:</p> <ul> <li>spark.sql.defaultColumn.allowedProviders</li> <li>spark.sql.defaultColumn.useNullsForMissingDefaultValues</li> <li>spark.sql.jsonGenerator.writeNullIfWithDefaultValue</li> </ul> <p>Default Columns are resolved using ResolveDefaultColumns logical resolution rule.</p>"},{"location":"demo/","title":"Demos","text":""},{"location":"demo/adaptive-query-execution/","title":"Demo: Adaptive Query Execution","text":"<p>This demo shows the internals of Adaptive Query Execution.</p>"},{"location":"demo/adaptive-query-execution/#before-you-begin","title":"Before you begin","text":"<p>Enable the following loggers:</p> <ul> <li>AQEOptimizer</li> <li>InsertAdaptiveSparkPlan</li> <li>ShuffleQueryStageExec</li> </ul>"},{"location":"demo/adaptive-query-execution/#query","title":"Query","text":"<p>Create a table.</p> <pre><code>sql(\"DROP TABLE IF EXISTS adaptive\")\nsql(\"CREATE TABLE adaptive USING parquet AS SELECT * FROM VALUES (1)\")\n</code></pre> <p>Create a query with an Exchange so Adaptive Query Execution can have a chance to step up (otherwise spark.sql.adaptive.forceApply would be required).</p> <pre><code>val q = spark.table(\"adaptive\").repartition(2)\n</code></pre> <pre><code>q.explain()\n</code></pre> <pre><code>== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Exchange RoundRobinPartitioning(2), REPARTITION_BY_NUM, [plan_id=58]\n   +- FileScan parquet default.adaptive[col1#14] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/jacek/dev/oss/spark/spark-warehouse/adaptive], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;col1:int&gt;\n</code></pre> <p>Note the value of the isFinalPlan flag that is <code>false</code>.</p>"},{"location":"demo/adaptive-query-execution/#access-adaptivesparkplan","title":"Access AdaptiveSparkPlan","text":"<pre><code>import org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec\n\nval adaptiveExec = q.queryExecution.executedPlan.collectFirst { case op: AdaptiveSparkPlanExec =&gt; op }.get\nassert(adaptiveExec.isInstanceOf[AdaptiveSparkPlanExec])\n</code></pre> <pre><code>println(adaptiveExec)\n</code></pre> <pre><code>AdaptiveSparkPlan isFinalPlan=false\n+- Exchange RoundRobinPartitioning(2), REPARTITION_BY_NUM, [plan_id=58]\n   +- FileScan parquet default.adaptive[col1#14] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/jacek/dev/oss/spark/spark-warehouse/adaptive], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;col1:int&gt;\n</code></pre>"},{"location":"demo/adaptive-query-execution/#execute-adaptivesparkplan","title":"Execute AdaptiveSparkPlan","text":"<p>Execute the query that in turn executes AdaptiveSparkPlanExec physical operator (and marks it as adaptively optimized using <code>isFinalPlan</code> flag).</p> <pre><code>val rdd = adaptiveExec.execute()\n</code></pre> <p><code>ShuffleQueryStageExec</code> and <code>AQEOptimizer</code> loggers should print out the following messages to the logs:</p> <pre><code>DEBUG ShuffleQueryStageExec: Materialize query stage ShuffleQueryStageExec: 0\nTRACE AQEOptimizer: Fixed point reached for batch Propagate Empty Relations after 1 iterations.\nTRACE AQEOptimizer: Fixed point reached for batch Dynamic Join Selection after 1 iterations.\nTRACE AQEOptimizer: Fixed point reached for batch Eliminate Limits after 1 iterations.\nTRACE AQEOptimizer: Fixed point reached for batch Optimize One Row Plan after 1 iterations.\n</code></pre> <pre><code>println(rdd.toDebugString)\n</code></pre> <pre><code>(2) ShuffledRowRDD[5] at execute at &lt;console&gt;:1 []\n +-(2) MapPartitionsRDD[4] at execute at &lt;console&gt;:1 []\n    |  MapPartitionsRDD[3] at execute at &lt;console&gt;:1 []\n    |  MapPartitionsRDD[2] at execute at &lt;console&gt;:1 []\n    |  MapPartitionsRDD[1] at execute at &lt;console&gt;:1 []\n    |  FileScanRDD[0] at execute at &lt;console&gt;:1 []\n</code></pre> <p>Alternatively, you could use one of the high-level operators (e.g. <code>tail</code>).</p> <pre><code>q.tail(1)\n</code></pre>"},{"location":"demo/adaptive-query-execution/#explain-query","title":"Explain Query","text":"<p>After execution, <code>AdaptiveSparkPlanExec</code> is final (and will never get re-optimized).</p> <pre><code>println(adaptiveExec)\n</code></pre> <pre><code>AdaptiveSparkPlan isFinalPlan=true\n+- == Final Plan ==\n   ShuffleQueryStage 0\n   +- Exchange RoundRobinPartitioning(2), REPARTITION_BY_NUM, [plan_id=15]\n      +- *(1) ColumnarToRow\n         +- FileScan parquet default.adaptive[id#0L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/jacek/dev/oss/spark/spark-warehouse/adaptive], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt;\n+- == Initial Plan ==\n   Exchange RoundRobinPartitioning(2), REPARTITION_BY_NUM, [plan_id=6]\n   +- FileScan parquet default.adaptive[id#0L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/jacek/dev/oss/spark/spark-warehouse/adaptive], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt;\n</code></pre> <p>Note the value of the isFinalPlan flag that is <code>true</code>.</p> <pre><code>q.explain()\n</code></pre> <pre><code>AdaptiveSparkPlan isFinalPlan=true\n+- == Final Plan ==\n   ShuffleQueryStage 0\n   +- Exchange RoundRobinPartitioning(2), REPARTITION_BY_NUM, [plan_id=15]\n      +- *(1) ColumnarToRow\n         +- FileScan parquet default.adaptive[id#0L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/jacek/dev/oss/spark/spark-warehouse/adaptive], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt;\n+- == Initial Plan ==\n   Exchange RoundRobinPartitioning(2), REPARTITION_BY_NUM, [plan_id=6]\n   +- FileScan parquet default.adaptive[id#0L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/jacek/dev/oss/spark/spark-warehouse/adaptive], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt;\n</code></pre>"},{"location":"demo/adaptive-query-execution/#internals","title":"Internals","text":"<p>AdaptiveSparkPlanExec is a leaf physical operator. That is why the following snippet gives a single physical operator in the optimized physical query plan.</p> <pre><code>q.queryExecution.executedPlan.foreach(op =&gt; println(op.getClass.getName))\n</code></pre> <pre><code>org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec\n</code></pre> <p>Let's access the underlying <code>AdaptiveSparkPlanExec</code> and the inputPlan and initialPlan physical query plans.</p> <pre><code>import org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec\n\nval adaptiveExec = q.queryExecution.executedPlan.collectFirst { case op: AdaptiveSparkPlanExec =&gt; op }.get\nassert(adaptiveExec.isInstanceOf[AdaptiveSparkPlanExec])\n\nval inputPlan = adaptiveExec.inputPlan\nval initialPlan = adaptiveExec.initialPlan\n</code></pre> <p>Before execution, <code>AdaptiveSparkPlanExec</code> should not be final.</p> <pre><code>println(adaptiveExec.numberedTreeString)\n</code></pre> <pre><code>00 AdaptiveSparkPlan isFinalPlan=false\n01 +- Exchange RoundRobinPartitioning(2), REPARTITION_BY_NUM, [plan_id=6]\n02    +- FileScan parquet default.adaptive[id#0L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/jacek/dev/oss/spark/spark-warehouse/adaptive], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt;\n</code></pre> <pre><code>val rdd = adaptiveExec.execute()\n</code></pre> <pre><code>DEBUG ShuffleQueryStageExec: Materialize query stage ShuffleQueryStageExec: 0\nTRACE AQEOptimizer: Fixed point reached for batch Propagate Empty Relations after 1 iterations.\nTRACE AQEOptimizer: Fixed point reached for batch Dynamic Join Selection after 1 iterations.\nTRACE AQEOptimizer: Fixed point reached for batch Eliminate Limits after 1 iterations.\nTRACE AQEOptimizer: Fixed point reached for batch Optimize One Row Plan after 1 iterations.\n</code></pre> <pre><code>println(rdd.toDebugString)\n</code></pre> <pre><code>(2) ShuffledRowRDD[5] at execute at &lt;console&gt;:1 []\n +-(2) MapPartitionsRDD[4] at execute at &lt;console&gt;:1 []\n    |  MapPartitionsRDD[3] at execute at &lt;console&gt;:1 []\n    |  MapPartitionsRDD[2] at execute at &lt;console&gt;:1 []\n    |  MapPartitionsRDD[1] at execute at &lt;console&gt;:1 []\n    |  FileScanRDD[0] at execute at &lt;console&gt;:1 []\n</code></pre> <p>Once executed, <code>AdaptiveSparkPlanExec</code> should be final.</p> <pre><code>println(adaptiveExec.numberedTreeString)\n</code></pre> <pre><code>00 AdaptiveSparkPlan isFinalPlan=true\n01 +- == Final Plan ==\n02    ShuffleQueryStage 0\n03    +- Exchange RoundRobinPartitioning(2), REPARTITION_BY_NUM, [plan_id=15]\n04       +- *(1) ColumnarToRow\n05          +- FileScan parquet default.adaptive[id#0L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/jacek/dev/oss/spark/spark-warehouse/adaptive], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt;\n06 +- == Initial Plan ==\n07    Exchange RoundRobinPartitioning(2), REPARTITION_BY_NUM, [plan_id=6]\n08    +- FileScan parquet default.adaptive[id#0L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/jacek/dev/oss/spark/spark-warehouse/adaptive], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt;\n</code></pre> <p>Note</p> <p>There seems no way to dig deeper and access QueryStageExecs though. Feeling sad</p>"},{"location":"demo/adaptive-query-execution/#future-work","title":"Future Work","text":"<ul> <li>Use <code>SparkListenerSQLAdaptiveExecutionUpdate</code> and <code>SparkListenerSQLAdaptiveSQLMetricUpdates</code> to intercept changes in query plans</li> <li>Enable <code>ALL</code> for AdaptiveSparkPlanExec logger with spark.sql.adaptive.logLevel to <code>TRACE</code></li> </ul>"},{"location":"demo/connecting-spark-sql-to-hive-metastore/","title":"Demo: Connecting Spark SQL to Hive Metastore (with Remote Metastore Server)","text":"<p>The demo shows how to run Apache Spark 3.4.0-rc5 with Apache Hive 2.3.9 (on Apache Hadoop 2.10.0).</p> <p>You'll be using a separate Remote Metastore Server to access table metadata via the Thrift protocol. It is in the discretion of the Remote Metastore Server to connect to the underlying JDBC-accessible relational database (e.g. PostgreSQL).</p> <p>Tip</p> <p>Read up External Apache Hive metastore in the official documentation of Databricks platform that describes the topic in more details from the perspective of Apache Spark developers.</p>"},{"location":"demo/connecting-spark-sql-to-hive-metastore/#install-java-8","title":"Install Java 8","text":"<p>As per Hadoop's Hadoop Java Versions:</p> <p>Apache Hadoop from 2.7.x to 2.x support Java 7 and 8</p> <p>As per Spark's Downloading:</p> <p>Spark runs on Java 8</p> <p>Make sure you have Java 8 installed.</p> <pre><code>$ java -version\nopenjdk version \"1.8.0_242\"\nOpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_242-b08)\nOpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.242-b08, mixed mode)\n</code></pre>"},{"location":"demo/connecting-spark-sql-to-hive-metastore/#build-apache-spark-for-apache-hadoop","title":"Build Apache Spark for Apache Hadoop","text":"<p>Build Apache Spark with support for Apache Hadoop 2.10.0.</p> <pre><code>$ cd $SPARK_HOME\n$ ./build/mvn \\\n    -Dhadoop.version=2.10.0 \\\n    -Pyarn,hive,hive-thriftserver \\\n    -Pscala-2.12 \\\n    -Pkubernetes \\\n    -DskipTests \\\n    clean install\n</code></pre> <pre><code>$ ./bin/spark-shell --version\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.5\n      /_/\n\nUsing Scala version 2.12.10, OpenJDK 64-Bit Server VM, 1.8.0_242\nBranch HEAD\nCompiled by user centos on 2020-02-02T21:10:50Z\nRevision cee4ecbb16917fa85f02c635925e2687400aa56b\nUrl https://gitbox.apache.org/repos/asf/spark.git\nType --help for more information.\n</code></pre> <p>Assert the versions work in <code>spark-shell</code> before proceeding.</p> <pre><code>$ ./bin/spark-shell\nscala&gt; assert(spark.version == \"2.4.5\")\n\nscala&gt; assert(org.apache.hadoop.util.VersionInfo.getVersion == \"2.10.0\")\n\nscala&gt; assert(org.apache.hadoop.hive.shims.ShimLoader.getMajorVersion == \"0.23\")\n</code></pre>"},{"location":"demo/connecting-spark-sql-to-hive-metastore/#set-up-single-node-hadoop-cluster","title":"Set Up Single-Node Hadoop Cluster","text":"<p>Hive uses Hadoop.</p> <p>Download and install Hadoop 2.10.0 (or more recent stable release of Apache Hadoop 2 line if available).</p> <pre><code>export HADOOP_HOME=/Users/jacek/dev/apps/hadoop\n</code></pre> <p>Follow the official documentation in Hadoop: Setting up a Single Node Cluster to set up a single-node Hadoop installation.</p> <pre><code>$ $HADOOP_HOME/bin/hadoop version\nHadoop 2.10.0\nSubversion ssh://git.corp.linkedin.com:29418/hadoop/hadoop.git -r e2f1f118e465e787d8567dfa6e2f3b72a0eb9194\nCompiled by jhung on 2019-10-22T19:10Z\nCompiled with protoc 2.5.0\nFrom source with checksum 7b2d8877c5ce8c9a2cca5c7e81aa4026\nThis command was run using /Users/jacek/dev/apps/hadoop-2.10.0/share/hadoop/common/hadoop-common-2.10.0.jar\n</code></pre> <p>This demo assumes running a single-node in a pseudo-distributed mode where each Hadoop daemon runs in a separate Java process.</p> <p>Tip</p> <p>Use <code>hadoop.tmp.dir</code> configuration property as the base for temporary directories.</p> <pre><code>&lt;property&gt;\n&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\n&lt;value&gt;/tmp/my-hadoop-tmp-dir/hdfs/tmp&lt;/value&gt;\n&lt;description&gt;The base for temporary directories.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre> <p>Use <code>./bin/hdfs getconf -confKey hadoop.tmp.dir</code> to check out the value</p> <pre><code>$ ./bin/hdfs getconf -confKey hadoop.tmp.dir\n/tmp/my-hadoop-tmp-dir/hdfs/tmp\n</code></pre>"},{"location":"demo/connecting-spark-sql-to-hive-metastore/#fsdefaultfs-configuration-property-core-sitexml","title":"fs.defaultFS Configuration Property (core-site.xml)","text":"<p>Edit <code>etc/hadoop/core-site.xml</code> and define <code>fs.defaultFS</code> and <code>hadoop.proxyuser.</code> properties.</p> <pre><code>&lt;configuration&gt;\n&lt;property&gt;\n&lt;name&gt;fs.defaultFS&lt;/name&gt;\n&lt;value&gt;hdfs://localhost:9000&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;name&gt;hadoop.proxyuser.[username].groups&lt;/name&gt;\n&lt;value&gt;*&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;name&gt;hadoop.proxyuser.[username].hosts&lt;/name&gt;\n&lt;value&gt;*&lt;/value&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Note</p> <p>Replace <code>[username]</code> above with the local user (e.g. <code>jacek</code>) that will be used in <code>beeline</code>. Consult this question on StackOverflow.</p>"},{"location":"demo/connecting-spark-sql-to-hive-metastore/#dfsreplication-configuration-property-hdfs-sitexml","title":"dfs.replication Configuration Property (hdfs-site.xml)","text":"<p>Edit <code>etc/hadoop/hdfs-site.xml</code> and define <code>dfs.replication</code> property as follows:</p> <pre><code>&lt;configuration&gt;\n&lt;property&gt;\n&lt;name&gt;dfs.replication&lt;/name&gt;\n&lt;value&gt;1&lt;/value&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"demo/connecting-spark-sql-to-hive-metastore/#passphrase-less-ssh-macos","title":"Passphrase-less SSH (macOS)","text":"<p>Turn Remote Login on in Mac OS X's Sharing preferences that allow remote users to connect to a Mac using the OpenSSH protocols.</p> <pre><code>$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa_hadoop\n$ cat ~/.ssh/id_rsa_hadoop.pub &gt;&gt; ~/.ssh/authorized_keys\n$ chmod 0600 ~/.ssh/authorized_keys\n</code></pre>"},{"location":"demo/connecting-spark-sql-to-hive-metastore/#other-steps","title":"Other Steps","text":"<p>You may want to set up <code>JAVA_HOME</code> in <code>etc/hadoop/hadoop-env.sh</code> as told in the file:</p> <p>Quote</p> <pre><code>$ $HADOOP_HOME/bin/hdfs namenode -format\n...\nINFO common.Storage: Storage directory /tmp/hadoop-jacek/dfs/name has been successfully formatted.\n...\n</code></pre> <p>Note</p> <p>Use <code>./bin/hdfs namenode</code> to start a NameNode that will tell you that the local filesystem is not ready.</p> <pre><code>$ ./bin/hdfs namenode\n18/01/09 15:43:11 INFO namenode.NameNode: STARTUP_MSG:\n/************************************************************\nSTARTUP_MSG: Starting NameNode\nSTARTUP_MSG:   host = japila.local/192.168.1.2\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 2.7.5\n...\n18/01/09 15:43:11 INFO namenode.NameNode: fs.defaultFS is hdfs://localhost:9000\n18/01/09 15:43:11 INFO namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.\n...\n18/01/09 15:43:12 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070\n...\n18/01/09 15:43:13 WARN common.Storage: Storage directory /private/tmp/hadoop-jacek/dfs/name does not exist\n18/01/09 15:43:13 WARN namenode.FSNamesystem: Encountered exception loading fsimage\norg.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /private/tmp/hadoop-jacek/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.\n  at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:382)\n  at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:233)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:984)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:686)\n  at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:586)\n  at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:646)\n  at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:820)\n  at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:804)\n  at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1516)\n  at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1582)\n...\n18/01/09 15:43:13 ERROR namenode.NameNode: Failed to start namenode.\norg.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /private/tmp/hadoop-jacek/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.\n  at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:382)\n  at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:233)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:984)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:686)\n  at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:586)\n  at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:646)\n  at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:820)\n  at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:804)\n  at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1516)\n  at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1582)\n</code></pre> <p>Start Hadoop DFS using <code>start-dfs.sh</code> (and <code>tail -f logs/hadoop-\\*-datanode-*.log</code>)</p> <pre><code>$ $HADOOP_HOME/sbin/start-dfs.sh\nStarting namenodes on [localhost]\nlocalhost: starting namenode, logging to /Users/jacek/dev/apps/hadoop-2.10.0/logs/hadoop-jacek-namenode-japila-new.local.out\nlocalhost: starting datanode, logging to /Users/jacek/dev/apps/hadoop-2.10.0/logs/hadoop-jacek-datanode-japila-new.local.out\nStarting secondary namenodes [0.0.0.0]\n0.0.0.0: starting secondarynamenode, logging to /Users/jacek/dev/apps/hadoop-2.10.0/logs/hadoop-jacek-secondarynamenode-japila-new.local.out\n</code></pre> <p>List Hadoop's JVM processes using <code>jps -lm</code>.</p> <pre><code>$ jps -lm\n50773 org.apache.hadoop.hdfs.server.datanode.DataNode\n50870 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode\n50695 org.apache.hadoop.hdfs.server.namenode.NameNode\n</code></pre> <p>Note</p> <p>FIXME Are the steps in YARN on a Single Node required for Hive?</p>"},{"location":"demo/connecting-spark-sql-to-hive-metastore/#the-only-required-environment-variable-is-java_home-all-others-are","title":"The only required environment variable is JAVA_HOME.  All others are","text":""},{"location":"demo/connecting-spark-sql-to-hive-metastore/#optional-when-running-a-distributed-configuration-it-is-best-to","title":"optional.  When running a distributed configuration it is best to","text":""},{"location":"demo/connecting-spark-sql-to-hive-metastore/#set-java_home-in-this-file-so-that-it-is-correctly-defined-on","title":"set JAVA_HOME in this file, so that it is correctly defined on","text":""},{"location":"demo/connecting-spark-sql-to-hive-metastore/#remote-nodes","title":"remote nodes.","text":""},{"location":"demo/connecting-spark-sql-to-hive-metastore/#running-hive","title":"Running Hive","text":"<p>Note</p> <p>Following the steps in Running Hive.</p> <pre><code>$HADOOP_HOME/bin/hadoop fs -mkdir /tmp\n$HADOOP_HOME/bin/hadoop fs -chmod g+w /tmp\n</code></pre> <pre><code>$HADOOP_HOME/bin/hadoop fs -mkdir -p /user/hive/warehouse\n$HADOOP_HOME/bin/hadoop fs -chmod g+w /user/hive/warehouse\n</code></pre> <p>Download and install Hive 2.3.9 (or more recent stable release of Apache Hive 2 line if available).</p> <pre><code>export HIVE_HOME=/Users/jacek/dev/apps/hive\n</code></pre>"},{"location":"demo/connecting-spark-sql-to-hive-metastore/#install-postgresql","title":"Install PostgreSQL","text":"<p>You'll set up a remote metastore database (as https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration#AdminManualMetastoreAdministration-RemoteMetastoreDatabase[This configuration of metastore database is recommended for any real use.]) and you'll be using https://www.enterprisedb.com/downloads/postgres-postgresql-downloads[PostgreSQL 12.2].</p> <pre><code>$ pg_ctl -D /usr/local/var/postgres start\nserver started\n</code></pre> <p>Download the most current version of PostgreSQL JDBC Driver (e.g. PostgreSQL JDBC 4.2 Driver, 42.2.11). Save the jar file (<code>postgresql-42.2.11.jar</code>) in <code>$HIVE_HOME/lib</code>.</p>"},{"location":"demo/connecting-spark-sql-to-hive-metastore/#setting-up-remote-metastore-database","title":"Setting Up Remote Metastore Database","text":"<p>Create a database and a user in PostgreSQL for Hive.</p> <pre><code>createdb hive_demo\n</code></pre> <pre><code>createuser APP\n</code></pre> <p>Create <code>conf/hive-site.xml</code> (based on <code>conf/hive-default.xml.template</code>) with the following properties:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n&lt;configuration&gt;\n&lt;property&gt;\n&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\n&lt;value&gt;jdbc:postgresql://localhost:5432/hive_demo&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\n&lt;value&gt;org.postgresql.Driver&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n&lt;value&gt;hdfs://localhost:9000/user/hive/warehouse&lt;/value&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Use the Hive Schema Tool to create the metastore tables.</p> <pre><code>$ $HIVE_HOME/bin/schematool -dbType postgres -initSchema\nMetastore connection URL:    jdbc:postgresql://localhost:5432/hive_demo\nMetastore Connection Driver :    org.postgresql.Driver\nMetastore connection User:   APP\nStarting metastore schema initialization to 2.3.0\nInitialization script hive-schema-2.3.0.postgres.sql\nInitialization script completed\nschemaTool completed\n</code></pre> <pre><code>$ $HIVE_HOME/bin/schematool -dbType postgres -info\nMetastore connection URL:    jdbc:postgresql://localhost:5432/hive_demo\nMetastore Connection Driver :    org.postgresql.Driver\nMetastore connection User:   APP\nHive distribution version:   2.3.0\nMetastore schema version:    2.3.0\nschemaTool completed\n</code></pre> <p>As per the official documentation of Hive:</p> <p>HiveCLI is now deprecated in favor of Beeline</p> <p>Run HiveServer2.</p> <pre><code>$HIVE_HOME/bin/hiveserver2\n</code></pre> <p>Run Beeline (the HiveServer2 CLI).</p> <pre><code>$ $HIVE_HOME/bin/beeline -u jdbc:hive2://localhost:10000\n...\nConnecting to jdbc:hive2://localhost:10000\nConnected to: Apache Hive (version 2.3.6)\nDriver: Hive JDBC (version 2.3.6)\nTransaction isolation: TRANSACTION_REPEATABLE_READ\nBeeline version 2.3.6 by Apache Hive\n0: jdbc:hive2://localhost:10000&gt;\n</code></pre>"},{"location":"demo/connecting-spark-sql-to-hive-metastore/#start-hive-metastore-server","title":"Start Hive Metastore Server","text":"<p>Start the Hive Metastore Server (as described in Remote Metastore Server).</p> <pre><code>$HIVE_HOME/bin/hive --service metastore\n...\nStarting Hive Metastore Server\n</code></pre> <p>That is the server Spark SQL applications are going to connect to for metadata of Hive tables.</p>"},{"location":"demo/connecting-spark-sql-to-hive-metastore/#connecting-apache-spark-to-apache-hive","title":"Connecting Apache Spark to Apache Hive","text":"<p>Create <code>$SPARK_HOME/conf/hive-site.xml</code> and define <code>hive.metastore.uris</code> configuration property (that is the thrift URL of the Hive Metastore Server).</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n&lt;configuration&gt;\n&lt;property&gt;\n&lt;name&gt;hive.metastore.uris&lt;/name&gt;\n&lt;value&gt;thrift://localhost:9083&lt;/value&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Optionally, you may want to add the following to <code>conf/log4j2.properties</code> for a more low-level logging:</p> <pre><code>log4j.logger.org.apache.spark.sql.hive.HiveUtils$=ALL\nlog4j.logger.org.apache.spark.sql.internal.SharedState=ALL\nlog4j.logger.org.apache.spark.sql.hive.client.HiveClientImpl=ALL\n</code></pre> <p>Start <code>spark-shell</code>.</p> <pre><code>$SPARK_HOME/bin/spark-shell \\\n  --jars \\\n    $HIVE_HOME/lib/hive-metastore-2.3.6.jar,\\\n    $HIVE_HOME/lib/hive-exec-2.3.6.jar,\\\n    $HIVE_HOME/lib/hive-common-2.3.6.jar,\\\n    $HIVE_HOME/lib/hive-serde-2.3.6.jar,\\\n    $HIVE_HOME/lib/guava-14.0.1.jar \\\n  --conf spark.sql.hive.metastore.version=2.3 \\\n  --conf spark.sql.hive.metastore.jars=$HIVE_HOME\"/lib/*\" \\\n  --conf spark.sql.warehouse.dir=hdfs://localhost:9000/user/hive/warehouse\n</code></pre> <p>You should see the following welcome message:</p> <pre><code>Welcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.5\n      /_/\n\nUsing Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_242)\nType in expressions to have them evaluated.\nType :help for more information.\n</code></pre> <p>With the <code>scala&gt;</code> prompt you made sure that the <code>spark.sql.hive.metastore.version</code> and the JAR files are all correct (as the check happens while the <code>SparkSession</code> is created). Congratulations!</p> <p>You may also want to check out the <code>spark.sql.catalogImplementation</code> internal property that should be <code>hive</code>. With the extra logging turned on, you should also see the configuration file loaded (<code>hive-site.xml</code>) and the warehouse location.</p> <pre><code>scala&gt; spark.conf.get(\"spark.sql.catalogImplementation\")\nINFO SharedState: loading hive config file: file:/Users/jacek/dev/oss/spark/conf/hive-site.xml\nINFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('hdfs://localhost:9000/user/hive/warehouse').\nINFO SharedState: Warehouse path is 'hdfs://localhost:9000/user/hive/warehouse'.\nres0: String = hive\n</code></pre> <p>The most critical step is to check out the remote connection with the Hive Metastore Server (via the thrift protocol). Execute the following command to list all tables known to Spark SQL (incl. Hive tables if there were any, but there are none by default).</p> <pre><code>scala&gt; spark.catalog.listTables.show\n+----+--------+-----------+---------+-----------+\n|name|database|description|tableType|isTemporary|\n+----+--------+-----------+---------+-----------+\n+----+--------+-----------+---------+-----------+\n</code></pre> <p>There is one database in Hive by default.</p> <pre><code>0: jdbc:hive2://localhost:10000&gt; show databases;\n+----------------+\n| database_name  |\n+----------------+\n| default        |\n+----------------+\n1 row selected (0.067 seconds)\n</code></pre> <p>List the tables in the <code>default</code> database. There should be some Hive tables listed.</p> <pre><code>scala&gt; spark.sharedState.externalCatalog.listTables(\"default\")\n</code></pre> <p>Create a partitioned table in Hive (based on the official documentation of Hive).</p> <p>Execute the following DDL in beeline.</p> <pre><code>CREATE TABLE demo_sales\n(id BIGINT, qty BIGINT, name STRING)\nCOMMENT 'Demo: Connecting Spark SQL to Hive Metastore'\nPARTITIONED BY (rx_mth_cd STRING COMMENT 'Prescription Date YYYYMM aggregated')\nSTORED AS PARQUET;\n</code></pre> <p>CREATE TABLE ... USING hive</p> <p>You can also create a Hive table from <code>spark-shell</code> using <code>CREATE TABLE ... USING hive</code> queries.</p> <pre><code>CREATE TABLE hive_table_name (id LONG) USING hive\n</code></pre> <p>In case of permission denied errors as the one below:</p> <pre><code>MetaException(message:Got exception: org.apache.hadoop.security.AccessControlException Permission denied: user=anonymous, access=WRITE, inode=\"/user/hive/warehouse\":jacek:supergroup:drwxrwxr-x\n</code></pre> <p>you may want to simply change the permissions of the warehouse directory to allow anybody to write:</p> <pre><code>$HADOOP_HOME/bin/hadoop fs -chmod 777 /user/hive/warehouse\n</code></pre> <pre><code>$HADOOP_HOME/bin/hadoop fs -ls /user/hive\n</code></pre> <pre><code>Found 1 items\ndrwxrwxrwx   - jacek supergroup          0 2020-03-21 11:15 /user/hive/warehouse\n</code></pre> <p>Check out the table directory on HDFS.</p> <pre><code>$HADOOP_HOME/bin/hadoop fs -ls /user/hive/warehouse\n</code></pre> <pre><code>Found 1 items\ndrwxrwxrwx   - anonymous supergroup          0 2020-03-22 16:07 /user/hive/warehouse/demo_sales\n</code></pre> <p>Insert some data.</p> <pre><code># (id BIGINT, qty BIGINT, name STRING)\n# PARTITIONED BY (rx_mth_cd STRING COMMENT 'Prescription Date YYYYMM aggregated')\nINSERT INTO demo_sales PARTITION (rx_mth_cd=\"202002\") VALUES (2, 2000, 'two');\n</code></pre> <p>Query the records in the table.</p> <pre><code>0: jdbc:hive2://localhost:10000&gt; SELECT * FROM demo_sales;\n+----------------+-----------------+------------------+-----------------------+\n| demo_sales.id  | demo_sales.qty  | demo_sales.name  | demo_sales.rx_mth_cd  |\n+----------------+-----------------+------------------+-----------------------+\n| 2              | 2000            | two              | 202002                |\n+----------------+-----------------+------------------+-----------------------+\n1 row selected (0.112 seconds)\n</code></pre> <p>Display the partitions (there should really be one).</p> <pre><code>0: jdbc:hive2://localhost:10000&gt; SHOW PARTITIONS demo_sales;\n+-------------------+\n|     partition     |\n+-------------------+\n| rx_mth_cd=202002  |\n+-------------------+\n1 row selected (0.084 seconds)\n</code></pre> <p>Check out the table directory on HDFS.</p> <pre><code>$ $HADOOP_HOME/bin/hadoop fs -ls -R /user/hive/warehouse/demo_sales\ndrwxrwxrwx   - anonymous supergroup          0 2020-03-22 16:10 /user/hive/warehouse/demo_sales/rx_mth_cd=202002\n-rwxrwxrwx   1 anonymous supergroup        454 2020-03-22 16:10 /user/hive/warehouse/demo_sales/rx_mth_cd=202002/000000_0\n</code></pre> <p>Time for some Spark.</p> <p>Query the tables in the <code>default</code> database. There should be at least the one you've just created.</p> <pre><code>scala&gt; spark.sharedState.externalCatalog.listTables(\"default\")\nres6: Seq[String] = Buffer(demo_sales)\n</code></pre> <p>Query the rows in the table.</p> <pre><code>scala&gt; spark.table(\"demo_sales\").show\n+---+----+----+---------+\n| id| qty|name|rx_mth_cd|\n+---+----+----+---------+\n|  2|2000| two|   202002|\n+---+----+----+---------+\n</code></pre> <p>Display the metadata of the table from the Spark catalog (<code>DESCRIBE EXTENDED</code> SQL command).</p> <pre><code>scala&gt; sql(\"DESCRIBE EXTENDED demo_sales\").show(Integer.MAX_VALUE, truncate = false)\n+----------------------------+--------------------------------------------------------------+-----------------------------------+\n|col_name                    |data_type                                                     |comment                            |\n+----------------------------+--------------------------------------------------------------+-----------------------------------+\n|id                          |bigint                                                        |null                               |\n|qty                         |bigint                                                        |null                               |\n|name                        |string                                                        |null                               |\n|rx_mth_cd                   |string                                                        |Prescription Date YYYYMM aggregated|\n|# Partition Information     |                                                              |                                   |\n|# col_name                  |data_type                                                     |comment                            |\n|rx_mth_cd                   |string                                                        |Prescription Date YYYYMM aggregated|\n|                            |                                                              |                                   |\n|# Detailed Table Information|                                                              |                                   |\n|Database                    |default                                                       |                                   |\n|Table                       |demo_sales                                                    |                                   |\n|Owner                       |anonymous                                                     |                                   |\n|Created Time                |Sun Mar 22 16:09:18 CET 2020                                  |                                   |\n|Last Access                 |Thu Jan 01 01:00:00 CET 1970                                  |                                   |\n|Created By                  |Spark 2.2 or prior                                            |                                   |\n|Type                        |MANAGED                                                       |                                   |\n|Provider                    |hive                                                          |                                   |\n|Comment                     |Demo: Connecting Spark SQL to Hive Metastore                  |                                   |\n|Table Properties            |[transient_lastDdlTime=1584889905]                            |                                   |\n|Location                    |hdfs://localhost:9000/user/hive/warehouse/demo_sales          |                                   |\n|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |                                   |\n|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |                                   |\n|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|                                   |\n|Storage Properties          |[serialization.format=1]                                      |                                   |\n|Partition Provider          |Catalog                                                       |                                   |\n+----------------------------+--------------------------------------------------------------+-----------------------------------+\n</code></pre> <p>It all worked fine. Congratulations!</p>"},{"location":"demo/developing-catalogplugin/","title":"Demo: Developing CatalogPlugin","text":"<p>The demo shows the internals of CatalogPlugin with support for TableCatalog and SupportsNamespaces.</p>"},{"location":"demo/developing-catalogplugin/#demo-catalogplugin","title":"Demo CatalogPlugin","text":"<p>Find the sources of a demo <code>CatalogPlugin</code> in the GitHub repo.</p>"},{"location":"demo/developing-catalogplugin/#install-demo-catalogplugin-spark-shell","title":"Install Demo CatalogPlugin (Spark Shell)","text":"<pre><code>./bin/spark-shell \\\n  --packages pl.japila.spark:spark-examples_2.13:1.0.0-SNAPSHOT \\\n  --conf spark.sql.catalog.demo=pl.japila.spark.sql.DemoCatalog \\\n  --conf spark.sql.catalog.demo.use-thing=true \\\n  --conf spark.sql.catalog.demo.delete-supported=false\n</code></pre> <p>SET</p> <p>You could instead use the following at runtime:</p> <pre><code>sql(\"SET spark.sql.catalog.demo=pl.japila.spark.sql.DemoCatalog\")\n</code></pre>"},{"location":"demo/developing-catalogplugin/#show-time","title":"Show Time","text":"<pre><code>sql(\"SHOW CATALOGS\").show(truncate = false)\n</code></pre> <pre><code>scala&gt; sql(\"SET CATALOG demo\")\n&gt;&gt;&gt; initialize(demo, Map(use-thing -&gt; true, delete-supported -&gt; false))\n</code></pre> <pre><code>scala&gt; sql(\"SHOW NAMESPACES IN demo_db\").show(false)\ndefaultNamespace=&lt;EMPTY&gt;\n&gt;&gt;&gt; listNamespaces(namespace=ArraySeq(demo_db))\ndefaultNamespace=&lt;EMPTY&gt;\n+---------+\n|namespace|\n+---------+\n+---------+\n</code></pre> <pre><code>-- FIXME Make it work\nSELECT * FROM demo_db.demo_schema.demo_table LIMIT 10\n</code></pre>"},{"location":"demo/developing-catalogplugin/#access-demo-catalog-using-catalogmanager","title":"Access Demo Catalog using CatalogManager","text":"<p>Let's use the CatalogManager to access the demo catalog.</p> <pre><code>val demo = spark.sessionState.catalogManager.catalog(\"demo\")\n</code></pre> <pre><code>scala&gt; val demo = spark.sessionState.catalogManager.catalog(\"demo\")\n&gt;&gt;&gt; initialize(demo, Map())\n</code></pre> <pre><code>demo.defaultNamespace\n</code></pre>"},{"location":"demo/developing-catalogplugin/#show-tables","title":"Show Tables","text":"<p>Let's use <code>SHOW TABLES</code> SQL command to show the tables in the demo catalog.</p> <pre><code>scala&gt; sql(\"SHOW TABLES IN demo\").show(truncate = false)\ndefaultNamespace=&lt;EMPTY&gt;\n&gt;&gt;&gt; listTables(ArraySeq())\ndefaultNamespace=&lt;EMPTY&gt;\n+---------+---------+-----------+\n|namespace|tableName|isTemporary|\n+---------+---------+-----------+\n+---------+---------+-----------+\n</code></pre>"},{"location":"demo/developing-catalogplugin/#create-namespace","title":"Create Namespace","text":"<pre><code>sql(\"CREATE NAMESPACE IF NOT EXISTS demo.hello\").show(truncate = false)\n</code></pre> <pre><code>scala&gt; sql(\"CREATE NAMESPACE IF NOT EXISTS demo.hello\").show(truncate = false)\n&gt;&gt;&gt; loadNamespaceMetadata(WrappedArray(hello))\n++\n||\n++\n++\n</code></pre>"},{"location":"demo/developing-catalogplugin/#show-namespaces","title":"Show Namespaces","text":"<p>Let's use <code>SHOW NAMESPACES</code> SQL command to show the catalogs (incl. ours).</p> <pre><code>sql(\"SHOW NAMESPACES IN demo\").show(truncate = false)\n</code></pre> <pre><code>scala&gt; sql(\"SHOW NAMESPACES IN demo\").show(truncate = false)\n&gt;&gt;&gt; listNamespaces()\n+---------+\n|namespace|\n+---------+\n+---------+\n</code></pre>"},{"location":"demo/developing-catalogplugin/#append-data-to-table","title":"Append Data to Table","text":"<pre><code>spark.range(5).writeTo(\"demo.t1\").append\n</code></pre> <pre><code>scala&gt; spark.range(5).writeTo(\"demo.t1\").append\n&gt;&gt;&gt; loadTable(t1)\nscala.NotImplementedError: an implementation is missing\n  at scala.Predef$.$qmark$qmark$qmark(Predef.scala:288)\n  at pl.japila.spark.sql.DemoCatalog.loadTable(&lt;pastie&gt;:67)\n  at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:283)\n  at org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:156)\n  ... 47 elided\n</code></pre>"},{"location":"demo/developing-catalogplugin/#possible-exceptions","title":"Possible Exceptions","text":""},{"location":"demo/developing-catalogplugin/#failed-to-get-database","title":"Failed to get database","text":"<pre><code>scala&gt; spark.range(5).writeTo(\"demo.t1\").append\n20/12/28 20:01:30 WARN ObjectStore: Failed to get database demo, returning NoSuchObjectException\norg.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table demo.t1 not found;\n  at org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:162)\n  ... 47 elided\n</code></pre>"},{"location":"demo/developing-catalogplugin/#cannot-find-catalog-plugin-class","title":"Cannot find catalog plugin class","text":"<pre><code>scala&gt; spark.range(5).writeTo(\"demo.t1\").append\norg.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'demo': pl.japila.spark.sql.DemoCatalog\n  at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:66)\n  at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:52)\n  at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n  at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)\n  at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:128)\n  at org.apache.spark.sql.DataFrameWriterV2.&lt;init&gt;(DataFrameWriterV2.scala:52)\n  at org.apache.spark.sql.Dataset.writeTo(Dataset.scala:3359)\n  ... 47 elided\n</code></pre>"},{"location":"demo/developing-catalogplugin/#cannot-use-catalog-demo-not-a-tablecatalog","title":"Cannot use catalog demo: not a TableCatalog","text":"<pre><code>scala&gt; spark.range(5).writeTo(\"demo.t1\").append\n&gt;&gt;&gt; initialize(demo, Map())\norg.apache.spark.sql.AnalysisException: Cannot use catalog demo: not a TableCatalog;\n  at org.apache.spark.sql.connector.catalog.CatalogV2Implicits$CatalogHelper.asTableCatalog(CatalogV2Implicits.scala:76)\n  at org.apache.spark.sql.DataFrameWriterV2.&lt;init&gt;(DataFrameWriterV2.scala:53)\n  at org.apache.spark.sql.Dataset.writeTo(Dataset.scala:3359)\n  ... 47 elided\n</code></pre>"},{"location":"demo/developing-catalogplugin/#cannot-use-catalog-demo-does-not-support-namespaces","title":"Cannot use catalog demo: does not support namespaces","text":"<pre><code>scala&gt; sql(\"SHOW NAMESPACES IN demo\").show(false)\norg.apache.spark.sql.AnalysisException: Cannot use catalog demo: does not support namespaces;\n  at org.apache.spark.sql.connector.catalog.CatalogV2Implicits$CatalogHelper.asNamespaceCatalog(CatalogV2Implicits.scala:83)\n  at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:277)\n</code></pre>"},{"location":"demo/dynamic-partition-pruning/","title":"Demo: Dynamic Partition Pruning","text":"<p>This demo shows Dynamic Partition Pruning in action.</p> <pre><code>Welcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.4.0\n      /_/\n\nUsing Scala version 2.13.8 (OpenJDK 64-Bit Server VM, Java 17.0.6)\n</code></pre>"},{"location":"demo/dynamic-partition-pruning/#before-you-begin","title":"Before you begin","text":"<p>Enable the following loggers:</p> <ul> <li>DataSourceStrategy</li> </ul>"},{"location":"demo/dynamic-partition-pruning/#create-partitioned-tables","title":"Create Partitioned Tables","text":"<pre><code>import org.apache.spark.sql.functions._\nspark.range(4000)\n.withColumn(\"part_id\", 'id % 4)\n.withColumn(\"value\", rand() * 100)\n.write\n.partitionBy(\"part_id\")\n.saveAsTable(\"dpp_facts_large\")\n</code></pre> <pre><code>import org.apache.spark.sql.functions._\nspark.range(4)\n.withColumn(\"name\", concat(lit(\"name_\"), 'id))\n.write\n.saveAsTable(\"dpp_dims_small\")\n</code></pre> <pre><code>val facts = spark.table(\"dpp_facts_large\")\nval dims = spark.table(\"dpp_dims_small\")\n</code></pre> ScalaSQL (FIXME) <pre><code>facts.printSchema()\n</code></pre> <p>FIXME</p> <pre><code>root\n |-- id: long (nullable = true)\n |-- value: double (nullable = true)\n |-- part_id: long (nullable = true)\n</code></pre> ScalaSQL (FIXME) <pre><code>dims.printSchema()\n</code></pre> <p>FIXME</p> <pre><code>root\n|-- id: long (nullable = true)\n|-- name: string (nullable = true)\n</code></pre>"},{"location":"demo/dynamic-partition-pruning/#selective-join-query","title":"Selective Join Query","text":"<pre><code>val q = facts.join(dims)\n.where(facts(\"part_id\") === dims(\"id\"))\n.where(dims(\"id\") isin (0, 1))\n</code></pre> <p>Execute the query (using Noop Data Source).</p> <pre><code>q.write.format(\"noop\").mode(\"overwrite\").save\n</code></pre> <p>DataSourceStrategy should print out the following INFO messages to the logs:</p> <pre><code>Pruning directories with: part_id#2L IN (0,1),isnotnull(part_id#2L),dynamicpruning#22 [part_id#2L]\nPruning directories with: part_id#2L IN (0,1),isnotnull(part_id#2L),dynamicpruning#22 [part_id#2L]\n</code></pre> Scala <pre><code>q.explain()\n</code></pre> <pre><code>== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- BroadcastHashJoin [part_id#27L], [id#31L], Inner, BuildRight, false\n   :- FileScan parquet spark_catalog.default.dpp_facts_large[id#25L,value#26,part_id#27L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(2 paths)[file:/Users/jacek/dev/oss/spark/spark-warehouse/dpp_facts_large/part_i..., PartitionFilters: [part_id#27L IN (0,1), isnotnull(part_id#27L), dynamicpruningexpression(part_id#27L IN dynamicpru..., PushedFilters: [], ReadSchema: struct&lt;id:bigint,value:double&gt;\n   :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, true, Filter (id#31L IN (0,1) AND isnotnull(id#31L)), [id#31L]\n   :        +- AdaptiveSparkPlan isFinalPlan=false\n   :           +- Filter (id#31L IN (0,1) AND isnotnull(id#31L))\n   :              +- FileScan parquet spark_catalog.default.dpp_dims_small[id#31L,name#32] Batched: true, DataFilters: [id#31L IN (0,1), isnotnull(id#31L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/jacek/dev/oss/spark/spark-warehouse/dpp_dims_small], PartitionFilters: [], PushedFilters: [In(id, [0,1]), IsNotNull(id)], ReadSchema: struct&lt;id:bigint,name:string&gt;\n   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=175]\n      +- Filter (id#31L IN (0,1) AND isnotnull(id#31L))\n         +- FileScan parquet spark_catalog.default.dpp_dims_small[id#31L,name#32] Batched: true, DataFilters: [id#31L IN (0,1), isnotnull(id#31L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/jacek/dev/oss/spark/spark-warehouse/dpp_dims_small], PartitionFilters: [], PushedFilters: [In(id, [0,1]), IsNotNull(id)], ReadSchema: struct&lt;id:bigint,name:string&gt;\n</code></pre> <p>Note the value of the isFinalPlan flag being <code>false</code>.</p>"},{"location":"demo/dynamic-partition-pruning/#filters-in-query-plan","title":"Filters in Query Plan","text":"<p><code>PartitionFilters</code> in the <code>Scan</code> operator over <code>dpp_facts_large</code> table should include <code>dims(\"id\") isin (0, 1)</code> predicate.</p> <pre><code>(1) Scan parquet spark_catalog.default.dpp_facts_large\nOutput [3]: [id#25L, value#26, part_id#27L]\nBatched: true\nLocation: InMemoryFileIndex [file:/Users/jacek/dev/oss/spark/spark-warehouse/dpp_facts_large/part_id=0, ... 1 entries]\nPartitionFilters: [part_id#27L IN (0,1), isnotnull(part_id#27L), dynamicpruningexpression(part_id#27L IN dynamicpruning#47)]\nReadSchema: struct&lt;id:bigint,value:double&gt;\n</code></pre> <p><code>PushedFilters</code> in the <code>Scan</code> operator over <code>dpp_dims_small</code> table should include <code>In(id, [0,1])</code> predicate.</p> <pre><code>(3) Scan parquet spark_catalog.default.dpp_dims_small\nOutput [2]: [id#31L, name#32]\nBatched: true\nLocation: InMemoryFileIndex [file:/Users/jacek/dev/oss/spark/spark-warehouse/dpp_dims_small]\nPushedFilters: [In(id, [0,1]), IsNotNull(id)]\nReadSchema: struct&lt;id:bigint,name:string&gt;\n</code></pre>"},{"location":"demo/hive-partitioned-parquet-table-partition-pruning/","title":"Demo: Hive Partitioned Parquet Table and Partition Pruning","text":"<p>The demo shows partition pruning optimization in Spark SQL for Hive partitioned tables in parquet format.</p> <p>NOTE: The demo is a follow-up to Demo: Connecting Spark SQL to Hive Metastore (with Remote Metastore Server). Please finish it first before this demo.</p> <p>The demo features the following:</p> <ul> <li> <p>spark.sql.hive.convertMetastoreParquet configuration property is enabled (which is the default)</p> </li> <li> <p>Hadoop DFS for spark.sql.warehouse.dir</p> </li> </ul>"},{"location":"demo/hive-partitioned-parquet-table-partition-pruning/#create-hive-partitioned-table-in-parquet-format","title":"Create Hive Partitioned Table in Parquet Format","text":"<p>Create a Hive partitioned table in parquet format with some data.</p> <pre><code>CREATE TABLE hive_partitioned_table\n(id BIGINT, name STRING)\nCOMMENT 'Demo: Hive Partitioned Parquet Table and Partition Pruning'\nPARTITIONED BY (city STRING COMMENT 'City')\nSTORED AS PARQUET;\n\nINSERT INTO hive_partitioned_table\nPARTITION (city=\"Warsaw\")\nVALUES (0, 'Jacek');\n\nINSERT INTO hive_partitioned_table\nPARTITION (city=\"Paris\")\nVALUES (1, 'Agata');\n</code></pre>"},{"location":"demo/hive-partitioned-parquet-table-partition-pruning/#accessing-hive-table-in-spark-shell","title":"Accessing Hive Table in Spark Shell","text":"<p>Make sure that the table is accessible in Spark SQL.</p> <pre><code>assert(spark.conf.get(\"spark.sql.warehouse.dir\").startsWith(\"hdfs\"))\n\nval tableName = \"hive_partitioned_table\"\nassert(spark.table(tableName).collect.length == 2 /* rows */)\n\n// Use the default value of spark.sql.hive.convertMetastoreParquet\nassert(spark.conf.get(\"spark.sql.hive.convertMetastoreParquet\").toBoolean)\n</code></pre> <p>Query the available partitions.</p> <pre><code>val parts = spark\n  .sharedState\n  .externalCatalog\n  .listPartitions(\"default\", tableName)\nscala&gt; parts.foreach(println)\nCatalogPartition(\n    Partition Values: [city=Warsaw]\n    Location: hdfs://localhost:9000/user/hive/warehouse/hive_partitioned_table/city=Warsaw\n    Serde Library: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n    InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\n    OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\n    Storage Properties: [serialization.format=1]\n    Partition Parameters: {rawDataSize=2, numFiles=1, transient_lastDdlTime=1585478385, totalSize=345, COLUMN_STATS_ACCURATE={\"BASIC_STATS\":\"true\"}, numRows=1}\n    Created Time: Sun Mar 29 12:39:45 CEST 2020\n    Last Access: Thu Jan 01 01:00:00 CET 1970\n    Partition Statistics: 345 bytes, 1 rows)\nCatalogPartition(\n    Partition Values: [city=Paris]\n    Location: hdfs://localhost:9000/user/hive/warehouse/hive_partitioned_table/city=Paris\n    Serde Library: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n    InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\n    OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\n    Storage Properties: [serialization.format=1]\n    Partition Parameters: {rawDataSize=2, numFiles=1, transient_lastDdlTime=1585478387, totalSize=345, COLUMN_STATS_ACCURATE={\"BASIC_STATS\":\"true\"}, numRows=1}\n    Created Time: Sun Mar 29 12:39:47 CEST 2020\n    Last Access: Thu Jan 01 01:00:00 CET 1970\n    Partition Statistics: 345 bytes, 1 rows)\n</code></pre> <p>Create another Hive table using Spark.</p> <pre><code>Seq(\"Warsaw\").toDF(\"name\").write.saveAsTable(\"cities\")\n</code></pre> <p>Check out the table in Hive using <code>beeline</code>.</p> <pre><code>0: jdbc:hive2://localhost:10000&gt; desc formatted cities;\n+-------------------------------+----------------------------------------------------+----------------------------------------------------+\n|           col_name            |                     data_type                      |                      comment                       |\n+-------------------------------+----------------------------------------------------+----------------------------------------------------+\n| # col_name                    | data_type                                          | comment                                            |\n|                               | NULL                                               | NULL                                               |\n| name                          | string                                             |                                                    |\n|                               | NULL                                               | NULL                                               |\n| # Detailed Table Information  | NULL                                               | NULL                                               |\n| Database:                     | default                                            | NULL                                               |\n| Owner:                        | jacek                                              | NULL                                               |\n| CreateTime:                   | Sun Mar 29 14:39:40 CEST 2020                      | NULL                                               |\n| LastAccessTime:               | UNKNOWN                                            | NULL                                               |\n| Retention:                    | 0                                                  | NULL                                               |\n| Location:                     | hdfs://localhost:9000/user/hive/warehouse/cities   | NULL                                               |\n| Table Type:                   | MANAGED_TABLE                                      | NULL                                               |\n| Table Parameters:             | NULL                                               | NULL                                               |\n|                               | numFiles                                           | 1                                                  |\n|                               | spark.sql.create.version                           | 2.4.5                                              |\n|                               | spark.sql.sources.provider                         | parquet                                            |\n|                               | spark.sql.sources.schema.numParts                  | 1                                                  |\n|                               | spark.sql.sources.schema.part.0                    | {\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]} |\n|                               | totalSize                                          | 425                                                |\n|                               | transient_lastDdlTime                              | 1585485580                                         |\n|                               | NULL                                               | NULL                                               |\n| # Storage Information         | NULL                                               | NULL                                               |\n| SerDe Library:                | org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe | NULL                                               |\n| InputFormat:                  | org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat | NULL                                               |\n| OutputFormat:                 | org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat | NULL                                               |\n| Compressed:                   | No                                                 | NULL                                               |\n| Num Buckets:                  | -1                                                 | NULL                                               |\n| Bucket Columns:               | []                                                 | NULL                                               |\n| Sort Columns:                 | []                                                 | NULL                                               |\n| Storage Desc Params:          | NULL                                               | NULL                                               |\n|                               | path                                               | hdfs://localhost:9000/user/hive/warehouse/cities   |\n|                               | serialization.format                               | 1                                                  |\n+-------------------------------+----------------------------------------------------+----------------------------------------------------+\n32 rows selected (0.075 seconds)\n</code></pre>"},{"location":"demo/hive-partitioned-parquet-table-partition-pruning/#explore-partition-pruning","title":"Explore Partition Pruning","text":"<p>You'll be using ../spark-sql-Expression-In.md[In] expression in structured queries to learn more on partition pruning.</p> <p>TIP: Enable <code>INFO</code> logging level for ../PrunedInMemoryFileIndex.md#logging[org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex] logger.</p> <p>Use a fixed list of cities to filter by (which should trigger partition pruning).</p> <pre><code>val q = sql(s\"\"\"SELECT * FROM $tableName WHERE city IN ('Warsaw')\"\"\")\nscala&gt; q.explain(extended = true)\n== Parsed Logical Plan ==\n'Project [*]\n+- 'Filter 'city IN (Warsaw)\n   +- 'UnresolvedRelation `hive_partitioned_table`\n\n== Analyzed Logical Plan ==\nid: bigint, name: string, city: string\nProject [id#101L, name#102, city#103]\n+- Filter city#103 IN (Warsaw)\n   +- SubqueryAlias `default`.`hive_partitioned_table`\n      +- Relation[id#101L,name#102,city#103] parquet\n\n== Optimized Logical Plan ==\nProject [id#101L, name#102, city#103]\n+- Filter (isnotnull(city#103) &amp;&amp; (city#103 = Warsaw))\n   +- Relation[id#101L,name#102,city#103] parquet\n\n== Physical Plan ==\n*(1) FileScan parquet default.hive_partitioned_table[id#101L,name#102,city#103] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[hdfs://localhost:9000/user/hive/warehouse/hive_partitioned_table/city=War..., PartitionCount: 1, PartitionFilters: [isnotnull(city#103), (city#103 = Warsaw)], PushedFilters: [], ReadSchema: struct&lt;id:bigint,name:string&gt;\n</code></pre> <p>Note the <code>PartitionFilters</code> field of the leaf <code>FileScan</code> node in the physical plan. It uses an ../PrunedInMemoryFileIndex.md[PrunedInMemoryFileIndex] (for the partition index). Let's explore it.</p> <pre><code>import org.apache.spark.sql.execution.FileSourceScanExec\nval scan = q.queryExecution.executedPlan.collect { case op: FileSourceScanExec =&gt; op }.head\n\nval index = scan.relation.location\nscala&gt; println(s\"Time of partition metadata listing: ${index.metadataOpsTimeNs.get}ns\")\nTime of partition metadata listing: 41703540ns\n\n// You may also want to review metadataTime metric in web UI\n// Includes the above time and the time to list files\n\n// You should see the following value (YMMV)\nscan.execute.collect\nscala&gt; println(scan.metrics(\"metadataTime\").value)\n41\n</code></pre> <p>Use a subquery to filter by and note the <code>PartitionFilters</code> field of <code>FileScan</code> operator (which is not supported for partition pruning since the values to filter partitions by are not known until the execution time).</p> <pre><code>val q = sql(s\"\"\"SELECT * FROM $tableName WHERE city IN (SELECT * FROM cities)\"\"\")\nscala&gt; q.explain(extended = true)\n== Parsed Logical Plan ==\n'Project [*]\n+- 'Filter 'city IN (list#104 [])\n   :  +- 'Project [*]\n   :     +- 'UnresolvedRelation `cities`\n   +- 'UnresolvedRelation `hive_partitioned_table`\n\n== Analyzed Logical Plan ==\nid: bigint, name: string, city: string\nProject [id#113L, name#114, city#115]\n+- Filter city#115 IN (list#104 [])\n   :  +- Project [name#108]\n   :     +- SubqueryAlias `default`.`cities`\n   :        +- Relation[name#108] parquet\n   +- SubqueryAlias `default`.`hive_partitioned_table`\n      +- Relation[id#113L,name#114,city#115] parquet\n\n== Optimized Logical Plan ==\nJoin LeftSemi, (city#115 = name#108)\n:- Relation[id#113L,name#114,city#115] parquet\n+- Relation[name#108] parquet\n\n== Physical Plan ==\n*(2) BroadcastHashJoin [city#115], [name#108], LeftSemi, BuildRight\n:- *(2) FileScan parquet default.hive_partitioned_table[id#113L,name#114,city#115] Batched: true, Format: Parquet, Location: CatalogFileIndex[hdfs://localhost:9000/user/hive/warehouse/hive_partitioned_table], PartitionCount: 2, PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint,name:string&gt;\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n   +- *(1) FileScan parquet default.cities[name#108] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://localhost:9000/user/hive/warehouse/cities], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;name:string&gt;\n</code></pre>"},{"location":"demo/objecthashaggregateexec-sort-based-fallback-tasks/","title":"Demo: ObjectHashAggregateExec and Sort-Based Fallback Tasks","text":"<p>This demo shows when ObjectHashAggregateExec physical operator falls back to sort-based aggregation (that Spark SQL hoped to avoid while planning an aggregation).</p> <p><code>ObjectHashAggregateExec</code> physical operator uses spark.sql.objectHashAggregate.sortBased.fallbackThreshold configuration property to control when to switch to sort-based aggregation.</p>"},{"location":"demo/objecthashaggregateexec-sort-based-fallback-tasks/#configure-sparksession","title":"Configure SparkSession","text":"<pre><code>./bin/spark-shell\n</code></pre> <pre><code>import org.apache.spark.sql.internal.SQLConf\nassert(SQLConf.get.objectAggSortBasedFallbackThreshold == 128)\n</code></pre> <pre><code>spark.sessionState.conf.setConf(SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD, 1)\nassert(SQLConf.get.objectAggSortBasedFallbackThreshold == 1)\n</code></pre>"},{"location":"demo/objecthashaggregateexec-sort-based-fallback-tasks/#no-sort-fallback-tasks","title":"No Sort Fallback Tasks","text":"<p>collect_set standard function (a TypedImperativeAggregate expression) is one of the built-in standard functions that are planned for execution using ObjectHashAggregateExec physical operator.</p> <p>The following query over a single-row dataset produces one group (so it is under <code>spark.sql.objectHashAggregate.sortBased.fallbackThreshold</code> of <code>1</code>) and hence there will be no sort fallback tasks.</p> <pre><code>val oneRowDataset = Seq(\n(0, 0)\n).toDF(\"id\", \"gid\")\nval q = oneRowDataset\n.groupBy(\"gid\")\n.agg(collect_set(\"id\") as \"ids\")\nq.write.format(\"noop\").mode(\"overwrite\").save\n</code></pre> <p></p>"},{"location":"demo/objecthashaggregateexec-sort-based-fallback-tasks/#sort-fallback-tasks","title":"Sort Fallback Tasks","text":"<p>When an aggregation happens to use more than one group (and crosses <code>spark.sql.objectHashAggregate.sortBased.fallbackThreshold</code>), there will be as many sort fallback tasks as there were partitions with enough groups above the threshold.</p> <pre><code>val threeRowDataset = Seq(\n(0, 0),\n(1, 1),\n(2, 1),\n).toDF(\"id\", \"gid\")\nval q = threeRowDataset\n.groupBy(\"gid\")\n.agg(collect_set(\"id\") as \"ids\")\nq.write.format(\"noop\").mode(\"overwrite\").save\n</code></pre> <p></p>"},{"location":"demo/objecthashaggregateexec-sort-based-fallback-tasks/#sort-fallback-tasks-after-repartition","title":"Sort Fallback Tasks After Repartition","text":"<pre><code>val q = threeRowDataset\n.coalesce(1) // all rows in one partition\n.groupBy(\"gid\")\n.agg(collect_set(\"id\") as \"ids\")\nq.write.format(\"noop\").mode(\"overwrite\").save\n</code></pre>"},{"location":"demo/spilling/","title":"Demo: Spilling","text":"<p>This demo shows in-memory data spilling while sorting (using SortExec physical operator).</p> <p></p>"},{"location":"demo/spilling/#configuration","title":"Configuration","text":"<p>Disable Adaptive Query Execution and force spilling at a very low threshold using <code>spark.shuffle.spill.numElementsForceSpillThreshold</code> (Spark Core).</p> <pre><code>./bin/spark-shell \\\n--conf spark.shuffle.spill.numElementsForceSpillThreshold=1 \\\n--conf spark.sql.adaptive.enabled=false \\\n--conf spark.sql.shuffle.partitions=1\n</code></pre>"},{"location":"demo/spilling/#create-table","title":"Create Table","text":"<pre><code>spark.range(2)\n.writeTo(\"tiny\")\n.using(\"parquet\")\n.create\n</code></pre>"},{"location":"demo/spilling/#spilling","title":"Spilling","text":"<p>One of the physical operators that are susceptible to spilling is SortExec.</p> <pre><code>spark.table(\"tiny\")\n.orderBy(\"id\")\n.write\n.format(\"noop\")\n.mode(\"overwrite\")\n.save\n</code></pre> FIXME <p>Why does <code>show</code> not work (as <code>format(\"noop\")</code> does)?</p>"},{"location":"demo/spilling/#web-ui","title":"web UI","text":""},{"location":"demo/spilling/#details-for-stage","title":"Details for Stage","text":""},{"location":"demo/spilling/#tasks","title":"Tasks","text":""},{"location":"demo/using-jdbc-data-source-to-access-postgresql/","title":"Demo: Using JDBC Data Source to Access PostgreSQL","text":"<p>This demo shows how to use JDBC Data Source to load data from PostgreSQL. These steps should be equally applicable to any relational database that allows access using a JDBC driver.</p>"},{"location":"demo/using-jdbc-data-source-to-access-postgresql/#start-postgres-instance","title":"Start Postgres Instance","text":"<pre><code>docker run --name postgres-demo -e POSTGRES_PASSWORD=mysecretpassword -p 5432:5432 -d postgres\n</code></pre> <pre><code>docker ps\n</code></pre>"},{"location":"demo/using-jdbc-data-source-to-access-postgresql/#download-jdbc-driver","title":"Download JDBC Driver","text":"<p>Download the most current version of PostgreSQL JDBC Driver (e.g. PostgreSQL JDBC 4.2 Driver, 42.2.19).</p> <p>Use an environment variable for the path of the jar file.</p> <pre><code>JDBC_DRIVER=/my/path/postgresql-42.2.11.jar\n</code></pre>"},{"location":"demo/using-jdbc-data-source-to-access-postgresql/#spark-shell","title":"spark-shell","text":"<pre><code>$SPARK_HOME/bin/spark-shell --version\n</code></pre> <pre><code>Welcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.1.1\n      /_/\n\nUsing Scala version 2.12.10, OpenJDK 64-Bit Server VM, 11.0.10\nBranch HEAD\nCompiled by user ubuntu on 2021-02-22T01:33:19Z\nRevision 1d550c4e90275ab418b9161925049239227f3dc9\nUrl https://github.com/apache/spark\nType --help for more information.\n</code></pre> <pre><code>$SPARK_HOME/bin/spark-shell --driver-class-path $JDBC_DRIVER --jars $JDBC_DRIVER\n</code></pre> <pre><code>val sampledata = spark.range(5)\nsampledata.write\n  .format(\"jdbc\")\n  .option(\"url\", \"jdbc:postgresql:postgres\")\n  .option(\"dbtable\", \"nums\")\n  .option(\"user\", \"postgres\")\n  .option(\"password\", \"mysecretpassword\")\n  .save\n</code></pre> <pre><code>val nums = spark.read\n  .format(\"jdbc\")\n  .option(\"url\", \"jdbc:postgresql:postgres\")\n  .option(\"dbtable\", \"nums\")\n  .option(\"user\", \"postgres\")\n  .option(\"password\", \"mysecretpassword\")\n  .load\n</code></pre> <pre><code>nums.show\n</code></pre>"},{"location":"demo/using-jdbc-data-source-to-access-postgresql/#clean-up","title":"Clean Up","text":"<pre><code>docker stop postgres-demo\n</code></pre> <pre><code>docker stop postgres-demo\n</code></pre> <p>That's it. Congratulations!</p>"},{"location":"dynamic-partition-pruning/","title":"Dynamic Partition Pruning","text":"<p>Dynamic Partition Pruning (DPP) is an optimization of JOIN batch queries of partitioned tables using partition columns in a join condition. The idea is to push filter conditions down to the large fact table and reduce the number of rows to scan.</p> <p>The best results are expected in JOIN queries between a large fact table and a much smaller dimension table (star-schema queries).</p> <p>Dynamic Partition Pruning is applied to a query at logical optimization phase using PartitionPruning and CleanupDynamicPruningFilters optimization rules.</p> <p>Dynamic Partition Pruning optimization is controlled by spark.sql.optimizer.dynamicPartitionPruning.enabled configuration property.</p> <p>Streaming Queries</p> <p>Dynamic Partition Pruning is not applied to streaming queries.</p>"},{"location":"dynamic-partition-pruning/#demo","title":"Demo","text":"<p>Demo: Dynamic Partition Pruning</p>"},{"location":"dynamic-partition-pruning/#references","title":"References","text":""},{"location":"dynamic-partition-pruning/#articles","title":"Articles","text":"<ul> <li>Dynamic Partition Pruning in Spark 3.0</li> </ul>"},{"location":"dynamic-partition-pruning/#videos","title":"Videos","text":"<ul> <li>Dynamic Partition Pruning in Apache Spark</li> <li>Apache Spark 3 | New Feature | Performance Optimization | Dynamic Partition Pruning</li> <li>Dynamic Partition Pruning | Spark Performance Tuning by Harjeet (aka Data Savvy)</li> </ul>"},{"location":"execution-planning-strategies/Aggregation/","title":"Aggregation Execution Planning Strategy","text":"<p><code>Aggregation</code> is an execution planning strategy that SparkPlanner uses for planning Aggregate logical operators (in the order of preference):</p> <ol> <li>HashAggregateExec</li> <li>ObjectHashAggregateExec</li> <li>SortAggregateExec</li> </ol>"},{"location":"execution-planning-strategies/Aggregation/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): Seq[SparkPlan]\n</code></pre> <p><code>apply</code>\u00a0is part of the GenericStrategy abstraction.</p> <p><code>apply</code> works with Aggregate logical operators with all the aggregate expressions being either AggregateExpressions or PythonUDFs only. Otherwise, <code>apply</code> throws an AnalysisException.</p> <p><code>apply</code> destructures the Aggregate logical operator (into a four-element tuple) with the following:</p> <ul> <li>Grouping Expressions</li> <li>Aggregration Expressions</li> <li>Result Expressions</li> <li>Child Logical Operator</li> </ul>","text":""},{"location":"execution-planning-strategies/Aggregation/#aggregateexpressions","title":"AggregateExpressions <p>For Aggregate logical operators with AggregateExpressions, <code>apply</code> splits them based on the isDistinct flag.</p> <p>Without distinct aggregate functions (expressions), <code>apply</code> planAggregateWithoutDistinct. Otherwise, <code>apply</code> planAggregateWithOneDistinct.</p> <p>In the end, <code>apply</code> creates one of the following physical operators based on whether there is distinct aggregate function or not.</p>  <p>Note</p> <p>It is assumed that all the distinct aggregate functions have the same column expressions.</p> <pre><code>COUNT(DISTINCT foo), MAX(DISTINCT foo)\n</code></pre> <p>The following is not valid due to different column expressions</p> <pre><code>COUNT(DISTINCT bar), COUNT(DISTINCT foo)\n</code></pre>","text":""},{"location":"execution-planning-strategies/Aggregation/#pythonudfs","title":"PythonUDFs <p>For Aggregate logical operators with <code>PythonUDF</code>s (PySpark)...FIXME</p>","text":""},{"location":"execution-planning-strategies/Aggregation/#analysisexception","title":"AnalysisException <p><code>apply</code> can throw an <code>AnalysisException</code>:</p> <pre><code>Cannot use a mixture of aggregate function and group aggregate pandas UDF\n</code></pre>","text":""},{"location":"execution-planning-strategies/Aggregation/#demo","title":"Demo <pre><code>scala&gt; :type spark\norg.apache.spark.sql.SparkSession\n\n// structured query with count aggregate function\nval q = spark\n  .range(5)\n  .groupBy($\"id\" % 2 as \"group\")\n  .agg(count(\"id\") as \"count\")\nval plan = q.queryExecution.optimizedPlan\nscala&gt; println(plan.numberedTreeString)\n00 Aggregate [(id#0L % 2)], [(id#0L % 2) AS group#3L, count(1) AS count#8L]\n01 +- Range (0, 5, step=1, splits=Some(8))\n\nimport spark.sessionState.planner.Aggregation\nval physicalPlan = Aggregation.apply(plan)\n\n// HashAggregateExec selected\nscala&gt; println(physicalPlan.head.numberedTreeString)\n00 HashAggregate(keys=[(id#0L % 2)#12L], functions=[count(1)], output=[group#3L, count#8L])\n01 +- HashAggregate(keys=[(id#0L % 2) AS (id#0L % 2)#12L], functions=[partial_count(1)], output=[(id#0L % 2)#12L, count#14L])\n02    +- PlanLater Range (0, 5, step=1, splits=Some(8))\n</code></pre>","text":""},{"location":"execution-planning-strategies/BasicOperators/","title":"BasicOperators Execution Planning Strategy","text":"<p><code>BasicOperators</code> is an execution planning strategy for basic conversions of logical operators to their physical representatives.</p>"},{"location":"execution-planning-strategies/BasicOperators/#conversions","title":"Conversions","text":"Logical Operator Physical Operator DataWritingCommand DataWritingCommandExec RunnableCommand ExecutedCommandExec MemoryPlan (Spark Structured Streaming) LocalTableScanExec DeserializeToObject DeserializeToObjectExec FlatMapGroupsWithState <code>CoGroupExec</code> or <code>MapGroupsExec</code> ... ... CollectMetrics CollectMetricsExec <p>Tip</p> <p>Refer to the source code of BasicOperators to confirm the most up-to-date operator mapping.</p>"},{"location":"execution-planning-strategies/DataSourceStrategy/","title":"DataSourceStrategy Execution Planning Strategy","text":"<p><code>DataSourceStrategy</code> is an execution planning strategy (of SparkPlanner) that plans LogicalRelation logical operators as RowDataSourceScanExec physical operators (possibly under FilterExec and ProjectExec logical operators).</p>"},{"location":"execution-planning-strategies/DataSourceStrategy/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): Seq[SparkPlan]\n</code></pre> <p><code>apply</code> plans the given LogicalPlan into a corresponding SparkPlan.</p>    Logical Operator Description     LogicalRelation with a <code>CatalystScan</code> relation <ul><li>pruneFilterProjectRaw (with the RDD conversion to RDD[InternalRow] as part of <code>scanBuilder</code>)</li><li><code>CatalystScan</code> does not seem to be used in Spark SQL</li></ul>   LogicalRelation with PrunedFilteredScan relation <ul><li>pruneFilterProject (with the RDD conversion to RDD[InternalRow] as part of <code>scanBuilder</code>)</li><li>Matches JDBCRelation exclusively</li></ul>   LogicalRelation with a PrunedScan relation <ul><li>pruneFilterProject (with the RDD conversion to RDD[InternalRow] as part of <code>scanBuilder</code>)</li><li><code>PrunedScan</code> does not seem to be used in Spark SQL</li></ul>   LogicalRelation with a TableScan relation <ul><li>Creates a RowDataSourceScanExec directly (requesting the <code>TableScan</code> to buildScan followed by RDD conversion to RDD[InternalRow])</li><li>Matches KafkaRelation exclusively</li></ul>","text":""},{"location":"execution-planning-strategies/DataSourceStrategy/#prunefilterproject","title":"pruneFilterProject <pre><code>pruneFilterProject(\n  relation: LogicalRelation,\n  projects: Seq[NamedExpression],\n  filterPredicates: Seq[Expression],\n  scanBuilder: (Seq[Attribute], Array[Filter]) =&gt; RDD[InternalRow]): SparkPlan\n</code></pre> <p><code>pruneFilterProject</code> pruneFilterProjectRaw (with <code>scanBuilder</code> ignoring the <code>Seq[Expression]</code> input argument).</p> <p><code>pruneFilterProject</code>\u00a0is used when:</p> <ul> <li><code>DataSourceStrategy</code> execution planning strategy is executed (with LogicalRelations over a PrunedFilteredScan or a PrunedScan)</li> </ul>","text":""},{"location":"execution-planning-strategies/DataSourceStrategy/#selecting-catalyst-expressions-convertible-to-data-source-filter-predicates","title":"Selecting Catalyst Expressions Convertible to Data Source Filter Predicates <pre><code>selectFilters(\n  relation: BaseRelation,\n  predicates: Seq[Expression]): (Seq[Expression], Seq[Filter], Set[Filter])\n</code></pre> <p><code>selectFilters</code> builds a map of Catalyst predicate expressions (from the input <code>predicates</code>) that can be translated to a data source filter predicate.</p> <p><code>selectFilters</code> then requests the input BaseRelation for unhandled filters (out of the convertible ones that <code>selectFilters</code> built the map with).</p> <p>In the end, <code>selectFilters</code> returns a 3-element tuple with the following:</p> <ol> <li> <p>Inconvertible and unhandled Catalyst predicate expressions</p> </li> <li> <p>All converted data source filters</p> </li> <li> <p>Pushed-down data source filters (that the input <code>BaseRelation</code> can handle)</p> </li> </ol> <p><code>selectFilters</code> is used when <code>DataSourceStrategy</code> execution planning strategy is executed (and creates a RowDataSourceScanExec physical operator).</p>","text":""},{"location":"execution-planning-strategies/DataSourceStrategy/#translating-catalyst-expression-into-data-source-filter-predicate","title":"Translating Catalyst Expression into Data Source Filter Predicate <pre><code>translateFilter(\n  predicate: Expression,\n  supportNestedPredicatePushdown: Boolean): Option[Filter]\n</code></pre> <p><code>translateFilter</code> translateFilterWithMapping (with the input parameters and an undefined (<code>None</code>) <code>translatedFilterToExpr</code>).</p> <p><code>translateFilter</code> is used when:</p> <ul> <li><code>FileSourceScanExec</code> physical operator is requested for the pushedDownFilters</li> <li><code>DataSourceStrategy</code> execution planning strategy is requested to selectFilters</li> <li><code>FileSourceStrategy</code> execution planning strategy is executed</li> <li><code>DataSourceV2Strategy</code> execution planning strategy is executed</li> <li>V2Writes logical optimization is requested to optimize a logical query</li> </ul>","text":""},{"location":"execution-planning-strategies/DataSourceStrategy/#translatefilterwithmapping","title":"translateFilterWithMapping <pre><code>translateFilterWithMapping(\n  predicate: Expression,\n  translatedFilterToExpr: Option[mutable.HashMap[sources.Filter, Expression]],\n  nestedPredicatePushdownEnabled: Boolean): Option[Filter]\n</code></pre> <p><code>translateFilterWithMapping</code> translates the input Catalyst Expression to a Data Source Filter predicate.</p>  <p><code>translateFilterWithMapping</code> branches off based on the given predicate expression:</p> <ul> <li> <p>For <code>And</code>s, <code>translateFilterWithMapping</code> translateFilterWithMapping with the left and right expressions and creates a <code>And</code> filter</p> </li> <li> <p>For <code>Or</code>s, <code>translateFilterWithMapping</code> translateFilterWithMapping with the left and right expressions and creates a <code>Or</code> filter</p> </li> <li> <p>For <code>Not</code>s, <code>translateFilterWithMapping</code> translateFilterWithMapping with the child expression and creates a <code>Not</code> filter</p> </li> <li> <p>For all the other cases, <code>translateFilterWithMapping</code> translateLeafNodeFilter and, if successful, adds the filter and the predicate expression to <code>translatedFilterToExpr</code> collection</p> </li> </ul> <p><code>translateFilterWithMapping</code> is used when:</p> <ul> <li><code>DataSourceStrategy</code> is requested to translateFilter</li> <li><code>PushDownUtils</code> is requested to pushFilters</li> </ul>","text":""},{"location":"execution-planning-strategies/DataSourceStrategy/#translateleafnodefilter","title":"translateLeafNodeFilter <pre><code>translateLeafNodeFilter(\n  predicate: Expression,\n  pushableColumn: PushableColumnBase): Option[Filter]\n</code></pre> <p><code>translateLeafNodeFilter</code> translates a given Catalyst Expression into a corresponding Filter predicate if possible. If not, <code>translateFilter</code> returns <code>None</code>.</p>    Catalyst Expression Filter Predicate     EqualTo (with a \"pushable\" column and a <code>Literal</code>) <code>EqualTo</code>   EqualNullSafe (with a \"pushable\" column and a <code>Literal</code>) <code>EqualNullSafe</code>   <code>GreaterThan</code> (with a \"pushable\" column and a <code>Literal</code>) <code>GreaterThan</code> or <code>LessThan</code>   <code>LessThan</code> (with a \"pushable\" column and a <code>Literal</code>) <code>LessThan</code> or <code>GreaterThan</code>   <code>GreaterThanOrEqual</code> (with a \"pushable\" column and a <code>Literal</code>) <code>GreaterThanOrEqual</code> or LessThanOrEqual   LessThanOrEqual (with a \"pushable\" column and a <code>Literal</code>) LessThanOrEqual or <code>GreaterThanOrEqual</code>   InSet (with a \"pushable\" column and values) <code>In</code>   InSet (with a \"pushable\" column and expressions) <code>In</code>   <code>IsNull</code> (with a \"pushable\" column) <code>IsNull</code>   <code>IsNotNull</code> (with a \"pushable\" column) <code>IsNotNull</code>   <code>StartsWith</code> (with a \"pushable\" column and a string <code>Literal</code>) <code>StringStartsWith</code>   <code>EndsWith</code> (with a \"pushable\" column and a string <code>Literal</code>) <code>StringEndsWith</code>   <code>Contains</code> (with a \"pushable\" column and a string <code>Literal</code>) <code>StringContains</code>   <code>Literal</code> (with <code>true</code>) <code>AlwaysTrue</code>   <code>Literal</code> (with <code>false</code>) <code>AlwaysFalse</code>     <p>Note</p> <p>The Catalyst expressions and their corresponding data source filter predicates have the same names in most cases but belong to different Scala packages (<code>org.apache.spark.sql.catalyst.expressions</code> and <code>org.apache.spark.sql.sources</code>, respectively).</p>","text":""},{"location":"execution-planning-strategies/DataSourceStrategy/#rdd-conversion-converting-rdd-of-rows-to-catalyst-rdd-of-internalrows","title":"RDD Conversion (Converting RDD of Rows to Catalyst RDD of InternalRows) <pre><code>toCatalystRDD(\n  relation: LogicalRelation,\n  output: Seq[Attribute],\n  rdd: RDD[Row]): RDD[InternalRow]\ntoCatalystRDD(\n  relation: LogicalRelation,\n  rdd: RDD[Row]) // &lt;1&gt;\n</code></pre> <p><code>toCatalystRDD</code> branches off per the needConversion flag of the BaseRelation of the input LogicalRelation:</p> <ul> <li>otherwise, <code>toCatalystRDD</code> casts the input <code>RDD[Row]</code> to an <code>RDD[InternalRow]</code> (using Java's <code>asInstanceOf</code> operator)</li> </ul> <p><code>toCatalystRDD</code> is used when <code>DataSourceStrategy</code> execution planning strategy is executed (for all kinds of BaseRelations).</p>","text":""},{"location":"execution-planning-strategies/DataSourceStrategy/#creating-rowdatasourcescanexec-physical-operator-for-logicalrelation","title":"Creating RowDataSourceScanExec Physical Operator for LogicalRelation <pre><code>pruneFilterProjectRaw(\n  relation: LogicalRelation,\n  projects: Seq[NamedExpression],\n  filterPredicates: Seq[Expression],\n  scanBuilder: (Seq[Attribute], Seq[Expression], Seq[Filter]) =&gt; RDD[InternalRow]): SparkPlan\n</code></pre> <p><code>pruneFilterProjectRaw</code> converts the given LogicalRelation leaf logical operator into a RowDataSourceScanExec leaf physical operator with the LogicalRelation leaf logical operator (possibly as a child of a FilterExec and a ProjectExec unary physical operators).</p>  <p>Note</p> <p><code>pruneFilterProjectRaw</code> is almost like SparkPlanner.pruneFilterProject.</p>  <p>Internally, <code>pruneFilterProjectRaw</code> splits the input <code>filterPredicates</code> expressions to select the Catalyst expressions that can be converted to data source filter predicates (and handled by the underlying BaseRelation of the <code>LogicalRelation</code>).</p> <p><code>pruneFilterProjectRaw</code> combines all expressions that are neither convertible to data source filters nor can be handled by the relation using <code>And</code> binary expression (that creates a so-called <code>filterCondition</code> that will eventually be used to create a FilterExec physical operator if non-empty).</p> <p><code>pruneFilterProjectRaw</code> creates a RowDataSourceScanExec leaf physical operator.</p>","text":""},{"location":"execution-planning-strategies/DataSourceStrategy/#getpusheddownfilters","title":"getPushedDownFilters <pre><code>getPushedDownFilters(\n  partitionColumns: Seq[Expression],\n  normalizedFilters: Seq[Expression]): ExpressionSet\n</code></pre> <p>For an empty <code>partitionColumns</code>, <code>getPushedDownFilters</code> an empty <code>ExpressionSet</code>.</p> <p>Otherwise, <code>getPushedDownFilters</code>...FIXME</p> <p><code>getPushedDownFilters</code> prints out the following INFO message to the logs:</p> <pre><code>Pruning directories with: [predicates]\n</code></pre>  <p><code>getPushedDownFilters</code> is used when executing the following execution planning strategies:</p> <ul> <li>HiveTableScans</li> <li>FileSourceStrategy</li> </ul>","text":""},{"location":"execution-planning-strategies/DataSourceStrategy/#demo","title":"Demo <pre><code>import org.apache.spark.sql.execution.datasources.DataSourceStrategy\nval strategy = DataSourceStrategy(spark.sessionState.conf)\n\nimport org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\nval plan: LogicalPlan = ???\n\nval sparkPlan = strategy(plan).head\n</code></pre>","text":""},{"location":"execution-planning-strategies/DataSourceStrategy/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.DataSourceStrategy</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.DataSourceStrategy.name = org.apache.spark.sql.execution.datasources.DataSourceStrategy\nlogger.DataSourceStrategy.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"execution-planning-strategies/DataSourceV2Strategy/","title":"DataSourceV2Strategy Execution Planning Strategy","text":"<p><code>DataSourceV2Strategy</code> is an execution planning strategy.</p> Logical Operator Physical Operator DataSourceV2ScanRelation with V1Scan RowDataSourceScanExec DataSourceV2ScanRelation BatchScanExec <code>StreamingDataSourceV2Relation</code> <code>WriteToDataSourceV2</code> (Spark Structured Streaming) <code>WriteToDataSourceV2Exec</code> (Spark Structured Streaming) CreateTableAsSelect <code>AtomicCreateTableAsSelectExec</code> or CreateTableAsSelectExec <code>RefreshTable</code> <code>RefreshTableExec</code> <code>ReplaceTable</code> <code>AtomicReplaceTableExec</code> or <code>ReplaceTableExec</code> <code>ReplaceTableAsSelect</code> <code>AtomicReplaceTableAsSelectExec</code> or <code>ReplaceTableAsSelectExec</code> AppendData <code>AppendDataExecV1</code> or <code>AppendDataExec</code> OverwriteByExpression with a DataSourceV2Relation <code>OverwriteByExpressionExecV1</code> or OverwriteByExpressionExec OverwritePartitionsDynamic <code>OverwritePartitionsDynamicExec</code> DeleteFromTable with DataSourceV2ScanRelation DeleteFromTableExec <code>WriteToContinuousDataSource</code> <code>WriteToContinuousDataSourceExec</code> <code>DescribeNamespace</code> <code>DescribeNamespaceExec</code> DescribeRelation DescribeTableExec <code>DropTable</code> <code>DropTableExec</code> <code>NoopDropTable</code> LocalTableScanExec AlterTable AlterTableExec others"},{"location":"execution-planning-strategies/DataSourceV2Strategy/#creating-instance","title":"Creating Instance","text":"<p><code>DataSourceV2Strategy</code> takes the following to be created:</p> <ul> <li> SparkSession <p><code>DataSourceV2Strategy</code> is created\u00a0when:</p> <ul> <li><code>SparkPlanner</code> is requested for the strategies</li> </ul>"},{"location":"execution-planning-strategies/DataSourceV2Strategy/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): Seq[SparkPlan]\n</code></pre> <p><code>apply</code> is part of GenericStrategy abstraction.</p> <p><code>apply</code> branches off per the type of the given logical operator.</p>","text":""},{"location":"execution-planning-strategies/DataSourceV2Strategy/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"execution-planning-strategies/FileSourceStrategy/","title":"FileSourceStrategy Execution Planning Strategy","text":"<p><code>FileSourceStrategy</code> is an execution planning strategy.</p> <p><code>FileSourceStrategy</code> is part of the predefined strategies of the Spark Planner (and is executed after DataSourceV2Strategy).</p>"},{"location":"execution-planning-strategies/InMemoryScans/","title":"InMemoryScans Execution Planning Strategy","text":"<p><code>InMemoryScans</code> is an execution planning strategy that &lt;&gt;."},{"location":"execution-planning-strategies/InMemoryScans/#source-scala","title":"[source, scala]","text":"<p>val spark: SparkSession = ... // query uses InMemoryRelation logical operator val q = spark.range(5).cache val plan = q.queryExecution.optimizedPlan scala&gt; println(plan.numberedTreeString) 00 InMemoryRelation [id#208L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas) 01    +- *Range (0, 5, step=1, splits=8)</p> <p>// InMemoryScans is an internal class of SparkStrategies import spark.sessionState.planner.InMemoryScans val physicalPlan = InMemoryScans.apply(plan).head scala&gt; println(physicalPlan.numberedTreeString) 00 InMemoryTableScan [id#208L] 01    +- InMemoryRelation [id#208L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas) 02          +- *Range (0, 5, step=1, splits=8)</p> <p><code>InMemoryScans</code> is part of the standard execution planning strategies of SparkPlanner.</p> <p>=== [[apply]] Applying InMemoryScans Strategy to Logical Plan (Executing InMemoryScans) -- <code>apply</code> Method</p>"},{"location":"execution-planning-strategies/InMemoryScans/#source-scala_1","title":"[source, scala]","text":""},{"location":"execution-planning-strategies/InMemoryScans/#applyplan-logicalplan-seqsparkplan","title":"apply(plan: LogicalPlan): Seq[SparkPlan]","text":"<p><code>apply</code> PhysicalOperation.md#unapply[destructures the input logical plan] to a InMemoryRelation logical operator.</p> <p>In the end, <code>apply</code> pruneFilterProject with a new InMemoryTableScanExec.md#creating-instance[InMemoryTableScanExec] physical operator.</p> <p><code>apply</code> is part of GenericStrategy abstraction.</p>"},{"location":"execution-planning-strategies/JoinSelection/","title":"JoinSelection Execution Planning Strategy","text":"<p><code>JoinSelection</code> is an execution planning strategy for Join logical operators.</p> <p><code>JoinSelection</code> is part of the strategies of the SparkPlanner.</p>"},{"location":"execution-planning-strategies/JoinSelection/#join-selection-priorities","title":"Join Selection Priorities","text":"<ol> <li>Join Type</li> <li>Hints</li> <li>Size (Cost-Based Optimization, Statistics)</li> </ol>"},{"location":"execution-planning-strategies/JoinSelection/#join-selection-requirements","title":"Join Selection Requirements","text":"<p>The following sections are in the order of preference.</p> <p>Danger</p> <p>These sections have to be reviewed for correctness.</p> <p><code>JoinSelection</code> considers join physical operators per whether join keys are used or not:</p> <ul> <li>If used, <code>JoinSelection</code> considers BroadcastHashJoinExec, ShuffledHashJoinExec or SortMergeJoinExec operators</li> <li>Otherwise, <code>JoinSelection</code> considers BroadcastNestedLoopJoinExec or CartesianProductExec</li> </ul>"},{"location":"execution-planning-strategies/JoinSelection/#broadcasthashjoinexec","title":"BroadcastHashJoinExec <p><code>JoinSelection</code> plans a BroadcastHashJoinExec when there are join keys and one of the following holds:</p> <ul> <li> <p>Join type is CROSS, INNER, LEFT ANTI, LEFT OUTER, LEFT SEMI or ExistenceJoin</p> </li> <li> <p>Join type is CROSS, INNER or RIGHT OUTER</p> </li> </ul> <p><code>BroadcastHashJoinExec</code> is created for ExtractEquiJoinKeys-destructurable logical query plans (INNER, CROSS, LEFT OUTER, LEFT SEMI, LEFT ANTI) of which the <code>right</code> physical operator can be broadcast.</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#shuffledhashjoinexec","title":"ShuffledHashJoinExec <p><code>JoinSelection</code> plans a ShuffledHashJoinExec when there are join keys and one of the following holds:</p> <ul> <li> <p>spark.sql.join.preferSortMergeJoin is disabled, the join type is CROSS, INNER, LEFT ANTI, LEFT OUTER, LEFT SEMI or ExistenceJoin</p> </li> <li> <p>spark.sql.join.preferSortMergeJoin is disabled, the join type is CROSS, INNER or RIGHT OUTER</p> </li> <li> <p>Left join keys are not orderable</p> </li> </ul>","text":""},{"location":"execution-planning-strategies/JoinSelection/#sortmergejoinexec","title":"SortMergeJoinExec <p><code>JoinSelection</code> plans a SortMergeJoinExec when the left join keys are orderable.</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#broadcastnestedloopjoinexec","title":"BroadcastNestedLoopJoinExec <p><code>JoinSelection</code> plans a BroadcastNestedLoopJoinExec when there are no join keys and one of the following holds:</p> <ul> <li> <p>Join type is CROSS, INNER, LEFT ANTI, LEFT OUTER, LEFT SEMI or ExistenceJoin</p> </li> <li> <p>Join type is CROSS, INNER or RIGHT OUTER</p> </li> </ul>","text":""},{"location":"execution-planning-strategies/JoinSelection/#cartesianproductexec","title":"CartesianProductExec <p><code>JoinSelection</code> plans a <code>CartesianProductExec</code> when there are no join keys and join type is CROSS or INNER</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#broadcastnestedloopjoinexec_1","title":"BroadcastNestedLoopJoinExec <p><code>JoinSelection</code> plans a BroadcastNestedLoopJoinExec when no other join operator has matched already</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): Seq[SparkPlan]\n</code></pre> <p><code>apply</code>\u00a0is part of the GenericStrategy abstraction.</p> <p><code>apply</code>\u00a0is made up of three parts (each with its own Scala extractor object to destructure the input LogicalPlan):</p> <ol> <li>ExtractEquiJoinKeys</li> <li>ExtractSingleColumnNullAwareAntiJoin</li> <li>Other Joins</li> </ol>","text":""},{"location":"execution-planning-strategies/JoinSelection/#extractequijoinkeys","title":"ExtractEquiJoinKeys <p><code>apply</code> uses ExtractEquiJoinKeys to match on Join logical operators with EqualTo and EqualNullSafe condition predicate expressions.</p> <p><code>apply</code> does the following (in the order until a join physical operator has been determined):</p> <ol> <li>createBroadcastHashJoin (based on the hints only)</li> <li>With a hintToSortMergeJoin defined, createSortMergeJoin</li> <li>createShuffleHashJoin (based on the hints only)</li> <li>With a hintToShuffleReplicateNL defined, createCartesianProduct</li> <li>createJoinWithoutHint</li> </ol>","text":""},{"location":"execution-planning-strategies/JoinSelection/#extractsinglecolumnnullawareantijoin","title":"ExtractSingleColumnNullAwareAntiJoin <p><code>apply</code> uses ExtractSingleColumnNullAwareAntiJoin to match on Join logical operators.</p> <p>For every <code>Join</code> operator, <code>apply</code> creates a BroadcastHashJoinExec physical operator with the following:</p> <ul> <li>LeftAnti join type</li> <li><code>BuildRight</code> build side</li> <li>Undefined join condition expressions</li> <li>isNullAwareAntiJoin flag enabled (<code>true</code>)</li> </ul>","text":""},{"location":"execution-planning-strategies/JoinSelection/#other-joins","title":"Other Joins <p><code>apply</code> determines the desired build side. For <code>InnerLike</code> and <code>FullOuter</code> join types, <code>apply</code> getSmallerSide. For all other join types, <code>apply</code> canBuildBroadcastLeft and prefers <code>BuildLeft</code> over <code>BuildRight</code>.</p> <p>In the end, <code>apply</code> does the following (in the order until a join physical operator has been determined):</p> <ol> <li>createBroadcastNLJoin (based on the hints only for the left and right side)</li> <li>With a hintToShuffleReplicateNL defined, createCartesianProduct</li> <li>createJoinWithoutHint</li> </ol>","text":""},{"location":"execution-planning-strategies/JoinSelection/#canbroadcastbysize","title":"canBroadcastBySize <pre><code>canBroadcastBySize(\n  plan: LogicalPlan,\n  conf: SQLConf): Boolean\n</code></pre> <p><code>canBroadcastBySize</code> is enabled (<code>true</code>) when the size of the table (the given LogicalPlan) is small for a broadcast join (between <code>0</code> and the spark.sql.autoBroadcastJoinThreshold configuration property inclusive).</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#canbuildbroadcastleft","title":"canBuildBroadcastLeft <pre><code>canBuildBroadcastLeft(\n  joinType: JoinType): Boolean\n</code></pre> <p><code>canBuildBroadcastLeft</code> is enabled (<code>true</code>) for <code>InnerLike</code> and <code>RightOuter</code> join types.</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#canbuildlocalhashmapbysize","title":"canBuildLocalHashMapBySize <pre><code>canBuildLocalHashMapBySize(\n  plan: LogicalPlan,\n  conf: SQLConf): Boolean\n</code></pre> <p><code>canBuildLocalHashMapBySize</code> is enabled (<code>true</code>) when the size of the table (the given LogicalPlan) is small for a shuffle hash join (below the spark.sql.autoBroadcastJoinThreshold configuration property multiplied by the configured number of shuffle partitions).</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#creating-broadcasthashjoinexec","title":"Creating BroadcastHashJoinExec <pre><code>createBroadcastHashJoin(\n  onlyLookingAtHint: Boolean): Option[Seq[BroadcastHashJoinExec]]\n</code></pre> <p><code>createBroadcastHashJoin</code> determines a BroadcastBuildSide and, if successful, creates a BroadcastHashJoinExec.</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#creating-broadcastnestedloopjoinexec","title":"Creating BroadcastNestedLoopJoinExec <pre><code>createBroadcastNLJoin(\n  buildLeft: Boolean,\n  buildRight: Boolean): Option[Seq[BroadcastNestedLoopJoinExec]]\n</code></pre> <p><code>createBroadcastNLJoin</code> creates a BroadcastNestedLoopJoinExec when at least one of the <code>buildLeft</code> or <code>buildRight</code> flags are enabled.</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#creating-shuffledhashjoinexec","title":"Creating ShuffledHashJoinExec <pre><code>createShuffleHashJoin(\n  onlyLookingAtHint: Boolean): Option[Seq[ShuffledHashJoinExec]]\n</code></pre> <p><code>createShuffleHashJoin</code> tries to determine the BuildSide for a ShuffleHashJoinExec and, if successful, creates a ShuffledHashJoinExec.</p> <p><code>createShuffleHashJoin</code> is used when:</p> <ul> <li><code>JoinSelection</code> is requested to createJoinWithoutHint (with <code>onlyLookingAtHint</code> disabled) and execute (with <code>onlyLookingAtHint</code> enabled)</li> </ul>","text":""},{"location":"execution-planning-strategies/JoinSelection/#creating-sortmergejoinexec","title":"Creating SortMergeJoinExec <pre><code>createSortMergeJoin(): Option[Seq[SortMergeJoinExec]]\n</code></pre> <p><code>createSortMergeJoin</code> creates a SortMergeJoinExec if the left keys are orderable.</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#createcartesianproduct","title":"createCartesianProduct <pre><code>createCartesianProduct(): Option[Seq[CartesianProductExec]]\n</code></pre> <p><code>createCartesianProduct</code> creates a <code>CartesianProductExec</code> for <code>InnerLike</code> join type.</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#createjoinwithouthint","title":"createJoinWithoutHint <pre><code>createJoinWithoutHint(): Seq[BaseJoinExec]\n</code></pre> <p><code>createJoinWithoutHint</code>...FIXME</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#build-side","title":"Build Side <pre><code>getBuildSide(\n  canBuildLeft: Boolean,\n  canBuildRight: Boolean,\n  left: LogicalPlan,\n  right: LogicalPlan): Option[BuildSide]\n</code></pre> <p><code>getBuildSide</code> is the following (in the order):</p> <ol> <li>The smaller side of the left and right operators when the <code>canBuildLeft</code> and <code>canBuildRight</code> flags are both enabled (<code>true</code>)</li> <li><code>BuildLeft</code> for <code>canBuildLeft</code> flag enabled</li> <li><code>BuildRight</code> for <code>canBuildRight</code> flag enabled</li> <li>Undefined (<code>None</code>)</li> </ol>","text":""},{"location":"execution-planning-strategies/JoinSelection/#getbroadcastbuildside","title":"getBroadcastBuildSide <pre><code>getBroadcastBuildSide(\n  left: LogicalPlan,\n  right: LogicalPlan,\n  joinType: JoinType,\n  hint: JoinHint,\n  hintOnly: Boolean,\n  conf: SQLConf): Option[BuildSide]\n</code></pre> <p><code>getBroadcastBuildSide</code> determines if build on the left side (<code>buildLeft</code>). With <code>hintOnly</code> enabled (<code>true</code>), <code>getBroadcastBuildSide</code> hintToBroadcastLeft. Otherwise, <code>getBroadcastBuildSide</code> checks if canBroadcastBySize and not hintToNotBroadcastLeft.</p> <p><code>getBroadcastBuildSide</code> determines if build on the right side (<code>buildRight</code>). With <code>hintOnly</code> enabled (<code>true</code>), <code>getBroadcastBuildSide</code> hintToBroadcastRight. Otherwise, <code>getBroadcastBuildSide</code> checks if canBroadcastBySize and not hintToNotBroadcastRight.</p> <p>In the end, <code>getBroadcastBuildSide</code> getBuildSide with the following:</p> <ul> <li>canBuildBroadcastLeft for the given <code>JoinType</code> and the <code>buildLeft</code> flag</li> <li>canBuildBroadcastRight for the given <code>JoinType</code> and the <code>buildRight</code> flag</li> <li>Left physical operator</li> <li>Right physical operator</li> </ul>","text":""},{"location":"execution-planning-strategies/JoinSelection/#buildside-for-shufflehashjoinexec","title":"BuildSide for ShuffleHashJoinExec <pre><code>getShuffleHashJoinBuildSide(\n  left: LogicalPlan,\n  right: LogicalPlan,\n  joinType: JoinType,\n  hint: JoinHint,\n  hintOnly: Boolean,\n  conf: SQLConf): Option[BuildSide]\n</code></pre> <p><code>getShuffleHashJoinBuildSide</code> determines if to build on the left side (<code>buildLeft</code>):</p> <ul> <li>With <code>hintOnly</code> enabled (<code>true</code>), <code>getShuffleHashJoinBuildSide</code> hintToShuffleHashJoinLeft</li> <li>Otherwise, <code>getShuffleHashJoinBuildSide</code> checks if canBuildLocalHashMapBySize and the left operator is muchSmaller than the right</li> </ul> <p><code>getShuffleHashJoinBuildSide</code> determines if to build on the right side (<code>buildRight</code>):</p> <ul> <li>With <code>hintOnly</code> enabled (<code>true</code>), <code>getShuffleHashJoinBuildSide</code> hintToShuffleHashJoinRight</li> <li>Otherwise, <code>getShuffleHashJoinBuildSide</code> checks if canBuildLocalHashMapBySize and the right operator is muchSmaller than the left</li> </ul> <p>In the end, <code>getShuffleHashJoinBuildSide</code> tries to determine the BuildSide based on the following:</p> <ul> <li>Checks for BuildLeft for the given JoinType and the <code>buildLeft</code> flag</li> <li>canBuildShuffledHashJoinRight for the given <code>JoinType</code> and the <code>buildRight</code> flag</li> <li>Left physical operator</li> <li>Right physical operator</li> </ul>","text":""},{"location":"execution-planning-strategies/JoinSelection/#smaller-side","title":"Smaller Side <pre><code>getSmallerSide(\n  left: LogicalPlan,\n  right: LogicalPlan): BuildSide\n</code></pre> <p><code>getSmallerSide</code> is <code>BuildLeft</code> unless the size of the right table (the given <code>right</code> LogicalPlan) is not larger than the size of the left table (the given <code>left</code> LogicalPlan). Otherwise, <code>getSmallerSide</code> is <code>BuildRight</code>.</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#hinttobroadcastleft","title":"hintToBroadcastLeft <pre><code>hintToBroadcastLeft(\n  hint: JoinHint): Boolean\n</code></pre> <p><code>hintToBroadcastLeft</code> is enabled (<code>true</code>) when the given <code>JoinHint</code> has <code>BROADCAST</code>, <code>BROADCASTJOIN</code> or <code>MAPJOIN</code> hints associated with the left operator.</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#hinttobroadcastright","title":"hintToBroadcastRight <pre><code>hintToBroadcastRight(\n  hint: JoinHint): Boolean\n</code></pre> <p><code>hintToBroadcastRight</code> is enabled (<code>true</code>) when the given <code>JoinHint</code> has <code>BROADCAST</code>, <code>BROADCASTJOIN</code> or <code>MAPJOIN</code> hints associated with the right operator.</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#hinttonotbroadcastleft","title":"hintToNotBroadcastLeft <pre><code>hintToNotBroadcastLeft(\n  hint: JoinHint): Boolean\n</code></pre> <p><code>hintToNotBroadcastLeft</code> is enabled (<code>true</code>) when the given <code>JoinHint</code> has the internal <code>NO_BROADCAST_HASH</code> hint associated with the left operator (to discourage broadcast hash join).</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#hinttonotbroadcastright","title":"hintToNotBroadcastRight <pre><code>hintToNotBroadcastRight(\n  hint: JoinHint): Boolean\n</code></pre> <p><code>hintToNotBroadcastRight</code> is enabled (<code>true</code>) when the given <code>JoinHint</code> has the internal <code>NO_BROADCAST_HASH</code> hint associated with the right operator (to discourage broadcast hash join).</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#hinttoshufflehashjoinleft","title":"hintToShuffleHashJoinLeft <pre><code>hintToShuffleHashJoinLeft(\n  hint: JoinHint): Boolean\n</code></pre> <p><code>hintToShuffleHashJoinLeft</code> is enabled (<code>true</code>) when the given <code>JoinHint</code> has <code>SHUFFLE_HASH</code> hint associated with the left operator.</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#hinttoshufflehashjoinright","title":"hintToShuffleHashJoinRight <pre><code>hintToShuffleHashJoinRight(\n  hint: JoinHint): Boolean\n</code></pre> <p><code>hintToShuffleHashJoinRight</code> is enabled (<code>true</code>) when the given <code>JoinHint</code> has <code>SHUFFLE_HASH</code> hint associated with the right operator.</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#hinttosortmergejoin","title":"hintToSortMergeJoin <pre><code>hintToSortMergeJoin(\n  hint: JoinHint): Boolean\n</code></pre> <p><code>hintToSortMergeJoin</code>...FIXME</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#hinttoshufflereplicatenl","title":"hintToShuffleReplicateNL <pre><code>hintToShuffleReplicateNL(\n  hint: JoinHint): Boolean\n</code></pre> <p><code>hintToShuffleReplicateNL</code>...FIXME</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#muchsmaller","title":"muchSmaller <pre><code>muchSmaller(\n  a: LogicalPlan,\n  b: LogicalPlan): Boolean\n</code></pre> <p><code>muchSmaller</code> is enabled (<code>true</code>) when the size of the left table (the given <code>a</code> LogicalPlan) is at least 3 times smaller than the size of the right table (the given <code>b</code> LogicalPlan).</p>","text":""},{"location":"execution-planning-strategies/JoinSelection/#checking-left-buildside-for-shuffledhashjoin","title":"Checking Left BuildSide for ShuffledHashJoin <pre><code>canBuildShuffledHashJoinLeft(\n  joinType: JoinType): Boolean\n</code></pre> <p><code>canBuildShuffledHashJoinLeft</code> is enabled (<code>true</code>) for the given JoinType among the following:</p> <ul> <li>InnerLikes: Inner and Cross</li> <li>RightOuter</li> <li>FullOuter</li> </ul>","text":""},{"location":"execution-planning-strategies/LogicalQueryStageStrategy/","title":"LogicalQueryStageStrategy Execution Planning Strategy","text":"<p><code>LogicalQueryStageStrategy</code> is an execution planning strategy that plans the following logical operators:</p> <ul> <li>LogicalQueryStage with BroadcastQueryStageExec physical operator</li> <li>Join thereof</li> </ul> <p><code>LogicalQueryStageStrategy</code> is part of the strategies of the SparkPlanner.</p>"},{"location":"execution-planning-strategies/LogicalQueryStageStrategy/#executing-strategy","title":"Executing Strategy <pre><code>apply(\n  plan: LogicalPlan): Seq[SparkPlan]\n</code></pre> <p>For Join operators with an equi-join condition and the left or right side being broadcast stages, <code>apply</code> gives a BroadcastHashJoinExec physical operator.</p> <p>For other <code>Join</code> operators and the left or right side being broadcast stages, <code>apply</code> gives a BroadcastNestedLoopJoinExec physical operator.</p> <p>For LogicalQueryStage operators, <code>apply</code> simply gives the associated physical plan.</p> <p><code>apply</code> is part of the GenericStrategy abstraction.</p>","text":""},{"location":"execution-planning-strategies/LogicalQueryStageStrategy/#isbroadcaststage","title":"isBroadcastStage <pre><code>isBroadcastStage(\n  plan: LogicalPlan): Boolean\n</code></pre> <p><code>isBroadcastStage</code> is <code>true</code> when the given LogicalPlan is a LogicalQueryStage leaf logical operator with a BroadcastQueryStageExec physical operator. Otherwise, <code>isBroadcastStage</code> is <code>false</code>.</p>","text":""},{"location":"execution-planning-strategies/SparkStrategies/","title":"SparkStrategies \u2014 Container of Execution Planning Strategies","text":"<p><code>SparkStrategies</code> is an abstract Catalyst query planner that merely serves as a \"container\" (or a namespace) of the concrete execution planning strategies (for SparkPlanner):</p> <ul> <li>Aggregation</li> <li>BasicOperators</li> <li><code>FlatMapGroupsWithStateStrategy</code></li> <li>InMemoryScans</li> <li>JoinSelection</li> <li>SpecialLimits</li> <li><code>StatefulAggregationStrategy</code></li> <li><code>StreamingDeduplicationStrategy</code></li> <li><code>StreamingRelationStrategy</code></li> </ul> <p>[[singleRowRdd]] <code>SparkStrategies</code> has a single lazily-instantiated <code>singleRowRdd</code> value that is an <code>RDD</code> of InternalRows that BasicOperators execution planning strategy uses when resolving OneRowRelation (to <code>RDDScanExec</code> leaf physical operator).</p> <p>NOTE: <code>OneRowRelation</code> logical operator represents SQL's SELECT clause without FROM clause or EXPLAIN DESCRIBE TABLE.</p>"},{"location":"execution-planning-strategies/SparkStrategy/","title":"SparkStrategy \u2014 Base for Execution Planning Strategies","text":"<p><code>SparkStrategy</code> is an extension of the GenericStrategy abstraction for execution planning strategies that can convert (plan) a logical query plan to zero or more physical query plans for execution.</p>"},{"location":"execution-planning-strategies/SparkStrategy/#sparksessionextensions","title":"SparkSessionExtensions","text":"<p>SparkSessionExtensions is used to inject a planner strategy to a SparkSession.</p>"},{"location":"execution-planning-strategies/SparkStrategy/#experimentalmethods","title":"ExperimentalMethods","text":"<p>ExperimentalMethods is used to register extraStrategies globally.</p>"},{"location":"execution-planning-strategies/SparkStrategy/#sparkplanner-and-extraplanningstrategies","title":"SparkPlanner and extraPlanningStrategies","text":"<p>SparkPlanner defines an extension point for extraPlanningStrategies.</p>"},{"location":"execution-planning-strategies/SparkStrategy/#strategy-type-alias","title":"Strategy Type Alias <p><code>SparkStrategy</code> is used for <code>Strategy</code> type alias (type synonym) in Spark's code base that is defined in org.apache.spark.sql package object.</p> <pre><code>type Strategy = SparkStrategy\n</code></pre>","text":""},{"location":"execution-planning-strategies/SparkStrategy/#planlater-leaf-physical-operator","title":"PlanLater Leaf Physical Operator <p><code>SparkStrategy</code> defines <code>PlanLater</code> physical operator that is used as a marker operator to be planned later.</p> <p><code>PlanLater</code> cannot be executed and, when requested to execute (using <code>doExecute</code>), simply throws an <code>UnsupportedOperationException</code>.</p>","text":""},{"location":"execution-planning-strategies/SparkStrategy/#planlater","title":"planLater <pre><code>planLater(\n  plan: LogicalPlan): SparkPlan\n</code></pre> <p><code>planLater</code> creates a PlanLater leaf physical operator for the given LogicalPlan.</p>","text":""},{"location":"execution-planning-strategies/SpecialLimits/","title":"SpecialLimits Execution Planning Strategy","text":"<p><code>SpecialLimits</code> is an execution planning strategy that Spark Planner uses to &lt;&gt;. <p>=== [[apply]] Applying SpecialLimits Strategy to Logical Plan (Executing SpecialLimits) -- <code>apply</code> Method</p>"},{"location":"execution-planning-strategies/SpecialLimits/#source-scala","title":"[source, scala]","text":""},{"location":"execution-planning-strategies/SpecialLimits/#applyplan-logicalplan-seqsparkplan","title":"apply(plan: LogicalPlan): Seq[SparkPlan]","text":"<p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of GenericStrategy abstraction.</p>"},{"location":"execution-planning-strategies/Window/","title":"Window Execution Planning Strategy","text":"<p><code>Window</code> is...FIXME</p>"},{"location":"execution-planning-strategies/WithCTEStrategy/","title":"WithCTEStrategy Execution Planning Strategy","text":"<p><code>WithCTEStrategy</code> is an execution planning strategy to plan WithCTE and CTERelationRef logical operators.</p>"},{"location":"execution-planning-strategies/WithCTEStrategy/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): Seq[SparkPlan]\n</code></pre> <p><code>apply</code> plans WithCTE and CTERelationRef logical operators.</p>  <p>For a <code>WithCTE</code>, <code>apply</code> removes the <code>WithCTE</code> layer by adding the CTERelationDefs to the (thread-local) collection of <code>CTERelationDef</code>s (by their IDs) and marking the logical plan to plan later.</p>  <p>For a <code>CTERelationRef</code>, <code>apply</code> finds the logical plan for the cteId (in the (thread-local) collection of <code>CTERelationDef</code>s) and creates a Project logical operator (over <code>Alias</code> expressions).</p> <p>In the end, <code>apply</code> creates a ShuffleExchangeExec physical operator with the <code>Project</code> (to plan later), <code>RoundRobinPartitioning</code> output partitioning and <code>REPARTITION_BY_COL</code> shuffle origin.</p>  <p><code>apply</code>\u00a0is part of the GenericStrategy abstraction.</p>","text":""},{"location":"expressions/","title":"Catalyst Expressions","text":"<p>The Catalyst Tree Manipulation Framework defines Expression abstraction for expression nodes in (the trees of) logical and physical query plans.</p> <p>Spark SQL uses <code>Expression</code> abstraction to represent standard and user-defined functions as well as subqueries in logical query plans. Every time you use one of them in a query it creates a new <code>Expression</code>.</p>"},{"location":"expressions/#execution-modes","title":"Execution Modes","text":"<p>When the query is executed, <code>Expression</code>s are evaluated (i.e. requested to produce a value for an input InternalRow). There are two execution modes:</p> <ol> <li>Code-Generated</li> <li>Interpreted</li> </ol>"},{"location":"expressions/#be-careful-with-user-defined-functions","title":"Be Careful With User-Defined Functions","text":"<p>There are many reasons why you should not write your own user-defined functions. First and foremost, they are a blackbox to Catalyst optimizer.</p> <p>Speaking of memory usage, UDFs are written in a programming language like Scala, Java or Python that require an internal representation of data (InternalRow) to be fully deserialized and available as an object to the UDFs (that most of the time and for a reason know nothing about InternalRow and such). If it happens that two or more UDFs share computation (unless the UDFs are deterministic) they cannot share anything. Spark SQL cannot do much to optimize such queries.</p> <p>There comes a thought that I'm still shaping in my head and haven't fully \"dissected\" yet.</p> <p>Given that expressions (incl. UDFs) can be executed in code-generated execution mode that begs the question about possible performance improvements when an UDF uses <code>Expression</code>s (as the \"programming language\"). I'm not really sure what the benefits could be yet, but gives some hope.</p>"},{"location":"expressions/#monotonicallyincreasingid-expression","title":"MonotonicallyIncreasingID Expression <p>MonotonicallyIncreasingID expression is an example of \"basic\" expressions.</p>","text":""},{"location":"expressions/AggregateExpression/","title":"AggregateExpression","text":"<p><code>AggregateExpression</code> is an unevaluable expression that acts as a container (wrapper) for an AggregateFunction.</p>"},{"location":"expressions/AggregateExpression/#creating-instance","title":"Creating Instance","text":"<p><code>AggregateExpression</code> takes the following to be created:</p> <ul> <li> AggregateFunction <li>AggregateMode</li> <li> <code>isDistinct</code> flag <li>Aggregate Filter</li> <li> Result <code>ExprId</code> <p><code>AggregateExpression</code> is created using apply utility.</p>"},{"location":"expressions/AggregateExpression/#aggregate-filter","title":"Aggregate Filter <p><code>AggregateExpression</code> can be given an Expression for an aggregate filter.</p> <p>The filter is assumed undefined by default when <code>AggregateExpression</code> is created.</p> <p>A filter is used in Partial and Complete modes only (cf. AggUtils).</p> <p><code>AggregationIterator</code> initializes predicates with <code>AggregateExpression</code>s with filters when requested to generateProcessRow.</p>","text":""},{"location":"expressions/AggregateExpression/#aggregatemode","title":"AggregateMode <p><code>AggregateExpression</code> is given an <code>AggregateMode</code> when created.</p> <ul> <li> <p>For <code>PartialMerge</code> or <code>Final</code> modes, the input to the AggregateFunction is immutable input aggregation buffers, and the actual children of the <code>AggregateFunction</code> are not used</p> </li> <li> <p>AggregateExpressions of an AggregationIterator cannot have more than 2 distinct modes nor the modes be among <code>Partial</code> and <code>PartialMerge</code> or <code>Final</code> and <code>Complete</code> mode pairs</p> </li> <li> <p><code>Partial</code> and <code>Complete</code> or <code>PartialMerge</code> and <code>Final</code> pairs are supported</p> </li> </ul>","text":""},{"location":"expressions/AggregateExpression/#complete","title":"Complete <p>No prefix (in toString)</p> <p>Used when:</p> <ul> <li><code>ObjectAggregationIterator</code> is requested for the mergeAggregationBuffers</li> <li><code>TungstenAggregationIterator</code> is requested for the switchToSortBasedAggregation</li> <li>others</li> </ul>","text":""},{"location":"expressions/AggregateExpression/#final","title":"Final <p>No prefix (in toString)</p>","text":""},{"location":"expressions/AggregateExpression/#partial","title":"Partial <ul> <li>Partial aggregation</li> <li><code>partial_</code> prefix (in toString)</li> </ul>","text":""},{"location":"expressions/AggregateExpression/#partialmerge","title":"PartialMerge <ul> <li><code>merge_</code> prefix (in toString)</li> </ul>","text":""},{"location":"expressions/AggregateExpression/#creating-aggregateexpression","title":"Creating AggregateExpression <pre><code>apply(\n  aggregateFunction: AggregateFunction,\n  mode: AggregateMode,\n  isDistinct: Boolean,\n  filter: Option[Expression] = None): AggregateExpression\n</code></pre> <p><code>apply</code> creates an AggregateExpression with a new autogenerated <code>ExprId</code>.</p> <p><code>apply</code> is used when:</p> <ul> <li><code>AggregateFunction</code> is requested to toAggregateExpression</li> <li>others</li> </ul>","text":""},{"location":"expressions/AggregateExpression/#human-friendly-textual-representation","title":"Human-Friendly Textual Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code> returns the following text:</p> <pre><code>[prefix][name]([args]) FILTER (WHERE [predicate])\n</code></pre> <p><code>toString</code> converts the mode to a prefix.</p>    mode prefix     Partial <code>partial_</code>   PartialMerge <code>merge_</code>   Final or Complete (empty)    <p><code>toString</code> requests the AggregateFunction for the toAggString (with the isDistinct flag).</p> <p>In the end, <code>toString</code> adds <code>FILTER (WHERE [predicate])</code> based on the optional filter expression.</p>","text":""},{"location":"expressions/AggregateFunction/","title":"AggregateFunction Expressions","text":"<p><code>AggregateFunction</code> is an extension of the Expression abstraction for aggregate functions.</p> <p><code>AggregateFunction</code> can never be foldable.</p>"},{"location":"expressions/AggregateFunction/#contract","title":"Contract","text":""},{"location":"expressions/AggregateFunction/#aggregation-buffer-attributes","title":"Aggregation Buffer Attributes <pre><code>aggBufferAttributes: Seq[AttributeReference]\n</code></pre> <p>See TypedImperativeAggregate</p>","text":""},{"location":"expressions/AggregateFunction/#aggregation-buffer-schema","title":"Aggregation Buffer Schema <pre><code>aggBufferSchema: StructType\n</code></pre> <p>Schema of an aggregation buffer with partial aggregate results</p> <p>Used when:</p> <ul> <li><code>AggregationIterator</code> is requested to initializeAggregateFunctions</li> </ul>","text":""},{"location":"expressions/AggregateFunction/#inputaggbufferattributes","title":"inputAggBufferAttributes <pre><code>inputAggBufferAttributes: Seq[AttributeReference]\n</code></pre>","text":""},{"location":"expressions/AggregateFunction/#implementations","title":"Implementations","text":"<ul> <li>DeclarativeAggregate</li> <li>ImperativeAggregate</li> <li><code>TypedAggregateExpression</code></li> </ul>"},{"location":"expressions/AggregateFunction/#foldable","title":"foldable <pre><code>foldable: Boolean\n</code></pre> <p><code>foldable</code> is part of the Expression abstraction.</p>  <p><code>foldable</code> is always <code>false</code>.</p>","text":""},{"location":"expressions/AggregateFunction/#converting-to-aggregateexpression","title":"Converting to AggregateExpression <pre><code>toAggregateExpression(): AggregateExpression // (1)\ntoAggregateExpression(\n  isDistinct: Boolean,\n  filter: Option[Expression] = None): AggregateExpression\n</code></pre> <ol> <li><code>isDistinct</code> flag is <code>false</code></li> </ol> <p><code>toAggregateExpression</code> creates an AggregateExpression for this <code>AggregateFunction</code> and <code>Complete</code> mode.</p> <pre><code>import org.apache.spark.sql.functions.collect_list\nval fn = collect_list(\"gid\")\n\nimport org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression\nval aggFn = fn.expr.asInstanceOf[AggregateExpression].aggregateFunction\n\nscala&gt; println(aggFn.numberedTreeString)\n00 collect_list('gid, 0, 0)\n01 +- 'gid\n</code></pre>  <p><code>toAggregateExpression</code> is used when:</p> <ul> <li><code>AggregateFunction</code> is requested to toAggregateExpression</li> <li><code>functions</code> utility is used to withAggregateFunction</li> <li>others</li> </ul>","text":""},{"location":"expressions/AggregateWindowFunction/","title":"AggregateWindowFunction Expressions","text":"<p><code>AggregateWindowFunction</code> is an extension of the DeclarativeAggregate and WindowFunction abstractions for aggregate window function expressions.</p>"},{"location":"expressions/AggregateWindowFunction/#implementations","title":"Implementations","text":"<ul> <li><code>NthValue</code></li> <li><code>RankLike</code></li> <li><code>RowNumberLike</code></li> <li><code>SizeBasedWindowFunction</code></li> </ul>"},{"location":"expressions/Aggregator/","title":"Aggregator Expressions","text":"<p><code>Aggregator</code> is an abstraction of typed user-defined aggregate functions (user-defined typed aggregations or UDAFs).</p> <pre><code>abstract class Aggregator[-IN, BUF, OUT]\n</code></pre> <p><code>Aggregator</code> is a <code>Serializable</code> (Java).</p> <p><code>Aggregator</code> is registered using udaf standard function.</p>"},{"location":"expressions/Aggregator/#contract","title":"Contract","text":""},{"location":"expressions/Aggregator/#bufferencoder","title":"bufferEncoder <pre><code>bufferEncoder: Encoder[BUF]\n</code></pre> <p>Used when:</p> <ul> <li><code>Aggregator</code> is requested to toColumn</li> <li><code>UserDefinedAggregator</code> is requested to scalaAggregator</li> </ul>","text":""},{"location":"expressions/Aggregator/#finish","title":"finish <pre><code>finish(\n  reduction: BUF): OUT\n</code></pre> <p>Used when:</p> <ul> <li><code>ComplexTypedAggregateExpression</code> is requested to <code>eval</code></li> <li><code>ScalaAggregator</code> is requested to eval</li> </ul>","text":""},{"location":"expressions/Aggregator/#merge","title":"merge <pre><code>merge(\n  b1: BUF,\n  b2: BUF): BUF\n</code></pre> <p>Used when:</p> <ul> <li><code>ComplexTypedAggregateExpression</code> is requested to <code>merge</code></li> <li><code>ScalaAggregator</code> is requested to merge</li> </ul>","text":""},{"location":"expressions/Aggregator/#outputencoder","title":"outputEncoder <pre><code>outputEncoder: Encoder[OUT]\n</code></pre> <p>Used when:</p> <ul> <li><code>Aggregator</code> is requested to toColumn</li> <li><code>ScalaAggregator</code> is requested for the outputEncoder</li> </ul>","text":""},{"location":"expressions/Aggregator/#reduce","title":"reduce <pre><code>reduce(\n  b: BUF,\n  a: IN): BUF\n</code></pre> <p>Used when:</p> <ul> <li><code>ComplexTypedAggregateExpression</code> is requested to <code>update</code></li> <li><code>ScalaAggregator</code> is requested to update</li> </ul>","text":""},{"location":"expressions/Aggregator/#zero","title":"zero <pre><code>zero: BUF\n</code></pre> <p>Used when:</p> <ul> <li><code>SimpleTypedAggregateExpression</code> is requested for <code>initialValues</code></li> <li><code>ComplexTypedAggregateExpression</code> is requested to <code>createAggregationBuffer</code></li> <li><code>ScalaAggregator</code> is requested to createAggregationBuffer</li> </ul>","text":""},{"location":"expressions/Aggregator/#implementations","title":"Implementations","text":"<ul> <li><code>ReduceAggregator</code></li> <li><code>TypedAverage</code></li> <li><code>TypedCount</code></li> <li><code>TypedSumDouble</code></li> <li><code>TypedSumLong</code></li> </ul>"},{"location":"expressions/Aggregator/#converting-to-typedcolumn","title":"Converting to TypedColumn <pre><code>toColumn: TypedColumn[IN, OUT]\n</code></pre> <p><code>toColumn</code> converts the <code>Aggregator</code> to a TypedColumn (that can be used with Dataset.select and KeyValueGroupedDataset.agg typed operators).</p>","text":""},{"location":"expressions/Aggregator/#demo","title":"Demo <pre><code>// From Spark MLlib's org.apache.spark.ml.recommendation.ALSModel\n// Step 1. Create Aggregator\nval topKAggregator: Aggregator[Int, Int, Float] = ???\nval recs = ratings\n  .as[(Int, Int, Float)]\n  .groupByKey(_._1)\n  .agg(topKAggregator.toColumn) // &lt;-- use the custom Aggregator\n  .toDF(\"id\", \"recommendations\")\n</code></pre> <p>Use <code>org.apache.spark.sql.expressions.scalalang.typed</code> object to access the type-safe aggregate functions (i.e. <code>avg</code>, <code>count</code>, <code>sum</code> and <code>sumLong</code>).</p> <pre><code>import org.apache.spark.sql.expressions.scalalang.typed\nds.groupByKey(_._1).agg(typed.sum(_._2))\nds.select(typed.sum((i: Int) =&gt; i))\n</code></pre>","text":""},{"location":"expressions/ArrayBasedSimpleHigherOrderFunction/","title":"ArrayBasedSimpleHigherOrderFunction","text":"<p><code>ArrayBasedSimpleHigherOrderFunction</code> is...FIXME</p>"},{"location":"expressions/ArrayFilter/","title":"ArrayFilter Expression","text":"<p><code>ArrayFilter</code> is a ArrayBasedSimpleHigherOrderFunction with CodegenFallback.</p>"},{"location":"expressions/ArrayFilter/#creating-instance","title":"Creating Instance","text":"<p><code>ArrayFilter</code> takes the following to be created:</p> <ul> <li> Argument Expression <li> Function Expression <p><code>ArrayFilter</code> is created for the following functions:</p> <ul> <li>filter standard function (Scala)</li> <li>filter function (SQL)</li> </ul>"},{"location":"expressions/ArrayFilter/#demo","title":"Demo","text":""},{"location":"expressions/ArrayFilter/#scala","title":"Scala","text":"<pre><code>import org.apache.spark.sql.Column\nval even: (Column =&gt; Column) = x =&gt; x % 2 === 1\nval filter_collect = filter(collect_set(\"id\") as \"ids\", even) as \"evens\"\n</code></pre> <pre><code>val q = spark.range(5).groupBy($\"id\" % 2 as \"gid\").agg(filter_collect)\n</code></pre> <pre><code>scala&gt; q.show\n+---+------+\n|gid| evens|\n+---+------+\n|  0|    []|\n|  1|[1, 3]|\n+---+------+\n</code></pre>"},{"location":"expressions/ArrayFilter/#sql","title":"SQL","text":"<pre><code>spark.range(5).createOrReplaceTempView(\"nums\")\n</code></pre> <pre><code>val q = sql(\"\"\"\nSELECT id % 2 gid, filter(collect_set(id), x -&gt; x % 2 == 1) evens\nFROM nums\nGROUP BY id % 2\n\"\"\")\n</code></pre> <pre><code>scala&gt; q.show\n+---+------+\n|gid| evens|\n+---+------+\n|  0|    []|\n|  1|[1, 3]|\n+---+------+\n</code></pre>"},{"location":"expressions/Attribute/","title":"Attribute Expressions","text":"<p><code>Attribute</code> is an extension of the LeafExpression and NamedExpression abstractions for named leaf expressions.</p> <p>Attributes are used by <code>QueryPlan</code> to build the schema of the structured query (it represents).</p>"},{"location":"expressions/Attribute/#contract","title":"Contract","text":""},{"location":"expressions/Attribute/#newinstance","title":"newInstance <pre><code>newInstance(): Attribute\n</code></pre>  <p>Note</p> <p><code>newInstance</code> is part of the NamedExpression abstraction but changes the return type to <code>Attribute</code> (from the base <code>NamedExpression</code>).</p>","text":""},{"location":"expressions/Attribute/#withdatatype","title":"withDataType <pre><code>withDataType(\n  newType: DataType): Attribute\n</code></pre>","text":""},{"location":"expressions/Attribute/#withexprid","title":"withExprId <pre><code>withExprId(\n  newExprId: ExprId): Attribute\n</code></pre>","text":""},{"location":"expressions/Attribute/#withmetadata","title":"withMetadata <pre><code>withMetadata(\n  newMetadata: Metadata): Attribute\n</code></pre>","text":""},{"location":"expressions/Attribute/#withname","title":"withName <pre><code>withName(\n  newName: String): Attribute\n</code></pre>","text":""},{"location":"expressions/Attribute/#withnullability","title":"withNullability <pre><code>withNullability(\n  newNullability: Boolean): Attribute\n</code></pre>","text":""},{"location":"expressions/Attribute/#withqualifier","title":"withQualifier <pre><code>withQualifier(\n  newQualifier: Seq[String]): Attribute\n</code></pre>","text":""},{"location":"expressions/Attribute/#implementations","title":"Implementations","text":"<ul> <li><code>AttributeReference</code></li> <li><code>PrettyAttribute</code></li> <li>UnresolvedAttribute</li> </ul>"},{"location":"expressions/AttributeSeq/","title":"AttributeSeq","text":"<p><code>AttributeSeq</code> is an implicit class (Scala) of Seq[Attribute] type.</p> <p><code>AttributeSeq</code> is a <code>Serializable</code> (Java).</p>"},{"location":"expressions/AttributeSeq/#creating-instance","title":"Creating Instance","text":"<p><code>AttributeSeq</code> takes the following to be created:</p> <ul> <li> Attributes"},{"location":"expressions/AttributeSeq/#resolving-attribute-names-to-namedexpressions","title":"Resolving Attribute Names (to NamedExpressions) <pre><code>resolve(\n  nameParts: Seq[String],\n  resolver: Resolver): Option[NamedExpression]\n</code></pre> <p><code>resolve</code> resolves the given <code>nameParts</code> to a NamedExpression.</p> <p><code>resolve</code> branches off based on whether the number of the qualifier parts of all the Attributes is 3 or less or not (more than 3).</p> <p><code>resolve</code> can return:</p> <ul> <li><code>Alias</code> with <code>ExtractValue</code>s for nested fields</li> <li>NamedExpression if there were no nested fields to resolve</li> <li><code>None</code> (undefined value) for no candidate</li> </ul> <p><code>resolve</code> throws an <code>AnalysisException</code> for more candidates:</p> <pre><code>Reference '[name]' is ambiguous, could be: [referenceNames].\n</code></pre> <p><code>resolve</code>\u00a0is used when:</p> <ul> <li><code>LateralJoin</code> is requested to <code>resolveChildren</code></li> <li><code>LogicalPlan</code> is requested to resolveChildren and resolve</li> </ul>","text":""},{"location":"expressions/BasePredicate/","title":"BasePredicate Expressions","text":"<p><code>BasePredicate</code> is an abstraction of predicate expressions that can be evaluated to a <code>Boolean</code> value.</p> <p><code>BasePredicate</code> is created using Predicate.create utility.</p>"},{"location":"expressions/BasePredicate/#contract","title":"Contract","text":""},{"location":"expressions/BasePredicate/#evaluating","title":"Evaluating <pre><code>eval(\n  r: InternalRow): Boolean\n</code></pre>","text":""},{"location":"expressions/BasePredicate/#initializing","title":"Initializing <pre><code>initialize(\n  partitionIndex: Int): Unit\n</code></pre>","text":""},{"location":"expressions/BasePredicate/#implementations","title":"Implementations","text":"<ul> <li><code>InterpretedPredicate</code></li> </ul>"},{"location":"expressions/BinaryComparison/","title":"BinaryComparison Expressions","text":"<p><code>BinaryComparison</code> is an extension of the <code>BinaryOperator</code> and Predicate abstractions for comparison expressions.</p>"},{"location":"expressions/BinaryComparison/#implementations","title":"Implementations","text":"<ul> <li>EqualNullSafe</li> <li>EqualTo</li> <li><code>GreaterThan</code></li> <li><code>GreaterThanOrEqual</code></li> <li><code>LessThan</code></li> <li>LessThanOrEqual</li> </ul>"},{"location":"expressions/BinaryOperator/","title":"BinaryOperator","text":"<p><code>BinaryOperator</code> is...FIXME</p>"},{"location":"expressions/BloomFilterAggregate/","title":"BloomFilterAggregate Expression","text":"<p><code>BloomFilterAggregate</code> is a TypedImperativeAggregate expression that uses BloomFilter for an aggregation buffer.</p>"},{"location":"expressions/BloomFilterAggregate/#creating-instance","title":"Creating Instance","text":"<p><code>BloomFilterAggregate</code> takes the following to be created:</p> <ul> <li> Child Expression <li>Estimated Number of Items</li> <li>Number of Bits</li> <li> Mutable Agg Buffer Offset (default: <code>0</code>) <li> Input Agg Buffer Offset (default: <code>0</code>) <p><code>BloomFilterAggregate</code> is created when:</p> <ul> <li><code>InjectRuntimeFilter</code> logical optimization is requested to inject a BloomFilter</li> </ul>"},{"location":"expressions/BloomFilterAggregate/#estimated-number-of-items-expression","title":"Estimated Number of Items Expression <p><code>BloomFilterAggregate</code> can be given Estimated Number of Items (as an Expression) when created.</p> <p>Unless given, <code>BloomFilterAggregate</code> uses spark.sql.optimizer.runtime.bloomFilter.expectedNumItems configuration property.</p>","text":""},{"location":"expressions/BloomFilterAggregate/#number-of-bits-expression","title":"Number of Bits Expression <p><code>BloomFilterAggregate</code> can be given Number of Bits (as an Expression) when created.</p> <p>The number of bits expression must be a constant literal (i.e., foldable) that evaluates to a long value.</p> <p>The maximum value for the number of bits is spark.sql.optimizer.runtime.bloomFilter.maxNumBits configuration property.</p> <p>The number of bits expression is the third expression (in this <code>TernaryLike</code> tree node).</p>","text":""},{"location":"expressions/BloomFilterAggregate/#number-of-bits","title":"Number of Bits <pre><code>numBits: Long\n</code></pre>  Lazy Value <p><code>numBits</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>BloomFilterAggregate</code> defines <code>numBits</code> value to be either the value of the numBitsExpression (after evaluating it to a number) or spark.sql.optimizer.runtime.bloomFilter.maxNumBits, whatever smaller.</p> <p>The <code>numBits</code> value must be a positive value.</p> <p><code>numBits</code> is used to create an aggregation buffer.</p>","text":""},{"location":"expressions/BloomFilterAggregate/#creating-aggregation-buffer","title":"Creating Aggregation Buffer <pre><code>createAggregationBuffer(): BloomFilter\n</code></pre> <p><code>createAggregationBuffer</code> is part of the TypedImperativeAggregate abstraction.</p>  <p><code>createAggregationBuffer</code> creates a BloomFilter (with the estimated number of items and the number of bits).</p>","text":""},{"location":"expressions/BloomFilterAggregate/#interpreted-execution","title":"Interpreted Execution <pre><code>eval(\n  buffer: BloomFilter): Any\n</code></pre> <p><code>eval</code> is part of the TypedImperativeAggregate abstraction.</p>  <p><code>eval</code>...FIXME</p>","text":""},{"location":"expressions/BloomFilterMightContain/","title":"BloomFilterMightContain Expression","text":"<p><code>BloomFilterMightContain</code> is a BinaryExpression.</p>"},{"location":"expressions/BloomFilterMightContain/#creating-instance","title":"Creating Instance","text":"<p><code>BloomFilterMightContain</code> takes the following to be created:</p> <ul> <li> Bloom Filter Expression <li> Value Expression <p><code>BloomFilterMightContain</code> is created when:</p> <ul> <li>InjectRuntimeFilter logical optimization is executed (and injects a BloomFilter)</li> </ul>"},{"location":"expressions/BoundReference/","title":"BoundReference","text":"<p><code>BoundReference</code> is a leaf expression that evaluates to a value (of the given data type) that is at the specified position in the given InternalRow.</p>"},{"location":"expressions/BoundReference/#creating-instance","title":"Creating Instance","text":"<p><code>BoundReference</code> takes the following to be created:</p> <ul> <li> Position (ordinal) <li> Data type of values <li> <code>nullable</code> flag <p><code>BoundReference</code> is created when:</p> <ul> <li><code>Encoders</code> utility is used to create a generic ExpressionEncoder</li> <li><code>ScalaReflection</code> utility is used to serializerForType</li> <li><code>ExpressionEncoder</code> utility is used to tuple</li> <li><code>RowEncoder</code> utility is used to create a RowEncoder</li> <li><code>BindReferences</code> utility is used to bind an AttributeReference</li> <li><code>UnsafeProjection</code> utility is used to create an UnsafeProjection</li> <li>others</li> </ul>"},{"location":"expressions/BoundReference/#code-generated-expression-evaluation","title":"Code-Generated Expression Evaluation <pre><code>doGenCode(\n  ctx: CodegenContext,\n  ev: ExprCode): ExprCode\n</code></pre> <p><code>doGenCode</code> is part of the Expression abstraction.</p>  <p><code>doGenCode</code>...FIXME</p>  <pre><code>import org.apache.spark.sql.catalyst.expressions.BoundReference\nimport org.apache.spark.sql.types.LongType\nval boundRef = BoundReference(ordinal = 0, dataType = LongType, nullable = true)\n\n// doGenCode is used when Expression.genCode is executed\n\nimport org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\nval ctx = new CodegenContext\nval code = boundRef.genCode(ctx).code\n</code></pre> <pre><code>scala&gt; println(code)\nboolean isNull_0 = i.isNullAt(0);\nlong value_0 = isNull_0 ?\n  -1L : (i.getLong(0));\n</code></pre>","text":""},{"location":"expressions/BoundReference/#interpreted-expression-evaluation","title":"Interpreted Expression Evaluation <pre><code>eval(\n  input: InternalRow): Any\n</code></pre> <p><code>eval</code> is part of the Expression abstraction.</p>  <p><code>eval</code> gives the value at position from the given InternalRow.</p>  <p><code>eval</code> returns <code>null</code> if the value at the position is <code>null</code>. Otherwise, <code>eval</code> uses the methods of <code>InternalRow</code> per the defined data type to access the value.</p>","text":""},{"location":"expressions/BoundReference/#string-representation","title":"String Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code> is part of the Expression abstraction.</p>  <p><code>toString</code> is the following text:</p> <pre><code>input[[ordinal], [dataType], [nullable]]\n</code></pre>","text":""},{"location":"expressions/BoundReference/#catalyst-dsl","title":"Catalyst DSL <p>Catalyst DSL's at can be used to create a <code>BoundReference</code>.</p> <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\nval boundRef = 'id.string.at(4)\n\nimport org.apache.spark.sql.catalyst.expressions.BoundReference\nassert(boundRef.isInstanceOf[BoundReference])\n</code></pre>","text":""},{"location":"expressions/BoundReference/#demo","title":"Demo <pre><code>import org.apache.spark.sql.catalyst.expressions.BoundReference\nimport org.apache.spark.sql.types.LongType\nval boundRef = BoundReference(ordinal = 0, dataType = LongType, nullable = true)\n</code></pre> <pre><code>scala&gt; println(boundRef)\ninput[0, bigint, true]\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.InternalRow\nval row = InternalRow(1L, \"hello\")\nval value = boundRef.eval(row).asInstanceOf[Long]\nassert(value == 1L)\n</code></pre>","text":""},{"location":"expressions/CallMethodViaReflection/","title":"CallMethodViaReflection Expression","text":"<p><code>CallMethodViaReflection</code> is an Expression.md[expression] that represents a static method call in Scala or Java using <code>reflect</code> and <code>java_method</code> functions.</p> <p>NOTE: <code>reflect</code> and <code>java_method</code> functions are only supported in SparkSession.md#sql[SQL] and spark-sql-dataset-operators.md#selectExpr[expression] modes.</p> <p>.CallMethodViaReflection's DataType to JVM Types Mapping [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | DataType | JVM Type</p> <p>| <code>BooleanType</code> | <code>java.lang.Boolean</code> / <code>scala.Boolean</code></p> <p>| <code>ByteType</code> | <code>java.lang.Byte</code> / <code>Byte</code></p> <p>| <code>ShortType</code> | <code>java.lang.Short</code> / <code>Short</code></p> <p>| <code>IntegerType</code> | <code>java.lang.Integer</code> / <code>Int</code></p> <p>| <code>LongType</code> | <code>java.lang.Long</code> / <code>Long</code></p> <p>| <code>FloatType</code> | <code>java.lang.Float</code> / <code>Float</code></p> <p>| <code>DoubleType</code> | <code>java.lang.Double</code> / <code>Double</code></p> <p>| <code>StringType</code> | <code>String</code> |===</p>"},{"location":"expressions/CallMethodViaReflection/#source-scala","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection import org.apache.spark.sql.catalyst.expressions.Literal scala&gt; val expr = CallMethodViaReflection(      |   Literal(\"java.time.LocalDateTime\") ::      |   Literal(\"now\") :: Nil) expr: org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection = reflect(java.time.LocalDateTime, now) scala&gt; println(expr.numberedTreeString) 00 reflect(java.time.LocalDateTime, now) 01 :- java.time.LocalDateTime 02 +- now</p> <p>// CallMethodViaReflection as the expression for reflect SQL function val q = \"\"\"   select reflect(\"java.time.LocalDateTime\", \"now\") as now   \"\"\" val plan = spark.sql(q).queryExecution.logical // CallMethodViaReflection shows itself under \"reflect\" name scala&gt; println(plan.numberedTreeString) 00 Project [reflect(java.time.LocalDateTime, now) AS now#39] 01 +- OneRowRelation$</p> <p><code>CallMethodViaReflection</code> supports a Expression.md#CodegenFallback[fallback mode for expression code generation].</p> <p>[[properties]] .CallMethodViaReflection's Properties [width=\"100%\",cols=\"1,2\",options=\"header\"] |=== | Property | Description</p> <p>| [[dataType]] <code>dataType</code> | <code>StringType</code></p> <p>| [[deterministic]] <code>deterministic</code> | Disabled (i.e. <code>false</code>)</p> <p>| [[nullable]] <code>nullable</code> | Enabled (i.e. <code>true</code>)</p> <p>| [[prettyName]] <code>prettyName</code> | <code>reflect</code> |===</p> <p>NOTE: <code>CallMethodViaReflection</code> is very similar to spark-sql-Expression-StaticInvoke.md[StaticInvoke] expression.</p>"},{"location":"expressions/CodeGeneratorWithInterpretedFallback/","title":"CodeGeneratorWithInterpretedFallback Generators","text":"<p><code>CodeGeneratorWithInterpretedFallback</code> is an abstraction of codegen object generators that can create objects for codegen and interpreted evaluation paths.</p>"},{"location":"expressions/CodeGeneratorWithInterpretedFallback/#type-constructor","title":"Type Constructor","text":"<p><code>CodeGeneratorWithInterpretedFallback</code> is a Scala type constructor (generic class) with <code>IN</code> and <code>OUT</code> type aliases.</p> <pre><code>CodeGeneratorWithInterpretedFallback[IN, OUT]\n</code></pre>"},{"location":"expressions/CodeGeneratorWithInterpretedFallback/#contract","title":"Contract","text":""},{"location":"expressions/CodeGeneratorWithInterpretedFallback/#createcodegeneratedobject","title":"createCodeGeneratedObject <pre><code>createCodeGeneratedObject(\n  in: IN): OUT\n</code></pre>","text":""},{"location":"expressions/CodeGeneratorWithInterpretedFallback/#createinterpretedobject","title":"createInterpretedObject <pre><code>createInterpretedObject(\n  in: IN): OUT\n</code></pre>","text":""},{"location":"expressions/CodeGeneratorWithInterpretedFallback/#implementations","title":"Implementations","text":"<ul> <li><code>MutableProjection</code></li> <li>Predicate</li> <li>RowOrdering</li> <li><code>SafeProjection</code></li> <li>UnsafeProjection</li> </ul>"},{"location":"expressions/CodeGeneratorWithInterpretedFallback/#creating-object","title":"Creating Object <pre><code>createObject(\n  in: IN): OUT\n</code></pre> <p><code>createObject</code> createCodeGeneratedObject.</p> <p>In case of a non-fatal exception, <code>createObject</code> prints out the following WARN message to the logs and createInterpretedObject.</p> <pre><code>Expr codegen error and falling back to interpreter mode\n</code></pre> <p><code>createObject</code> is used when:</p> <ul> <li><code>MutableProjection</code> utility is used to create a <code>MutableProjection</code></li> <li><code>Predicate</code> utility is used to create a BasePredicate</li> <li><code>RowOrdering</code> utility is used to create a BaseOrdering</li> <li><code>SafeProjection</code> utility is used to create a <code>Projection</code></li> <li><code>UnsafeProjection</code> utility is used to create a UnsafeProjection</li> </ul>","text":""},{"location":"expressions/CodegenFallback/","title":"CodegenFallback Expressions","text":"<p><code>CodegenFallback</code> is an extension of the Expression abstraction for Catalyst expressions that do not support Java code generation and fall back to interpreted mode (aka fallback mode).</p> <p><code>CodegenFallback</code> is used when:</p> <ul> <li>CollapseCodegenStages physical optimization is executed (and enforce whole-stage codegen requirements for Catalyst expressions)</li> <li><code>Generator</code> expressions is requested to supportCodegen</li> <li><code>EquivalentExpressions</code> is requested to childrenToRecurse and commonChildrenToRecurse</li> </ul>"},{"location":"expressions/CodegenFallback/#implementations","title":"Implementations","text":"<ul> <li>ArrayFilter</li> <li>CallMethodViaReflection</li> <li>ImperativeAggregate</li> <li>JsonToStructs</li> <li>others</li> </ul>"},{"location":"expressions/CodegenFallback/#generating-java-source-code","title":"Generating Java Source Code <pre><code>doGenCode(\n  ctx: CodegenContext,\n  ev: ExprCode): ExprCode\n</code></pre> <p><code>doGenCode</code> is part of the Expression abstraction.</p>  <p><code>doGenCode</code> requests the input <code>CodegenContext</code> to add itself to the references.</p> <p><code>doGenCode</code> walks down the expression tree to find Nondeterministic expressions. For every <code>Nondeterministic</code> expression, <code>doGenCode</code> does the following:</p> <ol> <li> <p>Requests the input <code>CodegenContext</code> to add it to the references</p> </li> <li> <p>Requests the input <code>CodegenContext</code> to addPartitionInitializationStatement that is a Java code block as follows:</p> <pre><code>((Nondeterministic) references[[childIndex]])\n  .initialize(partitionIndex);\n</code></pre> </li> </ol> <p>In the end, <code>doGenCode</code> generates a plain Java source code block that is one of the following code blocks per the nullable flag. <code>doGenCode</code> copies the input <code>ExprCode</code> with the code block added (as the <code>code</code> property).</p> <code>nullable</code> enabled<code>nullable</code> disabled   <pre><code>[placeHolder]\nObject [objectTerm] = ((Expression) references[[idx]]).eval([input]);\nboolean [isNull] = [objectTerm] == null;\n[javaType] [value] = [defaultValue];\nif (![isNull]) {\n  [value] = ([boxedType]) [objectTerm];\n}\n</code></pre>   <pre><code>[placeHolder]\nObject [objectTerm] = ((Expression) references[[idx]]).eval([input]);\n[javaType] [value] = ([boxedType]) [objectTerm];\n</code></pre>","text":""},{"location":"expressions/CodegenFallback/#demo-currenttimestamp-with-nullable-disabled","title":"Demo: CurrentTimestamp with nullable disabled <pre><code>import org.apache.spark.sql.catalyst.expressions.CurrentTimestamp\nval currTimestamp = CurrentTimestamp()\n\nimport org.apache.spark.sql.catalyst.expressions.codegen.CodegenFallback\nassert(currTimestamp.isInstanceOf[CodegenFallback], \"CurrentTimestamp should be a CodegenFallback\")\n\nassert(currTimestamp.nullable == false, \"CurrentTimestamp should not be nullable\")\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, ExprCode}\nval ctx = new CodegenContext\n// doGenCode is used when Expression.genCode is executed\nval ExprCode(code, _, _) = currTimestamp.genCode(ctx)\n</code></pre> <pre><code>println(code)\n</code></pre> <pre><code>Object obj_0 = ((Expression) references[0]).eval(null);\n        long value_0 = (Long) obj_0;\n</code></pre>","text":""},{"location":"expressions/Collect/","title":"Collect Expressions","text":"<p><code>Collect</code> is...FIXME</p>"},{"location":"expressions/CollectSet/","title":"CollectSet Expression","text":"<p><code>CollectSet</code> is a Collect expression (with a mutable.HashSet[Any]] aggregation buffer).</p> <p>CollectSet and Scala's <code>HashSet</code></p> <p>It's fair to say that <code>CollectSet</code> is merely a Spark SQL-enabled Scala mutable.HashSet[Any]].</p>"},{"location":"expressions/CollectSet/#creating-instance","title":"Creating Instance","text":"<p><code>CollectSet</code> takes the following to be created:</p> <ul> <li> Child Expression <li> <code>mutableAggBufferOffset</code> (default: <code>0</code>) <li> <code>inputAggBufferOffset</code> (default: <code>0</code>) <p><code>CollectSet</code> is created when:</p> <ul> <li>Catalyst DSL's collectSet is used</li> <li>collect_set standard function is used</li> <li>collect_set SQL function is used</li> </ul>"},{"location":"expressions/CollectSet/#pretty-name","title":"Pretty Name <pre><code>prettyName: String\n</code></pre> <p><code>prettyName</code> is part of the Expression abstraction.</p>  <p><code>prettyName</code> is <code>collect_set</code>.</p>","text":""},{"location":"expressions/CollectSet/#creating-aggregation-buffer","title":"Creating Aggregation Buffer <pre><code>createAggregationBuffer(): mutable.HashSet[Any]\n</code></pre> <p><code>createAggregationBuffer</code> is part of the TypedImperativeAggregate abstraction.</p>  <p><code>createAggregationBuffer</code> creates an empty <code>mutable.HashSet</code> (Scala).</p>","text":""},{"location":"expressions/CollectSet/#interpreted-execution","title":"Interpreted Execution <pre><code>eval(\n  buffer: mutable.HashSet[Any]): Any\n</code></pre> <p><code>eval</code> is part of the TypedImperativeAggregate abstraction.</p>  <p><code>eval</code> creates a <code>GenericArrayData</code> with an array based on the DataType of the child expression:</p> <ul> <li>For <code>BinaryType</code>, <code>eval</code>...FIXME</li> <li>Otherwise, <code>eval</code>...FIXME</li> </ul>","text":""},{"location":"expressions/CollectSet/#eliminatedistinct-logical-optimization","title":"EliminateDistinct Logical Optimization <p><code>CollectSet</code> is <code>isDuplicateAgnostic</code> per <code>EliminateDistinct</code> logical optimization.</p>","text":""},{"location":"expressions/CreateNamedStruct/","title":"CreateNamedStruct","text":"<p><code>CreateNamedStruct</code> is an Expression.</p>"},{"location":"expressions/CreateNamedStruct/#creating-instance","title":"Creating Instance","text":"<p><code>CreateNamedStruct</code> takes the following to be created:</p> <ul> <li> Child Expressions <p><code>CreateNamedStruct</code> is created when:</p> <ul> <li><code>SerializerBuildHelper</code> utility is used to createSerializerForObject</li> <li><code>ResolveExpressionsWithNamePlaceholders</code> logical analysis rule is executed</li> <li><code>ResolveUnion</code> logical analysis rule is executed</li> <li>Catalyst DSL's <code>namedStruct</code> operator is used</li> <li><code>ExpressionEncoder</code> is created</li> <li><code>RowEncoder</code> utility is used to create a serializer for a StructType</li> <li><code>CreateStruct</code> utility is used to create a CreateNamedStruct</li> <li><code>ObjectSerializerPruning</code> logical optimization is executed</li> <li>many many others</li> </ul>"},{"location":"expressions/CreateNamedStruct/#never-nullable","title":"Never Nullable <pre><code>nullable: Boolean\n</code></pre> <p><code>nullable</code> is always disabled (<code>false</code>).</p> <p><code>nullable</code> is part of the Expression abstraction.</p>","text":""},{"location":"expressions/CreateNamedStruct/#node-patterns","title":"Node Patterns <pre><code>nodePatterns: Seq[TreePattern]\n</code></pre> <p><code>nodePatterns</code> is CREATE_NAMED_STRUCT.</p> <p><code>nodePatterns</code> is part of the TreeNode abstraction.</p>","text":""},{"location":"expressions/CreateNamedStruct/#pretty-name","title":"Pretty Name <pre><code>prettyName: String\n</code></pre> <p><code>prettyName</code> is either function alias for the <code>functionAliasName</code> tag (if defined) or named_struct.</p> <p><code>prettyName</code> is part of the Expression abstraction.</p>","text":""},{"location":"expressions/CreateNamedStruct/#interpreted-expression-evaluation","title":"Interpreted Expression Evaluation <pre><code>eval(\n  input: InternalRow): Any\n</code></pre> <p><code>eval</code> creates an InternalRow with the result of evaluation of all the value expressions.</p> <p><code>eval</code> is part of the Expression abstraction.</p>","text":""},{"location":"expressions/CreateNamedStruct/#code-generated-expression-evaluation","title":"Code-Generated Expression Evaluation <pre><code>doGenCode(\n  ctx: CodegenContext,\n  ev: ExprCode): ExprCode\n</code></pre> <p><code>doGenCode</code> is part of the Expression abstraction.</p> <pre><code>import org.apache.spark.sql.functions.lit\nval exprs = Seq(\"a\", 1).map(lit).map(_.expr)\n\nimport org.apache.spark.sql.catalyst.dsl.expressions._\nval ns = namedStruct(exprs: _*)\n\n// doGenCode is used when Expression.genCode is executed\n\nimport org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\nval ctx = new CodegenContext\nval code = ns.genCode(ctx).code\n</code></pre> <pre><code>scala&gt; println(code)\nObject[] values_0 = new Object[1];\n\n\nif (false) {\n  values_0[0] = null;\n} else {\n  values_0[0] = 1;\n}\n\nfinal InternalRow value_0 = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(values_0);\nvalues_0 = null;\n</code></pre>","text":""},{"location":"expressions/CreateNamedStruct/#functionregistry","title":"FunctionRegistry <p><code>CreateNamedStruct</code> is registered in FunctionRegistry under the name of <code>named_struct</code> SQL function.</p> <pre><code>import org.apache.spark.sql.catalyst.FunctionIdentifier\nval fid = FunctionIdentifier(funcName = \"named_struct\")\nval className = spark.sessionState.functionRegistry.lookupFunction(fid).get.getClassName\n</code></pre> <pre><code>scala&gt; println(className)\norg.apache.spark.sql.catalyst.expressions.CreateNamedStruct\n</code></pre> <pre><code>val q = sql(\"SELECT named_struct('id', 0)\")\n// analyzed so the function is resolved already (using FunctionRegistry)\nval analyzedPlan = q.queryExecution.analyzed\n</code></pre> <pre><code>scala&gt; println(analyzedPlan.numberedTreeString)\n00 Project [named_struct(id, 0) AS named_struct(id, 0)#7]\n01 +- OneRowRelation\n</code></pre> <pre><code>val e = analyzedPlan.expressions.head.children.head\nimport org.apache.spark.sql.catalyst.expressions.CreateNamedStruct\nassert(e.isInstanceOf[CreateNamedStruct])\n</code></pre>","text":""},{"location":"expressions/CreateNamedStruct/#catalyst-dsl","title":"Catalyst DSL <p>Catalyst DSL defines namedStruct operator to create a <code>CreateNamedStruct</code> expression.</p> <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\nval s = namedStruct()\n\nimport org.apache.spark.sql.catalyst.expressions.CreateNamedStruct\nassert(s.isInstanceOf[CreateNamedStruct])\n\nval s = namedStruct(\"*\")\n</code></pre> <pre><code>scala&gt; println(s)\nnamed_struct(*)\n</code></pre>","text":""},{"location":"expressions/CreateStruct/","title":"CreateStruct Function Builder","text":"<p><code>CreateStruct</code> is a function builder (e.g. <code>Seq[Expression] =&gt; Expression</code>) that can &lt;&gt; and is the &lt;&gt; of the &lt;&gt; function. <p>=== [[apply]] Creating CreateNamedStruct Expression -- <code>apply</code> Method</p>"},{"location":"expressions/CreateStruct/#source-scala","title":"[source, scala]","text":""},{"location":"expressions/CreateStruct/#applychildren-seqexpression-createnamedstruct","title":"apply(children: Seq[Expression]): CreateNamedStruct","text":"<p>NOTE: <code>apply</code> is part of Scala's https://www.scala-lang.org/api/2.11.12/index.html#scala.Function1[scala.Function1] contract to create a function of one parameter (e.g. <code>Seq[Expression]</code>).</p> <p><code>apply</code> creates a &lt;&gt; expression with the input <code>children</code> &lt;&gt; as follows: <ul> <li> <p>For &lt;&gt; expressions that are &lt;&gt;, <code>apply</code> creates a pair of a &lt;&gt; expression (with the &lt;&gt; of the <code>NamedExpression</code>) and the <code>NamedExpression</code> itself <li> <p>For &lt;&gt; expressions that are not &lt;&gt; yet, <code>apply</code> creates a pair of a <code>NamePlaceholder</code> expression and the <code>NamedExpression</code> itself <li> <p>For all other &lt;&gt;, <code>apply</code> creates a pair of a &lt;&gt; expression (with the value as <code>col[index]</code>) and the <code>Expression</code> itself <p><code>apply</code> is used when:</p> <ul> <li> <p><code>ResolveReferences</code> logical resolution rule is requested to expandStarExpression</p> </li> <li> <p><code>InConversion</code> type coercion rule is requested to <code>coerceTypes</code></p> </li> <li> <p><code>ExpressionEncoder</code> is requested to create an ExpressionEncoder for a tuple</p> </li> <li> <p><code>Stack</code> generator expression is requested to generate a Java source code</p> </li> <li> <p><code>AstBuilder</code> is requested to parse a struct and row constructor</p> </li> <li> <p><code>ColumnStat</code> is requested to statExprs</p> </li> <li> <p><code>KeyValueGroupedDataset</code> is requested to aggUntyped (when KeyValueGroupedDataset.agg typed operator is used)</p> </li> <li> <p>&lt;&gt; typed transformation is used <li> <p>struct standard function is used</p> </li> <li> <p><code>SimpleTypedAggregateExpression</code> expression is requested <code>evaluateExpression</code> and <code>resultObjToRow</code></p> </li>"},{"location":"expressions/CumeDist/","title":"CumeDist","text":"<p><code>CumeDist</code> is a <code>SizeBasedWindowFunction</code> and a <code>RowNumberLike</code> expression that is used for the following:</p> <ul> <li> <p>cume_dist standard function</p> </li> <li> <p>cume_dist SQL function</p> </li> </ul>"},{"location":"expressions/CumeDist/#demo","title":"Demo","text":"<pre><code>import org.apache.spark.sql.catalyst.expressions.CumeDist\nval cume_dist = CumeDist()\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.expressions.CumeDist\nval cume_dist = CumeDist()\nscala&gt; println(cume_dist)\ncume_dist()\n</code></pre> <pre><code>scala&gt; println(cume_dist.evaluateExpression.numberedTreeString)\n00 (cast(rowNumber#0 as double) / cast(window__partition__size#1 as double))\n01 :- cast(rowNumber#0 as double)\n02 :  +- rowNumber#0: int\n03 +- cast(window__partition__size#1 as double)\n04    +- window__partition__size#1: int\n</code></pre>"},{"location":"expressions/CumeDist/#prettyname","title":"prettyName <pre><code>prettyName: String\n</code></pre> <p><code>prettyName</code> is cume_dist as the user-facing name.</p> <p><code>prettyName</code> is part of the Expression abstraction.</p>","text":""},{"location":"expressions/CumeDist/#frame","title":"frame <pre><code>frame: WindowFrame\n</code></pre> <p><code>frame</code> is a <code>SpecifiedWindowFrame</code> with the following:</p> <ul> <li><code>RangeFrame</code> frame type</li> <li><code>UnboundedPreceding</code> lower frame boundary</li> <li><code>CurrentRow</code> upper frame boundary</li> </ul>  <p>Note</p> <p>The frame of a <code>CumeDist</code> expression is range-based instead of row-based, because it has to return the same value for tie values in a window (equal values per <code>ORDER BY</code> specification).</p>  <p><code>frame</code> is part of the WindowFunction abstraction.</p>","text":""},{"location":"expressions/CumeDist/#evaluateexpression","title":"evaluateExpression <pre><code>evaluateExpression: Expression\n</code></pre> <p><code>evaluateExpression</code> is part of the DeclarativeAggregate abstraction.</p>  <p><code>evaluateExpression</code> uses the formula <code>rowNumber / n</code> where <code>rowNumber</code> is the row number in a window frame (the number of values before and including the current row) divided by the number of rows in the window frame.</p>","text":""},{"location":"expressions/DeclarativeAggregate/","title":"DeclarativeAggregate Expression-Based Functions","text":"<p><code>DeclarativeAggregate</code> is an extension of the AggregateFunction abstraction for Catalyst Expression-based aggregate functions that use Catalyst Expression for evaluation.</p> <p><code>DeclarativeAggregate</code> is an Unevaluable.</p>"},{"location":"expressions/DeclarativeAggregate/#contract","title":"Contract","text":""},{"location":"expressions/DeclarativeAggregate/#evaluateexpression","title":"evaluateExpression <pre><code>evaluateExpression: Expression\n</code></pre> <p>Catalyst Expression to calculate the final value of this aggregate function</p> <p>Used when:</p> <ul> <li><code>EliminateAggregateFilter</code> logical optimization is executed</li> <li><code>AggregatingAccumulator</code> is created</li> <li><code>AggregationIterator</code> is requested for the generateResultProjection</li> <li><code>HashAggregateExec</code> physical operator is requested to doProduceWithoutKeys and generateResultFunction</li> <li><code>AggregateProcessor</code> is created</li> </ul>","text":""},{"location":"expressions/DeclarativeAggregate/#expressions-to-initialize-empty-aggregation-buffers","title":"Expressions to Initialize Empty Aggregation Buffers <pre><code>initialValues: Seq[Expression]\n</code></pre> <p>Catalyst Expressions to initialize empty aggregation buffers (for the initial values of this aggregate function)</p> <p>Used when:</p> <ul> <li><code>EliminateAggregateFilter</code> logical optimization is executed</li> <li><code>AggregatingAccumulator</code> is created</li> <li><code>AggregateCodegenSupport</code> is requested to doProduceWithoutKeys</li> <li><code>AggregationIterator</code> is created</li> <li><code>HashAggregateExec</code> physical operator is requested to createHashMap, getEmptyAggregationBuffer</li> <li><code>HashMapGenerator</code> is created</li> <li><code>AggregateProcessor</code> is created</li> </ul>","text":""},{"location":"expressions/DeclarativeAggregate/#mergeexpressions","title":"mergeExpressions <pre><code>mergeExpressions: Seq[Expression]\n</code></pre> <p>Catalyst Expressions to...FIXME</p>","text":""},{"location":"expressions/DeclarativeAggregate/#updateexpressions","title":"updateExpressions <pre><code>updateExpressions: Seq[Expression]\n</code></pre> <p>Catalyst Expressions to update the mutable aggregation buffer based on an input row</p> <p>Used when:</p> <ul> <li><code>AggregateProcessor</code> is created</li> <li><code>AggregateCodegenSupport</code> is requested to doConsumeWithoutKeys</li> <li><code>AggregationIterator</code> is requested to generateProcessRow</li> <li><code>AggregatingAccumulator</code> is created</li> <li><code>HashAggregateExec</code> is requested to doConsumeWithKeys</li> </ul>","text":""},{"location":"expressions/DeclarativeAggregate/#implementations","title":"Implementations","text":"<ul> <li>AggregateWindowFunction</li> <li>First</li> <li>others</li> </ul>"},{"location":"expressions/DecodeUsingSerializer/","title":"DecodeUsingSerializer","text":"<p><code>DecodeUsingSerializer</code> is...FIXME</p>"},{"location":"expressions/DynamicPruningExpression/","title":"DynamicPruningExpression Unary Expression","text":"<p><code>DynamicPruningExpression</code> is a UnaryExpression and a <code>DynamicPruning</code> predicate expression.</p> <p><code>DynamicPruningExpression</code> delegates interpreted and code-generated expression evaluation to the child expression.</p>"},{"location":"expressions/DynamicPruningExpression/#creating-instance","title":"Creating Instance","text":"<p><code>DynamicPruningExpression</code> takes the following to be created:</p> <ul> <li> Child expression <p><code>DynamicPruningExpression</code> is created when the following physical optimizations are executed:</p> <ul> <li>PlanAdaptiveDynamicPruningFilters</li> <li>PlanAdaptiveSubqueries</li> <li>PlanDynamicPruningFilters</li> </ul>"},{"location":"expressions/DynamicPruningExpression/#node-patterns","title":"Node Patterns  Signature <pre><code>nodePatterns: Seq[TreePattern]\n</code></pre> <p><code>nodePatterns</code> is part of the TreeNode abstraction.</p>  <p><code>nodePatterns</code> is DYNAMIC_PRUNING_EXPRESSION.</p>","text":""},{"location":"expressions/DynamicPruningExpression/#query-planning","title":"Query Planning <ul> <li>PlanAdaptiveDynamicPruningFilters</li> </ul>","text":""},{"location":"expressions/DynamicPruningSubquery/","title":"DynamicPruningSubquery Unevaluable Subquery Unary Expression","text":"<p><code>DynamicPruningSubquery</code> is a SubqueryExpression and a <code>DynamicPruning</code> predicate expression.</p> <p><code>DynamicPruningSubquery</code> is an Unevaluable expression.</p> <p><code>DynamicPruningSubquery</code> is used in the following logical and physical optimizations:</p> <ul> <li>InjectRuntimeFilter logical optimization</li> <li>InsertAdaptiveSparkPlan physical optimization</li> <li>PlanDynamicPruningFilters physical optimization</li> </ul>"},{"location":"expressions/DynamicPruningSubquery/#creating-instance","title":"Creating Instance","text":"<p><code>DynamicPruningSubquery</code> takes the following to be created:</p> <ul> <li> Pruning Key Expression <li> Build Query LogicalPlan <li> Build Keys Expressions <li> Broadcast Key Index <li> <code>onlyInBroadcast</code> Flag <li> <code>ExprId</code> (default: <code>NamedExpression.newExprId</code>) <p><code>DynamicPruningSubquery</code> is created when:</p> <ul> <li>PartitionPruning logical optimization is executed</li> </ul>"},{"location":"expressions/DynamicPruningSubquery/#adaptive-query-planning","title":"Adaptive Query Planning","text":"<p><code>DynamicPruningSubquery</code> is planned as DynamicPruningExpression (with InSubqueryExec child expression) in PlanAdaptiveSubqueries physical optimization.</p>"},{"location":"expressions/DynamicPruningSubquery/#textual-representation","title":"Textual Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code> uses the exprId and conditionString to build a textual representation:</p> <pre><code>dynamicpruning#[exprId] [conditionString]\n</code></pre>","text":""},{"location":"expressions/EncodeUsingSerializer/","title":"EncodeUsingSerializer","text":"<p><code>EncodeUsingSerializer</code> is...FIXME</p>"},{"location":"expressions/EqualNullSafe/","title":"EqualNullSafe Predicate Expression","text":"<p><code>EqualNullSafe</code> is a BinaryComparison predicate expression that represents the following high-level operators in a logical plan:</p> <ul> <li><code>&lt;=&gt;</code> SQL operator</li> <li><code>Column.&lt;=&gt;</code> operator</li> </ul>"},{"location":"expressions/EqualNullSafe/#null-safeness","title":"Null Safeness","text":"<p><code>EqualNullSafe</code> is safe for <code>null</code> values, i.e. is like EqualTo with the following \"safeness\":</p> <ul> <li><code>true</code> if both operands are <code>null</code></li> <li><code>false</code> if either operand is <code>null</code></li> </ul>"},{"location":"expressions/EqualNullSafe/#creating-instance","title":"Creating Instance","text":"<p><code>EqualNullSafe</code> takes the following to be created:</p> <ul> <li> Left Expression <li> Right Expression <p><code>EqualNullSafe</code> is created when:</p> <ul> <li><code>AstBuilder</code> is requested to parse a comparison (for <code>&lt;=&gt;</code> operator) and withPredicate</li> <li><code>Column.&lt;=&gt;</code> operator is used</li> <li>others</li> </ul>"},{"location":"expressions/EqualNullSafe/#symbol","title":"Symbol <pre><code>symbol: String\n</code></pre> <p><code>symbol</code> is part of the <code>BinaryOperator</code> abstraction.</p> <p><code>symbol</code> is <code>&lt;=&gt;</code>.</p>","text":""},{"location":"expressions/EqualNullSafe/#nullable","title":"nullable <pre><code>nullable: Boolean\n</code></pre> <p><code>nullable</code> is part of the Expression abstraction.</p> <p><code>nullable</code> is always <code>false</code>.</p>","text":""},{"location":"expressions/EqualNullSafe/#catalyst-dsl","title":"Catalyst DSL <p>Catalyst DSL defines &lt;=&gt; operator to create an <code>EqualNullSafe</code> expression.</p> <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\nval right = 'right\nval left = 'left\nval e = right &lt;=&gt; left\n</code></pre> <pre><code>scala&gt; e.explain(extended = true)\n('right &lt;=&gt; 'left)\n</code></pre> <pre><code>scala&gt; println(e.expr.sql)\n(`right` &lt;=&gt; `left`)\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.expressions.EqualNullSafe\nassert(e.expr.isInstanceOf[EqualNullSafe])\n</code></pre>","text":""},{"location":"expressions/EqualTo/","title":"EqualTo Predicate Expression","text":"<p><code>EqualTo</code> is a BinaryComparison and <code>NullIntolerant</code> predicate expression that represents the following high-level operators in a logical plan:</p> <ul> <li><code>=</code>, <code>==</code>, <code>&lt;&gt;</code>, <code>!=</code> SQL operators</li> <li><code>Column.===</code>, <code>Column.=!=</code> and <code>Column.notEqual</code> operators</li> </ul>"},{"location":"expressions/EqualTo/#creating-instance","title":"Creating Instance","text":"<p><code>EqualTo</code> takes the following to be created:</p> <ul> <li> Left Expression <li> Right Expression <p><code>EqualTo</code> is created when:</p> <ul> <li><code>AstBuilder</code> is requested to parse a comparison (for <code>=</code>, <code>==</code>, <code>&lt;&gt;</code>, <code>!=</code> operators) and visitSimpleCase</li> <li><code>Column.===</code>, <code>Column.=!=</code> and <code>Column.notEqual</code> operators are used</li> <li>others</li> </ul>"},{"location":"expressions/EqualTo/#symbol","title":"Symbol <pre><code>symbol: String\n</code></pre> <p><code>symbol</code> is part of the <code>BinaryOperator</code> abstraction.</p> <p><code>symbol</code> is <code>=</code> (equal sign).</p>","text":""},{"location":"expressions/EqualTo/#catalyst-dsl","title":"Catalyst DSL <p>Catalyst DSL defines === operator to create an <code>EqualTo</code> expression.</p> <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\nval right = 'right\nval left = 'left\nval e = right === left\n</code></pre> <pre><code>scala&gt; e.explain(extended = true)\n('right = 'left)\n</code></pre> <pre><code>scala&gt; println(e.expr.sql)\n(`right` = `left`)\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.expressions.EqualTo\nassert(e.expr.isInstanceOf[EqualTo])\n</code></pre>","text":""},{"location":"expressions/ExecSubqueryExpression-ScalarSubquery/","title":"ScalarSubquery (ExecSubqueryExpression)","text":"<p><code>ScalarSubquery</code> is an ExecSubqueryExpression that &lt;&gt; (i.e. the value of executing &lt;&gt; subquery that can result in a single row and a single column or <code>null</code> if no row were computed). <p>IMPORTANT: Spark SQL uses the name of <code>ScalarSubquery</code> twice to represent an <code>ExecSubqueryExpression</code> (this page) and a SubqueryExpression. It is confusing and you should not be anymore.</p> <p><code>ScalarSubquery</code> is &lt;&gt; when PlanSubqueries physical optimization is executed (and plans a ScalarSubquery expression)."},{"location":"expressions/ExecSubqueryExpression-ScalarSubquery/#source-scala","title":"[source, scala]","text":"<p>// FIXME DEMO import org.apache.spark.sql.execution.PlanSubqueries val spark = ... val planSubqueries = PlanSubqueries(spark) val plan = ... val executedPlan = planSubqueries(plan)</p> <p>[[Unevaluable]] <code>ScalarSubquery</code> is an unevaluable expression.</p> <p>[[dataType]] <code>ScalarSubquery</code> uses...FIXME...for the &lt;&gt;. <p>[[internal-registries]] .ScalarSubquery's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| <code>result</code> | [[result]] The value of the single column in a single row after collecting the rows from executing the &lt;&gt; or <code>null</code> if no rows were collected. <p>| <code>updated</code> | [[updated]] Flag that says whether <code>ScalarSubquery</code> was &lt;&gt; with collected result of executing the &lt;&gt;. |=== <p>=== [[creating-instance]] Creating ScalarSubquery Instance</p> <p><code>ScalarSubquery</code> takes the following when created:</p> <ul> <li>[[plan]] SubqueryExec.md[SubqueryExec] plan</li> <li>[[exprId]] Expression ID (as <code>ExprId</code>)</li> </ul> <p>=== [[updateResult]] Updating ScalarSubquery With Collected Result -- <code>updateResult</code> Method</p>"},{"location":"expressions/ExecSubqueryExpression-ScalarSubquery/#source-scala_1","title":"[source, scala]","text":""},{"location":"expressions/ExecSubqueryExpression-ScalarSubquery/#updateresult-unit","title":"updateResult(): Unit","text":"<p>NOTE: <code>updateResult</code> is part of spark-sql-Expression-ExecSubqueryExpression.md#updateResult[ExecSubqueryExpression Contract] to fill an Catalyst expression with a collected result from executing a subquery plan.</p> <p><code>updateResult</code> requests &lt;&gt; physical plan to SubqueryExec.md#executeCollect[execute and collect internal rows]. <p><code>updateResult</code> sets &lt;&gt; to the value of the only column of the single row or <code>null</code> if no row were collected. <p>In the end, <code>updateResult</code> marks the <code>ScalarSubquery</code> instance as &lt;&gt;. <p><code>updateResult</code> reports a <code>RuntimeException</code> when there are more than 1 rows in the result.</p> <pre><code>more than one row returned by a subquery used as an expression:\n[plan]\n</code></pre> <p><code>updateResult</code> reports an <code>AssertionError</code> when the number of fields is not exactly 1.</p> <pre><code>Expects 1 field, but got [numFields] something went wrong in analysis\n</code></pre> <p>=== [[eval]] Evaluating Expression -- <code>eval</code> Method</p>"},{"location":"expressions/ExecSubqueryExpression-ScalarSubquery/#source-scala_2","title":"[source, scala]","text":""},{"location":"expressions/ExecSubqueryExpression-ScalarSubquery/#evalinput-internalrow-any","title":"eval(input: InternalRow): Any","text":"<p><code>eval</code> is part of the Expression abstraction.</p> <p><code>eval</code> simply returns &lt;&gt; value. <p><code>eval</code> reports an <code>IllegalArgumentException</code> if the <code>ScalarSubquery</code> expression has not been &lt;&gt; yet. <p>=== [[doGenCode]] Generating Java Source Code (ExprCode) For Code-Generated Expression Evaluation -- <code>doGenCode</code> Method</p>"},{"location":"expressions/ExecSubqueryExpression-ScalarSubquery/#source-scala_3","title":"[source, scala]","text":""},{"location":"expressions/ExecSubqueryExpression-ScalarSubquery/#dogencodectx-codegencontext-ev-exprcode-exprcode","title":"doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode","text":"<p>NOTE: <code>doGenCode</code> is part of &lt;&gt; to generate a Java source code (ExprCode) for code-generated expression evaluation. <p><code>doGenCode</code> first makes sure that the &lt;&gt; flag is on (<code>true</code>). If not, <code>doGenCode</code> throws an <code>IllegalArgumentException</code> exception with the following message: <pre><code>requirement failed: [this] has not finished\n</code></pre> <p><code>doGenCode</code> then creates a &lt;&gt; (for the &lt;&gt; and the &lt;&gt;) and simply requests it to &lt;&gt;."},{"location":"expressions/ExecSubqueryExpression/","title":"ExecSubqueryExpression Expressions","text":"<p><code>SubqueryExpression</code> is an extension of the PlanExpression abstraction for subquery expressions with BaseSubqueryExec physical operators (for a subquery).</p>"},{"location":"expressions/ExecSubqueryExpression/#contract","title":"Contract","text":""},{"location":"expressions/ExecSubqueryExpression/#updateresult","title":"updateResult","text":"<p> <pre><code>updateResult(): Unit\n</code></pre> <p>Updates the expression with a collected result from an executed plan</p> <p><code>updateResult</code> is used when <code>SparkPlan</code> is requested to waitForSubqueries.</p>"},{"location":"expressions/ExecSubqueryExpression/#withnewplan","title":"withNewPlan","text":"<p> <pre><code>withNewPlan(\nplan: BaseSubqueryExec): ExecSubqueryExpression\n</code></pre> <p>Note</p> <p><code>withNewPlan</code> is part of the PlanExpression abstraction and is defined as follows:</p> <pre><code>withNewPlan(plan: T): PlanExpression[T]\n</code></pre> <p>The purpose of this override method is to change the input and output generic types to the concrete BaseSubqueryExec and <code>ExecSubqueryExpression</code>, respectively.</p>"},{"location":"expressions/ExecSubqueryExpression/#implementations","title":"Implementations","text":"<ul> <li>InSubqueryExec</li> <li>ScalarSubquery</li> </ul>"},{"location":"expressions/Exists/","title":"Exists \u2014 Correlated Predicate Subquery Expression","text":"<p><code>Exists</code> is a SubqueryExpression and a predicate expression.</p> <p><code>Exists</code> is &lt;&gt; when: <ul> <li> <p><code>ResolveSubquery</code> is requested to resolveSubQueries</p> </li> <li> <p><code>PullupCorrelatedPredicates</code> is requested to spark-sql-PullupCorrelatedPredicates.md#rewriteSubQueries[rewriteSubQueries]</p> </li> <li> <p><code>AstBuilder</code> is requested to sql/AstBuilder.md#visitExists[visitExists] (in SQL statements)</p> </li> </ul> <p>[[Unevaluable]] <code>Exists</code> is unevaluable expression.</p> <p>[[eval]][[doGenCode]] When requested to evaluate or <code>doGenCode</code>, <code>Exists</code> simply reports a <code>UnsupportedOperationException</code>.</p> <pre><code>Cannot evaluate expression: [this]\n</code></pre> <p>[[nullable]] <code>Exists</code> is never Expression.md#nullable[nullable].</p> <p>[[toString]] <code>Exists</code> uses the following text representation:</p> <pre><code>exists#[exprId] [conditionString]\n</code></pre> <p>[[canonicalized]] When requested for a canonicalized version, <code>Exists</code> &lt;&gt; a new instance with...FIXME"},{"location":"expressions/Exists/#creating-instance","title":"Creating Instance","text":"<p><code>Exists</code> takes the following to be created:</p> <ul> <li>[[plan]] Subquery LogicalPlan</li> <li>[[children]] Child Expressions</li> <li>[[exprId]] <code>ExprId</code></li> </ul>"},{"location":"expressions/ExpectsInputTypes/","title":"ExpectsInputTypes Expressions","text":"<p><code>ExpectsInputTypes</code> is an extension of the Expression abstraction for Catalyst Expressions that can define the expected types from child expressions.</p>"},{"location":"expressions/ExpectsInputTypes/#contract","title":"Contract","text":""},{"location":"expressions/ExpectsInputTypes/#inputtypes","title":"inputTypes <pre><code>inputTypes: Seq[AbstractDataType]\n</code></pre> <p>Expected AbstractDataTypes of the child expressions</p> <p>Used when:</p> <ul> <li><code>ImplicitTypeCasts</code> utility (as <code>TypeCoercionRule</code>) is used to<code>transform</code></li> <li><code>ExpectsInputTypes</code> is requested to checkInputDataTypes</li> </ul>","text":""},{"location":"expressions/ExpectsInputTypes/#implementations","title":"Implementations","text":"<ul> <li>HigherOrderFunction</li> <li>JsonToStructs</li> <li>others</li> </ul>"},{"location":"expressions/ExpectsInputTypes/#checkinputdatatypes","title":"checkInputDataTypes <pre><code>checkInputDataTypes(): TypeCheckResult\n</code></pre> <p><code>checkInputDataTypes</code> is part of the Expression abstraction.</p>  <p><code>checkInputDataTypes</code> checks that the expected inputTypes are the DataTypes of the evaluation of the child expressions.</p>","text":""},{"location":"expressions/ExplodeBase/","title":"ExplodeBase Base Generator Expression","text":""},{"location":"expressions/Expression/","title":"Expression","text":"<p><code>Expression</code> is an extension of the TreeNode abstraction for executable expressions (in the Catalyst Tree Manipulation Framework).</p> <pre><code>abstract class Expression\nextends TreeNode[Expression]\n</code></pre> <p><code>Expression</code> is an executable TreeNode that can be evaluated and produce a JVM object (for an InternalRow) in the faster code-generated or the slower interpreted modes.</p>"},{"location":"expressions/Expression/#contract","title":"Contract","text":""},{"location":"expressions/Expression/#evaluation-result-datatype","title":"Evaluation Result DataType <pre><code>dataType: DataType\n</code></pre> <p>The DataType of the result of evaluating this expression</p>","text":""},{"location":"expressions/Expression/#generating-java-source-code-for-code-generated-expression-evaluation","title":"Generating Java Source Code for Code-Generated Expression Evaluation <pre><code>doGenCode(\n  ctx: CodegenContext,\n  ev: ExprCode): ExprCode\n</code></pre> <p>Generates a Java source code for Whole-Stage Java Code Generation execution</p> <p>See ScalaUDF</p> <p>Used when:</p> <ul> <li><code>Expression</code> is requested to generate a Java code</li> </ul>","text":""},{"location":"expressions/Expression/#interpreted-expression-evaluation","title":"Interpreted Expression Evaluation <pre><code>eval(\n  input: InternalRow = null): Any\n</code></pre> <p>Interpreted expression evaluation that evaluates this expression to a JVM object for a given InternalRow (and skipping generating a corresponding Java code)</p> <p><code>eval</code> is a slower \"relative\" of the code-generated expression evaluation</p>","text":""},{"location":"expressions/Expression/#nullable","title":"nullable <pre><code>nullable: Boolean\n</code></pre>","text":""},{"location":"expressions/Expression/#implementations","title":"Implementations","text":""},{"location":"expressions/Expression/#binaryexpression","title":"BinaryExpression","text":""},{"location":"expressions/Expression/#leafexpression","title":"LeafExpression","text":""},{"location":"expressions/Expression/#ternaryexpression","title":"TernaryExpression","text":""},{"location":"expressions/Expression/#other-expressions","title":"Other Expressions <ul> <li>CodegenFallback</li> <li>ExpectsInputTypes</li> <li>NamedExpression</li> <li>Nondeterministic</li> <li>Predicate</li> <li>UnaryExpression</li> <li>Unevaluable</li> <li>others</li> </ul>","text":""},{"location":"expressions/Expression/#code-generated-expression-evaluation","title":"Code-Generated Expression Evaluation <pre><code>genCode(\n  ctx: CodegenContext): ExprCode\n</code></pre> <p><code>genCode</code> returns an <code>ExprCode</code> with a Java source code for code-generated expression evaluation.</p> <p><code>genCode</code> is doGenCode but does Subexpression Elimination.</p> <p><code>genCode</code> is a faster \"relative\" of the interpreted expression evaluation.</p>  <p><code>genCode</code> is used when:</p> <ul> <li><code>CodegenContext</code> is requested to subexpressionEliminationForWholeStageCodegen and  generateExpressions (with subexpressionElimination)</li> <li><code>GenerateSafeProjection</code> utility is used to create a Projection</li> <li>others</li> </ul>","text":""},{"location":"expressions/Expression/#reducecodesize","title":"reduceCodeSize <pre><code>reduceCodeSize(\n  ctx: CodegenContext,\n  eval: ExprCode): Unit\n</code></pre> <p><code>reduceCodeSize</code> does its work only when all of the following are met:</p> <ol> <li> <p>Length of the generated code is above spark.sql.codegen.methodSplitThreshold</p> </li> <li> <p>INPUT_ROW (of the input <code>CodegenContext</code>) is defined</p> </li> <li> <p>currentVars (of the input <code>CodegenContext</code>) is not defined</p> </li> </ol>  This needs your help <p>FIXME When would the above not be met? What's so special about such an expression?</p>  <p><code>reduceCodeSize</code> sets the <code>value</code> of the input <code>ExprCode</code> to the fresh term name for the <code>value</code> name.</p> <p>In the end, <code>reduceCodeSize</code> sets the code of the input <code>ExprCode</code> to the following:</p> <pre><code>[javaType] [newValue] = [funcFullName]([INPUT_ROW]);\n</code></pre> <p>The <code>funcFullName</code> is the fresh term name for the name of the current expression node.</p>","text":""},{"location":"expressions/Expression/#deterministic","title":"deterministic <p><code>Expression</code> is deterministic when evaluates to the same result for the same input(s). An expression is deterministic if all the child expressions are.</p>  <p>Note</p> <p>A deterministic expression is like a pure function in functional programming languages.</p>  <pre><code>val e = $\"a\".expr\n\nimport org.apache.spark.sql.catalyst.expressions.Expression\nassert(e.isInstanceOf[Expression])\nassert(e.deterministic)\n</code></pre>","text":""},{"location":"expressions/Expression/#foldable","title":"foldable <pre><code>foldable: Boolean\n</code></pre> <p><code>foldable</code> is <code>false</code> (and is expected to be overriden by implementations).</p> <p><code>foldable</code> expression is a candidate for static evaluation before the query is executed.</p> <p>See:</p> <ul> <li>AggregateFunction</li> </ul>","text":""},{"location":"expressions/Expression/#demo","title":"Demo <pre><code>// evaluating an expression\n// Use Literal expression to create an expression from a Scala object\nimport org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\nval e: Expression = Literal(\"hello\")\n\nimport org.apache.spark.sql.catalyst.expressions.EmptyRow\nval v: Any = e.eval(EmptyRow)\n\n// Convert to Scala's String\nimport org.apache.spark.unsafe.types.UTF8String\nval s = v.asInstanceOf[UTF8String].toString\nassert(s == \"hello\")\n</code></pre>","text":""},{"location":"expressions/First/","title":"First Aggregate Function Expression","text":""},{"location":"expressions/Generator/","title":"Generator Expressions","text":"<p><code>Generator</code> is a &lt;&gt; for Catalyst expressions that can &lt;&gt; zero or more rows given a single input row. <p>NOTE: <code>Generator</code> corresponds to SQL's sql/AstBuilder.md#withGenerate[LATERAL VIEW].</p> <p>[[dataType]] <code>dataType</code> in <code>Generator</code> is simply an ArrayType of &lt;&gt;. <p>[[foldable]] [[nullable]] <code>Generator</code> is not Expression.md#foldable[foldable] and not Expression.md#nullable[nullable] by default.</p> <p>[[supportCodegen]] <code>Generator</code> supports Java code generation conditionally, i.e. only when a physical operator is not marked as CodegenFallback.</p> <p>[[terminate]] <code>Generator</code> uses <code>terminate</code> to inform that there are no more rows to process, clean up code, and additional rows can be made here.</p>"},{"location":"expressions/Generator/#source-scala","title":"[source, scala]","text":""},{"location":"expressions/Generator/#terminate-traversableonceinternalrow-nil","title":"terminate(): TraversableOnce[InternalRow] = Nil","text":"<p>[[generator-implementations]] .Generators [width=\"100%\",cols=\"1,2\",options=\"header\"] |=== | Name | Description</p> [[ExplodeBase]] spark-sql-Expression-ExplodeBase.md[ExplodeBase] [[Explode]] spark-sql-Expression-ExplodeBase.md#Explode[Explode] [[GeneratorOuter]] <code>GeneratorOuter</code> [[HiveGenericUDTF]] <code>HiveGenericUDTF</code> <p>| [[Inline]] spark-sql-Expression-Inline.md[Inline] | Corresponds to <code>inline</code> and <code>inline_outer</code> functions.</p> JsonTuple [[PosExplode]] spark-sql-Expression-ExplodeBase.md#PosExplode[PosExplode] <code>Stack</code> <p>| [[UnresolvedGenerator]] spark-sql-Expression-UnresolvedGenerator.md[UnresolvedGenerator] a| Represents an unresolved &lt;&gt;. <p>Created when <code>AstBuilder</code> sql/AstBuilder.md#withGenerate[creates] <code>Generate</code> unary logical operator for <code>LATERAL VIEW</code> that corresponds to the following:</p> <pre><code>LATERAL VIEW (OUTER)?\ngeneratorFunctionName (arg1, arg2, ...)\ntblName\nAS? col1, col2, ...\n</code></pre> <p><code>UnresolvedGenerator</code> is resolved to <code>Generator</code> by ResolveFunctions logical evaluation rule.</p> <p>| [[UserDefinedGenerator]] <code>UserDefinedGenerator</code> | Used exclusively in the deprecated <code>explode</code> operator |===</p> <p>[[lateral-view]] [NOTE] ==== You can only have one generator per select clause that is enforced by ExtractGenerator logical evaluation rule, e.g.</p> <pre><code>scala&gt; xys.select(explode($\"xs\"), explode($\"ys\")).show\norg.apache.spark.sql.AnalysisException: Only one generator allowed per select clause but found 2: explode(xs), explode(ys);\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractGenerator$$anonfun$apply$20.applyOrElse(Analyzer.scala:1670)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractGenerator$$anonfun$apply$20.applyOrElse(Analyzer.scala:1662)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:62)\n</code></pre> <p>If you want to have more than one generator in a structured query you should use <code>LATERAL VIEW</code> which is supported in SQL only, e.g.</p>"},{"location":"expressions/Generator/#source-scala_1","title":"[source, scala]","text":"<p>val arrayTuple = (Array(1,2,3), Array(\"a\",\"b\",\"c\")) val ncs = Seq(arrayTuple).toDF(\"ns\", \"cs\")</p> <p>scala&gt; ncs.show +---------+---------+ |       ns|       cs| +---------+---------+ |[1, 2, 3]|[a, b, c]| +---------+---------+</p> <p>scala&gt; ncs.createOrReplaceTempView(\"ncs\")</p> <p>val q = \"\"\"   SELECT n, c FROM ncs   LATERAL VIEW explode(ns) nsExpl AS n   LATERAL VIEW explode(cs) csExpl AS c \"\"\"</p> <p>scala&gt; sql(q).show +---+---+ |  n|  c| +---+---+ |  1|  a| |  1|  b| |  1|  c| |  2|  a| |  2|  b| |  2|  c| |  3|  a| |  3|  b| |  3|  c| +---+---+</p> <p>====</p> <p>=== [[contract]] Generator Contract</p>"},{"location":"expressions/Generator/#source-scala_2","title":"[source, scala]","text":"<p>package org.apache.spark.sql.catalyst.expressions</p> <p>trait Generator extends Expression {   // only required methods that have no implementation   def elementSchema: StructType   def eval(input: InternalRow): TraversableOnce[InternalRow] }</p> <p>.(Subset of) Generator Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| [[elementSchema]] <code>elementSchema</code> | Schema of the elements to be generated</p> [[eval]] <code>eval</code> ==="},{"location":"expressions/HashExpression/","title":"HashExpression","text":"<p><code>HashExpression[E]</code> is an extension of the Expression abstraction for hashing expressions that calculate hash value (for a group of expressions).</p>"},{"location":"expressions/HashExpression/#contract","title":"Contract","text":""},{"location":"expressions/HashExpression/#computing-hash","title":"Computing Hash <pre><code>computeHash(\n  value: Any,\n  dataType: DataType,\n  seed: E): E\n</code></pre> <p>Used when:</p> <ul> <li><code>HashExpression</code> is requested to eval and doGenCode</li> </ul>","text":""},{"location":"expressions/HashExpression/#hasherclassname","title":"hasherClassName <pre><code>hasherClassName: String\n</code></pre> <p>Used when:</p> <ul> <li><code>HashExpression</code> is requested to genHashInt, genHashLong, genHashBytes, genHashCalendarInterval, genHashString</li> </ul>","text":""},{"location":"expressions/HashExpression/#seed","title":"Seed <pre><code>seed: E\n</code></pre> <p>Used when:</p> <ul> <li><code>HashExpression</code> is requested to eval and doGenCode</li> </ul>","text":""},{"location":"expressions/HashExpression/#implementations","title":"Implementations","text":"<ul> <li>HiveHash</li> <li>Murmur3Hash</li> <li>XxHash64</li> </ul>"},{"location":"expressions/HashExpression/#interpreted-expression-evaluation","title":"Interpreted Expression Evaluation <pre><code>eval(\n  input: InternalRow): Any\n</code></pre> <p><code>eval</code>...FIXME</p> <p><code>eval</code> is part of the Expression abstraction.</p>","text":""},{"location":"expressions/HashExpression/#dogencode","title":"doGenCode <pre><code>doGenCode(\n  ctx: CodegenContext,\n  ev: ExprCode): ExprCode\n</code></pre> <p><code>doGenCode</code>...FIXME</p> <p><code>doGenCode</code> is part of the Expression abstraction.</p>","text":""},{"location":"expressions/HashPartitioning/","title":"HashPartitioning","text":"<p><code>HashPartitioning</code> is a Catalyst Partitioning in which rows are distributed across partitions using the MurMur3 hash (of the partitioning expressions) modulo the number of partitions.</p> <p><code>HashPartitioning</code> is an Expression that computes the partition Id for data distribution to be consistent across shuffling and bucketing (for joins of bucketed and regular tables).</p>"},{"location":"expressions/HashPartitioning/#creating-instance","title":"Creating Instance","text":"<p><code>HashPartitioning</code> takes the following to be created:</p> <ul> <li> Partitioning Expressions <li> Number of partitions <p><code>HashPartitioning</code> is created when:</p> <ul> <li><code>RepartitionByExpression</code> is requested for the partitioning</li> <li><code>RebalancePartitions</code> is requested for the partitioning</li> <li><code>ClusteredDistribution</code> is requested to create a Partitioning</li> <li><code>HashClusteredDistribution</code> is requested to create a Partitioning</li> <li><code>FileSourceScanExec</code> physical operator is requested for the output partitioning</li> <li><code>BucketingUtils</code> utility is used to <code>getBucketIdFromValue</code></li> <li><code>FileFormatWriter</code> utility is used to write out a query result (with a bucketing spec)</li> <li><code>BroadcastHashJoinExec</code> physical operator is requested to expandOutputPartitioning</li> </ul>"},{"location":"expressions/HashPartitioning/#unevaluable","title":"Unevaluable <p><code>HashPartitioning</code> is an Unevaluable expression.</p>","text":""},{"location":"expressions/HashPartitioning/#satisfying-distribution","title":"Satisfying Distribution <pre><code>satisfies0(\n  required: Distribution): Boolean\n</code></pre> <p><code>satisfies0</code> is positive (<code>true</code>) when either the base satisfies0 holds or one of the given Distribution satisfies the following:</p> <ul> <li> <p>For a HashClusteredDistribution, the number of the given partitioning expressions and the HashClusteredDistribution's are the same and semantically equal pair-wise</p> </li> <li> <p>For a ClusteredDistribution, the given partitioning expressions are among the clustering expressions (of the ClusteredDistribution) and they are semantically equal pair-wise</p> </li> </ul> <p>Otherwise, <code>satisfies0</code> is negative (<code>false</code>).</p> <p><code>satisfies0</code> is part of the Partitioning abstraction.</p>","text":""},{"location":"expressions/HashPartitioning/#partitionid-expression","title":"PartitionId Expression <pre><code>partitionIdExpression: Expression\n</code></pre> <p><code>partitionIdExpression</code> gives an Expression that produces a valid partition ID.</p> <p><code>partitionIdExpression</code> is a <code>Pmod</code> expression of a Murmur3Hash (with the partitioning expressions) and a Literal (with the number of partitions).</p> <p><code>partitionIdExpression</code> is used when:</p> <ul> <li><code>BucketingUtils</code> utility is used to <code>getBucketIdFromValue</code></li> <li><code>FileFormatWriter</code> utility is used to write out a query result (with a bucketing spec)</li> <li><code>ShuffleExchangeExec</code> utility is used to prepare a ShuffleDependency</li> </ul>","text":""},{"location":"expressions/HashPartitioning/#demo","title":"Demo <pre><code>val nums = spark.range(5)\nval numParts = 200 // the default number of partitions\nval partExprs = Seq(nums(\"id\"))\n\nval partitionIdExpression = pmod(hash(partExprs: _*), lit(numParts))\nscala&gt; partitionIdExpression.explain(extended = true)\npmod(hash(id#32L, 42), 200)\n\nval q = nums.withColumn(\"partitionId\", partitionIdExpression)\n</code></pre> <pre><code>scala&gt; q.show\n+---+-----------+\n| id|partitionId|\n+---+-----------+\n|  0|          5|\n|  1|         69|\n|  2|        128|\n|  3|        107|\n|  4|        140|\n+---+-----------+\n</code></pre>","text":""},{"location":"expressions/HigherOrderFunction/","title":"HigherOrderFunction","text":"<p><code>HigherOrderFunction</code> is an extension of the Expression abstraction for FIXME that method and...FIXME.</p>"},{"location":"expressions/HigherOrderFunction/#contract","title":"Contract","text":""},{"location":"expressions/HigherOrderFunction/#arguments","title":"arguments <pre><code>arguments: Seq[Expression]\n</code></pre> <p>Used when...FIXME</p>","text":""},{"location":"expressions/HigherOrderFunction/#argumenttypes","title":"argumentTypes <pre><code>argumentTypes: Seq[AbstractDataType]\n</code></pre> <p>Used when...FIXME</p>","text":""},{"location":"expressions/HigherOrderFunction/#bind","title":"bind <pre><code>bind(\n    f: (Expression, Seq[(DataType, Boolean)]) =&gt; LambdaFunction): HigherOrderFunction\n</code></pre> <p>Used when...FIXME</p>","text":""},{"location":"expressions/HigherOrderFunction/#functions","title":"functions <pre><code>functions: Seq[Expression]\n</code></pre> <p>Used when...FIXME</p>","text":""},{"location":"expressions/HigherOrderFunction/#functiontypes","title":"functionTypes <pre><code>functionTypes: Seq[AbstractDataType]\n</code></pre> <p>Used when...FIXME</p>","text":""},{"location":"expressions/HigherOrderFunction/#implementations","title":"Implementations","text":"<ul> <li><code>ArrayAggregate</code></li> <li><code>MapZipWith</code></li> <li>SimpleHigherOrderFunction</li> <li><code>ZipWith</code></li> </ul>"},{"location":"expressions/ImperativeAggregate/","title":"ImperativeAggregate Expressions","text":"<p><code>ImperativeAggregate</code> is an extension of the AggregateFunction abstraction for aggregate functions that are expressed using imperative initialize, merge and update methods (that operate on <code>Row</code>-based aggregation buffers).</p>"},{"location":"expressions/ImperativeAggregate/#contract-subset","title":"Contract (Subset)","text":""},{"location":"expressions/ImperativeAggregate/#initialize","title":"initialize <pre><code>initialize(\n  mutableAggBuffer: InternalRow): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>EliminateAggregateFilter</code> logical optimization is executed</li> <li><code>AggregatingAccumulator</code> is requested to createBuffer</li> <li><code>AggregationIterator</code> is requested to initializeBuffer</li> <li><code>ObjectAggregationIterator</code> is requested to initAggregationBuffer</li> <li><code>TungstenAggregationIterator</code> is requested to createNewAggregationBuffer</li> <li><code>AggregateProcessor</code> is requested to initialize</li> </ul>","text":""},{"location":"expressions/ImperativeAggregate/#merge","title":"merge <pre><code>merge(\n  mutableAggBuffer: InternalRow,\n  inputAggBuffer: InternalRow): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>AggregatingAccumulator</code> is requested to merge</li> <li><code>AggregationIterator</code> is requested to generateProcessRow</li> </ul>","text":""},{"location":"expressions/ImperativeAggregate/#update","title":"update <pre><code>update(\n  mutableAggBuffer: InternalRow,\n  inputRow: InternalRow): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>AggregatingAccumulator</code> is requested to add a value</li> <li><code>AggregationIterator</code> is requested to generateProcessRow</li> <li><code>AggregateProcessor</code> is requested to update</li> </ul>","text":""},{"location":"expressions/ImperativeAggregate/#implementations","title":"Implementations","text":"<ul> <li><code>HyperLogLogPlusPlus</code></li> <li><code>PivotFirst</code></li> <li><code>ScalaUDAF</code></li> <li>TypedImperativeAggregate</li> </ul>"},{"location":"expressions/ImperativeAggregate/#codegenfallback","title":"CodegenFallback <p><code>ImperativeAggregate</code> is a CodegenFallback.</p>","text":""},{"location":"expressions/In/","title":"In","text":"<p><code>In</code> is a predicate expression.</p>"},{"location":"expressions/InSet/","title":"InSet","text":"<p><code>InSet</code> is a predicate expression that is an optimized variant of the In predicate expression.</p>"},{"location":"expressions/InSubqueryExec/","title":"InSubqueryExec","text":"<p><code>InSubqueryExec</code> is a ExecSubqueryExpression that represents <code>InSubquery</code> and DynamicPruningSubquery expressions at execution time.</p>"},{"location":"expressions/InSubqueryExec/#creating-instance","title":"Creating Instance","text":"<p><code>InSubqueryExec</code> takes the following to be created:</p> <ul> <li> Child Expression <li> BaseSubqueryExec physical operator <li> Expression ID <li>Broadcast Variable</li> <p><code>InSubqueryExec</code> is created when:</p> <ul> <li>PlanSubqueries physical optimization is executed (and plans <code>InSubquery</code> expressions)</li> <li>PlanAdaptiveSubqueries physical optimization is executed (and plans <code>InSubquery</code> expressions)</li> <li>PlanDynamicPruningFilters physical optimization is executed (and plans DynamicPruningSubquery expressions)</li> </ul>"},{"location":"expressions/InSubqueryExec/#broadcasted-result","title":"Broadcasted Result","text":"<p> <pre><code>resultBroadcast: Broadcast[Array[Any]]\n</code></pre> <p><code>InSubqueryExec</code> is given a broadcast variable when created. It is uninitialized (<code>null</code>).</p> <p><code>resultBroadcast</code> is updated when <code>InSubqueryExec</code> is requested to update the collected result.</p>"},{"location":"expressions/InSubqueryExec/#interpreted-expression-evaluation","title":"Interpreted Expression Evaluation <pre><code>eval(\n  input: InternalRow): Any\n</code></pre> <p><code>eval</code> prepareResult.</p> <p><code>eval</code> requests the child expression to evaluate for the given InternalRow.</p> <p><code>eval</code> returns:</p> <ul> <li><code>null</code> for <code>null</code> evaluation result</li> <li><code>true</code> when the result contains the evaluation result or <code>false</code></li> </ul> <p><code>eval</code> is part of the Expression abstraction.</p>","text":""},{"location":"expressions/InSubqueryExec/#code-generated-expression-evaluation","title":"Code-Generated Expression Evaluation <p> <pre><code>doGenCode(\n  ctx: CodegenContext,\n  ev: ExprCode): ExprCode\n</code></pre> <p><code>doGenCode</code> prepareResult.</p> <p><code>doGenCode</code> creates a InSet expression (with the child expression and result) and requests it to doGenCode.</p> <p><code>doGenCode</code> is part of the Expression abstraction.</p>","text":""},{"location":"expressions/InSubqueryExec/#updating-result","title":"Updating Result <p> <pre><code>updateResult(): Unit\n</code></pre> <p><code>updateResult</code> requests the BaseSubqueryExec to executeCollect.</p> <p><code>updateResult</code> uses the collected result to update the result and resultBroadcast registries.</p> <p><code>updateResult</code> is part of the ExecSubqueryExpression abstraction.</p>","text":""},{"location":"expressions/InSubqueryExec/#result-registry","title":"result Registry <pre><code>result: Array[Any]\n</code></pre> <p><code>result</code>...FIXME</p>","text":""},{"location":"expressions/InSubqueryExec/#prepareresult","title":"prepareResult <pre><code>prepareResult(): Unit\n</code></pre> <p><code>prepareResult</code> simply requests the resultBroadcast broadcast variable for the broadcasted value when result is undefined (<code>null</code>). Otherwise, <code>prepareResult</code> does nothing.</p> <p><code>prepareResult</code> throws an <code>IllegalArgumentException</code> when resultBroadcast is undefined (<code>null</code>):</p> <pre><code>[this] has not finished\n</code></pre> <p><code>prepareResult</code> is used when <code>InSubqueryExec</code> expression is evaluated (interpreted or code-generated).</p>","text":""},{"location":"expressions/Inline/","title":"Inline","text":"<p><code>Inline</code> is a UnaryExpression and a <code>CollectionGenerator</code>.</p> <p><code>Inline</code> is created by <code>inline</code> and <code>inline_outer</code> standard functions.</p> <pre><code>// Query with inline function\nval q = spark.range(1)\n  .selectExpr(\"inline(array(struct(1, 'a'), struct(2, 'b')))\")\nval logicalPlan = q.queryExecution.analyzed\nscala&gt; println(logicalPlan.numberedTreeString)\n00 Project [col1#61, col2#62]\n01 +- Generate inline(array(named_struct(col1, 1, col2, a), named_struct(col1, 2, col2, b))), false, false, [col1#61, col2#62]\n02    +- Range (0, 1, step=1, splits=Some(8))\n\n// Query with inline_outer function\nval q = spark.range(1)\n  .selectExpr(\"inline_outer(array(struct(1, 'a'), struct(2, 'b')))\")\nval logicalPlan = q.queryExecution.analyzed\nscala&gt; println(logicalPlan.numberedTreeString)\n00 Project [col1#69, col2#70]\n01 +- Generate inline(array(named_struct(col1, 1, col2, a), named_struct(col1, 2, col2, b))), false, true, [col1#69, col2#70]\n02    +- Range (0, 1, step=1, splits=Some(8))\n\nimport org.apache.spark.sql.catalyst.plans.logical.Generate\n// get is safe since there is Generate logical operator\nval generator = logicalPlan.collectFirst { case g: Generate =&gt; g.generator }.get\nimport org.apache.spark.sql.catalyst.expressions.Inline\nval inline = generator.asInstanceOf[Inline]\n\n// Inline Generator expression is also CollectionGenerator\nscala&gt; inline.collectionType.catalogString\nres1: String = array&lt;struct&lt;col1:int,col2:string&gt;&gt;\n</code></pre>"},{"location":"expressions/InterpretedProjection/","title":"InterpretedProjection","text":"<p><code>InterpretedProjection</code> is a Projection.</p>"},{"location":"expressions/InterpretedProjection/#creating-instance","title":"Creating Instance","text":"<p><code>InterpretedProjection</code> takes the following to be created:</p> <ul> <li> Expressions <li> Input Schema (Attributes) <p><code>InterpretedProjection</code> is created when:</p> <ul> <li><code>HiveGenericUDTF</code> is requested to <code>eval</code></li> <li><code>HiveScriptTransformationExec</code> is requested to <code>processIterator</code></li> <li><code>SparkScriptTransformationExec</code> is requested to <code>processIterator</code></li> <li><code>UserDefinedGenerator</code> is requested to <code>initializeConverters</code></li> </ul>"},{"location":"expressions/InterpretedProjection/#demo","title":"Demo","text":"<pre><code>// HACK: Disable symbolToColumn implicit conversion\n// It is imported automatically in spark-shell (and makes demos impossible)\n// implicit def symbolToColumn(s: Symbol): org.apache.spark.sql.ColumnName\ntrait ThatWasABadIdea\nimplicit def symbolToColumn(ack: ThatWasABadIdea) = ack\n\nimport org.apache.spark.sql.catalyst.dsl.expressions._\nval boundRef = 'hello.string.at(4)\n\nimport org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\nval expressions: Seq[Expression] = Seq(Literal(1), boundRef)\n\nimport org.apache.spark.sql.catalyst.expressions.InterpretedProjection\nval ip = new InterpretedProjection(expressions)\nscala&gt; println(ip)\nRow =&gt; [1,input[4, string, true]]\n</code></pre>"},{"location":"expressions/JsonToStructs/","title":"JsonToStructs","text":"<p><code>JsonToStructs</code> is a UnaryExpression with timezone support and CodegenFallback.</p>"},{"location":"expressions/JsonToStructs/#expectsinputtypes","title":"ExpectsInputTypes <p><code>JsonToStructs</code> is an ExpectsInputTypes expression.</p>","text":""},{"location":"expressions/JsonToStructs/#creating-instance","title":"Creating Instance <p><code>JsonToStructs</code> takes the following to be created:</p> <ul> <li> DataType <li> Options <li> Child Expression <li> (optional) Time Zone ID","text":""},{"location":"expressions/JsonToStructs/#from_json-standard-function","title":"from_json Standard Function <p><code>JsonToStructs</code> represents from_json function.</p> <pre><code>import org.apache.spark.sql.functions.from_json\nval jsonCol = from_json($\"json\", new StructType())\n\nimport org.apache.spark.sql.catalyst.expressions.JsonToStructs\nval jsonExpr = jsonCol.expr.asInstanceOf[JsonToStructs]\nscala&gt; println(jsonExpr.numberedTreeString)\n00 jsontostructs('json, None)\n01 +- 'json\n</code></pre>","text":""},{"location":"expressions/JsonToStructs/#failfast","title":"FAILFAST <p><code>JsonToStructs</code> uses JacksonParser in <code>FAILFAST</code> mode that fails early when a corrupted/malformed record is found (and hence does not support <code>columnNameOfCorruptRecord</code> JSON option).</p>","text":""},{"location":"expressions/LessThanOrEqual/","title":"LessThanOrEqual","text":"<p><code>LessThanOrEqual</code> is a binary comparison expression and a <code>NullIntolerant</code>.</p> <p><code>LessThanOrEqual</code>'s symbol representation is <code>&lt;=</code>.</p>"},{"location":"expressions/LessThanOrEqual/#creating-instance","title":"Creating Instance","text":"<p><code>LessThanOrEqual</code> takes the following to be created:</p> <ul> <li> Left Expression <li> Right Expression"},{"location":"expressions/LessThanOrEqual/#symbol","title":"symbol <pre><code>symbol: String\n</code></pre> <p><code>symbol</code> is part of the BinaryOperator abstraction.</p>  <p><code>symbol</code> is <code>&lt;=</code>.</p>","text":""},{"location":"expressions/LessThanOrEqual/#nullsafeeval","title":"nullSafeEval <pre><code>nullSafeEval(\n  input1: Any,\n  input2: Any): Any\n</code></pre> <p><code>nullSafeEval</code> is part of the BinaryOperator abstraction.</p>  <p><code>nullSafeEval</code> requests the Ordering to <code>lteq</code> the inputs.</p>","text":""},{"location":"expressions/LessThanOrEqual/#catalyst-dsl","title":"Catalyst DSL <p><code>expressions</code> defines &lt;= operator to create a <code>LessThanOrEqual</code> expression.</p> <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\n\n// LessThanOrEqual\nval e = col(\"a\").expr &lt;= 5\n\nimport org.apache.spark.sql.catalyst.expressions.LessThanOrEqual\nval lessThanOrEqual = e.asInstanceOf[LessThanOrEqual]\n</code></pre>  <p>FIXME</p> <pre><code>import org.apache.spark.sql.catalyst.InternalRow\nval input = InternalRow(1, 2)\n</code></pre> <pre><code>scala&gt; lessThanOrEqual.eval(input)\norg.apache.spark.SparkUnsupportedOperationException: Cannot evaluate expression: 'a\nat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotEvaluateExpressionError(QueryExecutionErrors.scala:73)\nat org.apache.spark.sql.catalyst.expressions.Unevaluable.eval(Expression.scala:344)\nat org.apache.spark.sql.catalyst.expressions.Unevaluable.eval$(Expression.scala:343)\nat org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.eval(unresolved.scala:131)\nat org.apache.spark.sql.catalyst.expressions.BinaryExpression.eval(Expression.scala:634)\n... 54 elided\n</code></pre>","text":""},{"location":"expressions/ListQuery/","title":"ListQuery","text":"<p><code>ListQuery</code> is a SubqueryExpression that represents SQL's IN predicate with a subquery.</p>"},{"location":"expressions/Literal/","title":"Literal","text":"<p><code>Literal</code> is a leaf expression to represent a Scala value of a specific type.</p>"},{"location":"expressions/MapBasedSimpleHigherOrderFunction/","title":"MapBasedSimpleHigherOrderFunction","text":"<p><code>MapBasedSimpleHigherOrderFunction</code> is...FIXME</p>"},{"location":"expressions/MonotonicallyIncreasingID/","title":"MonotonicallyIncreasingID","text":"<p><code>MonotonicallyIncreasingID</code> is a non-deterministic leaf expression that represents <code>monotonically_increasing_id</code> standard and SQL functions in logical query plans.</p> <p><code>MonotonicallyIncreasingID</code> supports code-generated and interpreted execution modes.</p>"},{"location":"expressions/MonotonicallyIncreasingID/#result-datatype","title":"Result DataType <pre><code>dataType: DataType\n</code></pre> <p><code>dataType</code> is always LongType</p> <p><code>dataType</code> is part of the Expression abstraction.</p>","text":""},{"location":"expressions/MonotonicallyIncreasingID/#never-nullable","title":"Never Nullable <pre><code>nullable: Boolean\n</code></pre> <p><code>nullable</code> is always <code>false</code>.</p> <p><code>nullable</code> is part of the Expression abstraction.</p>","text":""},{"location":"expressions/MonotonicallyIncreasingID/#initialization","title":"Initialization <pre><code>initializeInternal(\n  partitionIndex: Int): Unit\n</code></pre> <p><code>initializeInternal</code> initializes the following internal registries:</p> <ul> <li>count to <code>0</code></li> <li>partitionMask as <code>partitionIndex.toLong &lt;&lt; 33</code></li> </ul> <pre><code>val partitionIndex = 1\nval partitionMask = partitionIndex.toLong &lt;&lt; 33\nscala&gt; println(partitionMask.toBinaryString)\n1000000000000000000000000000000000\n</code></pre> <p><code>initializeInternal</code> is part of the Nondeterministic abstraction.</p>","text":""},{"location":"expressions/MonotonicallyIncreasingID/#interpreted-expression-evaluation","title":"Interpreted Expression Evaluation <pre><code>evalInternal(\n  input: InternalRow): Long\n</code></pre> <p><code>evalInternal</code> increments the count internal counter.</p> <p><code>evalInternal</code> increments the partitionMask internal registry by the previous count.</p> <p><code>evalInternal</code> is part of the Nondeterministic abstraction.</p>","text":""},{"location":"expressions/MonotonicallyIncreasingID/#code-generated-expression-evaluation","title":"Code-Generated Expression Evaluation <pre><code>doGenCode(\n  ctx: CodegenContext,\n  ev: ExprCode): ExprCode\n</code></pre> <p><code>doGenCode</code> is part of the Expression abstraction.</p>  <p><code>doGenCode</code> requests the <code>CodegenContext</code> to add a mutable state as <code>count</code> name and <code>long</code> Java type.</p> <p><code>doGenCode</code> requests the <code>CodegenContext</code> to add an immutable state (unless exists already) as <code>partitionMask</code> name and <code>long</code> Java type.</p> <p><code>doGenCode</code> requests the <code>CodegenContext</code> to addPartitionInitializationStatement with <code>[countTerm] = 0L;</code> statement.</p> <p><code>doGenCode</code> requests the <code>CodegenContext</code> to addPartitionInitializationStatement with <code>[partitionMaskTerm] = ((long) partitionIndex) &lt;&lt; 33;</code> statement.</p> <p>In the end, <code>doGenCode</code> returns the input <code>ExprCode</code> with the <code>code</code> as follows and <code>isNull</code> property disabled (<code>false</code>):</p> <pre><code>final [dataType] [value] = [partitionMaskTerm] + [countTerm];\n      [countTerm]++;\n</code></pre>  <pre><code>import org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID\nval monotonicallyIncreasingID = MonotonicallyIncreasingID()\n\n// doGenCode is used when Expression.genCode is executed\n\nimport org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\nval ctx = new CodegenContext\nval code = monotonicallyIncreasingID.genCode(ctx).code\n</code></pre> <pre><code>scala&gt; println(code)\nfinal long value_0 = partitionMask + count_0;\n      count_0++;\n</code></pre>","text":""},{"location":"expressions/MonotonicallyIncreasingID/#stateful","title":"Stateful <p><code>MonotonicallyIncreasingID</code> is a Stateful.</p>","text":""},{"location":"expressions/Murmur3Hash/","title":"Murmur3Hash","text":"<p><code>Murmur3Hash</code> is a HashExpression to calculate the hash code (integer) of the given child expressions.</p>"},{"location":"expressions/Murmur3Hash/#creating-instance","title":"Creating Instance","text":"<p><code>Murmur3Hash</code> takes the following to be created:</p> <ul> <li> Child Expressions <li> Seed (default: <code>42</code>) <p><code>Murmur3Hash</code> is created\u00a0when:</p> <ul> <li><code>HashPartitioning</code> is requested for the partitionId expression</li> <li>hash standard and SQL functions are used</li> </ul>"},{"location":"expressions/Murmur3Hash/#demo","title":"Demo","text":"<pre><code>val data = Seq[Option[Int]](Some(0), None, None, None, Some(4), None)\n.toDF\n.withColumn(\"hash\", hash('value))\n</code></pre> <pre><code>scala&gt; data.show\n+-----+----------+\n|value|      hash|\n+-----+----------+\n|    0| 933211791|\n| null|        42|\n| null|        42|\n| null|        42|\n|    4|-397064898|\n| null|        42|\n+-----+----------+\n</code></pre> <pre><code>scala&gt; data.printSchema\nroot\n |-- value: integer (nullable = true)\n |-- hash: integer (nullable = false)\n</code></pre> <pre><code>val defaultSeed = 42\n\nval nonEmptyPartitions = data\n.repartition(numPartitions = defaultSeed, partitionExprs = 'value)\n.mapPartitions { it: Iterator[org.apache.spark.sql.Row] =&gt;\nimport org.apache.spark.TaskContext\nval ns = it.map(_.get(0)).mkString(\",\")\nIterator((TaskContext.getPartitionId, ns))\n}\n.as[(Long, String)]\n.collect\n.filterNot { case (pid, ns) =&gt; ns.isEmpty }\n\nnonEmptyPartitions.foreach { case (pid, ns) =&gt; printf(\"%2s: %s%n\", pid, ns) }\n</code></pre> <pre><code> 0: null,null,null,null\n25: 0\n32: 4\n</code></pre>"},{"location":"expressions/MutableProjection/","title":"MutableProjection Expression","text":"<p><code>MutableProjection</code> is...FIXME</p>"},{"location":"expressions/NamedExpression/","title":"NamedExpression","text":"<p><code>NamedExpression</code> is an extension of the Expression abstraction for named expressions (with an ExprId and an optional qualifier).</p>"},{"location":"expressions/NamedExpression/#contract","title":"Contract","text":""},{"location":"expressions/NamedExpression/#exprid","title":"ExprId <pre><code>exprId: ExprId\n</code></pre>","text":""},{"location":"expressions/NamedExpression/#name","title":"Name <pre><code>name: String\n</code></pre>","text":""},{"location":"expressions/NamedExpression/#creating-namedexpression","title":"Creating NamedExpression <pre><code>newInstance(): NamedExpression\n</code></pre>","text":""},{"location":"expressions/NamedExpression/#qualifier-parts","title":"Qualifier (Parts) <pre><code>qualifier: Seq[String]\n</code></pre> <p>Optional qualifier parts (with the names of catalog, database, table, fields, incl. nested fields)</p>","text":""},{"location":"expressions/NamedExpression/#converting-to-attribute","title":"Converting to Attribute <pre><code>toAttribute: Attribute\n</code></pre>","text":""},{"location":"expressions/NamedExpression/#implementations","title":"Implementations","text":"<ul> <li>Attribute</li> <li>others</li> </ul>"},{"location":"expressions/NamedExpression/#foldable","title":"foldable <pre><code>foldable: Boolean\n</code></pre> <p><code>foldable</code> is part of the Expression abstraction.</p>  <p><code>foldable</code> is always <code>false</code> (in order to not remove the alias).</p>","text":""},{"location":"expressions/Nondeterministic/","title":"Nondeterministic Expressions","text":"<p><code>Nondeterministic</code> is an extension of the Expression abstraction for non-deterministic and non-foldable expressions.</p> <p>Nondeterministic expression should be initialized (with the partition ID) before evaluation.</p>"},{"location":"expressions/Nondeterministic/#contract","title":"Contract","text":""},{"location":"expressions/Nondeterministic/#evalinternal","title":"evalInternal <pre><code>evalInternal(\n  input: InternalRow): Any\n</code></pre> <p>Used when:</p> <ul> <li><code>Nondeterministic</code> is requested to eval</li> </ul>","text":""},{"location":"expressions/Nondeterministic/#initializeinternal","title":"initializeInternal <pre><code>initializeInternal(\n  partitionIndex: Int): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>Nondeterministic</code> is requested to initialize</li> </ul>","text":""},{"location":"expressions/Nondeterministic/#implementations","title":"Implementations","text":"<ul> <li>CallMethodViaReflection</li> <li><code>CurrentBatchTimestamp</code></li> <li><code>InputFileBlockLength</code></li> <li><code>InputFileBlockStart</code></li> <li><code>InputFileName</code></li> <li><code>SparkPartitionID</code></li> <li><code>Stateful</code></li> </ul>"},{"location":"expressions/Nondeterministic/#review-me","title":"Review Me","text":"<p>NOTE: <code>Nondeterministic</code> expressions are the target of <code>PullOutNondeterministic</code> logical plan rule.</p> <p>=== [[initialize]] Initializing Expression -- <code>initialize</code> Method</p>"},{"location":"expressions/Nondeterministic/#source-scala","title":"[source, scala]","text":""},{"location":"expressions/Nondeterministic/#initializepartitionindex-int-unit","title":"initialize(partitionIndex: Int): Unit","text":"<p>Internally, <code>initialize</code> &lt;&gt; itself (with the input partition index) and turns the internal &lt;&gt; flag on. <p><code>initialize</code> is used when InterpretedProjection and <code>InterpretedMutableProjection</code> are requested to <code>initialize</code> themselves.</p> <p>=== [[eval]] Evaluating Expression -- <code>eval</code> Method</p>"},{"location":"expressions/Nondeterministic/#source-scala_1","title":"[source, scala]","text":""},{"location":"expressions/Nondeterministic/#evalinput-internalrow-any","title":"eval(input: InternalRow): Any","text":"<p><code>eval</code> is part of the Expression abstraction.</p> <p><code>eval</code> is just a wrapper of &lt;&gt; that makes sure that &lt;&gt; has already been executed (and so the expression is initialized). <p>Internally, <code>eval</code> makes sure that the expression was &lt;&gt; and calls &lt;&gt;. <p><code>eval</code> reports a <code>IllegalArgumentException</code> exception when the internal &lt;&gt; flag is off, i.e. &lt;&gt; has not yet been executed. <pre><code>requirement failed: Nondeterministic expression [name] should be initialized before eval.\n</code></pre>"},{"location":"expressions/OffsetWindowFunction/","title":"OffsetWindowFunction Expressions","text":"<p><code>OffsetWindowFunction</code> is an extension of the WindowFunction abstraction for offset-based window functions.</p>"},{"location":"expressions/OffsetWindowFunction/#contract","title":"Contract","text":""},{"location":"expressions/OffsetWindowFunction/#default-expression","title":"Default Expression <pre><code>default: Expression\n</code></pre> <p>Used when:</p> <ul> <li><code>FrameLessOffsetWindowFunction</code> is requested to <code>nullable</code>, <code>toString</code></li> <li><code>OffsetWindowFunctionFrameBase</code> is requested for <code>fillDefaultValue</code></li> </ul>","text":""},{"location":"expressions/OffsetWindowFunction/#ignorenulls","title":"ignoreNulls <pre><code>ignoreNulls: Boolean\n</code></pre> <p>Used when:</p> <ul> <li><code>WindowExecBase</code> unary physical operator is requested for windowFrameExpressionFactoryPairs</li> </ul>","text":""},{"location":"expressions/OffsetWindowFunction/#input-expression","title":"Input Expression <pre><code>input: Expression\n</code></pre> <p>Input Expression</p> <p>Used when:</p> <ul> <li><code>FrameLessOffsetWindowFunction</code> is requested to <code>nullable</code>, <code>dataType</code>, <code>inputTypes</code>, <code>toString</code></li> <li><code>OffsetWindowFunctionFrameBase</code> is requested for <code>projection</code>, <code>project</code></li> </ul>","text":""},{"location":"expressions/OffsetWindowFunction/#offset-expression","title":"Offset Expression <pre><code>offset: Expression\n</code></pre> <p>Offset (foldable) Expression</p> <p>Used when:</p> <ul> <li><code>OffsetWindowFunction</code> is requested to <code>fakeFrame</code></li> <li><code>FrameLessOffsetWindowFunction</code> is requested to <code>checkInputDataTypes</code>, <code>toString</code></li> </ul>","text":""},{"location":"expressions/OffsetWindowFunction/#implementations","title":"Implementations","text":"<ul> <li><code>FrameLessOffsetWindowFunction</code></li> <li><code>NthValue</code></li> </ul>"},{"location":"expressions/ParseToDate/","title":"ParseToDate","text":"<p><code>ParseToDate</code> is a RuntimeReplaceable expression to represent to_date function (in logical query plans).</p> <p>As a <code>RuntimeReplaceable</code> expression, <code>ParseToDate</code> is replaced by Logical Query Optimizer with the child expression:</p> <ul> <li> <p><code>Cast(left, DateType)</code> for <code>to_date(e: Column): Column</code> function</p> </li> <li> <p><code>Cast(Cast(UnixTimestamp(left, format), TimestampType), DateType)</code> for <code>to_date(e: Column, fmt: String): Column</code> function</p> </li> </ul>"},{"location":"expressions/ParseToTimestamp/","title":"ParseToTimestamp","text":"<p><code>ParseToTimestamp</code> is a RuntimeReplaceable expression to represent to_timestamp standard function (in logical query plans).</p> <p>As a <code>RuntimeReplaceable</code> expression, <code>ParseToTimestamp</code> is replaced by Logical Optimizer with the child expression:</p> <ul> <li> <p><code>Cast(left, TimestampType)</code> for <code>to_timestamp(s: Column): Column</code> function</p> </li> <li> <p><code>Cast(UnixTimestamp(left, format), TimestampType)</code> for <code>to_timestamp(s: Column, fmt: String): Column</code> function</p> </li> </ul>"},{"location":"expressions/ParseToTimestamp/#demo","title":"Demo","text":"<pre><code>// DEMO to_timestamp(s: Column): Column\nimport java.sql.Timestamp\nimport java.time.LocalDateTime\nval times = Seq(Timestamp.valueOf(LocalDateTime.of(2018, 5, 30, 0, 0, 0)).toString).toDF(\"time\")\nscala&gt; times.printSchema\nroot\n |-- time: string (nullable = true)\n\nimport org.apache.spark.sql.functions.to_timestamp\nval q = times.select(to_timestamp($\"time\") as \"ts\")\nscala&gt; q.printSchema\nroot\n |-- ts: timestamp (nullable = true)\n\nval plan = q.queryExecution.analyzed\nscala&gt; println(plan.numberedTreeString)\n00 Project [to_timestamp('time, None) AS ts#29]\n01 +- Project [value#16 AS time#18]\n02    +- LocalRelation [value#16]\n\nimport org.apache.spark.sql.catalyst.expressions.ParseToTimestamp\nval ptt = plan.expressions.head.children.head.asInstanceOf[ParseToTimestamp]\nscala&gt; println(ptt.numberedTreeString)\n00 to_timestamp('time, None)\n01 +- cast(time#18 as timestamp)\n02    +- time#18: string\n</code></pre>"},{"location":"expressions/PlanExpression/","title":"PlanExpression Expressions","text":"<p><code>PlanExpression</code> is an extension of the Expression abstraction for subquery expressions (that are expressions with query plans).</p>"},{"location":"expressions/PlanExpression/#contract","title":"Contract","text":""},{"location":"expressions/PlanExpression/#exprid","title":"exprId","text":"<p> <pre><code>exprId: ExprId\n</code></pre> <p>Expression ID</p>"},{"location":"expressions/PlanExpression/#plan","title":"plan","text":"<p> <pre><code>plan: T\n</code></pre> <p>Query plan of a subquery</p>"},{"location":"expressions/PlanExpression/#withnewplan","title":"withNewPlan","text":"<p> <pre><code>withNewPlan(\nplan: T): PlanExpression[T]\n</code></pre> <p>Updates the expression with a new plan</p>"},{"location":"expressions/PlanExpression/#implementations","title":"Implementations","text":"<ul> <li>ExecSubqueryExpression</li> <li>SubqueryExpression</li> </ul>"},{"location":"expressions/PlanExpression/#conditionstring","title":"conditionString","text":"<p> <pre><code>conditionString: String\n</code></pre> <p><code>conditionString</code> simply concatenates all children's text representation between <code>[</code> and <code>]</code> characters, separated by <code>&amp;&amp;</code>.</p> <p><code>conditionString</code> is used when DynamicPruningSubquery, ScalarSubquery, ListQuery and Exists expressions are requested for a text representation.</p>"},{"location":"expressions/Predicate/","title":"Predicate Expressions","text":"<p><code>Predicate</code> is an extension of the Expression abstraction for predicate expressions that evaluate to a value of BooleanType type.</p>"},{"location":"expressions/Predicate/#implementations","title":"Implementations","text":"<ul> <li>BinaryComparison</li> <li>Exists</li> <li>In</li> <li>InSet</li> <li>others</li> </ul>"},{"location":"expressions/Predicate/#datatype","title":"DataType <pre><code>dataType: DataType\n</code></pre> <p><code>dataType</code> is part of the Expression abstraction.</p>  <p><code>dataType</code> is always BooleanType.</p>","text":""},{"location":"expressions/Predicate/#creating-basepredicate-for-bound-expression","title":"Creating BasePredicate for Bound Expression <pre><code>create(\n  e: Expression): BasePredicate\ncreate(\n  e: Expression,\n  inputSchema: Seq[Attribute]): BasePredicate\n</code></pre> <p><code>create</code> creates a BasePredicate for the given Expression that is bound to the input schema (Attributes).</p>","text":""},{"location":"expressions/Projection/","title":"Projection Functions","text":"<p><code>Projection</code> is an abstraction of InternalRow converter functions to produce an InternalRow for a given <code>InternalRow</code>.</p> <pre><code>Projection: InternalRow =&gt; InternalRow\n</code></pre>"},{"location":"expressions/Projection/#contract","title":"Contract","text":""},{"location":"expressions/Projection/#initialization","title":"Initialization <pre><code>initialize(\n  partitionIndex: Int): Unit\n</code></pre>","text":""},{"location":"expressions/Projection/#implementations","title":"Implementations","text":"<ul> <li><code>IdentityProjection</code></li> <li>InterpretedProjection</li> <li><code>InterpretedSafeProjection</code></li> <li><code>MutableProjection</code></li> <li>UnsafeProjection</li> </ul>"},{"location":"expressions/RowOrdering/","title":"RowOrdering","text":""},{"location":"expressions/RowOrdering/#isorderable","title":"isOrderable <pre><code>isOrderable(\n  dataType: DataType): Boolean\nisOrderable(\n  exprs: Seq[Expression]): Boolean\n</code></pre> <p><code>isOrderable</code> holds <code>true</code> when the DataType is one of the following:</p> <ul> <li>NullType</li> <li>AtomicType</li> <li>StructType with all fields orderable (recursive)</li> <li>ArrayType with orderable type of the elements</li> <li>UserDefinedType</li> </ul> <p><code>isOrderable</code>\u00a0is used when:</p> <ul> <li>JoinSelection execution planning strategy is executed (and SortMergeJoinExec is considered)</li> <li>FIXME</li> </ul>","text":""},{"location":"expressions/RuntimeReplaceable/","title":"RuntimeReplaceable Expressions","text":"<p><code>RuntimeReplaceable</code> is the marker contract for unary expressions that are replaced by Logical Optimizer with their child expression (that can then be evaluated).</p> <p>Note</p> <p>Catalyst Optimizer uses ReplaceExpressions logical optimization to replace <code>RuntimeReplaceable</code> expressions.</p> <p><code>RuntimeReplaceable</code> contract allows for expression aliases, i.e. expressions that are fairly complex in the inside than on the outside, and is used to provide compatibility with other SQL databases by supporting SQL functions with their more complex Catalyst expressions (that are already supported by Spark SQL).</p> <p>Note</p> <p><code>RuntimeReplaceables</code> are tied up to their SQL functions in FunctionRegistry.</p> <p>Note</p> <p>To make sure the <code>explain</code> plan and expression SQL works correctly, a <code>RuntimeReplaceable</code> implementation should override flatArguments and sql methods.</p>"},{"location":"expressions/ScalaAggregator/","title":"ScalaAggregator","text":"<p><code>ScalaAggregator[IN, BUF, OUT]</code> is a TypedImperativeAggregate (of <code>BUF</code> values).</p> <p><code>ScalaAggregator</code> is a UserDefinedExpression.</p>"},{"location":"expressions/ScalaAggregator/#creating-instance","title":"Creating Instance","text":"<p><code>ScalaAggregator</code> takes the following to be created:</p> <ul> <li> Children Expressions <li>Aggregator</li> <li> Input ExpressionEncoder (of <code>IN</code>s) <li> Buffer ExpressionEncoder (of <code>BUF</code>s) <li> <code>nullable</code> flag (default: <code>false</code>) <li> <code>isDeterministic</code> flag (default: <code>true</code>) <li> <code>mutableAggBufferOffset</code> (default: <code>0</code>) <li> <code>inputAggBufferOffset</code> (default: <code>0</code>) <li> Aggregator Name (default: undefined) <p><code>ScalaAggregator</code> is created when:</p> <ul> <li><code>UserDefinedAggregator</code> is requested to scalaAggregator</li> </ul>"},{"location":"expressions/ScalaAggregator/#aggregator","title":"Aggregator <p><code>ScalaAggregator</code> is given an Aggregator when created.</p>    ScalaAggregator Aggregator     outputEncoder outputEncoder   createAggregationBuffer zero   update reduce   merge merge   Interpreted Execution finish   name Simple class name","text":""},{"location":"expressions/ScalaAggregator/#interpreted-execution","title":"Interpreted Execution <pre><code>eval(\n  buffer: BUF): Any\n</code></pre> <p><code>eval</code> is part of the TypedImperativeAggregate abstraction.</p>  <p><code>eval</code> requests the Aggregator to finish with the given (reduction) <code>buffer</code>.</p> <p><code>eval</code> requests the outputSerializer to convert the result (of type <code>OUT</code> to an InternalRow).</p> <p>In the end, <code>eval</code> returns one of the following:</p> <ul> <li>The row if isSerializedAsStruct (per the outputEncoder)</li> <li>The object at the 0<sup>th</sup> index (that is assumed to be of DataType)</li> </ul>","text":""},{"location":"expressions/ScalaAggregator/#logical-analysis","title":"Logical Analysis <p>The input and buffer encoders are resolved and bound using <code>ResolveEncodersInScalaAgg</code> logical resolution rule.</p>","text":""},{"location":"expressions/ScalaAggregator/#execution-planning","title":"Execution Planning <p><code>ScalaAggregator</code> (as a TypedImperativeAggregate) uses aggBufferAttributes with BinaryType.</p> <p><code>BinaryType</code> is among unsupported types of HashAggregateExec and makes the physical operator out of scope for aggregation planning.</p> <p>Because of this <code>BinaryType</code> (in an aggregation buffer) <code>ScalaAggregator</code> will always be planned as ObjectHashAggregateExec or SortAggregateExec physical operators.</p>","text":""},{"location":"expressions/ScalaUDF/","title":"ScalaUDF Expression","text":"<p><code>ScalaUDF</code> is an Expression to manage the lifecycle of a user-defined function (and hook it to Catalyst execution path).</p> <p><code>ScalaUDF</code> is a <code>NonSQLExpression</code> (and so has no representation in SQL).</p> <p><code>ScalaUDF</code> is a UserDefinedExpression.</p>"},{"location":"expressions/ScalaUDF/#creating-instance","title":"Creating Instance","text":"<p><code>ScalaUDF</code> takes the following to be created:</p> <ul> <li> Function <li> DataType <li> Child Expressions <li> Input ExpressionEncoders <li> Output ExpressionEncoder <li> Name <li> <code>nullable</code> flag (default: <code>true</code>) <li> <code>udfDeterministic</code> flag (default: <code>true</code>) <p><code>ScalaUDF</code> is created when:</p> <ul> <li><code>UDFRegistration</code> is requested to register a UDF</li> <li><code>BaseDynamicPartitionDataWriter</code> is requested for partitionPathExpression</li> <li><code>SparkUserDefinedFunction</code> is requested to createScalaUDF</li> </ul>"},{"location":"expressions/ScalaUDF/#deterministic","title":"deterministic <pre><code>deterministic: Boolean\n</code></pre> <p><code>deterministic</code> is part of the Expression abstraction.</p>  <p><code>ScalaUDF</code> is <code>deterministic</code> when all the following hold:</p> <ol> <li>udfDeterministic is enabled</li> <li>All the children are deterministic</li> </ol>","text":""},{"location":"expressions/ScalaUDF/#text-representation","title":"Text Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code> is part of the TreeNode abstraction.</p>  <p><code>toString</code> uses the name and the [children] for the text representation:</p> <pre><code>[name]([comma-separated children])\n</code></pre>","text":""},{"location":"expressions/ScalaUDF/#name","title":"Name <pre><code>name: String\n</code></pre> <p><code>name</code> is part of the UserDefinedExpression abstraction.</p>  <p><code>name</code> is the udfName (if defined) or <code>UDF</code>.</p>","text":""},{"location":"expressions/ScalaUDF/#generating-java-source-code-for-code-generated-expression-evaluation","title":"Generating Java Source Code for Code-Generated Expression Evaluation <pre><code>doGenCode(\n  ctx: CodegenContext,\n  ev: ExprCode): ExprCode\n</code></pre> <p><code>doGenCode</code> is part of the Expression abstraction.</p>  <p><code>doGenCode</code> requests the given CodegenContext to register a reference (that gives a <code>udf</code> reference):</p>    Input Argument Value     <code>objName</code> <code>udf</code>   <code>obj</code> The given function   <code>className</code> <code>scala.FunctionN</code> (where <code>N</code> is the number of the given children)    <p>Since Scala functions are executed using <code>apply</code> method, <code>doGenCode</code> creates a string with the following source code:</p> <pre><code>[udf].apply([comma-separated funcArgs])\n</code></pre>  <p>Note</p> <p>There is more in <code>doGenCode</code>.</p>  <p>In the end, <code>doGenCode</code> generates a block of a Java code in the following format:</p> <pre><code>[evalCode]\n[initArgs]\n[boxedType] [resultTerm] = null;\ntry {\n  [funcInvocation];\n} catch (Throwable e) {\n  throw QueryExecutionErrors.failedExecuteUserDefinedFunctionError(\n    \"[funcCls]\", \"[inputTypesString]\", \"[outputType]\", e);\n}\n\n\nboolean [isNull] = [resultTerm] == null;\n[dataType] [value] = [defaultValue];\nif (![isNull]) {\n  [value] = [resultTerm];\n}\n</code></pre>","text":""},{"location":"expressions/ScalaUDF/#interpreted-expression-evaluation","title":"Interpreted Expression Evaluation <pre><code>eval(\n  input: InternalRow): Any\n</code></pre> <p><code>eval</code> is part of the Expression abstraction.</p>  <p><code>eval</code>...FIXME</p>","text":""},{"location":"expressions/ScalaUDF/#node-patterns","title":"Node Patterns <pre><code>nodePatterns: Seq[TreePattern]\n</code></pre> <p><code>nodePatterns</code> is part of the TreeNode abstraction.</p>  <p><code>nodePatterns</code> is SCALA_UDF.</p>","text":""},{"location":"expressions/ScalaUDF/#analysis","title":"Analysis <p>Logical Analyzer uses HandleNullInputsForUDF and <code>ResolveEncodersInUDF</code> logical evaluation rules to analyze queries with <code>ScalaUDF</code> expressions.</p>","text":""},{"location":"expressions/ScalaUDF/#demo","title":"Demo","text":""},{"location":"expressions/ScalaUDF/#zero-argument-udf","title":"Zero-Argument UDF","text":"<p>Let's define a zero-argument UDF.</p> <pre><code>val myUDF = udf { () =&gt; \"Hello World\" }\n</code></pre> <pre><code>// \"Execute\" the UDF\n// Attach it to an \"execution environment\", i.e. a Dataset\n// by specifying zero columns to execute on (since the UDF is no-arg)\nimport org.apache.spark.sql.catalyst.expressions.ScalaUDF\nval scalaUDF = myUDF().expr.asInstanceOf[ScalaUDF]\n\nassert(scalaUDF.resolved)\n</code></pre> <p>Let's execute the UDF (on every row in a <code>Dataset</code>). We simulate it relying on the <code>EmptyRow</code> that is the default <code>InternalRow</code> of <code>eval</code>.</p> <pre><code>scala&gt; scalaUDF.eval()\nres2: Any = Hello World\n</code></pre>"},{"location":"expressions/ScalaUDF/#whole-stage-code-gen","title":"Whole-Stage Code Gen","text":"<pre><code>import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\nval ctx = new CodegenContext\nval code = scalaUDF.genCode(ctx).code\n</code></pre> <pre><code>scala&gt; println(code)\nUTF8String result_1 = null;\ntry {\n  result_1 = (UTF8String)((scala.Function1[]) references[2] /* converters */)[0].apply(((scala.Function0) references[3] /* udf */).apply());\n} catch (Throwable e) {\n  throw QueryExecutionErrors.failedExecuteUserDefinedFunctionError(\n    \"$read$$iw$$Lambda$2020/0x000000080153ad78\", \"\", \"string\", e);\n}\n\n\nboolean isNull_1 = result_1 == null;\nUTF8String value_1 = null;\nif (!isNull_1) {\n  value_1 = result_1;\n}\n</code></pre>"},{"location":"expressions/ScalaUDF/#one-argument-udf","title":"One-Argument UDF","text":"<p>Let's define a UDF of one argument.</p> <pre><code>val lengthUDF = udf { s: String =&gt; s.length }.withName(\"lengthUDF\")\nval c = lengthUDF($\"name\")\n</code></pre> <pre><code>scala&gt; println(c.expr.treeString)\nUDF:lengthUDF('name)\n+- 'name\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.expressions.ScalaUDF\nassert(c.expr.isInstanceOf[ScalaUDF])\n</code></pre> <p>Let's define another UDF of one argument.</p> <pre><code>val hello = udf { s: String =&gt; s\"Hello $s\" }\n\n// Binding the hello UDF to a column name\nimport org.apache.spark.sql.catalyst.expressions.ScalaUDF\nval helloScalaUDF = hello($\"name\").expr.asInstanceOf[ScalaUDF]\n\nassert(helloScalaUDF.resolved == false)\n</code></pre> <pre><code>// Resolve helloScalaUDF, i.e. the only `name` column reference\n\nscala&gt; helloScalaUDF.children\nres4: Seq[org.apache.spark.sql.catalyst.expressions.Expression] = ArrayBuffer('name)\n</code></pre> <pre><code>// The column is free (i.e. not bound to a Dataset)\n// Define a Dataset that becomes the rows for the UDF\nval names = Seq(\"Jacek\", \"Agata\").toDF(\"name\")\n</code></pre> <pre><code>scala&gt; println(names.queryExecution.analyzed.numberedTreeString)\n00 Project [value#1 AS name#3]\n01 +- LocalRelation [value#1]\n</code></pre> <p>Resolve the references using the <code>Dataset</code>.</p> <pre><code>val plan = names.queryExecution.analyzed\nval resolver = spark.sessionState.analyzer.resolver\nimport org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute\nval resolvedUDF = helloScalaUDF.transformUp { case a @ UnresolvedAttribute(names) =&gt;\n// we're in controlled environment\n// so get is safe\nplan.resolve(names, resolver).get\n}\nassert(resolvedUDF.resolved)\n</code></pre> <pre><code>scala&gt; println(resolvedUDF.numberedTreeString)\n00 UDF(name#3)\n01 +- name#3: string\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.expressions.BindReferences\nval attrs = names.queryExecution.sparkPlan.output\nval boundUDF = BindReferences.bindReference(resolvedUDF, attrs)\n\n// Create an internal binary row, i.e. InternalRow\nimport org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\nval stringEncoder = ExpressionEncoder[String]\nval row = stringEncoder.toRow(\"world\")\n</code></pre> <p>Yay! It works!</p> <pre><code>scala&gt; boundUDF.eval(row)\nres8: Any = Hello world\n</code></pre> <p>Just to show the regular execution path (i.e. how to execute an UDF in a context of a <code>Dataset</code>).</p> <pre><code>val q = names.select(hello($\"name\"))\n</code></pre> <pre><code>scala&gt; q.show\n+-----------+\n|  UDF(name)|\n+-----------+\n|Hello Jacek|\n|Hello Agata|\n+-----------+\n</code></pre>"},{"location":"expressions/ScalarSubquery/","title":"ScalarSubquery","text":"<p><code>ScalarSubquery</code> is a SubqueryExpression that returns a single row and a single column only.</p> <p><code>ScalarSubquery</code> represents a structured query that can be used as a \"column\".</p> <p><code>ScalarSubquery</code> represents a subquery expression in a logical plan.</p> <p>Important</p> <p>Spark SQL uses the name of <code>ScalarSubquery</code> twice to represent a <code>SubqueryExpression</code> (this page) and an ExecSubqueryExpression.</p>"},{"location":"expressions/ScalarSubquery/#demo","title":"Demo","text":"<pre><code>// FIXME DEMO\n\n// Borrowed from ExpressionParserSuite.scala\n// ScalarSubquery(table(\"tbl\").select('max.function('val))) &gt; 'current)\nval sql = \"(select max(val) from tbl) &gt; current\"\n\n// 'a === ScalarSubquery(table(\"s\").select('b))\nval sql = \"a = (select b from s)\"\n\n// Borrowed from PlanParserSuite.scala\n// table(\"t\").select(ScalarSubquery(table(\"s\").select('max.function('b))).as(\"ss\"))\nval sql = \"select (select max(b) from s) ss from t\"\n\n// table(\"t\").where('a === ScalarSubquery(table(\"s\").select('b))).select(star())\nval sql = \"select * from t where a = (select b from s)\"\n\n// table(\"t\").groupBy('g)('g).where('a &gt; ScalarSubquery(table(\"s\").select('b)))\nval sql = \"select g from t group by g having a &gt; (select b from s)\"\n</code></pre>"},{"location":"expressions/SimpleHigherOrderFunction/","title":"SimpleHigherOrderFunction","text":"<p><code>SimpleHigherOrderFunction</code> is an extension of the HigherOrderFunction abstraction for FIXME that method and...FIXME.</p>"},{"location":"expressions/SimpleHigherOrderFunction/#contract","title":"Contract","text":""},{"location":"expressions/SimpleHigherOrderFunction/#argument","title":"argument <pre><code>argument: Expression\n</code></pre> <p>Used when...FIXME</p>","text":""},{"location":"expressions/SimpleHigherOrderFunction/#argumenttype","title":"argumentType <pre><code>argumentType: AbstractDataType\n</code></pre> <p>Used when...FIXME</p>","text":""},{"location":"expressions/SimpleHigherOrderFunction/#function","title":"function <pre><code>function: Expression\n</code></pre> <p>Used when...FIXME</p>","text":""},{"location":"expressions/SimpleHigherOrderFunction/#implementations","title":"Implementations","text":"<ul> <li>ArrayBasedSimpleHigherOrderFunction</li> <li>MapBasedSimpleHigherOrderFunction</li> </ul>"},{"location":"expressions/SortOrder/","title":"SortOrder","text":"<p><code>SortOrder</code> is an Expression that represents the following operators in a structured query:</p> <ul> <li> <p><code>AstBuilder</code> is requested to parse ORDER BY or SORT BY sort specifications</p> </li> <li> <p>Column.asc, Column.asc_nulls_first, Column.asc_nulls_last, Column.desc, Column.desc_nulls_first, and Column.desc_nulls_last high-level operators</p> </li> </ul>"},{"location":"expressions/SortOrder/#creating-instance","title":"Creating Instance","text":"<p><code>SortOrder</code> takes the following to be created:</p> <ul> <li> Child Expression <li> SortDirection <li> NullOrdering <li> \"Same Order\" Expressions"},{"location":"expressions/SortOrder/#sortdirection","title":"SortDirection    SortDirection Default Null Ordering SQL     <code>Ascending</code> NullsFirst ASC   <code>Descending</code> NullsLast DESC","text":""},{"location":"expressions/SortOrder/#nullordering","title":"NullOrdering    NullOrdering SQL     <code>NullsFirst</code> NULLS FIRST   <code>NullsLast</code> NULLS LAST","text":""},{"location":"expressions/SortOrder/#creating-sortorder","title":"Creating SortOrder <pre><code>apply(\n  child: Expression,\n  direction: SortDirection,\n  sameOrderExpressions: Seq[Expression] = Seq.empty): SortOrder\n</code></pre> <p><code>apply</code> creates a SortOrder (with the <code>defaultNullOrdering</code> of the given SortDirection).</p>","text":""},{"location":"expressions/SortOrder/#catalyst-dsl","title":"Catalyst DSL <p>Catalyst DSL defines the following operators to create <code>SortOrder</code>s:</p> <ul> <li>asc (with <code>null</code>s first)</li> <li>asc_nullsLast</li> <li>desc (with <code>null</code>s last)</li> <li>desc_nullsFirst</li> </ul> <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\nval sortNullsLast = 'id.asc_nullsLast\nscala&gt; println(sortNullsLast.sql)\n`id` ASC NULLS LAST\n</code></pre>","text":""},{"location":"expressions/SortOrder/#unevaluable","title":"Unevaluable <p><code>SortOrder</code> is an Unevaluable expression.</p>","text":""},{"location":"expressions/SortOrder/#physical-operators","title":"Physical Operators <p><code>SortOrder</code> is used to specify the output data ordering requirements and the required child ordering of physical operators.</p>","text":""},{"location":"expressions/SparkUserDefinedFunction/","title":"SparkUserDefinedFunction","text":"<p><code>SparkUserDefinedFunction</code> is a UserDefinedFunction that uses ScalaUDF for execution.</p> <p><code>SparkUserDefinedFunction</code> is created using udf standard function (among the other less interesting means).</p>"},{"location":"expressions/SparkUserDefinedFunction/#creating-instance","title":"Creating Instance","text":"<p><code>SparkUserDefinedFunction</code> takes the following to be created:</p> <ul> <li> Scala Function <li> DataType <li> Input ExpressionEncoders <li> Output ExpressionEncoder <li> Name <li> <code>nullable</code> flag (default: <code>true</code>) <li> <code>deterministic</code> flag (default: <code>true</code>) <p><code>SparkUserDefinedFunction</code> is created when:</p> <ul> <li><code>FPGrowthModel</code> (Spark MLlib) is requested to <code>genericTransform</code></li> <li>udf standard function is used</li> <li><code>UDFRegistration</code> is requested to register a named user-defined function</li> </ul>"},{"location":"expressions/SparkUserDefinedFunction/#creating-column-for-function-execution","title":"Creating Column (for Function Execution) <pre><code>apply(\n  exprs: Column*): Column\n</code></pre> <p><code>apply</code> is part of the UserDefinedFunction abstraction.</p>  <p><code>apply</code> creates a Column with a ScalaUDF (with the given <code>exprs</code>).</p>","text":""},{"location":"expressions/SparkUserDefinedFunction/#creating-scalaudf","title":"Creating ScalaUDF <pre><code>createScalaUDF(\n  exprs: Seq[Expression]): ScalaUDF\n</code></pre> <p><code>createScalaUDF</code> creates a ScalaUDF expression.</p>  <p><code>createScalaUDF</code> is used when:</p> <ul> <li><code>UDFRegistration</code> is requested to register a named user-defined function</li> <li><code>SparkUserDefinedFunction</code> is requested to create a Column (for function execution)</li> </ul>","text":""},{"location":"expressions/Stateful/","title":"Stateful","text":"<p><code>Stateful</code> is...FIXME</p>"},{"location":"expressions/StaticInvoke/","title":"StaticInvoke","text":"<p><code>StaticInvoke</code> is an expression with no SQL representation that represents a static method call in Scala or Java.</p> <p><code>StaticInvoke</code> supports Java code generation to evaluate itself.</p> <p><code>StaticInvoke</code> is &lt;&gt; when: <ul> <li> <p><code>ScalaReflection</code> is requested for the deserializer or serializer for a Scala type</p> </li> <li> <p>RowEncoder is requested for <code>deserializerFor</code> or serializer for a Scala type</p> </li> <li> <p><code>JavaTypeInference</code> is requested for <code>deserializerFor</code> or <code>serializerFor</code></p> </li> </ul> <pre><code>import org.apache.spark.sql.types.StructType\nval schema = new StructType()\n  .add($\"id\".long.copy(nullable = false))\n  .add($\"name\".string.copy(nullable = false))\n\nimport org.apache.spark.sql.catalyst.encoders.RowEncoder\nval encoder = RowEncoder(schema)\nscala&gt; println(encoder.serializer(0).numberedTreeString)\n00 validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, id), LongType) AS id#1640L\n01 +- validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, id), LongType)\n02    +- getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, id)\n03       +- assertnotnull(input[0, org.apache.spark.sql.Row, true])\n04          +- input[0, org.apache.spark.sql.Row, true]\n</code></pre> <p>NOTE: <code>StaticInvoke</code> is similar to <code>CallMethodViaReflection</code> expression.</p>"},{"location":"expressions/StaticInvoke/#creating-instance","title":"Creating Instance","text":"<p><code>StaticInvoke</code> takes the following when created:</p> <ul> <li>[[staticObject]] Target object of the static call</li> <li>[[dataType]] Data type of the return value of the &lt;&gt; <li>[[functionName]] Name of the method to call on the &lt;&gt; <li>[[arguments]] Optional Expression.md[expressions] to pass as input arguments to the &lt;&gt; <li>[[propagateNull]] Flag to control whether to propagate <code>nulls</code> or not (enabled by default). If any of the arguments is <code>null</code>, <code>null</code> is returned instead of calling the &lt;&gt;"},{"location":"expressions/SubqueryExpression/","title":"SubqueryExpression Expressions","text":"<p><code>SubqueryExpression</code> is an extension of the PlanExpression abstraction for subquery expressions with logical plans (for a subquery).</p>"},{"location":"expressions/SubqueryExpression/#contract","title":"Contract","text":""},{"location":"expressions/SubqueryExpression/#withnewplan","title":"withNewPlan","text":"<p> <pre><code>withNewPlan(\nplan: LogicalPlan): SubqueryExpression\n</code></pre> <p>Note</p> <p><code>withNewPlan</code> is part of the PlanExpression abstraction and is defined as follows:</p> <pre><code>withNewPlan(plan: T): PlanExpression[T]\n</code></pre> <p>The purpose of this override method is to change the input and output generic types to the concrete LogicalPlan and <code>SubqueryExpression</code>, respectively.</p>"},{"location":"expressions/SubqueryExpression/#implementations","title":"Implementations","text":"<ul> <li>DynamicPruningSubquery</li> <li>Exists</li> <li>ListQuery</li> <li>ScalarSubquery</li> </ul>"},{"location":"expressions/SubqueryExpression/#creating-instance","title":"Creating Instance","text":"<p><code>SubqueryExpression</code> takes the following to be created:</p> <ul> <li> Subquery logical plan <li> Child Expressions <li> Expression ID <p>Abstract Class</p> <p><code>SubqueryExpression</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete SubqueryExpressions.</p>"},{"location":"expressions/SubqueryExpression/#references","title":"References","text":"<p> <pre><code>references: AttributeSet\n</code></pre> <p><code>references</code> is...FIXME</p> <p><code>references</code> is part of the Expression abstraction.</p>"},{"location":"expressions/SubqueryExpression/#resolved-predicate","title":"resolved Predicate","text":"<p> <pre><code>resolved: Boolean\n</code></pre> <p><code>resolved</code> is <code>true</code> when all of the following hold:</p> <ul> <li>children are all resolved</li> <li>subquery logical plan is resolved</li> </ul> <p><code>resolved</code> is part of the Expression abstraction.</p>"},{"location":"expressions/SubqueryExpression/#hasinorcorrelatedexistssubquery-utility","title":"hasInOrCorrelatedExistsSubquery Utility","text":"<p> <pre><code>hasInOrCorrelatedExistsSubquery(\ne: Expression): Boolean\n</code></pre> <p><code>hasInOrCorrelatedExistsSubquery</code>...FIXME</p> <p><code>hasInOrCorrelatedExistsSubquery</code> is used when RewritePredicateSubquery logical optimization is executed.</p>"},{"location":"expressions/SubqueryExpression/#hascorrelatedsubquery-utility","title":"hasCorrelatedSubquery Utility","text":"<p> <pre><code>hasCorrelatedSubquery(\ne: Expression): Boolean\n</code></pre> <p><code>hasCorrelatedSubquery</code>...FIXME</p> <p><code>hasCorrelatedSubquery</code> is used when:</p> <ul> <li><code>EliminateOuterJoin</code> logical optimization is executed</li> <li><code>Subquery</code> is created (from an expression)</li> <li><code>Filter</code> logical operator is requested for <code>validConstraints</code></li> </ul>"},{"location":"expressions/SubqueryExpression/#hassubquery-utility","title":"hasSubquery Utility","text":"<p> <pre><code>hasSubquery(\ne: Expression): Boolean\n</code></pre> <p><code>hasSubquery</code>...FIXME</p> <p><code>hasSubquery</code> is used when...FIXME</p>"},{"location":"expressions/TimeWindow/","title":"TimeWindow","text":"<p><code>TimeWindow</code> is an Unevaluable, <code>NonSQLExpression</code> UnaryExpression that represents window function.</p> <pre><code>import org.apache.spark.sql.functions.window\nval w = window(\n'timeColumn,\nwindowDuration = \"10 seconds\",\nslideDuration = \"5 seconds\",\nstartTime = \"0 seconds\")\n</code></pre> <pre><code>scala&gt; println(w.expr.numberedTreeString)\n00 window('timeColumn, 10000000, 5000000, 0) AS window#1\n01 +- window('timeColumn, 10000000, 5000000, 0)\n02    +- 'timeColumn\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.expressions.TimeWindow\nval tw = w.expr.children.head.asInstanceOf[TimeWindow]\n</code></pre>"},{"location":"expressions/TimeWindow/#creating-instance","title":"Creating Instance","text":"<p><code>TimeWindow</code> takes the following to be created:</p> <ul> <li> Time Column (Expression) <li> Window Duration <li> Slide Duration <li> Start Time <p><code>TimeWindow</code> is created using apply factory method.</p>"},{"location":"expressions/TimeWindow/#creating-timewindow","title":"Creating TimeWindow <pre><code>apply(\n  timeColumn: Expression,\n  windowDuration: String,\n  slideDuration: String,\n  startTime: String): TimeWindow\n</code></pre> <p><code>apply</code> creates a TimeWindow (for the given <code>timeColumn</code> expression with the window and slide durations and start time converted to seconds).</p> <p><code>apply</code> is used when:</p> <ul> <li>window standard function is used</li> </ul>","text":""},{"location":"expressions/TimeWindow/#parsing-time-interval-to-microseconds","title":"Parsing Time Interval to Microseconds <pre><code>getIntervalInMicroSeconds(\n  interval: String): Long\n</code></pre> <p><code>getIntervalInMicroSeconds</code>...FIXME</p> <p><code>getIntervalInMicroSeconds</code> is used when:</p> <ul> <li><code>TimeWindow</code> utility is used to parseExpression and apply</li> </ul>","text":""},{"location":"expressions/TimeWindow/#analysis-phase","title":"Analysis Phase <p><code>TimeWindow</code> is resolved to Expand unary logical operator (when <code>TimeWindowing</code> logical evaluation rule is executed).</p> <pre><code>import java.time.LocalDateTime\nimport java.sql.Timestamp\n\nval levels = Seq(\n  // (year, month, dayOfMonth, hour, minute, second)\n  ((2012, 12, 12, 12, 12, 12), 5),\n  ((2012, 12, 12, 12, 12, 14), 9),\n  ((2012, 12, 12, 13, 13, 14), 4),\n  ((2016, 8,  13, 0, 0, 0), 10),\n  ((2017, 5,  27, 0, 0, 0), 15)).\n  map { case ((yy, mm, dd, h, m, s), a) =&gt; (LocalDateTime.of(yy, mm, dd, h, m, s), a) }.\n  map { case (ts, a) =&gt; (Timestamp.valueOf(ts), a) }.\n  toDF(\"time\", \"level\")\nscala&gt; levels.show\n+-------------------+-----+\n|               time|level|\n+-------------------+-----+\n|2012-12-12 12:12:12|    5|\n|2012-12-12 12:12:14|    9|\n|2012-12-12 13:13:14|    4|\n|2016-08-13 00:00:00|   10|\n|2017-05-27 00:00:00|   15|\n+-------------------+-----+\n\nval q = levels.select(window($\"time\", \"5 seconds\"))\n\n// Before Analyzer\nscala&gt; println(q.queryExecution.logical.numberedTreeString)\n00 'Project [timewindow('time, 5000000, 5000000, 0) AS window#18]\n01 +- Project [_1#6 AS time#9, _2#7 AS level#10]\n02    +- LocalRelation [_1#6, _2#7]\n\n// After Analyzer\nscala&gt; println(q.queryExecution.analyzed.numberedTreeString)\n00 Project [window#19 AS window#18]\n01 +- Filter ((time#9 &gt;= window#19.start) &amp;&amp; (time#9 &lt; window#19.end))\n02    +- Expand [List(named_struct(start, ((((CEIL((cast((precisetimestamp(time#9) - 0) as double) / cast(5000000 as double))) + cast(0 as bigint)) - cast(1 as bigint)) * 5000000) + 0), end, (((((CEIL((cast((precisetimestamp(time#9) - 0) as double) / cast(5000000 as double))) + cast(0 as bigint)) - cast(1 as bigint)) * 5000000) + 0) + 5000000)), time#9, level#10), List(named_struct(start, ((((CEIL((cast((precisetimestamp(time#9) - 0) as double) / cast(5000000 as double))) + cast(1 as bigint)) - cast(1 as bigint)) * 5000000) + 0), end, (((((CEIL((cast((precisetimestamp(time#9) - 0) as double) / cast(5000000 as double))) + cast(1 as bigint)) - cast(1 as bigint)) * 5000000) + 0) + 5000000)), time#9, level#10)], [window#19, time#9, level#10]\n03       +- Project [_1#6 AS time#9, _2#7 AS level#10]\n04          +- LocalRelation [_1#6, _2#7]\n</code></pre>","text":""},{"location":"expressions/TypedImperativeAggregate/","title":"TypedImperativeAggregate Expressions","text":"<p><code>TypedImperativeAggregate</code> is an extension of the ImperativeAggregate abstraction for typed ImperativeAggregates.</p>"},{"location":"expressions/TypedImperativeAggregate/#contract","title":"Contract","text":""},{"location":"expressions/TypedImperativeAggregate/#creating-aggregation-buffer","title":"Creating Aggregation Buffer <pre><code>createAggregationBuffer(): T\n</code></pre> <p>See:</p> <ul> <li>BloomFilterAggregate</li> <li>ScalaAggregator</li> </ul> <p>Used when:</p> <ul> <li><code>Collect</code> is requested to deserialize</li> <li><code>TypedImperativeAggregate</code> is requested to initialize</li> </ul>","text":""},{"location":"expressions/TypedImperativeAggregate/#deserializing","title":"Deserializing <pre><code>deserialize(\n  storageFormat: Array[Byte]): T\n</code></pre> <p>See:</p> <ul> <li>BloomFilterAggregate</li> <li>ScalaAggregator</li> </ul> <p>Used when:</p> <ul> <li><code>TypedImperativeAggregate</code> is requested to merge</li> </ul>","text":""},{"location":"expressions/TypedImperativeAggregate/#interpreted-execution","title":"Interpreted Execution <pre><code>eval(\n  buffer: T): Any\n</code></pre> <p>See:</p> <ul> <li>BloomFilterAggregate</li> <li>ScalaAggregator</li> </ul> <p>Used when:</p> <ul> <li><code>TypedImperativeAggregate</code> is requested to execute (interpreted mode)</li> </ul>","text":""},{"location":"expressions/TypedImperativeAggregate/#merge","title":"merge <pre><code>merge(\n  buffer: T,\n  input: T): T\n</code></pre> <p>See:</p> <ul> <li>BloomFilterAggregate</li> <li>ScalaAggregator</li> </ul> <p>Used when:</p> <ul> <li><code>TypedImperativeAggregate</code> is requested to merge and mergeBuffersObjects</li> </ul>","text":""},{"location":"expressions/TypedImperativeAggregate/#serialize","title":"serialize <pre><code>serialize(\n  buffer: T): Array[Byte]\n</code></pre> <p>See:</p> <ul> <li>BloomFilterAggregate</li> <li>ScalaAggregator</li> </ul> <p>Used when:</p> <ul> <li><code>TypedImperativeAggregate</code> is requested to serializeAggregateBufferInPlace</li> </ul>","text":""},{"location":"expressions/TypedImperativeAggregate/#update","title":"update <pre><code>update(\n  buffer: T,\n  input: InternalRow): T\n</code></pre> <p>See:</p> <ul> <li>BloomFilterAggregate</li> <li>ScalaAggregator</li> </ul> <p>Used when:</p> <ul> <li><code>TypedImperativeAggregate</code> is requested to update</li> </ul>","text":""},{"location":"expressions/TypedImperativeAggregate/#implementations","title":"Implementations","text":"<ul> <li>BloomFilterAggregate</li> <li>ScalaAggregator</li> <li>others</li> </ul>"},{"location":"expressions/TypedImperativeAggregate/#interpreted-expression-evaluation","title":"Interpreted Expression Evaluation <pre><code>eval(\n  buffer: InternalRow): Any\n</code></pre> <p><code>eval</code> is part of the Expression abstraction.</p>  <p><code>eval</code> extracts the buffer object (for the given InternalRow) and evaluates the result.</p>","text":""},{"location":"expressions/TypedImperativeAggregate/#aggregation-buffer-attributes","title":"Aggregation Buffer Attributes <pre><code>aggBufferAttributes: Seq[AttributeReference]\n</code></pre> <p><code>aggBufferAttributes</code> is part of the AggregateFunction abstraction.</p>  <p><code>aggBufferAttributes</code> is a single AttributeReference:</p>    Name DataType     <code>buf</code> BinaryType","text":""},{"location":"expressions/TypedImperativeAggregate/#extracting-buffer-object","title":"Extracting Buffer Object <pre><code>getBufferObject(\n  bufferRow: InternalRow): T // (1)!\ngetBufferObject(\n  buffer: InternalRow,\n  offset: Int): T\n</code></pre> <ol> <li>Uses the mutableAggBufferOffset as the <code>offset</code></li> </ol> <p><code>getBufferObject</code> requests the given InternalRow for the value (of type <code>T</code>) at the given <code>offset</code> that is of ObjectType type.</p>  <p><code>getBufferObject</code> is used when:</p> <ul> <li><code>TypedImperativeAggregate</code> is requested to mergeBuffersObjects, update, merge, eval, serializeAggregateBufferInPlace</li> </ul>","text":""},{"location":"expressions/TypedImperativeAggregate/#objecttype","title":"ObjectType <pre><code>anyObjectType: ObjectType\n</code></pre> <p>When created, <code>TypedImperativeAggregate</code> creates an <code>ObjectType</code> of a value of Scala <code>AnyRef</code> type.</p> <p>The <code>ObjectType</code> is used in getBufferObject.</p>","text":""},{"location":"expressions/UnaryExpression/","title":"UnaryExpression","text":"<p><code>UnaryExpression</code> is an extension of the Expression abstraction for expressions with a single child.</p>"},{"location":"expressions/UnaryExpression/#contract","title":"Contract","text":""},{"location":"expressions/UnaryExpression/#child-expression","title":"Child Expression <pre><code>child: Expression\n</code></pre> <p>Child Expression</p>","text":""},{"location":"expressions/UnaryExpression/#implementations","title":"Implementations","text":"<ul> <li><code>Alias</code></li> <li>ArrayDistinct</li> <li>ArrayMax</li> <li>ArrayMin</li> <li>BitwiseCount</li> <li>CsvToStructs</li> <li>MapEntries</li> <li>MapKeys</li> <li>MapValues</li> <li>MultiAlias</li> <li>PrintToStderr</li> <li>SchemaOfCsv</li> <li>others...</li> </ul>"},{"location":"expressions/UnaryExpression/#interpreted-expression-evaluation","title":"Interpreted Expression Evaluation <pre><code>eval(\n  input: InternalRow): Any\n</code></pre> <p><code>eval</code>...FIXME</p> <p><code>eval</code> is part of the Expression abstraction.</p>","text":""},{"location":"expressions/UnaryExpression/#definecodegen","title":"defineCodeGen <pre><code>defineCodeGen(\n  ctx: CodegenContext,\n  ev: ExprCode,\n  f: String =&gt; String): ExprCode\n</code></pre> <p><code>defineCodeGen</code>...FIXME</p>","text":""},{"location":"expressions/UnaryExpression/#nullsafecodegen","title":"nullSafeCodeGen <pre><code>nullSafeCodeGen(\n  ctx: CodegenContext,\n  ev: ExprCode,\n  f: String =&gt; String): ExprCode\n</code></pre> <p><code>nullSafeCodeGen</code>...FIXME</p> <p><code>nullSafeCodeGen</code> is used when...FIXME</p>","text":""},{"location":"expressions/UnaryExpression/#nullsafeeval","title":"nullSafeEval <pre><code>nullSafeEval(\n  input: Any): Any\n</code></pre> <p><code>nullSafeEval</code> simply fails with the following error (and is expected to be overrided to save null-check code):</p> <pre><code>UnaryExpressions must override either eval or nullSafeEval\n</code></pre> <p><code>nullSafeEval</code> is used when <code>UnaryExpression</code> is requested to evaluate (in interpreted mode).</p>","text":""},{"location":"expressions/Unevaluable/","title":"Unevaluable Expressions","text":"<p><code>Unevaluable</code> is an extension of the Expression abstraction for unevaluable expression that cannot be evaluated to produce a value (neither in the interpreted nor code-generated mode).</p> <p><code>Unevaluable</code> expressions are expected to be resolved (replaced) to \"evaluable\" expressions or logical operators at analysis or optimization phases or they fail analysis.</p> <p>Unevaluable expressions cannot be evaluated (neither in &lt;&gt; nor &lt;&gt; expression evaluations) and has to be"},{"location":"expressions/Unevaluable/#implementations","title":"Implementations","text":"<ul> <li>AggregateExpression</li> <li>DeclarativeAggregate</li> <li>DynamicPruningSubquery</li> <li>Exists</li> <li>HashPartitioning</li> <li>ListQuery</li> <li>RuntimeReplaceable</li> <li>SortOrder</li> <li>TimeWindow</li> <li>UnresolvedAttribute</li> <li>UnresolvedFunction</li> <li>UnresolvedOrdinal</li> <li>UnresolvedStar</li> <li>WindowExpression</li> <li>WindowSpecDefinition</li> <li>others</li> </ul>"},{"location":"expressions/Unevaluable/#demo","title":"Demo","text":"<pre><code>/**\nExample: Analysis failure due to an Unevaluable expression\nUnresolvedFunction is an Unevaluable expression\nUsing Catalyst DSL to create a UnresolvedFunction\n*/\nimport org.apache.spark.sql.catalyst.dsl.expressions._\nval f = 'f.function()\n\nimport org.apache.spark.sql.catalyst.dsl.plans._\nval logicalPlan = table(\"t1\").select(f)\nscala&gt; println(logicalPlan.numberedTreeString)\n00 'Project [unresolvedalias('f(), None)]\n01 +- 'UnresolvedRelation `t1`\n\nscala&gt; spark.sessionState.analyzer.execute(logicalPlan)\norg.apache.spark.sql.AnalysisException: Undefined function: 'f'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.;\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15$$anonfun$applyOrElse$49.apply(Analyzer.scala:1198)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15$$anonfun$applyOrElse$49.apply(Analyzer.scala:1198)\n  at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:48)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1197)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1195)\n</code></pre>"},{"location":"expressions/UnixTimestamp/","title":"UnixTimestamp","text":"<p><code>UnixTimestamp</code> is a binary expression with timezone support that represents unix_timestamp function (and indirectly to_date and to_timestamp).</p> <pre><code>import org.apache.spark.sql.functions.unix_timestamp\nval c1 = unix_timestamp()\n\nscala&gt; c1.explain(true)\nunix_timestamp(current_timestamp(), yyyy-MM-dd HH:mm:ss, None)\n\nscala&gt; println(c1.expr.numberedTreeString)\n00 unix_timestamp(current_timestamp(), yyyy-MM-dd HH:mm:ss, None)\n01 :- current_timestamp()\n02 +- yyyy-MM-dd HH:mm:ss\n\nimport org.apache.spark.sql.catalyst.expressions.UnixTimestamp\nscala&gt; c1.expr.isInstanceOf[UnixTimestamp]\nres0: Boolean = true\n</code></pre> <p>NOTE: <code>UnixTimestamp</code> is <code>UnixTime</code> expression internally (as is <code>ToUnixTimestamp</code> expression).</p> <p>[[inputTypes]][[dataType]] <code>UnixTimestamp</code> supports <code>StringType</code>, DateType and <code>TimestampType</code> as input types for a time expression and returns <code>LongType</code>.</p> <pre><code>scala&gt; c1.expr.eval()\nres1: Any = 1493354303\n</code></pre> <p>[[formatter]] <code>UnixTimestamp</code> uses <code>DateTimeUtils.newDateFormat</code> for date/time format (as Java's java.text.DateFormat).</p>"},{"location":"expressions/UnresolvedAttribute/","title":"UnresolvedAttribute","text":"<p>[[name]] <code>UnresolvedAttribute</code> is a named spark-sql-Expression-Attribute.md[Attribute] leaf expression (i.e. it has a name) that represents a reference to an entity in a logical query plan.</p> <p><code>UnresolvedAttribute</code> is &lt;&gt; when: <ul> <li> <p><code>AstBuilder</code> is requested to sql/AstBuilder.md#visitDereference[visitDereference]</p> </li> <li> <p><code>LogicalPlan</code> is requested to spark-sql-LogicalPlan.md#resolve[resolve an attribute by name parts]</p> </li> <li> <p><code>DescribeColumnCommand</code> is DescribeColumnCommand.md#run[executed]</p> </li> </ul> <p>[[resolved]] <code>UnresolvedAttribute</code> can never be Expression.md#resolved[resolved] (and is replaced at &lt;&gt;). <p>[[analysis-phase]] [NOTE] ==== <code>UnresolvedAttribute</code> is resolved when Logical Analyzer is executed by the following logical resolution rules:</p> <ul> <li> <p>ResolveReferences</p> </li> <li> <p><code>ResolveMissingReferences</code></p> </li> <li> <p><code>ResolveDeserializer</code></p> </li> </ul>"},{"location":"expressions/UnresolvedAttribute/#resolvesubquery","title":"* <code>ResolveSubquery</code>","text":"<p>[[Unevaluable]][[eval]][[doGenCode]] Given <code>UnresolvedAttribute</code> can never be resolved it should not come as a surprise that it cannot be evaluated either (i.e. produce a value given an internal row). When requested to evaluate, <code>UnresolvedAttribute</code> simply reports a <code>UnsupportedOperationException</code>.</p> <pre><code>Cannot evaluate expression: [this]\n</code></pre> <p>[[creating-instance]] [[nameParts]] <code>UnresolvedAttribute</code> takes name parts when created.</p> <p>[[apply]] <code>UnresolvedAttribute</code> can be created with a fully-qualified name with dots to separate name parts.</p>"},{"location":"expressions/UnresolvedAttribute/#source-scala","title":"[source, scala]","text":""},{"location":"expressions/UnresolvedAttribute/#applyname-string-unresolvedattribute","title":"apply(name: String): UnresolvedAttribute","text":""},{"location":"expressions/UnresolvedAttribute/#tip","title":"[TIP]","text":"<p>Use backticks (<code>```) around names with dots (</code>.`) to disable them as separators.</p> <p>The following is a two-part attribute name with <code>a.b</code> and <code>c</code> name parts.</p>"},{"location":"expressions/UnresolvedAttribute/#abc","title":"<pre><code>`a.b`.c\n</code></pre>","text":"<p>[[quoted]] <code>UnresolvedAttribute</code> can also be created without the dots with the special meaning.</p>"},{"location":"expressions/UnresolvedAttribute/#source-scala_1","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute val attr1 = UnresolvedAttribute.quoted(\"a.b.c\") scala&gt; println(s\"Number of name parts: ${attr1.nameParts.length}\") Number of name parts: 1</p>"},{"location":"expressions/UnresolvedAttribute/#note","title":"[NOTE]","text":"<p>Catalyst DSL defines two Scala implicits to create an <code>UnresolvedAttribute</code>:</p> <ul> <li> <p><code>StringToAttributeConversionHelper</code> is a Scala implicit class that converts <code>$\"colName\"</code> into an <code>UnresolvedAttribute</code></p> </li> <li> <p><code>symbolToUnresolvedAttribute</code> is a Scala implicit method that converts <code>'colName</code> into an <code>UnresolvedAttribute</code></p> </li> </ul> <p>Both implicits are part of ExpressionConversions Scala trait of Catalyst DSL.</p> <p>Import <code>expressions</code> object to get access to the expression conversions.</p>"},{"location":"expressions/UnresolvedAttribute/#source-scala_2","title":"[source, scala]","text":"<p>// Use <code>sbt console</code> with Spark libraries defined (in <code>build.sbt</code>) import org.apache.spark.sql.catalyst.dsl.expressions._ scala&gt; :type $\"name\" org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute</p> <p>import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute val nameAttr: UnresolvedAttribute = 'name</p> <p>scala&gt; :type nameAttr org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute</p> <p>====</p>"},{"location":"expressions/UnresolvedAttribute/#note_1","title":"[NOTE]","text":"<p>A <code>UnresolvedAttribute</code> can be replaced by (resolved) a <code>NamedExpression</code> using an spark-sql-LogicalPlan.md#resolveQuoted[analyzed logical plan] (of the structured query the attribute is part of).</p>"},{"location":"expressions/UnresolvedAttribute/#source-scala_3","title":"[source, scala]","text":"<p>val analyzedPlan = Seq((0, \"zero\")).toDF(\"id\", \"name\").queryExecution.analyzed</p> <p>import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute val nameAttr = UnresolvedAttribute(\"name\") val nameResolved = analyzedPlan.resolveQuoted(   name = nameAttr.name,   resolver = spark.sessionState.analyzer.resolver).getOrElse(nameAttr)</p> <p>scala&gt; println(nameResolved.numberedTreeString) 00 name#47: string</p> <p>scala&gt; :type nameResolved org.apache.spark.sql.catalyst.expressions.NamedExpression</p> <p>====</p>"},{"location":"expressions/UnresolvedAttribute/#demo","title":"Demo","text":"<pre><code>import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute\n\nscala&gt; val t1 = UnresolvedAttribute(\"t1\")\nt1: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute = 't1\n\nscala&gt; val t2 = UnresolvedAttribute(\"db1.t2\")\nt2: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute = 'db1.t2\n\nscala&gt; println(s\"Number of name parts: ${t2.nameParts.length}\")\nNumber of name parts: 2\n\nscala&gt; println(s\"Name parts: ${t2.nameParts.mkString(\",\")}\")\nName parts: db1,t2\n</code></pre>"},{"location":"expressions/UnresolvedFunction/","title":"UnresolvedFunction","text":"<p><code>UnresolvedFunction</code> is an Expression that represents a function (application) in a logical query plan.</p> <p><code>UnresolvedFunction</code> is &lt;&gt; as a result of the following: <ul> <li> <p>callUDF standard function</p> </li> <li> <p>RelationalGroupedDataset.agg operator with aggregation functions specified by name (that converts function names to UnresolvedFunction expressions)</p> </li> <li> <p><code>AstBuilder</code> is requested to sql/AstBuilder.md#visitFunctionCall[visitFunctionCall] (in SQL queries)</p> </li> </ul> <p>[[resolved]] <code>UnresolvedFunction</code> can never be Expression.md#resolved[resolved] (and is replaced at analysis phase).</p> <p>NOTE: <code>UnresolvedFunction</code> is first looked up in LookupFunctions logical rule and then resolved in ResolveFunctions logical resolution rule.</p> <p>[[Unevaluable]][[eval]][[doGenCode]] Given <code>UnresolvedFunction</code> can never be resolved it should not come as a surprise that it cannot be evaluated either (i.e. produce a value given an internal row). When requested to evaluate, <code>UnresolvedFunction</code> simply reports a <code>UnsupportedOperationException</code>.</p> <pre><code>Cannot evaluate expression: [this]\n</code></pre> <p>TIP: Use Catalyst DSL's function or distinctFunction to create a <code>UnresolvedFunction</code> with &lt;&gt; flag off and on, respectively. <p>=== [[apply]] Creating UnresolvedFunction (With Database Undefined) -- <code>apply</code> Factory Method</p>"},{"location":"expressions/UnresolvedFunction/#source-scala","title":"[source, scala]","text":""},{"location":"expressions/UnresolvedFunction/#applyname-string-children-seqexpression-isdistinct-boolean-unresolvedfunction","title":"apply(name: String, children: Seq[Expression], isDistinct: Boolean): UnresolvedFunction","text":"<p><code>apply</code> creates a <code>FunctionIdentifier</code> with the <code>name</code> and no database first and then creates a &lt;&gt; with the <code>FunctionIdentifier</code>, <code>children</code> and <code>isDistinct</code> flag. <p><code>apply</code> is used when:</p> <ul> <li> <p>callUDF standard function is used</p> </li> <li> <p><code>RelationalGroupedDataset</code> is requested to agg with aggregation functions specified by name (and converts function names to UnresolvedFunction expressions)</p> </li> </ul>"},{"location":"expressions/UnresolvedFunction/#creating-instance","title":"Creating Instance","text":"<p><code>UnresolvedFunction</code> takes the following to be created:</p> <ul> <li>[[name]] <code>FunctionIdentifier</code></li> <li>[[children]] Child Expression.md[expressions]</li> <li>[[isDistinct]] <code>isDistinct</code> flag</li> </ul>"},{"location":"expressions/UnresolvedFunction/#demo","title":"Demo","text":"<pre><code>// Using Catalyst DSL to create UnresolvedFunctions\nimport org.apache.spark.sql.catalyst.dsl.expressions._\n\n// Scala Symbols supported only\nval f = 'f.function()\nscala&gt; :type f\norg.apache.spark.sql.catalyst.analysis.UnresolvedFunction\n\nscala&gt; f.isDistinct\nres0: Boolean = false\n\nval g = 'g.distinctFunction()\nscala&gt; g.isDistinct\nres1: Boolean = true\n</code></pre>"},{"location":"expressions/UnresolvedGenerator/","title":"UnresolvedGenerator","text":"<p><code>UnresolvedGenerator</code> is a Generator that represents an unresolved generator in a logical query plan.</p> <p><code>UnresolvedGenerator</code> is &lt;&gt; exclusively when <code>AstBuilder</code> is requested to sql/AstBuilder.md#withGenerate[withGenerate] (as part of Generate.md#generator[Generate] logical operator) for SQL's <code>LATERAL VIEW</code> (in <code>SELECT</code> or <code>FROM</code> clauses). <p>[[resolved]] <code>UnresolvedGenerator</code> can never be Expression.md#resolved[resolved] (and is replaced at &lt;&gt;). <p>[[Unevaluable]][[eval]][[doGenCode]] Given <code>UnresolvedGenerator</code> can never be resolved it should not come as a surprise that it Expression.md#Unevaluable[cannot be evaluated] either (i.e. produce a value given an internal row). When requested to evaluate, <code>UnresolvedGenerator</code> simply reports a <code>UnsupportedOperationException</code>.</p> <pre><code>Cannot evaluate expression: [this]\n</code></pre> <p>[[analysis-phase]] [NOTE] ==== <code>UnresolvedGenerator</code> is resolved to a concrete expressions/Generator.md[Generator] expression when ResolveFunctions logical resolution rule is executed. ====</p> <p>NOTE: <code>UnresolvedGenerator</code> is similar to spark-sql-Expression-UnresolvedFunction.md[UnresolvedFunction] and differs mostly by the type (to make Spark development with Scala easier?)</p> <p>=== [[creating-instance]] Creating UnresolvedGenerator Instance</p> <p><code>UnresolvedGenerator</code> takes the following when created:</p> <ul> <li>[[name]] <code>FunctionIdentifier</code></li> <li>[[children]] Child Expression.md[expressions]</li> </ul>"},{"location":"expressions/UnresolvedGenerator/#demo","title":"Demo","text":"<pre><code>// SQLs borrowed from https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView\nval sqlText = \"\"\"\n  SELECT pageid, adid\n  FROM pageAds LATERAL VIEW explode(adid_list) adTable AS adid\n\"\"\"\n// Register pageAds table\nSeq(\n  (\"front_page\", Array(1, 2, 3)),\n  (\"contact_page\", Array(3, 4, 5)))\n  .toDF(\"pageid\", \"adid_list\")\n  .createOrReplaceTempView(\"pageAds\")\nval query = sql(sqlText)\nval logicalPlan = query.queryExecution.logical\nscala&gt; println(logicalPlan.numberedTreeString)\n00 'Project ['pageid, 'adid]\n01 +- 'Generate 'explode('adid_list), false, adtable, ['adid]\n02    +- 'UnresolvedRelation `pageAds`\n\nimport org.apache.spark.sql.catalyst.plans.logical.Generate\nval generator = logicalPlan.collectFirst { case g: Generate =&gt; g.generator }.get\n\nscala&gt; :type generator\norg.apache.spark.sql.catalyst.expressions.Generator\n\nimport org.apache.spark.sql.catalyst.analysis.UnresolvedGenerator\nscala&gt; generator.isInstanceOf[UnresolvedGenerator]\nres1: Boolean = true\n</code></pre>"},{"location":"expressions/UnresolvedOrdinal/","title":"UnresolvedOrdinal","text":"<p><code>UnresolvedOrdinal</code> is a leaf expression that represents a single integer literal in Sort logical operators (in SortOrder ordering expressions) and in Aggregate logical operators (in grouping expressions) in a logical plan.</p> <p><code>UnresolvedOrdinal</code> is &lt;&gt; when <code>SubstituteUnresolvedOrdinals</code> logical resolution rule is executed. <pre><code>// Note \"order by 1\" clause\nval sqlText = \"select id from VALUES 1, 2, 3 t1(id) order by 1\"\nval logicalPlan = spark.sql(sqlText).queryExecution.logical\nscala&gt; println(logicalPlan.numberedTreeString)\n00 'Sort [1 ASC NULLS FIRST], true\n01 +- 'Project ['id]\n02    +- 'SubqueryAlias t1\n03       +- 'UnresolvedInlineTable [id], [List(1), List(2), List(3)]\n\nimport org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinals\nval rule = new SubstituteUnresolvedOrdinals(spark.sessionState.conf)\n\nval logicalPlanWithUnresolvedOrdinals = rule.apply(logicalPlan)\nscala&gt; println(logicalPlanWithUnresolvedOrdinals.numberedTreeString)\n00 'Sort [unresolvedordinal(1) ASC NULLS FIRST], true\n01 +- 'Project ['id]\n02    +- 'SubqueryAlias t1\n03       +- 'UnresolvedInlineTable [id], [List(1), List(2), List(3)]\n\nimport org.apache.spark.sql.catalyst.plans.logical.Sort\nval sortOp = logicalPlanWithUnresolvedOrdinals.collect { case s: Sort =&gt; s }.head\nval sortOrder = sortOp.order.head\n\nimport org.apache.spark.sql.catalyst.analysis.UnresolvedOrdinal\nval unresolvedOrdinalExpr = sortOrder.child.asInstanceOf[UnresolvedOrdinal]\nscala&gt; println(unresolvedOrdinalExpr)\nunresolvedordinal(1)\n</code></pre> <p>[[creating-instance]] [[ordinal]] <code>UnresolvedOrdinal</code> takes a single <code>ordinal</code> integer when created.</p> <p><code>UnresolvedOrdinal</code> is an unevaluable expression.</p> <p>[[resolved]] <code>UnresolvedOrdinal</code> can never be &lt;&gt; (and is replaced at &lt;&gt;). <p>[[analysis-phase]] NOTE: <code>UnresolvedOrdinal</code> is resolved when ResolveOrdinalInOrderByAndGroupBy logical resolution rule is executed.</p> <p>NOTE: <code>UnresolvedOrdinal</code> in GROUP BY ordinal position is not allowed for a select list with a star (<code>*</code>).</p>"},{"location":"expressions/UnresolvedStar/","title":"UnresolvedStar","text":"<p><code>UnresolvedStar</code> is a <code>Star</code> expression that represents a star (i.e. all) expression in a logical query plan.</p> <p><code>UnresolvedStar</code> is created when:</p> <ul> <li> <p><code>Column</code> is created with <code>*</code></p> </li> <li> <p><code>AstBuilder</code> is requested to visitStar</p> </li> </ul> <pre><code>val q = spark.range(5).select(\"*\")\nval plan = q.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'Project [*]\n01 +- AnalysisBarrier\n02       +- Range (0, 5, step=1, splits=Some(8))\n\nimport org.apache.spark.sql.catalyst.analysis.UnresolvedStar\nval starExpr = plan.expressions.head.asInstanceOf[UnresolvedStar]\n\nval namedExprs = starExpr.expand(input = q.queryExecution.analyzed, spark.sessionState.analyzer.resolver)\nscala&gt; println(namedExprs.head.numberedTreeString)\n00 id#0: bigint\n</code></pre> <p>[[resolved]] <code>UnresolvedStar</code> can never be Expression.md#resolved[resolved], and is &lt;&gt; at analysis (when ResolveReferences logical resolution rule is executed). <p>Note</p> <p><code>UnresolvedStar</code> can only be used in <code>Project</code>, <code>Aggregate</code> or <code>ScriptTransformation</code> logical operators.</p> <p>[[Unevaluable]][[eval]][[doGenCode]] Given <code>UnresolvedStar</code> can never be &lt;&gt; it should not come as a surprise that it cannot be evaluated either (i.e. produce a value given an internal row). When requested to evaluate, <code>UnresolvedStar</code> simply reports a <code>UnsupportedOperationException</code>. <pre><code>Cannot evaluate expression: [this]\n</code></pre> <p>[[creating-instance]] [[target]] When created, <code>UnresolvedStar</code> takes name parts that, once concatenated, is the target of the star expansion.</p>"},{"location":"expressions/UnresolvedStar/#source-scala","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.analysis.UnresolvedStar scala&gt; val us = UnresolvedStar(None) us: org.apache.spark.sql.catalyst.analysis.UnresolvedStar = *</p> <p>scala&gt; val ab = UnresolvedStar(Some(\"a\" :: \"b\" :: Nil)) ab: org.apache.spark.sql.catalyst.analysis.UnresolvedStar = List(a, b).*</p>"},{"location":"expressions/UnresolvedStar/#tip","title":"[TIP]","text":"<p>Use <code>star</code> operator from Catalyst DSL's expressions to create an <code>UnresolvedStar</code>.</p>"},{"location":"expressions/UnresolvedStar/#source-scala_1","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.dsl.expressions._ val s = star() scala&gt; :type s org.apache.spark.sql.catalyst.expressions.Expression</p> <p>import org.apache.spark.sql.catalyst.analysis.UnresolvedStar assert(s.isInstanceOf[UnresolvedStar])</p> <p>val s = star(\"a\", \"b\") scala&gt; println(s) WrappedArray(a, b).*</p>"},{"location":"expressions/UnresolvedStar/#you-could-also-use-or-to-create-an-unresolvedstar-but-that-requires-sbt-console-with-spark-libraries-defined-in-buildsbt-as-the-catalyst-dsl-expressions-implicits-interfere-with-the-spark-implicits-to-create-columns","title":"You could also use <code>$\"*\"</code> or <code>'*</code> to create an <code>UnresolvedStar</code>, but that requires <code>sbt console</code> (with Spark libraries defined in <code>build.sbt</code>) as the Catalyst DSL <code>expressions</code> implicits interfere with the Spark implicits to create columns.","text":""},{"location":"expressions/UnresolvedStar/#note","title":"[NOTE]","text":"<p><code>AstBuilder</code> sql/AstBuilder.md#visitFunctionCall[replaces] <code>count(*)</code> (with no <code>DISTINCT</code> keyword) to <code>count(1)</code>.</p>"},{"location":"expressions/UnresolvedStar/#val-q-sqlselect-count-from-range123-scala-printlnqqueryexecutionlogicalnumberedtreestring-00-project-unresolvedaliascount1-none-01-unresolvedtablevaluedfunction-range-1-2-3-val-q-sqlselect-countdistinct-from-range123-scala-printlnqqueryexecutionlogicalnumberedtreestring-00-project-unresolvedaliascount-none-01-unresolvedtablevaluedfunction-range-1-2-3","title":"<pre><code>val q = sql(\"SELECT COUNT(*) FROM RANGE(1,2,3)\")\nscala&gt; println(q.queryExecution.logical.numberedTreeString)\n00 'Project [unresolvedalias('count(1), None)]\n01 +- 'UnresolvedTableValuedFunction range, [1, 2, 3]\n\nval q = sql(\"SELECT COUNT(DISTINCT *) FROM RANGE(1,2,3)\")\nscala&gt; println(q.queryExecution.logical.numberedTreeString)\n00 'Project [unresolvedalias('COUNT(*), None)]\n01 +- 'UnresolvedTableValuedFunction RANGE, [1, 2, 3]\n</code></pre>","text":"<p>=== [[expand]] Star Expansion -- <code>expand</code> Method</p>"},{"location":"expressions/UnresolvedStar/#source-scala_2","title":"[source, scala]","text":""},{"location":"expressions/UnresolvedStar/#expandinput-logicalplan-resolver-resolver-seqnamedexpression","title":"expand(input: LogicalPlan, resolver: Resolver): Seq[NamedExpression]","text":"<p><code>expand</code> first expands to named expressions per &lt;&gt;: <ul> <li> <p>For unspecified &lt;&gt;, <code>expand</code> gives the catalyst/QueryPlan.md#output[output] schema of the <code>input</code> logical query plan (that assumes that the star refers to a relation / table) <li> <p>For &lt;&gt; with one element, <code>expand</code> gives the table (attribute) in the catalyst/QueryPlan.md#output[output] schema of the <code>input</code> logical query plan (using NamedExpression.md#qualifier[qualifiers]) if available <p>With no result earlier, <code>expand</code> then requests the <code>input</code> logical query plan to spark-sql-LogicalPlan.md#resolve[resolve] the &lt;&gt; name parts to a named expression. <p>For a named expression of StructType data type, <code>expand</code> creates an spark-sql-Expression-Alias.md#creating-instance[Alias] expression with a <code>GetStructField</code> unary expression (with the resolved named expression and the field index).</p> <pre><code>val q = Seq((0, \"zero\")).toDF(\"id\", \"name\").select(struct(\"id\", \"name\") as \"s\")\nval analyzedPlan = q.queryExecution.analyzed\n\nimport org.apache.spark.sql.catalyst.analysis.UnresolvedStar\nimport org.apache.spark.sql.catalyst.dsl.expressions._\nval s = star(\"s\").asInstanceOf[UnresolvedStar]\nval exprs = s.expand(input = analyzedPlan, spark.sessionState.analyzer.resolver)\n\n// star(\"s\") should expand to two Alias(GetStructField) expressions\n// s is a struct of id and name in the query\n\nimport org.apache.spark.sql.catalyst.expressions.{Alias, GetStructField}\nval getStructFields = exprs.collect { case Alias(g: GetStructField, _) =&gt; g }.map(_.sql)\nscala&gt; getStructFields.foreach(println)\n`s`.`id`\n`s`.`name`\n</code></pre> <p><code>expand</code> reports a <code>AnalysisException</code> when:</p> <ul> <li> <p>The Expression.md#dataType[data type] of the named expression (when the <code>input</code> logical plan was requested to spark-sql-LogicalPlan.md#resolve[resolve] the &lt;&gt;) is not a StructType. + <pre><code>Can only star expand struct data types. Attribute: `[target]`\n</code></pre> <li> <p>Earlier attempts gave no results + <pre><code>cannot resolve '[target].*' given input columns '[from]'\n</code></pre></p> </li>"},{"location":"expressions/UnsafeProjection/","title":"UnsafeProjection","text":"<p><code>UnsafeProjection</code> is an extension of the Projection abstraction for expressions that encode InternalRows to UnsafeRows.</p> <pre><code>UnsafeProjection: InternalRow =[apply]=&gt; UnsafeRow\n</code></pre>"},{"location":"expressions/UnsafeProjection/#contract","title":"Contract","text":""},{"location":"expressions/UnsafeProjection/#encoding-internalrow-as-unsaferow","title":"Encoding InternalRow as UnsafeRow <pre><code>apply(\n  row: InternalRow): UnsafeRow\n</code></pre> <p>Encodes the given InternalRow to an UnsafeRow</p>","text":""},{"location":"expressions/UnsafeProjection/#implementations","title":"Implementations","text":"<ul> <li><code>InterpretedUnsafeProjection</code></li> </ul>"},{"location":"expressions/UnsafeProjection/#codegeneratorwithinterpretedfallback","title":"CodeGeneratorWithInterpretedFallback <p><code>UnsafeProjection</code> factory object is a CodeGeneratorWithInterpretedFallback of <code>UnsafeProjection</code>s (based on Expressions).</p> <pre><code>CodeGeneratorWithInterpretedFallback[Seq[Expression], UnsafeProjection]\n</code></pre>","text":""},{"location":"expressions/UnsafeProjection/#createcodegeneratedobject","title":"createCodeGeneratedObject <pre><code>createCodeGeneratedObject(\n  in: Seq[Expression]): UnsafeProjection\n</code></pre> <p><code>createCodeGeneratedObject</code> is part of the CodeGeneratorWithInterpretedFallback abstraction.</p> <p><code>createCodeGeneratedObject</code>...FIXME</p>","text":""},{"location":"expressions/UnsafeProjection/#createinterpretedobject","title":"createInterpretedObject <pre><code>createInterpretedObject(\n  in: Seq[Expression]): UnsafeProjection\n</code></pre> <p><code>createInterpretedObject</code> is part of the CodeGeneratorWithInterpretedFallback abstraction.</p> <p><code>createInterpretedObject</code>...FIXME</p>","text":""},{"location":"expressions/UnsafeProjection/#create","title":"create <pre><code>create(\n  fields: Array[DataType]): UnsafeProjection\ncreate(\n  expr: Expression): UnsafeProjection\ncreate(\n  exprs: Seq[Expression]): UnsafeProjection\ncreate(\n  exprs: Seq[Expression],\n  inputSchema: Seq[Attribute]): UnsafeProjection\ncreate(\n  schema: StructType): UnsafeProjection\n</code></pre> <p><code>create</code> creates an UnsafeProjection for the given BoundReferences.</p>","text":""},{"location":"expressions/UserDefinedAggregateFunction/","title":"UserDefinedAggregateFunction \u2014 User-Defined Untyped Aggregate Functions (UDAFs)","text":"<p><code>UserDefinedAggregateFunction</code> is the &lt;&gt; to define user-defined aggregate functions (UDAFs). <pre><code>// Custom UDAF to count rows\n\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}\nimport org.apache.spark.sql.types.{DataType, LongType, StructType}\n\nclass MyCountUDAF extends UserDefinedAggregateFunction {\n  override def inputSchema: StructType = {\n    new StructType().add(\"id\", LongType, nullable = true)\n  }\n\n  override def bufferSchema: StructType = {\n    new StructType().add(\"count\", LongType, nullable = true)\n  }\n\n  override def dataType: DataType = LongType\n\n  override def deterministic: Boolean = true\n\n  override def initialize(buffer: MutableAggregationBuffer): Unit = {\n    println(s\"&gt;&gt;&gt; initialize (buffer: $buffer)\")\n    // NOTE: Scala's update used under the covers\n    buffer(0) = 0L\n  }\n\n  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n    println(s\"&gt;&gt;&gt; update (buffer: $buffer -&gt; input: $input)\")\n    buffer(0) = buffer.getLong(0) + 1\n  }\n\n  override def merge(buffer: MutableAggregationBuffer, row: Row): Unit = {\n    println(s\"&gt;&gt;&gt; merge (buffer: $buffer -&gt; row: $row)\")\n    buffer(0) = buffer.getLong(0) + row.getLong(0)\n  }\n\n  override def evaluate(buffer: Row): Any = {\n    println(s\"&gt;&gt;&gt; evaluate (buffer: $buffer)\")\n    buffer.getLong(0)\n  }\n}\n</code></pre> <p><code>UserDefinedAggregateFunction</code> is created using &lt;&gt; or &lt;&gt; factory methods. <pre><code>val dataset = spark.range(start = 0, end = 4, step = 1, numPartitions = 2)\n\n// Use the UDAF\nval mycount = new MyCountUDAF\nval q = dataset.\n  withColumn(\"group\", 'id % 2).\n  groupBy('group).\n  agg(mycount.distinct('id) as \"count\")\nscala&gt; q.show\n+-----+-----+\n|group|count|\n+-----+-----+\n|    0|    2|\n|    1|    2|\n+-----+-----+\n</code></pre> <p>The &lt;&gt; of <code>UserDefinedAggregateFunction</code> is entirely managed using <code>ScalaUDAF</code> expression container. <p></p>"},{"location":"expressions/UserDefinedAggregateFunction/#note","title":"[NOTE]","text":"<p>Use UDFRegistration.md[UDFRegistration] to register a (temporary) <code>UserDefinedAggregateFunction</code> and use it in SparkSession.md#sql[SQL mode].</p>"},{"location":"expressions/UserDefinedAggregateFunction/#source-scala","title":"[source, scala]","text":"<p>import org.apache.spark.sql.expressions.UserDefinedAggregateFunction val mycount: UserDefinedAggregateFunction = ... spark.udf.register(\"mycount\", mycount)</p>"},{"location":"expressions/UserDefinedAggregateFunction/#sparksqlselect-mycount-from-range5","title":"spark.sql(\"SELECT mycount(*) FROM range(5)\")","text":"<p>====</p> <p>=== [[contract]] UserDefinedAggregateFunction Contract</p>"},{"location":"expressions/UserDefinedAggregateFunction/#source-scala_1","title":"[source, scala]","text":"<p>package org.apache.spark.sql.expressions</p> <p>abstract class UserDefinedAggregateFunction {   // only required methods that have no implementation   def bufferSchema: StructType   def dataType: DataType   def deterministic: Boolean   def evaluate(buffer: Row): Any   def initialize(buffer: MutableAggregationBuffer): Unit   def inputSchema: StructType   def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit   def update(buffer: MutableAggregationBuffer, input: Row): Unit }</p> <p>.(Subset of) UserDefinedAggregateFunction Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> [[bufferSchema]] <code>bufferSchema</code> [[dataType]] <code>dataType</code> [[deterministic]] <code>deterministic</code> [[evaluate]] <code>evaluate</code> [[initialize]] <code>initialize</code> [[inputSchema]] <code>inputSchema</code> [[merge]] <code>merge</code> [[update]] <code>update</code> === <p>=== [[apply]] Creating Column for UDAF -- <code>apply</code> Method</p> <pre><code>apply(\nexprs: Column*): Column\n</code></pre> <p><code>apply</code> creates a Column with <code>ScalaUDAF</code> (inside AggregateExpression).</p> <p>Note</p> <p><code>AggregateExpression</code> uses <code>Complete</code> mode and <code>isDistinct</code> flag is disabled.</p> <pre><code>import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\nval myUDAF: UserDefinedAggregateFunction = ...\nval myUdafCol = myUDAF.apply($\"id\", $\"name\")\nscala&gt; myUdafCol.explain(extended = true)\nmycountudaf('id, 'name, $line17.$read$$iw$$iw$MyCountUDAF@4704b66a, 0, 0)\n\nscala&gt; println(myUdafCol.expr.numberedTreeString)\n00 mycountudaf('id, 'name, $line17.$read$$iw$$iw$MyCountUDAF@4704b66a, 0, 0)\n01 +- MyCountUDAF('id,'name)\n02    :- 'id\n03    +- 'name\n\nimport org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression\nmyUdafCol.expr.asInstanceOf[AggregateExpression]\n\nimport org.apache.spark.sql.execution.aggregate.ScalaUDAF\nval scalaUdaf = myUdafCol.expr.children.head.asInstanceOf[ScalaUDAF]\nscala&gt; println(scalaUdaf.toString)\nMyCountUDAF('id,'name)\n</code></pre> <p>=== [[distinct]] Creating Column for UDAF with Distinct Values -- <code>distinct</code> Method</p> <pre><code>distinct(\nexprs: Column*): Column\n</code></pre> <p><code>distinct</code> creates a Column with <code>ScalaUDAF</code> (inside AggregateExpression).</p> <p>Note</p> <p><code>AggregateExpression</code> uses <code>Complete</code> mode and <code>isDistinct</code> flag is enabled.</p> <p>Note</p> <p><code>distinct</code> is like apply but has <code>isDistinct</code> flag enabled.</p> <pre><code>import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\nval myUDAF: UserDefinedAggregateFunction = ...\nscala&gt; val myUdafCol = myUDAF.distinct($\"id\", $\"name\")\nmyUdafCol: org.apache.spark.sql.Column = mycountudaf(DISTINCT id, name)\n\nscala&gt; myUdafCol.explain(extended = true)\nmycountudaf(distinct 'id, 'name, $line17.$read$$iw$$iw$MyCountUDAF@4704b66a, 0, 0)\n\nimport org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression\nval aggExpr = myUdafCol.expr\nscala&gt; println(aggExpr.numberedTreeString)\n00 mycountudaf(distinct 'id, 'name, $line17.$read$$iw$$iw$MyCountUDAF@4704b66a, 0, 0)\n01 +- MyCountUDAF('id,'name)\n02    :- 'id\n03    +- 'name\n\nscala&gt; aggExpr.asInstanceOf[AggregateExpression].isDistinct\nres0: Boolean = true\n</code></pre>"},{"location":"expressions/UserDefinedAggregator/","title":"UserDefinedAggregator","text":"<p><code>UserDefinedAggregator[IN, BUF, OUT]</code> is a UserDefinedFunction that uses ScalaAggregator for execution.</p> <p><code>UserDefinedAggregator</code> is created using udaf standard function.</p>"},{"location":"expressions/UserDefinedAggregator/#creating-instance","title":"Creating Instance","text":"<p><code>UserDefinedAggregator</code> takes the following to be created:</p> <ul> <li> Aggregator <li> Encoder (of <code>IN</code>s and assumed an ExpressionEncoder) <li> Name <li> <code>nullable</code> flag (default: <code>true</code>) <li> <code>deterministic</code> flag (default: <code>true</code>) <p><code>UserDefinedAggregator</code> is created using udaf standard function.</p>"},{"location":"expressions/UserDefinedAggregator/#creating-column-for-function-execution","title":"Creating Column (for Function Execution) <pre><code>apply(\n  exprs: Column*): Column\n</code></pre> <p><code>apply</code> is part of the UserDefinedFunction abstraction.</p>  <p><code>apply</code> creates a Column with an AggregateExpression with the following:</p> <ul> <li>ScalaAggregator aggregate function</li> <li><code>Complete</code> aggregate mode</li> <li><code>isDistinct</code> flag disabled (<code>false</code>)</li> </ul>","text":""},{"location":"expressions/UserDefinedAggregator/#creating-scalaaggregator","title":"Creating ScalaAggregator <pre><code>scalaAggregator(\n  exprs: Seq[Expression]): ScalaAggregator[IN, BUF, OUT]\n</code></pre> <p><code>scalaAggregator</code> assumes the following are all ExpressionEncoders:</p> <ol> <li>Input Encoder (of <code>IN</code>s)</li> <li>Buffer Encoder (of <code>BUF</code>s) of the Aggregator</li> </ol> <p>In the end, <code>scalaAggregator</code> creates a ScalaAggregator.</p>  <p><code>scalaAggregator</code> is used when:</p> <ul> <li><code>UDFRegistration</code> is requested to register a UserDefinedAggregator</li> <li><code>UserDefinedAggregator</code> is requested to create a Column (for execution)</li> </ul>","text":""},{"location":"expressions/UserDefinedAggregator/#creates-named-userdefinedaggregator","title":"Creates Named UserDefinedAggregator <pre><code>withName(\n  name: String): UserDefinedAggregator[IN, BUF, OUT]\n</code></pre> <p><code>withName</code> is part of the UserDefinedFunction abstraction.</p>  <p><code>withName</code> creates a copy of this <code>UserDefinedAggregator</code> to use the given <code>name</code> as the name.</p>","text":""},{"location":"expressions/UserDefinedExpression/","title":"UserDefinedExpression","text":"<p><code>UserDefinedExpression</code> is...FIXME</p>"},{"location":"expressions/UserDefinedFunction/","title":"UserDefinedFunction","text":"<p><code>UserDefinedFunction</code> is an abstraction of user-defined functions (UDFs).</p>"},{"location":"expressions/UserDefinedFunction/#contract-subset","title":"Contract (Subset)","text":""},{"location":"expressions/UserDefinedFunction/#creating-column-for-function-execution","title":"Creating Column (for Function Execution) <pre><code>apply(\n  exprs: Column*): Column\n</code></pre> <p>Column with Expression to execute this <code>UserDefinedFunction</code></p>","text":""},{"location":"expressions/UserDefinedFunction/#withname","title":"withName <pre><code>withName(\n  name: String): UserDefinedFunction\n</code></pre> <p>Associates the given <code>name</code> with this <code>UserDefinedFunction</code></p> <p>Used when:</p> <ul> <li><code>UDFRegistration</code> is requested to register a named UserDefinedFunction</li> </ul>","text":""},{"location":"expressions/UserDefinedFunction/#implementations","title":"Implementations","text":"<ul> <li>SparkUserDefinedFunction</li> <li>UserDefinedAggregator</li> </ul>"},{"location":"expressions/UserDefinedFunction/#demo","title":"Demo","text":"<pre><code>import org.apache.spark.sql.functions.udf\nval lengthUDF = udf { s: String =&gt; s.length }\n</code></pre> <pre><code>scala&gt; :type lengthUDF\norg.apache.spark.sql.expressions.UserDefinedFunction\n</code></pre> <pre><code>val r = lengthUDF($\"name\")\n</code></pre> <pre><code>scala&gt; :type r\norg.apache.spark.sql.Column\n</code></pre> <pre><code>val namedLengthUDF = lengthUDF.withName(\"lengthUDF\")\n</code></pre> <pre><code>scala&gt; namedLengthUDF($\"name\")\nres2: org.apache.spark.sql.Column = UDF:lengthUDF(name)\n</code></pre> <pre><code>val nonNullableLengthUDF = lengthUDF.asNonNullable\nassert(nonNullableLengthUDF.nullable == false)\n</code></pre>"},{"location":"expressions/WindowExpression/","title":"WindowExpression","text":"<p><code>WindowExpression</code> is an unevaluable expression that represents a window function (over some WindowSpecDefinition).</p> <p><code>WindowExpression</code> is created when:</p> <ul> <li> <p><code>WindowSpec</code> is requested to withAggregate (when Column.over operator is used)</p> </li> <li> <p>WindowsSubstitution logical evaluation rule is executed (with WithWindowDefinition logical operators with <code>UnresolvedWindowExpression</code> expressions)</p> </li> <li> <p><code>AstBuilder</code> is requested to parse a function call in a SQL statement</p> </li> </ul> <p><code>WindowExpression</code> can only be with AggregateExpression, AggregateWindowFunction or OffsetWindowFunction expressions which is enforced at analysis.</p> <pre><code>// Using Catalyst DSL\nval wf = 'count.function(star())\nval windowSpec = ???\n</code></pre> <p><code>WindowExpression</code> is resolved in ExtractWindowExpressions, ResolveWindowFrame and ResolveWindowOrder logical rules.</p> <p><code>WindowExpression</code> is subject to NullPropagation and DecimalAggregates logical optimizations.</p> <p>Distinct window functions are not supported which is enforced at analysis.</p> <p>An offset window function can only be evaluated in an ordered row-based window frame with a single offset which is enforced at analysis.</p>"},{"location":"expressions/WindowExpression/#catalyst-dsl","title":"Catalyst DSL <pre><code>windowExpr(\n  windowFunc: Expression,\n  windowSpec: WindowSpecDefinition): WindowExpression\n</code></pre> <p>windowExpr operator in Catalyst DSL creates a WindowExpression expression, e.g. for testing or Spark SQL internals exploration.</p>","text":""},{"location":"expressions/WindowExpression/#creating-instance","title":"Creating Instance <p><code>WindowExpression</code> takes the following when created:</p> <ul> <li> Window function expression <li> WindowSpecDefinition expression","text":""},{"location":"expressions/WindowExpression/#demo","title":"Demo <pre><code>import org.apache.spark.sql.catalyst.expressions.WindowExpression\n// relation - Dataset as a table to query\nval table = spark.emptyDataset[Int]\n\nscala&gt; val windowExpr = table\n  .selectExpr(\"count() OVER (PARTITION BY value) AS count\")\n  .queryExecution\n  .logical      // (1)!\n  .expressions\n  .toList(0)\n  .children(0)\n  .asInstanceOf[WindowExpression]\nwindowExpr: org.apache.spark.sql.catalyst.expressions.WindowExpression = 'count() windowspecdefinition('value, UnspecifiedFrame)\n\nscala&gt; windowExpr.sql\nres2: String = count() OVER (PARTITION BY `value` UnspecifiedFrame)\n</code></pre> <ol> <li>Use <code>sqlParser</code> directly as in WithWindowDefinition Example</li> </ol>","text":""},{"location":"expressions/WindowFunction/","title":"WindowFunction Expressions","text":"<p><code>WindowFunction</code> is an extension of the Expression abstraction for window functions.</p> <p><code>WindowFunction</code> (along with AggregateFunction) is of <code>SQL</code> window function type.</p>"},{"location":"expressions/WindowFunction/#over-clause","title":"OVER Clause","text":"<p><code>WindowFunction</code> can only be evaluated in the context of <code>OVER</code> window operator.</p> <p>CheckAnalysis enforces that <code>WindowFunction</code>s can only be children of WindowExpression.</p>"},{"location":"expressions/WindowFunction/#implementations","title":"Implementations","text":"<ul> <li>AggregateWindowFunction</li> <li>OffsetWindowFunction</li> </ul>"},{"location":"expressions/WindowFunction/#logical-resolution-rules","title":"Logical Resolution Rules","text":"<p>Analyzer uses the following rules to work with <code>WindowFunction</code>s:</p> <ol> <li>ResolveWindowFrame</li> <li>ResolveWindowOrder</li> <li>ExtractWindowExpressions</li> </ol>"},{"location":"expressions/WindowFunction/#windowframe","title":"WindowFrame <pre><code>frame: WindowFrame\n</code></pre> <p><code>frame</code> is <code>UnspecifiedFrame</code> by default.</p> <p><code>frame</code> is used when:</p> <ul> <li>ResolveWindowFrame logical resolution rule is executed</li> </ul>","text":""},{"location":"expressions/WindowSpecDefinition/","title":"WindowSpecDefinition","text":"<p><code>WindowSpecDefinition</code> is an unevaluable expression.</p> <p><code>WindowSpecDefinition</code> is created when:</p> <ul> <li> <p><code>AstBuilder</code> is requested to parse a window specification in a SQL query</p> </li> <li> <p>Column.over operator is used</p> </li> </ul> <pre><code>import org.apache.spark.sql.expressions.Window\nval byValueDesc = Window.partitionBy(\"value\").orderBy($\"value\".desc)\n\nval q = table.withColumn(\"count over window\", count(\"*\") over byValueDesc)\n\nimport org.apache.spark.sql.catalyst.expressions.WindowExpression\nval windowExpr = q.queryExecution\n  .logical\n  .expressions(1)\n  .children(0)\n  .asInstanceOf[WindowExpression]\n\nscala&gt; windowExpr.windowSpec\nres0: org.apache.spark.sql.catalyst.expressions.WindowSpecDefinition = windowspecdefinition('value, 'value DESC NULLS LAST, UnspecifiedFrame)\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.expressions.WindowSpecDefinition\n\nSeq((0, \"hello\"), (1, \"windows\"))\n  .toDF(\"id\", \"token\")\n  .createOrReplaceTempView(\"mytable\")\n\nval sqlText = \"\"\"\n  SELECT count(*) OVER myWindowSpec\n  FROM mytable\n  WINDOW\n    myWindowSpec AS (\n      PARTITION BY token\n      ORDER BY id\n      RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    )\n\"\"\"\n\nimport spark.sessionState.{analyzer,sqlParser}\n\nscala&gt; val parsedPlan = sqlParser.parsePlan(sqlText)\nparsedPlan: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =\n'WithWindowDefinition Map(myWindowSpec -&gt; windowspecdefinition('token, 'id ASC NULLS FIRST, RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW))\n+- 'Project [unresolvedalias(unresolvedwindowexpression('count(1), WindowSpecReference(myWindowSpec)), None)]\n   +- 'UnresolvedRelation `mytable`\n\nimport org.apache.spark.sql.catalyst.plans.logical.WithWindowDefinition\nval myWindowSpec = parsedPlan.asInstanceOf[WithWindowDefinition].windowDefinitions(\"myWindowSpec\")\n\nscala&gt; println(myWindowSpec)\nwindowspecdefinition('token, 'id ASC NULLS FIRST, RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n\nscala&gt; println(myWindowSpec.sql)\n(PARTITION BY `token` ORDER BY `id` ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n\nscala&gt; sql(sqlText)\nres4: org.apache.spark.sql.DataFrame = [count(1) OVER (PARTITION BY token ORDER BY id ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW): bigint]\n\nscala&gt; println(analyzer.execute(sqlParser.parsePlan(sqlText)))\nProject [count(1) OVER (PARTITION BY token ORDER BY id ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#25L]\n+- Project [token#13, id#12, count(1) OVER (PARTITION BY token ORDER BY id ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#25L, count(1) OVER (PARTITION BY token ORDER BY id ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#25L]\n   +- Window [count(1) windowspecdefinition(token#13, id#12 ASC NULLS FIRST, RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS count(1) OVER (PARTITION BY token ORDER BY id ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#25L], [token#13], [id#12 ASC NULLS FIRST]\n      +- Project [token#13, id#12]\n         +- SubqueryAlias mytable\n            +- Project [_1#9 AS id#12, _2#10 AS token#13]\n               +- LocalRelation [_1#9, _2#10]\n</code></pre> <p>[[properties]] .WindowSpecDefinition's Properties [width=\"100%\",cols=\"1,2\",options=\"header\"] |=== | Name | Description</p> <p>| [[children]] <code>children</code> | Window &lt;&gt; and &lt;&gt; specifications (for which <code>WindowExpression</code> was created). <p>| <code>dataType</code> | Unsupported (i.e. reports a <code>UnsupportedOperationException</code>)</p> <p>| <code>foldable</code> | Disabled (i.e. <code>false</code>)</p> <p>| <code>nullable</code> | Enabled (i.e. <code>true</code>)</p> <p>| <code>resolved</code> | Enabled when &lt;&gt; are and the input DataType is valid and the input frameSpecification is a <code>SpecifiedWindowFrame</code>. <p>| <code>sql</code> a| Contains <code>PARTITION BY</code> with comma-separated elements of &lt;&gt; (if defined) with <code>ORDER BY</code> with comma-separated elements of &lt;&gt; (if defined) followed by &lt;&gt;."},{"location":"expressions/WindowSpecDefinition/#optionswrap","title":"[options=\"wrap\"]","text":""},{"location":"expressions/WindowSpecDefinition/#partition-by-token-order-by-id-asc-nulls-first-range-between-unbounded-preceding-and-current-row","title":"(PARTITION BY <code>token</code> ORDER BY <code>id</code> ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)","text":"<p>|===</p>"},{"location":"expressions/WindowSpecDefinition/#creating-instance","title":"Creating Instance","text":"<p><code>WindowSpecDefinition</code> takes the following when created:</p> <ul> <li>[[partitionSpec]] &lt;&gt; for window partition specification <li>[[orderSpec]] Window order specifications (as <code>SortOrder</code> unary expressions)</li> <li>[[frameSpecification]] Window frame specification (as <code>WindowFrame</code>)</li> <p>=== [[isValidFrameType]] Validating Data Type Of Window Order-- <code>isValidFrameType</code> Internal Method</p>"},{"location":"expressions/WindowSpecDefinition/#source-scala","title":"[source, scala]","text":""},{"location":"expressions/WindowSpecDefinition/#isvalidframetypeft-datatype-boolean","title":"isValidFrameType(ft: DataType): Boolean","text":"<p><code>isValidFrameType</code> is positive (<code>true</code>) when the data type of the &lt;&gt; and the input <code>ft</code> data type are as follows: <ul> <li> <p>DateType and IntegerType</p> </li> <li> <p>TimestampType and CalendarIntervalType</p> </li> <li> <p>Equal</p> </li> </ul> <p>Otherwise, <code>isValidFrameType</code> is negative (<code>false</code>).</p> <p>NOTE: <code>isValidFrameType</code> is used exclusively when <code>WindowSpecDefinition</code> is requested to &lt;&gt; (with <code>RangeFrame</code> as the &lt;&gt;) <p>=== [[checkInputDataTypes]] Checking Input Data Types -- <code>checkInputDataTypes</code> Method</p>"},{"location":"expressions/WindowSpecDefinition/#source-scala_1","title":"[source, scala]","text":""},{"location":"expressions/WindowSpecDefinition/#checkinputdatatypes-typecheckresult","title":"checkInputDataTypes(): TypeCheckResult","text":"<p><code>checkInputDataTypes</code> is part of the Expression abstraction.</p> <p><code>checkInputDataTypes</code>...FIXME</p>"},{"location":"features/","title":"Features","text":"<p>The following are the features of Spark SQL that help place it in the top of the modern distributed SQL query processing engines:</p> <ul> <li>Adaptive Query Execution</li> <li>Bucketing</li> <li>Catalog Plugin API</li> <li>Columnar Execution</li> <li>Connector API</li> <li>Default Columns </li> <li>Dynamic Partition Pruning</li> <li>File-Based Data Scanning</li> <li>Hints</li> <li>Metadata Columns</li> <li>Spark Connect </li> <li>Table-Valued Functions </li> <li>Variable Substitution</li> <li>Vectorized Parquet Decoding (Reader)</li> <li>Whole-Stage Code Generation</li> <li>others (listed in the menu on the left)</li> </ul>"},{"location":"file-based-data-scanning/","title":"File-Based Data Scanning","text":"<p>Spark SQL uses FileScanRDD for table scans of File-Based Data Sources (e.g., parquet).</p> <p>The number of partitions in data scanning is based on the following:</p> <ul> <li>maxSplitBytes hint</li> <li>Whether FileFormat is splitable or not</li> <li>Number of split files</li> <li>Bucket Pruning</li> </ul> <p>File-Based Data Scanning can be bucketed or not.</p>"},{"location":"files/","title":"File-Based Connectors (Files)","text":""},{"location":"functions/","title":"Standard Functions","text":"<p><code>org.apache.spark.sql.functions</code> object defines built-in standard functions to work with (values produced by) columns.</p> <p>You can access the standard functions using the following <code>import</code> statement in your Scala application:</p> <pre><code>import org.apache.spark.sql.functions._\n</code></pre>"},{"location":"functions/#udaf","title":"udaf <pre><code>udaf[IN: TypeTag, BUF, OUT](\n  agg: Aggregator[IN, BUF, OUT]): UserDefinedFunction // (1)!\nudaf[IN, BUF, OUT](\n  agg: Aggregator[IN, BUF, OUT],\n  inputEncoder: Encoder[IN]): UserDefinedFunction\n</code></pre> <ol> <li>Uses an ExpressionEncoder of <code>IN</code> type</li> </ol> <p><code>udaf</code> creates a UserDefinedAggregator with the given Aggregator and Encoder.</p>","text":""},{"location":"functions/aggregate-functions/","title":"Standard Aggregate Functions","text":""},{"location":"functions/aggregate-functions/#collect_set","title":"collect_set <pre><code>collect_set(\n  e: Column): Column\ncollect_set(\n  columnName: String): Column\n</code></pre> <p><code>collect_set</code> creates a CollectSet expression (for the expr of the given Column) and requests it to toAggregateExpression.</p> <p>In the end, <code>collect_set</code> wraps the AggregateExpression up in a Column.</p>","text":""},{"location":"functions/collection-functions/","title":"Standard Collection Functions","text":""},{"location":"functions/collection-functions/#filter","title":"filter <pre><code>filter(\n  column: Column,\n  f: (Column, Column) =&gt; Column): Column\nfilter(\n  column: Column,\n  f: Column =&gt; Column): Column\n</code></pre> <p><code>filter</code> creates an ArrayFilter expression (for the expr of the given Column and a LambdaFunction).</p> <p>In the end, <code>collect_set</code> wraps the <code>ArrayFilter</code> up in a Column.</p>","text":""},{"location":"generated-columns/","title":"Generated Columns","text":"<p>Spark SQL 3.4.0 comes with support for <code>GENERATED ALWAYS AS</code> syntax for defining Generated Columns in <code>CREATE TABLE</code> and <code>REPLACE TABLE</code> statements for data sources that support it.</p> <pre><code>CREATE TABLE default.example (\ntime TIMESTAMP,\ndate DATE GENERATED ALWAYS AS (CAST(time AS DATE))\n)\n</code></pre> <p>Generated Columns are part of column definition.</p> <pre><code>colDefinitionOption\n: NOT NULL\n| defaultExpression\n| generationExpression\n| commentSpec\n;\n\ngenerationExpression\n: GENERATED ALWAYS AS '(' expression ')'\n;\n</code></pre>"},{"location":"generated-columns/GeneratedColumn/","title":"GeneratedColumn","text":"<p><code>GeneratedColumn</code> is...FIXME</p>"},{"location":"hints/","title":"Hint Framework","text":"<p>Structured queries can be optimized using Hint Framework that allows for specifying query hints.</p> <p>Query hints allow for annotating a query and give a hint to the query optimizer how to optimize logical plans. This can be very useful when the query optimizer cannot make optimal decision, e.g. with respect to join methods due to conservativeness or the lack of proper statistics.</p> <p>Spark SQL supports COALESCE and REPARTITION and BROADCAST hints. All remaining unresolved hints are silently removed from a query plan at analysis.</p> <p>Hint Framework was added in Apache Spark 2.2.</p>"},{"location":"hints/#specifying-query-hints","title":"Specifying Query Hints","text":"<p>Hints are defined using Dataset.hint operator or SQL hints.</p>"},{"location":"hints/#dataset-api","title":"Dataset API","text":"<pre><code>val q = spark.range(1).hint(name = \"myHint\", 100, true)\nval plan = q.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'UnresolvedHint myHint, [100, true]\n01 +- Range (0, 1, step=1, splits=Some(8))\n</code></pre>"},{"location":"hints/#sql","title":"SQL","text":"<pre><code>val q = sql(\"SELECT /*+ myHint (100, true) */ 1\")\nval plan = q.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'UnresolvedHint myHint, [100, true]\n01 +- 'Project [unresolvedalias(1, None)]\n02    +- OneRowRelation\n</code></pre>"},{"location":"hints/#select-sql-statements-with-hints","title":"SELECT SQL Statements With Hints <p><code>SELECT</code> SQL statement supports query hints as comments in SQL query that Spark SQL translates into a UnresolvedHint unary logical operator.</p>","text":""},{"location":"hints/#coalesce-and-repartition-hints","title":"COALESCE and REPARTITION Hints <p>Spark SQL 2.4 added support for COALESCE and REPARTITION hints (using SQL comments):</p> <ul> <li> <p><code>SELECT /*+ COALESCE(5) */ ...</code></p> </li> <li> <p><code>SELECT /*+ REPARTITION(3) */ ...</code></p> </li> </ul>","text":""},{"location":"hints/#broadcast-hints","title":"Broadcast Hints <p>Spark SQL 2.2 supports BROADCAST hints using broadcast standard function or SQL comments:</p> <ul> <li> <p><code>SELECT /*+ MAPJOIN(b) */ ...</code></p> </li> <li> <p><code>SELECT /*+ BROADCASTJOIN(b) */ ...</code></p> </li> <li> <p><code>SELECT /*+ BROADCAST(b) */ ...</code></p> </li> </ul>","text":""},{"location":"hints/#broadcast-standard-function","title":"broadcast Standard Function <p>While <code>hint</code> operator allows for attaching any hint to a logical plan broadcast standard function attaches the broadcast hint only (that actually makes it a special case of <code>hint</code> operator).</p> <p><code>broadcast</code> standard function is used for broadcast joins (aka map-side joins) (and to hint the Spark planner to broadcast a dataset regardless of the size).</p> <pre><code>val small = spark.range(1)\nval large = spark.range(100)\n\n// Let's use broadcast standard function first\nval q = large.join(broadcast(small), \"id\")\nval plan = q.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'Join UsingJoin(Inner,List(id))\n01 :- Range (0, 100, step=1, splits=Some(8))\n02 +- ResolvedHint (broadcast)\n03    +- Range (0, 1, step=1, splits=Some(8))\n\n// Please note that broadcast standard function uses ResolvedHint not UnresolvedHint\n\n// Let's \"replicate\" standard function using hint operator\n// Any of the names would work (case-insensitive)\n// \"BROADCAST\", \"BROADCASTJOIN\", \"MAPJOIN\"\nval smallHinted = small.hint(\"broadcast\")\nval plan = smallHinted.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'UnresolvedHint broadcast\n01 +- Range (0, 1, step=1, splits=Some(8))\n\n// join is \"clever\"\n// i.e. resolves UnresolvedHint into ResolvedHint immediately\nval q = large.join(smallHinted, \"id\")\nval plan = q.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'Join UsingJoin(Inner,List(id))\n01 :- Range (0, 100, step=1, splits=Some(8))\n02 +- ResolvedHint (broadcast)\n03    +- Range (0, 1, step=1, splits=Some(8))\n</code></pre>","text":""},{"location":"hints/#spark-analyzer","title":"Spark Analyzer <p>Logical Analyzer uses the following logical rules to resolve UnresolvedHint logical operators:</p> <ul> <li> <p>ResolveJoinStrategyHints</p> </li> <li> <p>ResolveCoalesceHints</p> </li> <li> <p><code>RemoveAllHints</code> (to remove all <code>UnresolvedHint</code> operators left unresolved)</p> </li> </ul> <p>The order of executing the above rules matters.</p> <pre><code>// Let's hint the query twice\n// The order of hints matters as every hint operator executes Spark analyzer\n// That will resolve all but the last hint\nval q = spark.range(100).\n  hint(\"broadcast\").\n  hint(\"myHint\", 100, true)\nval plan = q.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'UnresolvedHint myHint, [100, true]\n01 +- ResolvedHint (broadcast)\n02    +- Range (0, 100, step=1, splits=Some(8))\n\n// Let's resolve unresolved hints\nimport org.apache.spark.sql.catalyst.rules.RuleExecutor\nimport org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\nimport org.apache.spark.sql.catalyst.analysis.ResolveHints\nimport org.apache.spark.sql.internal.SQLConf\nobject HintResolver extends RuleExecutor[LogicalPlan] {\n  lazy val batches =\n    Batch(\"Hints\", FixedPoint(maxIterations = 100),\n      new ResolveHints.ResolveBroadcastHints(SQLConf.get),\n      ResolveHints.RemoveAllHints) :: Nil\n}\nval resolvedPlan = HintResolver.execute(plan)\nscala&gt; println(resolvedPlan.numberedTreeString)\n00 ResolvedHint (broadcast)\n01 +- Range (0, 100, step=1, splits=Some(8))\n</code></pre>","text":""},{"location":"hints/#hint-operator-in-catalyst-dsl","title":"Hint Operator in Catalyst DSL <p>hint operator in Catalyst DSL allows to create a UnresolvedHint logical operator.</p> <pre><code>// Create a logical plan to add hint to\nimport org.apache.spark.sql.catalyst.plans.logical.LocalRelation\nval r1 = LocalRelation('a.int, 'b.timestamp, 'c.boolean)\nscala&gt; println(r1.numberedTreeString)\n00 LocalRelation &lt;empty&gt;, [a#0, b#1, c#2]\n\n// Attach hint to the plan\nimport org.apache.spark.sql.catalyst.dsl.plans._\nval plan = r1.hint(name = \"myHint\", 100, true)\nscala&gt; println(plan.numberedTreeString)\n00 'UnresolvedHint myHint, [100, true]\n01 +- LocalRelation &lt;empty&gt;, [a#0, b#1, c#2]\n</code></pre>","text":""},{"location":"hints/HintErrorHandler/","title":"HintErrorHandler","text":"<p><code>HintErrorHandler</code> is...FIXME</p>"},{"location":"hints/HintInfo/","title":"HintInfo","text":""},{"location":"hints/HintInfo/#creating-instance","title":"Creating Instance","text":"<p><code>HintInfo</code> takes the following to be created:</p> <ul> <li> Optional JoinStrategyHint (default: undefined) <p><code>HintInfo</code> is created\u00a0when:</p> <ul> <li>ResolveJoinStrategyHints logical resolution rule is executed</li> <li>ResolvedHint logical operator is created</li> <li><code>HintInfo</code> is requested to merge with another HintInfo</li> <li>broadcast standard function is used (on a <code>Dataset</code>)</li> </ul>"},{"location":"hints/HintInfo/#text-representation","title":"Text Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code>\u00a0is part of the <code>Object</code> (Java) abstraction.</p> <p><code>toString</code> returns the following (with the strategy if defined or <code>none</code>):</p> <pre><code>(strategy=[strategy|none])\n</code></pre>","text":""},{"location":"hints/HintInfo/#merging-hints","title":"Merging Hints <pre><code>merge(\n  other: HintInfo,\n  hintErrorHandler: HintErrorHandler): HintInfo\n</code></pre> <p><code>merge</code>...FIXME</p> <p><code>merge</code>\u00a0is used when:</p> <ul> <li>ResolveJoinStrategyHints logical resolution rule is executed</li> <li>EliminateResolvedHint logical optimization is executed</li> </ul>","text":""},{"location":"hints/JoinHint/","title":"JoinHint","text":"<p><code>JoinHint</code> holds the hints of the left and right sides of a Join logical operator.</p>"},{"location":"hints/JoinHint/#creating-instance","title":"Creating Instance","text":"<p><code>JoinHint</code> takes the following to be created:</p> <ul> <li> Left-Side Join HintInfo <li> Right-Side Join HintInfo <p><code>JoinHint</code> is created\u00a0when:</p> <ul> <li>EliminateResolvedHint logical optimization is executed (on Join logical operators with no hint associated explicitly)</li> </ul>"},{"location":"hints/JoinHint/#text-representation","title":"Text Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code>\u00a0is part of the <code>Object</code> (Java) abstraction.</p> <p><code>toString</code> returns the following (with the leftHint and rightHint if defined):</p> <pre><code>leftHint=[leftHint], rightHint=[rightHint]\n</code></pre>","text":""},{"location":"hints/JoinStrategyHint/","title":"JoinStrategyHint","text":"<p><code>JoinStrategyHint</code> is an abstraction of join hints.</p> JoinStrategyHint displayName hintAliases  BROADCAST broadcast <ul><li>BROADCAST</li><li>BROADCASTJOIN</li><li>MAPJOIN</li></ul>  NO_BROADCAST_HASH no_broadcast_hash  PREFER_SHUFFLE_HASH prefer_shuffle_hash  SHUFFLE_HASH shuffle_hash <ul><li>SHUFFLE_HASH</li></ul>  SHUFFLE_MERGE merge <ul><li>SHUFFLE_MERGE</li><li>MERGE</li><li>MERGEJOIN</li></ul>  SHUFFLE_REPLICATE_NL shuffle_replicate_nl <ul><li>SHUFFLE_REPLICATE_NL</li></ul> <p><code>JoinStrategyHint</code> is resolved using ResolveJoinStrategyHints logical resolution rule.</p> sealed abstract class <p><code>JoinStrategyHint</code> is a Scala sealed abstract class which means that all possible implementations (<code>JoinStrategyHint</code>s) are all in the same compilation unit (file).</p>"},{"location":"hints/JoinStrategyHint/#contract","title":"Contract","text":""},{"location":"hints/JoinStrategyHint/#displayname","title":"displayName <pre><code>displayName: String\n</code></pre>","text":""},{"location":"hints/JoinStrategyHint/#hintaliases","title":"hintAliases <pre><code>hintAliases: Set[String]\n</code></pre> <p>Used when:</p> <ul> <li>ResolveJoinStrategyHints logical resolution rule is executed</li> </ul>","text":""},{"location":"hints/JoinStrategyHint/#joinstrategyhints","title":"JoinStrategyHints <p><code>JoinStrategyHint</code> defines <code>strategies</code> collection of <code>JoinStrategyHint</code>s for ResolveJoinStrategyHints logical resolution rule:</p> <ul> <li>BROADCAST</li> <li>SHUFFLE_MERGE</li> <li>SHUFFLE_HASH</li> <li>SHUFFLE_REPLICATE_NL</li> </ul>","text":""},{"location":"hints/join-strategy-hints/","title":"Join Strategy Hints","text":"<p>Join Strategy Hints extend the existing BROADCAST join hint with other join strategy hints for shuffle-hash, sort-merge, cartesian-product.</p> <p>Join Strategy Hints were introduced to Apache Spark 3.0.0 as SPARK-27225.</p> <p>Main abstractions:</p> <ul> <li>JoinStrategyHint</li> <li>ResolveJoinStrategyHints logical resolution rule</li> </ul>"},{"location":"hive/","title":"Hive Connector","text":"<p>Spark SQL supports Apache Hive using Hive data source. Spark SQL allows executing structured queries on Hive tables, a persistent Hive metastore, support for Hive serdes and user-defined functions.</p> <p>Tip</p> <p>Consult Demo: Connecting Spark SQL to Hive Metastore (with Remote Metastore Server) to learn in a more practical approach.</p> <p>In order to use Hive-related features in a Spark SQL application a ../SparkSession.md[SparkSession] has to be created with ../SparkSession-Builder.md#enableHiveSupport[Builder.enableHiveSupport].</p> <p>Hive Data Source uses custom Spark SQL configuration properties (in addition to Hive's).</p> <p>Hive Data Source uses HiveTableRelation.md[HiveTableRelation] to represent Hive tables. <code>HiveTableRelation</code> can be RelationConversions.md#convert[converted to a HadoopFsRelation] based on spark.sql.hive.convertMetastoreParquet and spark.sql.hive.convertMetastoreOrc configuration properties (and \"disappears\" from a logical plan when enabled).</p> <p>Hive Data Source uses HiveSessionStateBuilder.md[HiveSessionStateBuilder] (to build a Hive-specific ../SparkSession.md#sessionState[SessionState]) and HiveExternalCatalog.</p> <p>Hive Data Source uses HiveClientImpl.md[HiveClientImpl] for meta data/DDL operations (using calls to a Hive metastore).</p>"},{"location":"hive/#hive-configuration-properties","title":"Hive Configuration Properties","text":"<p>Hive uses &lt;&gt; that can be defined in &lt;&gt; and as System properties (that have a higher precedence). <p>In Spark applications, you could use <code>--driver-java-options</code> or <code>--conf spark.hadoop.[property]</code> to define Hive configuration properties.</p> <pre><code>$SPARK_HOME/bin/spark-shell \\\n--jars \\\n$HIVE_HOME/lib/hive-metastore-2.3.6.jar,\\\n$HIVE_HOME/lib/hive-exec-2.3.6.jar,\\\n$HIVE_HOME/lib/hive-common-2.3.6.jar,\\\n$HIVE_HOME/lib/hive-serde-2.3.6.jar,\\\n$HIVE_HOME/lib/guava-14.0.1.jar \\\n--conf spark.sql.hive.metastore.version=2.3 \\\n--conf spark.sql.hive.metastore.jars=$HIVE_HOME\"/lib/*\" \\\n--conf spark.sql.warehouse.dir=hdfs://localhost:9000/user/hive/warehouse \\\n--driver-java-options=\"-Dhive.exec.scratchdir=/tmp/hive-scratchdir\" \\\n--conf spark.hadoop.hive.exec.scratchdir=/tmp/hive-scratchdir-conf\n</code></pre> <p>[[properties]] .HiveConf's Configuration Properties [cols=\"1a\",options=\"header\",width=\"100%\"] |=== | Configuration Property</p> <p>| [[hive.enforce.bucketing]] hive.enforce.bucketing</p> <p>| [[hive.enforce.sorting]] hive.enforce.sorting</p> <p>| [[hive.exec.compress.output]] hive.exec.compress.output</p> <p>Default: <code>false</code></p> <p>| [[hive.exec.dynamic.partition]] hive.exec.dynamic.partition</p> <p>| [[hive.exec.dynamic.partition.mode]] hive.exec.dynamic.partition.mode</p> <p>| [[hive.exec.log4j.file]][[HIVE_EXEC_LOG4J_FILE]] hive.exec.log4j.file</p> <p>Hive log4j configuration file for execution mode(sub command).</p> <p>If the property is not set, then logging will be initialized using <code>hive-exec-log4j2.properties</code> found on the classpath.</p> <p>If the property is set, the value must be a valid URI (<code>java.net.URI</code>, e.g. <code>file:///tmp/my-logging.xml</code>).</p> <p>| [[hive.exec.scratchdir]][[SCRATCHDIR]] hive.exec.scratchdir</p> <p>HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: <code>hive.exec.scratchdir</code>/ is created, with &lt;&gt;. <p>Default: <code>/tmp/hive</code></p> <p>| [[hive.exec.stagingdir]][[STAGINGDIR]] hive.exec.stagingdir</p> <p>Temporary staging directory that will be created inside table locations in order to support HDFS encryption.</p> <p>Default: <code>.hive-staging</code></p> <p>This replaces &lt;&gt; for query results with the exception of read-only tables. In all cases &lt;&gt; is still used for other temporary files, such as job plans. <p>| [[hive.log4j.file]][[HIVE_LOG4J_FILE]] hive.log4j.file</p> <p>Hive log4j configuration file.</p> <p>If the property is not set, then logging will be initialized using <code>hive-log4j2.properties</code> found on the classpath.</p> <p>If the property is set, the value must be a valid URI (<code>java.net.URI</code>, e.g. <code>file:///tmp/my-logging.xml</code>).</p> <p>| [[hive.metastore.connect.retries]][[METASTORETHRIFTCONNECTIONRETRIES]] hive.metastore.connect.retries</p> <p>Number of retries while opening a connection to metastore</p> <p>Default: <code>3</code></p> <p>| [[hive.metastore.port]][[METASTORE_SERVER_PORT]] hive.metastore.port</p> <p>Port of the Hive metastore</p> <p>Default: <code>9083</code></p> <p>| [[hive.metastore.uris]][[METASTOREURIS]] hive.metastore.uris</p> <p>Comma-separated list of the thrift URIs of remote metastores that is used by metastore clients to connect (incl. Spark SQL applications)</p> <p>| [[hive.scratch.dir.permission]][[SCRATCHDIRPERMISSION]] hive.scratch.dir.permission</p> <p>The permission for the user specific scratch directories that get created.</p> <p>Default: <code>700</code></p> <p>| [[javax.jdo.option.ConnectionDriverName]][[METASTORE_CONNECTION_DRIVER]] javax.jdo.option.ConnectionDriverName</p> <p>Driver class name for a JDBC metastore</p> <p>Default: <code>org.apache.derby.jdbc.EmbeddedDriver</code></p> <p>| [[javax.jdo.option.ConnectionPassword]][[METASTOREPWD]] javax.jdo.option.ConnectionPassword</p> <p>Password to use against metastore database</p> <p>Default: <code>mine</code></p> <p>| [[javax.jdo.option.ConnectionURL]][[METASTORECONNECTURLKEY]] javax.jdo.option.ConnectionURL</p> <p>JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL (<code>jdbc:postgresql://myhost/db?ssl=true</code> for postgres database).</p> <p>Default: <code>jdbc:derby:;databaseName=metastore_db;create=true</code></p> <p>| [[javax.jdo.option.ConnectionUserName]][[METASTORE_CONNECTION_USER_NAME]] javax.jdo.option.ConnectionUserName</p> <p>Username to use against metastore database</p> <p>Default: <code>APP</code></p> <p>|===</p>"},{"location":"hive/#hive-configuration-files","title":"Hive Configuration Files","text":"<p><code>HiveConf</code> loads <code>hive-default.xml</code> when on the classpath.</p> <p><code>HiveConf</code> loads and prints out the location of <code>hive-site.xml</code> configuration file (when on the classpath, in <code>$HIVE_CONF_DIR</code> or <code>$HIVE_HOME/conf</code> directories, or in the directory with the jar file with <code>HiveConf</code> class).</p> <p>Enable ALL logging level in <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.hadoop.hive=ALL\n</code></pre> <p>Execute the following <code>spark.sharedState.externalCatalog.getTable(\"default\", \"t1\")</code> to have the following INFO message in the logs:</p> <pre><code>Found configuration file [url]\n</code></pre> <p>IMPORTANT: Spark SQL loads <code>hive-site.xml</code> found in <code>$SPARK_HOME/conf</code> while Hive in <code>$SPARK_HOME</code>. Make sure there are no two configuration files that could lead to hard to diagnose issues at runtime.</p>"},{"location":"hive/CreateHiveTableAsSelectCommand/","title":"CreateHiveTableAsSelectCommand Logical Command","text":"<p><code>CreateHiveTableAsSelectCommand</code> is a logical command that writes the result of executing a &lt;&gt; to a &lt;&gt; (per &lt;&gt;). <p><code>CreateHiveTableAsSelectCommand</code> uses the given &lt;&gt; for the table name. <p><code>CreateHiveTableAsSelectCommand</code> is &lt;&gt; when HiveAnalysis.md[HiveAnalysis] logical resolution rule is executed and resolves a ../CreateTable.md[CreateTable] logical operator with a child structured query and a Hive table. <p>When &lt;&gt;, <code>CreateHiveTableAsSelectCommand</code> runs (morphs itself into) a InsertIntoHiveTable.md[InsertIntoHiveTable] logical command. <pre><code>val tableName = \"create_hive_table_as_select_demo\"\nval q = sql(s\"\"\"CREATE TABLE IF NOT EXISTS $tableName USING hive SELECT 1L AS id\"\"\")\nscala&gt; q.explain(extended = true)\n== Parsed Logical Plan ==\n'CreateTable `create_hive_table_as_select_demo`, Ignore\n+- Project [1 AS id#74L]\n   +- OneRowRelation\n\n== Analyzed Logical Plan ==\nCreateHiveTableAsSelectCommand [Database:default, TableName: create_hive_table_as_select_demo, InsertIntoHiveTable]\n+- Project [1 AS id#74L]\n   +- OneRowRelation\n\n== Optimized Logical Plan ==\nCreateHiveTableAsSelectCommand [Database:default, TableName: create_hive_table_as_select_demo, InsertIntoHiveTable]\n+- Project [1 AS id#74L]\n   +- OneRowRelation\n\n== Physical Plan ==\nExecute CreateHiveTableAsSelectCommand CreateHiveTableAsSelectCommand [Database:default, TableName: create_hive_table_as_select_demo, InsertIntoHiveTable]\n+- *(1) Project [1 AS id#74L]\n   +- Scan OneRowRelation[]\n\nscala&gt; spark.table(tableName).show\n+---+\n| id|\n+---+\n|  1|\n+---+\n</code></pre>"},{"location":"hive/CreateHiveTableAsSelectCommand/#creating-instance","title":"Creating Instance","text":"<p><code>CreateHiveTableAsSelectCommand</code> takes the following to be created:</p> <ul> <li>[[tableDesc]] CatalogTable</li> <li>[[query]] Structured query (as a LogicalPlan)</li> <li>[[outputColumnNames]] Names of the output columns</li> <li>[[mode]] SaveMode</li> </ul> <p>=== [[run]] Executing Data-Writing Logical Command -- <code>run</code> Method</p>"},{"location":"hive/CreateHiveTableAsSelectCommand/#source-scala","title":"[source, scala]","text":"<p>run(   sparkSession: SparkSession,   child: SparkPlan): Seq[Row]</p> <p>NOTE: <code>run</code> is part of ../DataWritingCommand.md#run[DataWritingCommand] contract.</p> <p>In summary, <code>run</code> runs a InsertIntoHiveTable.md[InsertIntoHiveTable] logical command.</p> <p><code>run</code> requests the input ../SparkSession.md[SparkSession] for ../SparkSession.md#sessionState[SessionState] that is then requested for the ../SessionState.md#catalog[SessionCatalog].</p> <p><code>run</code> requests the <code>SessionCatalog</code> to ../SessionCatalog.md#tableExists[check out whether the table exists or not].</p> <p>With the Hive table available, <code>run</code> validates the &lt;&gt; and runs a InsertIntoHiveTable.md[InsertIntoHiveTable] logical command (with <code>overwrite</code> and <code>ifPartitionNotExists</code> flags disabled). <p>When the Hive table is not available, <code>run</code> asserts that the schema (of the &lt;&gt;) is not defined and requests the <code>SessionCatalog</code> to ../SessionCatalog.md#createTable[create the table] (with the <code>ignoreIfExists</code> flag disabled). In the end, <code>run</code> runs a InsertIntoHiveTable.md[InsertIntoHiveTable] logical command (with <code>overwrite</code> flag enabled and <code>ifPartitionNotExists</code> flag disabled)."},{"location":"hive/DataSinks/","title":"DataSinks","text":"<p>CAUTION: FIXME</p>"},{"location":"hive/DetermineTableStats/","title":"DetermineTableStats Logical PostHoc Resolution Rule -- Computing Total Size Table Statistic for HiveTableRelations","text":"<p><code>DetermineTableStats</code> is a HiveSessionStateBuilder.md#postHocResolutionRules[logical posthoc resolution rule] that the HiveSessionStateBuilder.md#analyzer[Hive-specific logical query plan analyzer] uses to &lt;&gt;. <p>Technically, <code>DetermineTableStats</code> is a ../catalyst/Rule.md[Catalyst rule] for transforming ../spark-sql-LogicalPlan.md[logical plans], i.e. <code>Rule[LogicalPlan]</code>.</p> <p>=== [[apply]] <code>apply</code> Method</p>"},{"location":"hive/DetermineTableStats/#source-scala","title":"[source, scala]","text":""},{"location":"hive/DetermineTableStats/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p>NOTE: <code>apply</code> is part of ../catalyst/Rule.md#apply[Rule Contract] to apply a rule to a ../spark-sql-LogicalPlan.md[logical plan] (aka execute a rule).</p> <p><code>apply</code>...FIXME</p>"},{"location":"hive/HadoopTableReader/","title":"HadoopTableReader","text":"<p>:hive-version: 2.3.6 :hadoop-version: 2.10.0 :url-hive-javadoc: https://hive.apache.org/javadocs/r{hive-version}/api :url-hadoop-javadoc: https://hadoop.apache.org/docs/r{hadoop-version}/api</p> <p><code>HadoopTableReader</code> is a TableReader.md[TableReader] to create an <code>HadoopRDD</code> for scanning &lt;&gt; or &lt;&gt; tables stored in Hadoop. <p><code>HadoopTableReader</code> is used by HiveTableScanExec.md[HiveTableScanExec] physical operator when requested to HiveTableScanExec.md#doExecute[execute].</p> <p>=== [[creating-instance]] Creating HadoopTableReader Instance</p> <p><code>HadoopTableReader</code> takes the following to be created:</p> <ul> <li>[[attributes]] Attributes</li> <li>[[partitionKeys]] Partition Keys (<code>Seq[Attribute]</code>)</li> <li>[[tableDesc]] Hive {url-hive-javadoc}/org/apache/hive/hcatalog/templeton/TableDesc.html[TableDesc]</li> <li>[[sparkSession]] SparkSession.md[SparkSession]</li> <li>[[hadoopConf]] Hadoop {url-hadoop-javadoc}/org/apache/hadoop/conf/Configuration.html[Configuration]</li> </ul> <p><code>HadoopTableReader</code> initializes the &lt;&gt;. <p>=== [[makeRDDForTable]] <code>makeRDDForTable</code> Method</p>"},{"location":"hive/HadoopTableReader/#source-scala","title":"[source, scala]","text":"<p>makeRDDForTable(   hiveTable: HiveTable): RDD[InternalRow]</p> <p>NOTE: <code>makeRDDForTable</code> is part of the TableReader.md#makeRDDForTable[TableReader] contract to...FIXME.</p> <p><code>makeRDDForTable</code> simply calls the private &lt;&gt; with...FIXME <p>==== [[makeRDDForTable-private]] <code>makeRDDForTable</code> Method</p>"},{"location":"hive/HadoopTableReader/#source-scala_1","title":"[source, scala]","text":"<p>makeRDDForTable(   hiveTable: HiveTable,   deserializerClass: Class[_ &lt;: Deserializer],   filterOpt: Option[PathFilter]): RDD[InternalRow]</p> <p><code>makeRDDForTable</code>...FIXME</p> <p>NOTE: <code>makeRDDForTable</code> is used when...FIXME</p> <p>=== [[makeRDDForPartitionedTable]] <code>makeRDDForPartitionedTable</code> Method</p>"},{"location":"hive/HadoopTableReader/#source-scala_2","title":"[source, scala]","text":"<p>makeRDDForPartitionedTable(   partitions: Seq[HivePartition]): RDD[InternalRow]</p> <p>NOTE: <code>makeRDDForPartitionedTable</code> is part of the TableReader.md#makeRDDForPartitionedTable[TableReader] contract to...FIXME.</p> <p><code>makeRDDForPartitionedTable</code> simply calls the private &lt;&gt; with...FIXME <p>==== [[makeRDDForPartitionedTable-private]] <code>makeRDDForPartitionedTable</code> Method</p>"},{"location":"hive/HadoopTableReader/#source-scala_3","title":"[source, scala]","text":"<p>makeRDDForPartitionedTable(   partitionToDeserializer: Map[HivePartition, Class[_ &lt;: Deserializer]],   filterOpt: Option[PathFilter]): RDD[InternalRow]</p> <p><code>makeRDDForPartitionedTable</code>...FIXME</p> <p>NOTE: <code>makeRDDForPartitionedTable</code> is used when...FIXME</p> <p>=== [[createHadoopRdd]] Creating HadoopRDD -- <code>createHadoopRdd</code> Internal Method</p>"},{"location":"hive/HadoopTableReader/#source-scala_4","title":"[source, scala]","text":"<p>createHadoopRdd(   tableDesc: TableDesc,   path: String,   inputFormatClass: Class[InputFormat[Writable, Writable]]): RDD[Writable]</p> <p><code>createHadoopRdd</code> &lt;&gt; for the input <code>path</code> and <code>tableDesc</code>. <p><code>createHadoopRdd</code> creates an <code>HadoopRDD</code> (with the &lt;&lt;broadcastedHadoopConf, broadcast Hadoop Configuration&gt;&gt;, the input <code>inputFormatClass</code>, and the &lt;&lt;_minSplitsPerRDD, minimum number of partitions&gt;&gt;) and takes (_maps over) the values.</p> <p>NOTE: <code>createHadoopRdd</code> adds a <code>HadoopRDD</code> and a <code>MapPartitionsRDD</code> to a RDD lineage.</p> <p>NOTE: <code>createHadoopRdd</code> is used when <code>HadoopTableReader</code> is requested to &lt;&gt; and &lt;&gt;. <p>=== [[initializeLocalJobConfFunc]] <code>initializeLocalJobConfFunc</code> Utility</p>"},{"location":"hive/HadoopTableReader/#source-scala_5","title":"[source, scala]","text":"<p>initializeLocalJobConfFunc(   path: String,   tableDesc: TableDesc)(     jobConf: JobConf): Unit</p> <p><code>initializeLocalJobConfFunc</code>...FIXME</p> <p>NOTE: <code>initializeLocalJobConfFunc</code> is used when <code>HadoopTableReader</code> is requested to &lt;&gt;. <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| _broadcastedHadoopConf a| [[_broadcastedHadoopConf]] Hadoop {url-hadoop-javadoc}/org/apache/hadoop/conf/Configuration.html[Configuration] broadcast to executors</p> <p>| _minSplitsPerRDD a| [[_minSplitsPerRDD]] Minimum number of partitions for a &lt;&gt;: <ul> <li><code>0</code> for local mode</li> <li>The greatest of Hadoop's <code>mapreduce.job.maps</code> (default: <code>1</code>) and Spark Core's default minimum number of partitions for Hadoop RDDs (not higher than <code>2</code>)</li> </ul> <p>|===</p>"},{"location":"hive/HiveAnalysis/","title":"HiveAnalysis PostHoc Logical Resolution Rule","text":"<p><code>HiveAnalysis</code> is a HiveSessionStateBuilder.md#postHocResolutionRules[logical posthoc resolution rule] that the HiveSessionStateBuilder.md#analyzer[Hive-specific logical query plan analyzer] uses to &lt;&gt;. <p>Technically, <code>HiveAnalysis</code> is a ../catalyst/Rule.md[Catalyst rule] for transforming ../spark-sql-LogicalPlan.md[logical plans], i.e. <code>Rule[LogicalPlan]</code>.</p>"},{"location":"hive/HiveAnalysis/#source-scala","title":"[source, scala]","text":""},{"location":"hive/HiveAnalysis/#fixme-example-of-hiveanalysis","title":"// FIXME Example of HiveAnalysis","text":"<p>=== [[apply]] Applying HiveAnalysis Rule to Logical Plan (Executing HiveAnalysis) -- <code>apply</code> Method</p>"},{"location":"hive/HiveAnalysis/#source-scala_1","title":"[source, scala]","text":""},{"location":"hive/HiveAnalysis/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p>NOTE: <code>apply</code> is part of ../catalyst/Rule.md#apply[Rule Contract] to apply a rule to a ../spark-sql-LogicalPlan.md[logical plan].</p> <p><code>apply</code>...FIXME</p>"},{"location":"hive/HiveClient/","title":"HiveClient","text":""},{"location":"hive/HiveClientImpl/","title":"HiveClientImpl","text":"<p><code>HiveClientImpl</code> is a HiveClient that uses a Hive metastore client to communicate with a Hive metastore.</p>"},{"location":"hive/HiveClientImpl/#creating-instance","title":"Creating Instance","text":"<p><code>HiveClientImpl</code> takes the following to be created:</p> <ul> <li> <code>HiveVersion</code> <li>Metastore Warehouse Directory</li> <li> <code>SparkConf</code> (Spark Core) <li> Hadoop Configuration (<code>Iterable[Map.Entry[String, String]]</code>) <li> Extra Configuration (<code>Map[String, String]</code>) <li> Init <code>ClassLoader</code> <li> IsolatedClientLoader <p>When created, <code>HiveClientImpl</code> prints out the following INFO message to the logs:</p> <pre><code>Warehouse location for Hive client (version [fullVersion]) is [the value of hive.metastore.warehouse.dir]\n</code></pre> <p><code>HiveClientImpl</code> is created when:</p> <ul> <li><code>IsolatedClientLoader</code> is requested to create a HiveClient</li> </ul>"},{"location":"hive/HiveClientImpl/#metastore-warehouse-directory","title":"Metastore Warehouse Directory <p><code>HiveClientImpl</code> is given the directory of the default database of a Hive warehouse.</p> <p>The directory is the value of <code>hive.metastore.warehouse.dir</code> configuration property (default: <code>/user/hive/warehouse</code>).</p>","text":""},{"location":"hive/HiveClientImpl/#hive-metastore-client","title":"Hive Metastore Client <pre><code>client: Hive\n</code></pre> <p><code>client</code> is a Hive metastore client (for meta data/DDL operations using calls to the metastore).</p>","text":""},{"location":"hive/HiveClientImpl/#creating-catalogstatistics","title":"Creating CatalogStatistics <pre><code>readHiveStats(\n  properties: Map[String, String]): Option[CatalogStatistics]\n</code></pre> <p><code>readHiveStats</code> creates a CatalogStatistics from the input Hive <code>properties</code> (with table and possibly partition parameters). <code>readHiveStats</code> uses the following Hive properties, if available and greater than 0.</p>    Hive Property Table Statistic     <code>totalSize</code> or <code>rawDataSize</code> sizeInBytes   <code>numRows</code> rowCount     <p><code>readHiveStats</code> is used when:</p> <ul> <li><code>HiveClientImpl</code> is requested for the metadata of a table or partition</li> </ul>","text":""},{"location":"hive/HiveClientImpl/#converthivetabletocatalogtable","title":"convertHiveTableToCatalogTable <pre><code>convertHiveTableToCatalogTable(\n  h: Table): CatalogTable\n</code></pre> <p><code>convertHiveTableToCatalogTable</code> creates a CatalogTable based on the given Hive Table as follows:</p>    CatalogTable Hive Table     Table Statistics readHiveStats   ...      <p><code>convertHiveTableToCatalogTable</code> is used when:</p> <ul> <li><code>HiveClientImpl</code> is requested to getRawHiveTableOption (and requests <code>RawHiveTableImpl</code> to <code>getRawHiveTableOption</code>), getTablesByName, getTableOption</li> </ul>","text":""},{"location":"hive/HiveClientImpl/#fromhivepartition","title":"fromHivePartition <pre><code>fromHivePartition(\n  hp: HivePartition): CatalogTablePartition\n</code></pre> <p><code>fromHivePartition</code>...FIXME</p>  <p><code>fromHivePartition</code> is used when:</p> <ul> <li><code>HiveClientImpl</code> is requested to getPartitionOption, getPartitions, getPartitionsByFilter</li> </ul>","text":""},{"location":"hive/HiveClientImpl/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.hive.client.HiveClientImpl</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.hive.client.HiveClientImpl=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"hive/HiveExternalCatalog/","title":"HiveExternalCatalog","text":"<p><code>HiveExternalCatalog</code> is an ExternalCatalog for <code>SparkSession</code> with Hive support enabled.</p> <p></p> <p><code>HiveExternalCatalog</code> uses an HiveClient to interact with a Hive metastore.</p>"},{"location":"hive/HiveExternalCatalog/#creating-instance","title":"Creating Instance","text":"<p><code>HiveExternalCatalog</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> (Spark Core) <li> <code>Configuration</code> (Apache Hadoop) <p><code>HiveExternalCatalog</code> is created when:</p> <ul> <li><code>SharedState</code> is requested for the ExternalCatalog (and spark.sql.catalogImplementation is <code>hive</code>).</li> </ul>"},{"location":"hive/HiveExternalCatalog/#restoretablemetadata","title":"restoreTableMetadata <pre><code>restoreTableMetadata(\n  inputTable: CatalogTable): CatalogTable\n</code></pre> <p><code>restoreTableMetadata</code>...FIXME</p>  <p><code>restoreTableMetadata</code> is used when:</p> <ul> <li><code>HiveExternalCatalog</code> is requested for table metadata, the metadata of tables and listPartitionsByFilter</li> </ul>","text":""},{"location":"hive/HiveExternalCatalog/#restorehiveserdetable","title":"restoreHiveSerdeTable <pre><code>restoreHiveSerdeTable(\n  table: CatalogTable): CatalogTable\n</code></pre> <p><code>restoreHiveSerdeTable</code>...FIXME</p>","text":""},{"location":"hive/HiveExternalCatalog/#restoredatasourcetable","title":"restoreDataSourceTable <pre><code>restoreDataSourceTable(\n  table: CatalogTable,\n  provider: String): CatalogTable\n</code></pre> <p><code>restoreDataSourceTable</code>...FIXME</p>","text":""},{"location":"hive/HiveExternalCatalog/#looking-up-bucketspec-in-table-properties","title":"Looking Up BucketSpec in Table Properties <pre><code>getBucketSpecFromTableProperties(\n  metadata: CatalogTable): Option[BucketSpec]\n</code></pre> <p><code>getBucketSpecFromTableProperties</code> looks up the value of <code>spark.sql.sources.schema.numBuckets</code> property (among the properties) in the given CatalogTable metadata.</p> <p>If found, <code>getBucketSpecFromTableProperties</code> creates a BucketSpec with the following:</p>    BucketSpec Metadata Property     numBuckets <code>spark.sql.sources.schema.numBuckets</code>   bucketColumnNames <code>spark.sql.sources.schema.numBucketCols</code><code>spark.sql.sources.schema.bucketCol.N</code>   sortColumnNames <code>spark.sql.sources.schema.numSortCols</code><code>spark.sql.sources.schema.sortCol.N</code>","text":""},{"location":"hive/HiveExternalCatalog/#restorepartitionmetadata","title":"restorePartitionMetadata <pre><code>restorePartitionMetadata(\n  partition: CatalogTablePartition,\n  table: CatalogTable): CatalogTablePartition\n</code></pre> <p><code>restorePartitionMetadata</code>...FIXME</p>  <p><code>restorePartitionMetadata</code> is used when:</p> <ul> <li><code>HiveExternalCatalog</code> is requested to getPartition and getPartitionOption</li> </ul>","text":""},{"location":"hive/HiveExternalCatalog/#restoring-table-statistics-from-table-properties-from-hive-metastore","title":"Restoring Table Statistics from Table Properties (from Hive Metastore) <pre><code>statsFromProperties(\n  properties: Map[String, String],\n  table: String): Option[CatalogStatistics]\n</code></pre> <p><code>statsFromProperties</code> collects statistics-related <code>spark.sql.statistics</code>-prefixed properties.</p> <p>For no keys with the prefix, <code>statsFromProperties</code> returns <code>None</code>.</p> <p>If there are keys with <code>spark.sql.statistics</code> prefix, <code>statsFromProperties</code> creates a CatalogColumnStat for every column in the <code>schema</code>.</p> <p>For every column name in <code>schema</code>, <code>statsFromProperties</code> collects all the keys that start with <code>spark.sql.statistics.colStats.[name]</code> prefix (after having checked that the key <code>spark.sql.statistics.colStats.[name].version</code> exists that is a marker that the column statistics exist in the statistics properties) and converts them to a <code>ColumnStat</code> (for the column name).</p> <p>In the end, <code>statsFromProperties</code> creates a CatalogStatistics as follows:</p>    Catalog Statistic Value     sizeInBytes <code>spark.sql.statistics.totalSize</code>   rowCount <code>spark.sql.statistics.numRows</code> property   colStats Column Names and their CatalogColumnStats     <p><code>statsFromProperties</code> is used when:</p> <ul> <li><code>HiveExternalCatalog</code> is requested to restore metadata of a table or a partition</li> </ul>","text":""},{"location":"hive/HiveExternalCatalog/#demo","title":"Demo <pre><code>import org.apache.spark.sql.internal.StaticSQLConf\nval catalogType = spark.conf.get(StaticSQLConf.CATALOG_IMPLEMENTATION.key)\nassert(catalogType == \"hive\")\n</code></pre> <pre><code>assert(spark.sessionState.conf.getConf(StaticSQLConf.CATALOG_IMPLEMENTATION) == \"hive\")\n</code></pre> <pre><code>assert(spark.conf.get(\"spark.sql.catalogImplementation\") == \"hive\")\n</code></pre> <pre><code>val metastore = spark.sharedState.externalCatalog\nimport org.apache.spark.sql.catalyst.catalog.ExternalCatalog\nassert(metastore.isInstanceOf[ExternalCatalog])\n</code></pre> <pre><code>scala&gt; println(metastore)\norg.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener@277ffb17\n\nscala&gt; println(metastore.unwrapped)\norg.apache.spark.sql.hive.HiveExternalCatalog@4eda3af9\n</code></pre>","text":""},{"location":"hive/HiveExternalCatalog/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.hive.HiveExternalCatalog</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.HiveExternalCatalog.name = org.apache.spark.sql.hive.HiveExternalCatalog\nlogger.HiveExternalCatalog.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"hive/HiveFileFormat/","title":"HiveFileFormat","text":"<p><code>HiveFileFormat</code> is a FileFormat for writing Hive tables.</p> <p>[[shortName]] <code>HiveFileFormat</code> is a DataSourceRegister and registers itself as hive data source.</p> <p>NOTE: Hive data source can only be used with tables and you cannot read or write files of Hive data source directly. Use DataFrameReader.table to load from or DataFrameWriter.saveAsTable to write data to a Hive table.</p> <p><code>HiveFileFormat</code> is &lt;&gt; exclusively when <code>SaveAsHiveFile</code> is requested to ../hive/SaveAsHiveFile.md#saveAsHiveFile[saveAsHiveFile] (when InsertIntoHiveDirCommand.md[InsertIntoHiveDirCommand] and InsertIntoHiveTable.md[InsertIntoHiveTable] logical commands are executed). <p>[[fileSinkConf]][[creating-instance]] <code>HiveFileFormat</code> takes a <code>FileSinkDesc</code> when created.</p> <p>[[inferSchema]] <code>HiveFileFormat</code> throws a <code>UnsupportedOperationException</code> when requested to inferSchema.</p> <pre><code>inferSchema is not supported for hive data source.\n</code></pre> <p>=== [[prepareWrite]] Preparing Write Job -- <code>prepareWrite</code> Method</p>"},{"location":"hive/HiveFileFormat/#source-scala","title":"[source, scala]","text":"<p>prepareWrite(   sparkSession: SparkSession,   job: Job,   options: Map[String, String],   dataSchema: StructType): OutputWriterFactory</p> <p><code>prepareWrite</code> sets the mapred.output.format.class property to be the <code>getOutputFileFormatClassName</code> of the Hive <code>TableDesc</code> of the &lt;&gt;. <p><code>prepareWrite</code> requests the <code>HiveTableUtil</code> helper object to <code>configureJobPropertiesForStorageHandler</code>.</p> <p><code>prepareWrite</code> requests the Hive <code>Utilities</code> helper object to <code>copyTableJobPropertiesToConf</code>.</p> <p>In the end, <code>prepareWrite</code> creates a new <code>OutputWriterFactory</code> that creates a new <code>HiveOutputWriter</code> when requested for a new <code>OutputWriter</code> instance.</p> <p><code>prepareWrite</code> is part of the FileFormat abstraction.</p>"},{"location":"hive/HiveMetastoreCatalog/","title":"HiveMetastoreCatalog \u2014 Legacy SessionCatalog for Converting Hive Metastore Relations to Data Source Relations","text":"<p><code>HiveMetastoreCatalog</code> is a session-scoped catalog of relational entities that knows how to &lt;&gt;. <p><code>HiveMetastoreCatalog</code> is used by HiveSessionCatalog for RelationConversions logical evaluation rule.</p> <p><code>HiveMetastoreCatalog</code> is &lt;&gt; when <code>HiveSessionStateBuilder</code> is requested for a HiveSessionStateBuilder.md#catalog[SessionCatalog] (and creates a HiveSessionCatalog). <p></p>"},{"location":"hive/HiveMetastoreCatalog/#creating-instance","title":"Creating Instance","text":"<p><code>HiveMetastoreCatalog</code> takes the following to be created:</p> <ul> <li>[[sparkSession]] SparkSession</li> </ul>"},{"location":"hive/HiveMetastoreCatalog/#converting-hivetablerelation-to-logicalrelation","title":"Converting HiveTableRelation to LogicalRelation <pre><code>convert(\n  relation: HiveTableRelation): LogicalRelation\n</code></pre> <p><code>convert</code>...FIXME</p> <p><code>convert</code> is used when:</p> <ul> <li>RelationConversions logical rule is executed (for a <code>InsertIntoStatement</code> over a HiveTableRelation or a HiveTableRelation)</li> <li><code>OptimizedCreateHiveTableAsSelectCommand</code> logical command is executed</li> </ul>","text":""},{"location":"hive/HiveMetastoreCatalog/#converttologicalrelation","title":"convertToLogicalRelation <pre><code>convertToLogicalRelation(\n  relation: HiveTableRelation,\n  options: Map[String, String],\n  fileFormatClass: Class[_ &lt;: FileFormat],\n  fileType: String): LogicalRelation\n</code></pre> <p><code>convertToLogicalRelation</code> branches based on whether the input HiveTableRelation.md[HiveTableRelation] is &lt;&gt; or &lt;&gt;. <p>[[convertToLogicalRelation-partitioned]] When the <code>HiveTableRelation</code> is HiveTableRelation.md#isPartitioned[partitioned], <code>convertToLogicalRelation</code> uses spark.sql.hive.manageFilesourcePartitions configuration property to compute the root paths. With the property enabled, the root path is simply the table location (aka locationUri). Otherwise, the root paths are the <code>locationUri</code> of the partitions (using the shared ExternalCatalog).</p> <p><code>convertToLogicalRelation</code> creates a new ../LogicalRelation.md[LogicalRelation] with a HadoopFsRelation (with no bucketing specification among things) unless a <code>LogicalRelation</code> for the table is already in a &lt;&gt;. <p>[[convertToLogicalRelation-not-partitioned]] When the <code>HiveTableRelation</code> is not partitioned, <code>convertToLogicalRelation</code>...FIXME</p> <p>In the end, <code>convertToLogicalRelation</code> replaces <code>exprIds</code> in the ../LogicalRelation.md#output[table relation output (schema)].</p> <p>NOTE: <code>convertToLogicalRelation</code> is used when RelationConversions.md[RelationConversions] logical evaluation rule is executed (with Hive tables in <code>parquet</code> as well as <code>native</code> and <code>hive</code> ORC storage formats).</p>","text":""},{"location":"hive/HiveMetastoreCatalog/#inferifneeded","title":"inferIfNeeded <pre><code>inferIfNeeded(\n  relation: HiveTableRelation,\n  options: Map[String, String],\n  fileFormat: FileFormat,\n  fileIndexOpt: Option[FileIndex] = None): CatalogTable\n</code></pre> <p><code>inferIfNeeded</code>...FIXME</p> <p>=== [[getCached]] <code>getCached</code> Internal Method</p>","text":""},{"location":"hive/HiveMetastoreCatalog/#source-scala","title":"[source, scala] <p>getCached(   tableIdentifier: QualifiedTableName,   pathsInMetastore: Seq[Path],   schemaInMetastore: StructType,   expectedFileFormat: Class[_ &lt;: FileFormat],   partitionSchema: Option[StructType]): Option[LogicalRelation]</p>  <p><code>getCached</code>...FIXME</p> <p>NOTE: <code>getCached</code> is used when <code>HiveMetastoreCatalog</code> is requested to &lt;&gt;. <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| catalogProxy a| [[catalogProxy]] SessionCatalog (of the &lt;&gt;). <p>Used when <code>HiveMetastoreCatalog</code> is requested to &lt;&gt;, &lt;&gt; <p>|===</p>","text":""},{"location":"hive/HiveSessionCatalog/","title":"HiveSessionCatalog -- Hive-Specific Catalog of Relational Entities","text":"<p>:hive-version: 2.3.6 :hadoop-version: 2.10.0 :url-hive-javadoc: https://hive.apache.org/javadocs/r{hive-version}/api :url-hadoop-javadoc: https://hadoop.apache.org/docs/r{hadoop-version}/api</p> <p><code>HiveSessionCatalog</code> is a session-scoped catalog of relational entities that is used when <code>SparkSession</code> was created with ../SparkSession-Builder.md#enableHiveSupport[Hive support enabled].</p> <p>.HiveSessionCatalog and HiveSessionStateBuilder image::../images/spark-sql-HiveSessionCatalog.png[align=\"center\"]</p> <p><code>HiveSessionCatalog</code> is available as ../SessionState.md#catalog[catalog] property of <code>SessionState</code> when <code>SparkSession</code> was created with ../SparkSession-Builder.md#enableHiveSupport[Hive support enabled] (that in the end sets ../StaticSQLConf.md#spark.sql.catalogImplementation[spark.sql.catalogImplementation] internal configuration property to <code>hive</code>).</p>"},{"location":"hive/HiveSessionCatalog/#source-scala","title":"[source, scala]","text":"<p>import org.apache.spark.sql.internal.StaticSQLConf val catalogType = spark.conf.get(StaticSQLConf.CATALOG_IMPLEMENTATION.key) scala&gt; println(catalogType) hive</p> <p>// You could also use the property key by name scala&gt; spark.conf.get(\"spark.sql.catalogImplementation\") res1: String = hive</p> <p>// Since Hive is enabled HiveSessionCatalog is the implementation scala&gt; spark.sessionState.catalog res2: org.apache.spark.sql.catalyst.catalog.SessionCatalog = org.apache.spark.sql.hive.HiveSessionCatalog@1ae3d0a8</p> <p><code>HiveSessionCatalog</code> is &lt;&gt; exclusively when <code>HiveSessionStateBuilder</code> is requested for the HiveSessionStateBuilder.md#catalog[SessionCatalog]. <p><code>HiveSessionCatalog</code> uses the legacy &lt;&gt; (which is another session-scoped catalog of relational entities) exclusively to allow <code>RelationConversions</code> logical evaluation rule to &lt;&gt; when RelationConversions.md#apply[executed]."},{"location":"hive/HiveSessionCatalog/#creating-instance","title":"Creating Instance","text":"<p><code>HiveSessionCatalog</code> takes the following to be created:</p> <ul> <li>[[externalCatalog]] HiveExternalCatalog</li> <li>[[globalTempViewManager]] GlobalTempViewManager</li> <li>[[metastoreCatalog]] Legacy HiveMetastoreCatalog</li> <li>[[functionRegistry]] FunctionRegistry</li> <li>[[conf]] SQLConf</li> <li>[[hadoopConf]] Hadoop {url-hadoop-javadoc}/org/apache/hadoop/conf/Configuration.html[Configuration]</li> <li>[[parser]] ParserInterface</li> <li>[[functionResourceLoader]] <code>FunctionResourceLoader</code></li> </ul> <p>=== [[lookupFunction0]] <code>lookupFunction0</code> Internal Method</p>"},{"location":"hive/HiveSessionCatalog/#source-scala_1","title":"[source, scala]","text":"<p>lookupFunction0(   name: FunctionIdentifier,   children: Seq[Expression]): Expression</p> <p><code>lookupFunction0</code>...FIXME</p> <p>NOTE: <code>lookupFunction0</code> is used when...FIXME</p>"},{"location":"hive/HiveSessionStateBuilder/","title":"HiveSessionStateBuilder","text":"<p><code>HiveSessionStateBuilder</code> is a concrete ../BaseSessionStateBuilder.md[builder] to produce a Hive-aware ../SessionState.md[SessionState] for...FIXME</p> <p><code>HiveSessionStateBuilder</code> comes with Hive-specific &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt; and &lt;&gt;. <p>.HiveSessionStateBuilder's Hive-Specific Properties image::../images/spark-sql-HiveSessionStateBuilder.png[align=\"center\"]</p> <p><code>HiveSessionStateBuilder</code> is &lt;&gt; (using &lt;&gt;) when...FIXME <p>.HiveSessionStateBuilder and SessionState (in SparkSession) image::../images/spark-sql-HiveSessionStateBuilder-SessionState.png[align=\"center\"]</p> <p>[[properties]] .HiveSessionStateBuilder's Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| &lt;&gt; a| [[analyzer]] Hive-specific logical query plan analyzer with the Hive-specific rules. <p>| <code>catalog</code> a| [[catalog]] HiveSessionCatalog with the following:</p> <ul> <li>&lt;&gt; <li>../SharedState.md#globalTempViewManager[GlobalTempViewManager] from the session-specific <code>SharedState</code></li> <li>New HiveMetastoreCatalog.md[HiveMetastoreCatalog]</li> <li>../BaseSessionStateBuilder.md#functionRegistry[FunctionRegistry]</li> <li>../BaseSessionStateBuilder.md#conf[SQLConf]</li> <li>New Hadoop ../SessionState.md#newHadoopConf[Configuration]</li> <li>../BaseSessionStateBuilder.md#sqlParser[ParserInterface]</li> <li>&lt;&gt; <p>NOTE: If &lt;&gt; is defined, the state is copied to <code>catalog</code> <p>Used to create &lt;&gt; and a RelationConversions.md#creating-instance[RelationConversions] logical evaluation rule (as part of &lt;&gt;) <p>| <code>externalCatalog</code> | [[externalCatalog]] HiveExternalCatalog</p> <p>| &lt;&gt; | [[planner]] SparkPlanner with &lt;&gt;. <p>| <code>resourceLoader</code> | [[resourceLoader]] <code>HiveSessionResourceLoader</code> |===</p> <p>=== [[planner-indepth]] SparkPlanner with Hive-Specific Strategies -- <code>planner</code> Property</p>"},{"location":"hive/HiveSessionStateBuilder/#source-scala","title":"[source, scala]","text":""},{"location":"hive/HiveSessionStateBuilder/#planner-sparkplanner","title":"planner: SparkPlanner","text":"<p>NOTE: <code>planner</code> is part of ../BaseSessionStateBuilder.md#planner[BaseSessionStateBuilder Contract] to create a query planner.</p> <p><code>planner</code> is a SparkPlanner with...FIXME</p> <p><code>planner</code> uses the &lt;&gt;. <p>[[planner-strategies]] .Hive-Specific SparkPlanner's Hive-Specific Strategies [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Strategy | Description</p> <p>| HiveTableScans.md[HiveTableScans] | [[HiveTableScans]] Replaces HiveTableRelation.md[HiveTableRelation] logical operators with HiveTableScanExec.md[HiveTableScanExec] physical operators</p> <p>| <code>Scripts</code> | [[Scripts]] |===</p> <p>=== [[analyzer-indepth]] Logical Query Plan Analyzer with Hive-Specific Rules -- <code>analyzer</code> Property</p> <pre><code>analyzer: Analyzer\n</code></pre> <p><code>analyzer</code> is a Logical Analyzer with &lt;&gt; (and ../BaseSessionStateBuilder.md#conf[SQLConf]). <p><code>analyzer</code> uses the Hive-specific &lt;&gt;, &lt;&gt; and &lt;&gt; rules. <p><code>analyzer</code> is part of the BaseSessionStateBuilder abstraction.</p> <p>[[extendedResolutionRules]] .Hive-Specific Analyzer's Extended Resolution Rules (in the order of execution) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Logical Rule | Description</p> <p>| ResolveHiveSerdeTable | [[ResolveHiveSerdeTable]]</p> <p>| FindDataSourceTable | [[FindDataSourceTable]]</p> <p>| ResolveSQLOnFile | [[ResolveSQLOnFile]]</p> <p>|===</p> <p>[[postHocResolutionRules]] .Hive-Specific Analyzer's PostHoc Resolution Rules [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Logical Rule | Description</p> [[DetermineTableStats]] DetermineTableStats.md[DetermineTableStats] [[RelationConversions]] RelationConversions.md[RelationConversions] [[PreprocessTableCreation]] PreprocessTableCreation [[PreprocessTableInsertion]] <code>PreprocessTableInsertion</code> [[DataSourceAnalysis]] .DataSourceAnalysis [[HiveAnalysis]] HiveAnalysis.md[HiveAnalysis] === <p>[[extendedCheckRules]] .Hive-Specific Analyzer's Extended Check Rules [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Logical Rule | Description</p> [[PreWriteCheck]] <code>PreWriteCheck</code> [[PreReadCheck]] <code>PreReadCheck</code> === <p>=== [[creating-instance]] Creating HiveSessionStateBuilder Instance</p> <p><code>HiveSessionStateBuilder</code> takes the following when created:</p> <ul> <li>[[session]] ../SparkSession.md[SparkSession]</li> <li>[[parentState]] Optional ../SessionState.md[SessionState] (default: <code>None</code>)</li> </ul> <p>=== [[newBuilder]] Builder Function to Create HiveSessionStateBuilder -- <code>newBuilder</code> Method</p>"},{"location":"hive/HiveSessionStateBuilder/#source-scala_1","title":"[source, scala]","text":""},{"location":"hive/HiveSessionStateBuilder/#newbuilder-sparksession-optionsessionstate-basesessionstatebuilder","title":"newBuilder: (SparkSession, Option[SessionState]) =&gt; BaseSessionStateBuilder","text":"<p>NOTE: <code>newBuilder</code> is part of ../BaseSessionStateBuilder.md#newBuilder[BaseSessionStateBuilder] contract to...FIXME.</p> <p><code>newBuilder</code>...FIXME</p>"},{"location":"hive/HiveTableRelation/","title":"HiveTableRelation Leaf Logical Operator","text":"<p><code>HiveTableRelation</code> is a ../LeafNode.md[leaf logical operator] that represents a Hive table in a ../spark-sql-LogicalPlan.md[logical query plan].</p> <p><code>HiveTableRelation</code> is &lt;&gt; when <code>FindDataSourceTable</code> logical evaluation rule is requested to resolve UnresolvedCatalogRelations in a logical plan (for Hive tables). <p>NOTE: <code>HiveTableRelation</code> can be RelationConversions.md#convert[converted to a HadoopFsRelation] based on spark.sql.hive.convertMetastoreParquet and spark.sql.hive.convertMetastoreOrc properties (and \"disappears\" from a logical plan when enabled).</p> <p><code>HiveTableRelation</code> is &lt;&gt; when it has at least one &lt;&gt;. <p>[[MultiInstanceRelation]] <code>HiveTableRelation</code> is a MultiInstanceRelation.</p> <p><code>HiveTableRelation</code> is converted (resolved) to as follows:</p> <ul> <li> <p>HiveTableScanExec.md[HiveTableScanExec] physical operator in HiveTableScans.md[HiveTableScans] execution planning strategy</p> </li> <li> <p>InsertIntoHiveTable.md[InsertIntoHiveTable] command in HiveAnalysis.md[HiveAnalysis] logical resolution rule</p> </li> </ul> <pre><code>val tableName = \"h1\"\n\n// Make the example reproducible\nval db = spark.catalog.currentDatabase\nimport spark.sharedState.{externalCatalog =&gt; extCatalog}\nextCatalog.dropTable(\n  db, table = tableName, ignoreIfNotExists = true, purge = true)\n\n// sql(\"CREATE TABLE h1 (id LONG) USING hive\")\nimport org.apache.spark.sql.types.StructType\nspark.catalog.createTable(\n  tableName,\n  source = \"hive\",\n  schema = new StructType().add($\"id\".long),\n  options = Map.empty[String, String])\n\nval h1meta = extCatalog.getTable(db, tableName)\nscala&gt; println(h1meta.provider.get)\nhive\n\n// Looks like we've got the testing space ready for the experiment\nval h1 = spark.table(tableName)\n\nimport org.apache.spark.sql.catalyst.dsl.plans._\nval plan = table(tableName).insertInto(\"t2\", overwrite = true)\nscala&gt; println(plan.numberedTreeString)\n00 'InsertIntoTable 'UnresolvedRelation `t2`, true, false\n01 +- 'UnresolvedRelation `h1`\n\n// ResolveRelations logical rule first to resolve UnresolvedRelations\nimport spark.sessionState.analyzer.ResolveRelations\nval rrPlan = ResolveRelations(plan)\nscala&gt; println(rrPlan.numberedTreeString)\n00 'InsertIntoTable 'UnresolvedRelation `t2`, true, false\n01 +- 'SubqueryAlias h1\n02    +- 'UnresolvedCatalogRelation `default`.`h1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n\n// FindDataSourceTable logical rule next to resolve UnresolvedCatalogRelations\nimport org.apache.spark.sql.execution.datasources.FindDataSourceTable\nval findTablesRule = new FindDataSourceTable(spark)\nval planWithTables = findTablesRule(rrPlan)\n\n// At long last...\n// Note HiveTableRelation in the logical plan\nscala&gt; println(planWithTables.numberedTreeString)\n00 'InsertIntoTable 'UnresolvedRelation `t2`, true, false\n01 +- SubqueryAlias h1\n02    +- HiveTableRelation `default`.`h1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#13L]\n</code></pre> <p>The metadata of a <code>HiveTableRelation</code> (in a catalog) has to meet the requirements:</p> <ul> <li>The database is defined</li> <li>The partition schema is of the same type as &lt;&gt; <li>The data schema is of the same type as &lt;&gt; <p>[[output]] <code>HiveTableRelation</code> has the output attributes made up of &lt;&gt; followed by &lt;&gt; columns. <p>=== [[computeStats]] Computing Statistics -- <code>computeStats</code> Method</p>"},{"location":"hive/HiveTableRelation/#source-scala","title":"[source, scala]","text":""},{"location":"hive/HiveTableRelation/#computestats-statistics","title":"computeStats(): Statistics","text":"<p>NOTE: <code>computeStats</code> is part of ../LeafNode.md#computeStats[LeafNode Contract] to compute statistics for ../cost-based-optimization/index.md[cost-based optimizer].</p> <p><code>computeStats</code> takes the table statistics from the &lt;&gt; if defined and ../CatalogStatistics.md#toPlanStats[converts them to Spark statistics] (with &lt;&gt;). <p>If the table statistics are not available, <code>computeStats</code> reports an <code>IllegalStateException</code>.</p> <pre><code>table stats must be specified.\n</code></pre>"},{"location":"hive/HiveTableRelation/#creating-instance","title":"Creating Instance","text":"<p><code>HiveTableRelation</code> takes the following when created:</p> <ul> <li>[[tableMeta]] Table metadata</li> <li>[[dataCols]] Columns (as a collection of <code>AttributeReferences</code>)</li> <li>[[partitionCols]] Partition columns (as a collection of <code>AttributeReferences</code>)</li> </ul> <p>=== [[partition-columns]] Partition Columns</p> <p>When created, <code>HiveTableRelation</code> is given the &lt;&gt;. <p>FindDataSourceTable.mdFindDataSourceTable logical evaluation rule creates a <code>HiveTableRelation</code> based on a table specification (from a catalog).</p> <p>The &lt;&gt; are exactly partitions of the table specification. <p>=== [[isPartitioned]] <code>isPartitioned</code> Method</p>"},{"location":"hive/HiveTableRelation/#source-scala_1","title":"[source, scala]","text":""},{"location":"hive/HiveTableRelation/#ispartitioned-boolean","title":"isPartitioned: Boolean","text":"<p><code>isPartitioned</code> is <code>true</code> when there is at least one &lt;&gt;."},{"location":"hive/HiveTableRelation/#note","title":"[NOTE]","text":"<p><code>isPartitioned</code> is used when:</p> <ul> <li> <p><code>HiveMetastoreCatalog</code> is requested to HiveMetastoreCatalog.md#convertToLogicalRelation[convert a HiveTableRelation to a LogicalRelation over a HadoopFsRelation]</p> </li> <li> <p>RelationConversions.md[RelationConversions] logical posthoc evaluation rule is executed (on a RelationConversions.md#apply-InsertIntoTable[InsertIntoTable])</p> </li> </ul>"},{"location":"hive/HiveTableRelation/#hivetablescanexec-physical-operator-is-hivetablescanexecmddoexecuteexecuted","title":"* <code>HiveTableScanExec</code> physical operator is HiveTableScanExec.md#doExecute[executed]","text":""},{"location":"hive/HiveTableScanExec/","title":"HiveTableScanExec Leaf Physical Operator","text":"<p><code>HiveTableScanExec</code> is a leaf physical operator that represents a HiveTableRelation logical operator at execution time.</p> <p><code>HiveTableScanExec</code> is &lt;&gt; exclusively when HiveTableScans.md[HiveTableScans] execution planning strategy plans a <code>HiveTableRelation</code> logical operator (i.e. is executed on a logical query plan with a <code>HiveTableRelation</code> logical operator). <p>[[nodeName]] <code>HiveTableScanExec</code> uses the HiveTableRelation.md#tableMeta[fully-qualified name of the Hive table] (of the &lt;&gt;) for the node name: <pre><code>Scan hive [table]\n</code></pre>"},{"location":"hive/HiveTableScanExec/#creating-instance","title":"Creating Instance","text":"<p><code>HiveTableScanExec</code> takes the following when created:</p> <ul> <li>[[requestedAttributes]] Requested attributes</li> <li>[[relation]] HiveTableRelation</li> <li>[[partitionPruningPred]] Partition pruning predicates</li> <li>[[sparkSession]] SparkSession</li> </ul> <p><code>HiveTableScanExec</code> initializes the &lt;&gt;. <p>=== [[partition-pruning-predicates]] Partition Pruning Predicates</p> <p><code>HiveTableScanExec</code> physical operator supports partition pruning for &lt;&gt; that are HiveTableRelation.md#isPartitioned[partitioned]. <p><code>HiveTableScanExec</code> requires that either the &lt;&gt; has no expressions or the &lt;&gt; is partitioned. Otherwise, <code>HiveTableScanExec</code> throws an <code>IllegalArgumentException</code>. <p>HiveTableScans.md[HiveTableScans] execution planning strategy creates a <code>HiveTableScanExec</code> physical operator for every HiveTableRelation.md[HiveTableRelation] operator in a query plan. When created, <code>HiveTableScanExec</code> is given the &lt;&gt; that are predicate expressions with no references and among the HiveTableRelation.md#partitionCols[partition columns] of the <code>HiveTableRelation</code>."},{"location":"hive/HiveTableScanExec/#performance-metrics","title":"Performance Metrics    Key Name (in web UI) Description     numOutputRows number of output rows Number of output rows    <p>=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- <code>doExecute</code> Method</p>","text":""},{"location":"hive/HiveTableScanExec/#source-scala","title":"[source, scala]","text":""},{"location":"hive/HiveTableScanExec/#doexecute-rddinternalrow","title":"doExecute(): RDD[InternalRow] <p><code>doExecute</code> is part of the SparkPlan abstraction.</p> <p><code>doExecute</code>...FIXME</p> <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| boundPruningPred a| [[boundPruningPred]] Catalyst ../expressions/Expression.md[expression] for the &lt;&gt; bound to (the HiveTableRelation.md#partitionCols[partitionCols] of) the &lt;&gt; <p>| hiveQlTable a| [[hiveQlTable]] Hive {url-hive-javadoc}/org/apache/hadoop/hive/ql/metadata/Table.html[Table] metadata (HiveClientImpl.md#toHiveTable[converted] from the HiveTableRelation.md#tableMeta[CatalogTable] of the &lt;&gt;) <p>Used when <code>HiveTableScanExec</code> is requested for the &lt;&gt;, &lt;&gt; and is &lt;&gt; <p>| hadoopReader a| [[hadoopReader]] HadoopTableReader.md[HadoopTableReader]</p> <p>| rawPartitions a| [[rawPartitions]] HiveClientImpl.md#toHivePartition[Hive partitions] (<code>Seq[Partition]</code>)</p> <p>Used when <code>HiveTableScanExec</code> physical operator is &lt;&gt; with a partitioned table <p>| tableDesc a| [[tableDesc]] Hive {url-hive-javadoc}/org/apache/hive/hcatalog/templeton/TableDesc.html[TableDesc]</p> <p>|===</p>","text":""},{"location":"hive/HiveTableScans/","title":"HiveTableScans Execution Planning Strategy","text":"<p><code>HiveTableScans</code> is an execution planning strategy (of Hive-specific SparkPlanner) that &lt;&gt;. <p>=== [[apply]] Planning Logical Plan for Execution -- <code>apply</code> Method</p>"},{"location":"hive/HiveTableScans/#source-scala","title":"[source, scala]","text":"<p>apply(   plan: LogicalPlan): Seq[SparkPlan]</p> <p><code>apply</code> converts (destructures) the input ../spark-sql-LogicalPlan.md[logical query plan] into projection expressions, predicate expressions, and a HiveTableRelation.md[HiveTableRelation].</p> <p><code>apply</code> requests the <code>HiveTableRelation</code> for the HiveTableRelation.md#partitionCols[partition columns] and filters the predicates to find so-called pruning predicates (that are expressions with no references and among the partition columns).</p> <p>In the end, <code>apply</code> creates a \"partial\" HiveTableScanExec.md[HiveTableScanExec] physical operator (with the <code>HiveTableRelation</code> and the pruning predicates only) and pruneFilterProject.</p> <p><code>apply</code> is part of GenericStrategy abstraction.</p>"},{"location":"hive/HiveUtils/","title":"HiveUtils","text":"<p><code>HiveUtils</code> is an utility that is used to create a &lt;&gt; that HiveExternalCatalog uses to interact with a Hive metastore. <p><code>HiveUtils</code> is a Scala object with <code>private[spark]</code> access modifier. Use the following utility to access the properties.</p>"},{"location":"hive/HiveUtils/#source-scala","title":"[source, scala]","text":"<p>// Use :paste -raw to paste the following code in spark-shell // BEGIN package org.apache.spark import org.apache.spark.sql.hive.HiveUtils object opener {   def CONVERT_METASTORE_PARQUET = HiveUtils.CONVERT_METASTORE_PARQUET } // END</p> <p>import org.apache.spark.opener spark.sessionState.conf.getConf(opener.CONVERT_METASTORE_PARQUET)</p> <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.hive.HiveUtils$</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.hive.HiveUtils=ALL\n</code></pre>"},{"location":"hive/HiveUtils/#refer-to-spark-loggingmdlogging","title":"Refer to ../spark-logging.md[Logging].","text":"<p>=== [[builtinHiveVersion]] <code>builtinHiveVersion</code> Property</p>"},{"location":"hive/HiveUtils/#source-scala_1","title":"[source, scala]","text":""},{"location":"hive/HiveUtils/#builtinhiveversion-string-121","title":"builtinHiveVersion: String = \"1.2.1\"","text":"<p><code>builtinHiveVersion</code> is used when:</p> <ul> <li>spark.sql.hive.metastore.version configuration property is used</li> <li><code>HiveUtils</code> utility is used to newClientForExecution and newClientForMetadata</li> <li>Spark Thrift Server is used</li> </ul> <p>=== [[newClientForMetadata]] Creating HiveClientImpl -- <code>newClientForMetadata</code> Method</p>"},{"location":"hive/HiveUtils/#source-scala_2","title":"[source, scala]","text":"<p>newClientForMetadata(   conf: SparkConf,   hadoopConf: Configuration): HiveClient  // &lt;1&gt; newClientForMetadata(   conf: SparkConf,   hadoopConf: Configuration,   configurations: Map[String, String]): HiveClient</p> <p>&lt;1&gt; Uses time configurations formatted</p> <p>Internally, <code>newClientForMetadata</code> creates a new SQLConf with spark.sql properties only (from the input <code>SparkConf</code>).</p> <p><code>newClientForMetadata</code> then creates an IsolatedClientLoader per the input parameters and the following configuration properties:</p> <ul> <li> <p>spark.sql.hive.metastore.version</p> </li> <li> <p>spark.sql.hive.metastore.jars</p> </li> <li> <p>spark.sql.hive.metastore.sharedPrefixes</p> </li> <li> <p>spark.sql.hive.metastore.barrierPrefixes</p> </li> </ul> <p>You should see one of the following INFO messages in the logs:</p> <pre><code>Initializing HiveMetastoreConnection version [hiveMetastoreVersion] using Spark classes.\nInitializing HiveMetastoreConnection version [hiveMetastoreVersion] using maven.\nInitializing HiveMetastoreConnection version [hiveMetastoreVersion] using [jars]\n</code></pre> <p>In the end, <code>newClientForMetadata</code> requests the <code>IsolatedClientLoader</code> for a IsolatedClientLoader.md#createClient[HiveClient].</p> <p><code>newClientForMetadata</code> is used when <code>HiveExternalCatalog</code> is requested for a HiveClient.</p> <p>=== [[newClientForExecution]] <code>newClientForExecution</code> Utility</p>"},{"location":"hive/HiveUtils/#source-scala_3","title":"[source, scala]","text":"<p>newClientForExecution(   conf: SparkConf,   hadoopConf: Configuration): HiveClientImpl</p> <p><code>newClientForExecution</code>...FIXME</p> <p><code>newClientForExecution</code> is used for HiveThriftServer2.</p> <p>=== [[inferSchema]] <code>inferSchema</code> Method</p>"},{"location":"hive/HiveUtils/#source-scala_4","title":"[source, scala]","text":"<p>inferSchema(   table: CatalogTable): CatalogTable</p> <p><code>inferSchema</code>...FIXME</p> <p>NOTE: <code>inferSchema</code> is used when ResolveHiveSerdeTable.md[ResolveHiveSerdeTable] logical resolution rule is executed.</p> <p>=== [[withHiveExternalCatalog]] <code>withHiveExternalCatalog</code> Utility</p>"},{"location":"hive/HiveUtils/#source-scala_5","title":"[source, scala]","text":"<p>withHiveExternalCatalog(   sc: SparkContext): SparkContext</p> <p><code>withHiveExternalCatalog</code> simply sets the ../StaticSQLConf.md#spark.sql.catalogImplementation[spark.sql.catalogImplementation] configuration property to <code>hive</code> for the input <code>SparkContext</code>.</p> <p>NOTE: <code>withHiveExternalCatalog</code> is used when the deprecated <code>HiveContext</code> is created.</p>"},{"location":"hive/InsertIntoHiveDirCommand/","title":"InsertIntoHiveDirCommand Logical Command","text":"<p>:spark-version: 2.4.5 :hive-version: 2.3.6 :hadoop-version: 2.10.0 :url-hive-javadoc: https://hive.apache.org/javadocs/r{hive-version}/api :url-hadoop-javadoc: https://hadoop.apache.org/docs/r{hadoop-version}/api</p> <p><code>InsertIntoHiveDirCommand</code> is a SaveAsHiveFile.md[logical command] that writes the result of executing a &lt;&gt; to a Hadoop DFS location of a &lt;&gt;. <p><code>InsertIntoHiveDirCommand</code> is &lt;&gt; when HiveAnalysis.md[HiveAnalysis] logical resolution rule is executed and resolves a ../InsertIntoDir.md[InsertIntoDir] logical operator with a Hive table."},{"location":"hive/InsertIntoHiveDirCommand/#source-scala","title":"[source, scala]","text":"<p>// // The example does NOT work when executed // \"Data not in the BIGINT data type range so converted to null\" // It is enough to show the InsertIntoHiveDirCommand operator though // assert(spark.version == \"2.4.5\")</p> <p>val tableName = \"insert_into_hive_dir_demo\" sql(s\"\"\"CREATE TABLE IF NOT EXISTS $tableName (id LONG) USING hive\"\"\")</p> <p>val locationUri = spark.sharedState.externalCatalog.getTable(\"default\", tableName).location.toString val q = sql(s\"\"\"INSERT OVERWRITE DIRECTORY '$locationUri' USING hive SELECT 1L AS id\"\"\") scala&gt; q.explain(true) == Parsed Logical Plan == 'InsertIntoDir false, Storage(Location: hdfs://localhost:9000/user/hive/warehouse/insert_into_hive_dir_demo), hive, true +- Project [1 AS id#49L]    +- OneRowRelation</p> <p>== Analyzed Logical Plan == InsertIntoHiveDirCommand false, Storage(Location: hdfs://localhost:9000/user/hive/warehouse/insert_into_hive_dir_demo), true, [id] +- Project [1 AS id#49L]    +- OneRowRelation</p> <p>== Optimized Logical Plan == InsertIntoHiveDirCommand false, Storage(Location: hdfs://localhost:9000/user/hive/warehouse/insert_into_hive_dir_demo), true, [id] +- Project [1 AS id#49L]    +- OneRowRelation</p> <p>== Physical Plan == Execute InsertIntoHiveDirCommand InsertIntoHiveDirCommand false, Storage(Location: hdfs://localhost:9000/user/hive/warehouse/insert_into_hive_dir_demo), true, [id] +- *(1) Project [1 AS id#49L]    +- Scan OneRowRelation[]</p> <p>// FIXME Why does the following throw an exception? // spark.table(tableName)</p>"},{"location":"hive/InsertIntoHiveDirCommand/#creating-instance","title":"Creating Instance","text":"<p><code>InsertIntoHiveDirCommand</code> takes the following to be created:</p> <ul> <li>[[isLocal]] <code>isLocal</code> Flag</li> <li>[[storage]] CatalogStorageFormat</li> <li>[[query]] Structured query (as a ../spark-sql-LogicalPlan.md[LogicalPlan])</li> <li>[[overwrite]] <code>overwrite</code> Flag</li> <li>[[outputColumnNames]] Names of the output columns</li> </ul> <p>=== [[run]] Executing Logical Command -- <code>run</code> Method</p>"},{"location":"hive/InsertIntoHiveDirCommand/#source-scala_1","title":"[source, scala]","text":"<p>run(   sparkSession: SparkSession,   child: SparkPlan): Seq[Row]</p> <p>NOTE: <code>run</code> is part of DataWritingCommand contract.</p> <p><code>run</code> asserts that the table location of the CatalogStorageFormat is specified (or throws an <code>AssertionError</code>).</p> <p><code>run</code> makes sure that there are no duplicates among the given output columns.</p> <p><code>run</code> creates a CatalogTable for the table location (and the <code>VIEW</code> table type) and HiveClientImpl.md#toHiveTable[converts it to a Hive Table metadata].</p> <p><code>run</code> specifies <code>serialization.lib</code> metadata to the serde of the given CatalogStorageFormat or <code>LazySimpleSerDe</code> if not defined.</p> <p><code>run</code> creates a new map-reduce job for execution (a Hadoop JobConf) with a new Hadoop Configuration (from the input SparkSession).</p> <p><code>run</code> prepares the path to write to (based on the given &lt;&gt; flag and creating it if necessary). <code>run</code> SaveAsHiveFile.md#getExternalTmpPath[getExternalTmpPath]. <p><code>run</code> saveAsHiveFile.</p> <p>In the end, <code>run</code> SaveAsHiveFile.md#deleteExternalTmpPath[deleteExternalTmpPath].</p> <p>In case of any error (<code>Throwable</code>), <code>run</code> throws an <code>SparkException</code>:</p> <pre><code>Failed inserting overwrite directory [locationUri]\n</code></pre>"},{"location":"hive/InsertIntoHiveTable/","title":"InsertIntoHiveTable Logical Command","text":"<p><code>InsertIntoHiveTable</code> is a SaveAsHiveFile.md[logical command] that writes the result of executing a &lt;&gt; to a &lt;&gt;. <p><code>InsertIntoHiveTable</code> is &lt;&gt; when: <ul> <li> <p>HiveAnalysis.md[HiveAnalysis] logical resolution rule is executed and resolves a ../InsertIntoTable.md[InsertIntoTable] logical operator with a HiveTableRelation.md[Hive table]</p> </li> <li> <p>CreateHiveTableAsSelectCommand.md[CreateHiveTableAsSelectCommand] logical command is executed</p> </li> </ul>"},{"location":"hive/InsertIntoHiveTable/#creating-instance","title":"Creating Instance","text":"<p><code>InsertIntoHiveTable</code> takes the following to be created:</p> <ul> <li>[[table]] CatalogTable</li> <li>[[partition]] Partition keys with optional values (<code>Map[String, Option[String]]</code>)</li> <li>[[query]] Structured query (as a LogicalPlan)</li> <li>[[overwrite]] <code>overwrite</code> Flag</li> <li>[[ifPartitionNotExists]] <code>ifPartitionNotExists</code> Flag</li> <li>[[outputColumnNames]] Names of the output columns</li> </ul> <p>=== [[run]] Executing Data-Writing Logical Command -- <code>run</code> Method</p>"},{"location":"hive/InsertIntoHiveTable/#source-scala","title":"[source, scala]","text":"<p>run(   sparkSession: SparkSession,   child: SparkPlan): Seq[Row]</p> <p>NOTE: <code>run</code> is part of ../DataWritingCommand.md#run[DataWritingCommand] contract.</p> <p><code>run</code> requests the input ../SparkSession.md[SparkSession] for ../SparkSession.md#sharedState[SharedState] that is then requested for the ../SharedState.md#externalCatalog[ExternalCatalog].</p> <p><code>run</code> requests the ../SessionState.md[SessionState] for a new ../SessionState.md#newHadoopConf[Hadoop Configuration].</p> <p><code>run</code> HiveClientImpl.md#toHiveTable[converts the CatalogTable metadata to Hive's].</p> <p><code>run</code> SaveAsHiveFile.md#getExternalTmpPath[getExternalTmpPath].</p> <p><code>run</code> &lt;&gt; (and SaveAsHiveFile.md#deleteExternalTmpPath[deleteExternalTmpPath]). <p><code>run</code> requests the input ../SparkSession.md[SparkSession] for ../SparkSession.md#catalog[Catalog] that is requested to uncache the table.</p> <p><code>run</code> un-caches the Hive table. <code>run</code> requests the input ../SparkSession.md[SparkSession] for ../SparkSession.md#sessionState[SessionState]. <code>run</code> requests the <code>SessionState</code> for the ../SessionState.md#catalog[SessionCatalog] that is requested to invalidate the cache for the table.</p> <p>In the end, <code>run</code> update the table statistics.</p> <p>=== [[processInsert]] <code>processInsert</code> Internal Method</p>"},{"location":"hive/InsertIntoHiveTable/#source-scala_1","title":"[source, scala]","text":"<p>processInsert(   sparkSession: SparkSession,   externalCatalog: ExternalCatalog,   hadoopConf: Configuration,   tableDesc: TableDesc,   tmpLocation: Path,   child: SparkPlan): Unit</p> <p><code>processInsert</code>...FIXME</p> <p>NOTE: <code>processInsert</code> is used when <code>InsertIntoHiveTable</code> logical command is &lt;&gt;."},{"location":"hive/IsolatedClientLoader/","title":"IsolatedClientLoader Utility","text":"<p>:hive-version: 2.3.6 :hadoop-version: 2.10.0 :url-hive-javadoc: https://hive.apache.org/javadocs/r{hive-version}/api :url-hadoop-javadoc: https://hadoop.apache.org/docs/r{hadoop-version}/api</p> <p><code>IsolatedClientLoader</code> is &lt;&gt; for <code>HiveUtils</code> utility for HiveUtils.md#newClientForExecution[newClientForExecution] and HiveUtils.md#newClientForMetadata[newClientForMetadata]. <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.hive.client.IsolatedClientLoader</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.hive.client.IsolatedClientLoader=ALL\n</code></pre>"},{"location":"hive/IsolatedClientLoader/#refer-to-spark-loggingmdlogging","title":"Refer to ../spark-logging.md[Logging].","text":"<p>=== [[creating-instance]] Creating IsolatedClientLoader Instance</p> <p><code>IsolatedClientLoader</code> takes the following to be created:</p> <ul> <li>[[version]] <code>HiveVersion</code></li> <li>[[sparkConf]] <code>SparkConf</code></li> <li>[[hadoopConf]] Hadoop {url-hadoop-javadoc}/org/apache/hadoop/conf/Configuration.html[Configuration]</li> <li>[[execJars]] Execution JARs (default: empty)</li> <li>[[config]] Configuration (default: empty)</li> <li>[[isolationOn]] <code>isolationOn</code> flag (default: <code>true</code>)</li> <li>[[sharesHadoopClasses]] <code>sharesHadoopClasses</code> flag (default: <code>true</code>)</li> <li>[[rootClassLoader]] Root <code>ClassLoader</code> (default: <code>ClassLoader.getSystemClassLoader.getParent.getParent</code>)</li> <li>[[baseClassLoader]] Base class <code>ClassLoader</code> (default: <code>Thread.currentThread().getContextClassLoader</code>)</li> <li>[[sharedPrefixes]] Shared prefixes (default: empty)</li> <li>[[barrierPrefixes]] Barrier prefixes (default: empty)</li> </ul> <p><code>IsolatedClientLoader</code> initializes the &lt;&gt;. <p>=== [[hiveVersion]] Hive Metastore Version -- <code>hiveVersion</code> Utility</p>"},{"location":"hive/IsolatedClientLoader/#source-scala","title":"[source, scala]","text":"<p>hiveVersion(   version: String): HiveVersion</p> <p><code>hiveVersion</code> creates a <code>HiveVersion</code> based on the input <code>version</code>.</p> <p>Acceptable versions and synonyms:</p> <ul> <li><code>12</code>, <code>0.12</code>, <code>0.12.0</code></li> <li><code>13</code>, <code>0.13</code>, <code>0.13.0</code>, <code>0.13.1</code></li> <li><code>14</code>, <code>0.14</code>, <code>0.14.0</code></li> <li><code>1.0</code>, <code>1.0.0</code></li> <li><code>1.1</code>, <code>1.1.0</code></li> <li><code>1.2</code>, <code>1.2.0</code>, <code>1.2.1</code>, <code>1.2.2</code></li> <li><code>2.0</code>, <code>2.0.0</code>, <code>2.0.1</code></li> <li><code>2.1</code>, <code>2.1.0</code>, <code>2.1.1</code></li> <li><code>2.2</code>, <code>2.2.0</code></li> <li><code>2.3</code>, <code>2.3.0</code>, <code>2.3.1</code>, <code>2.3.2</code>, <code>2.3.3</code></li> </ul>"},{"location":"hive/IsolatedClientLoader/#note","title":"[NOTE]","text":"<p><code>hiveVersion</code> is used when:</p> <ul> <li><code>HiveUtils</code> utility is used to HiveUtils.md#newClientForExecution[newClientForExecution] and HiveUtils.md#newClientForMetadata[newClientForMetadata]</li> </ul>"},{"location":"hive/IsolatedClientLoader/#isolatedclientloader-utility-is-used-for-an","title":"* <code>IsolatedClientLoader</code> utility is used for an &lt;&gt; <p>=== [[forVersion]] Creating IsolatedClientLoader for Given Hive Metastore Version -- <code>forVersion</code> Utility</p>","text":""},{"location":"hive/IsolatedClientLoader/#source-scala_1","title":"[source, scala]","text":"<p>forVersion(   hiveMetastoreVersion: String,   hadoopVersion: String,   sparkConf: SparkConf,   hadoopConf: Configuration,   config: Map[String, String] = Map.empty,   ivyPath: Option[String] = None,   sharedPrefixes: Seq[String] = Seq.empty,   barrierPrefixes: Seq[String] = Seq.empty,   sharesHadoopClasses: Boolean = true): IsolatedClientLoader</p> <p><code>forVersion</code>...FIXME</p> <p>NOTE: <code>forVersion</code> is used when <code>HiveUtils</code> utility is used to HiveUtils.md#newClientForMetadata[newClientForMetadata].</p> <p>=== [[createClient]] Creating HiveClient -- <code>createClient</code> Method</p>"},{"location":"hive/IsolatedClientLoader/#source-scala_2","title":"[source, scala]","text":""},{"location":"hive/IsolatedClientLoader/#createclient-hiveclient","title":"createClient(): HiveClient","text":"<p><code>createClient</code>...FIXME</p>"},{"location":"hive/IsolatedClientLoader/#note_1","title":"[NOTE] <p><code>createClient</code> is used when:</p> <ul> <li><code>HiveUtils</code> utility is used to HiveUtils.md#newClientForExecution[newClientForExecution] and HiveUtils.md#newClientForMetadata[newClientForMetadata]</li> </ul>","text":""},{"location":"hive/IsolatedClientLoader/#hiveclientimpl-is-requested-for-a-hiveclientimplmdnewsessionnew-hiveclientimpl","title":"* <code>HiveClientImpl</code> is requested for a HiveClientImpl.md#newSession[new HiveClientImpl]","text":""},{"location":"hive/RelationConversions/","title":"RelationConversions PostHoc Logical Evaluation Rule","text":"<p><code>RelationConversions</code> is a HiveSessionStateBuilder.md#postHocResolutionRules[posthoc logical resolution rule] that the HiveSessionStateBuilder.md#analyzer[Hive-specific logical analyzer] uses to &lt;&gt; with Parquet and ORC storage formats. <p>CAUTION: FIXME Show example of a hive table, e.g. <code>spark.table(...)</code></p> <p><code>RelationConversions</code> is &lt;&gt; when the HiveSessionStateBuilder.md#analyzer[Hive-specific logical analyzer] is created. <p>=== [[creating-instance]] Creating RelationConversions Instance</p> <p><code>RelationConversions</code> takes the following when created:</p> <ul> <li>[[conf]] SQLConf</li> <li>[[sessionCatalog]] Hive-specific session catalog</li> </ul> <p>=== [[apply]] Executing Rule -- <code>apply</code> Method</p>"},{"location":"hive/RelationConversions/#source-scala","title":"[source, scala]","text":"<p>apply(   plan: LogicalPlan): LogicalPlan</p> <p>NOTE: <code>apply</code> is part of the ../catalyst/Rule.md#apply[Rule] contract to execute (apply) a rule on a ../spark-sql-LogicalPlan.md[LogicalPlan].</p> <p><code>apply</code> traverses the input ../spark-sql-LogicalPlan.md[logical plan] looking for ../InsertIntoTable.md[InsertIntoTables] (over a HiveTableRelation.md[HiveTableRelation]) or HiveTableRelation.md[HiveTableRelation] logical operators:</p> <p>[[apply-InsertIntoTable]] * For an ../InsertIntoTable.md[InsertIntoTable] over a HiveTableRelation.md[HiveTableRelation] that is HiveTableRelation.md#isPartitioned[non-partitioned] and &lt;&gt;, <code>apply</code> creates a new <code>InsertIntoTable</code> with the <code>HiveTableRelation</code> &lt;&gt;. <p>[[apply-HiveTableRelation]] * For a <code>HiveTableRelation</code> logical operator alone <code>apply</code>...FIXME</p> <p>=== [[isConvertible]] Does Table Use Parquet or ORC SerDe? -- <code>isConvertible</code> Internal Method</p>"},{"location":"hive/RelationConversions/#source-scala_1","title":"[source, scala]","text":"<p>isConvertible(   relation: HiveTableRelation): Boolean</p> <p><code>isConvertible</code> is positive when the input HiveTableRelation.md#tableMeta[HiveTableRelation] is a parquet or ORC table (and corresponding SQL properties are enabled).</p> <p>Internally, <code>isConvertible</code> takes the Hive SerDe of the table (from HiveTableRelation.md#tableMeta[table metadata]) if available or assumes no SerDe.</p> <p><code>isConvertible</code> is turned on when either condition holds:</p> <ul> <li> <p>The Hive SerDe is <code>parquet</code> (aka parquet table) and spark.sql.hive.convertMetastoreParquet configuration property is enabled</p> </li> <li> <p>The Hive SerDe is <code>orc</code> (aka orc table) and spark.sql.hive.convertMetastoreOrc configuration property is enabled</p> </li> </ul> <p>NOTE: <code>isConvertible</code> is used when <code>RelationConversions</code> is &lt;&gt;. <p>=== [[convert]] Converting HiveTableRelation to HadoopFsRelation -- <code>convert</code> Internal Method</p>"},{"location":"hive/RelationConversions/#source-scala_2","title":"[source, scala]","text":"<p>convert(   relation: HiveTableRelation): LogicalRelation</p> <p><code>convert</code> branches based on the SerDe of (the storage format of) the input HiveTableRelation logical operator.</p> <p>For Hive tables in parquet format, <code>convert</code> creates options with one extra <code>mergeSchema</code> per spark.sql.hive.convertMetastoreParquet.mergeSchema configuration property and requests the HiveMetastoreCatalog to convert a HiveTableRelation to a LogicalRelation (with ParquetFileFormat).</p> <p>For non-<code>parquet</code> Hive tables, <code>convert</code> assumes ORC format:</p> <ul> <li> <p>When spark.sql.orc.impl configuration property is <code>native</code> (default) <code>convert</code> requests <code>HiveMetastoreCatalog</code> to HiveMetastoreCatalog.md#convertToLogicalRelation[convert a HiveTableRelation to a LogicalRelation over a HadoopFsRelation] (with <code>org.apache.spark.sql.execution.datasources.orc.OrcFileFormat</code> as <code>fileFormatClass</code>).</p> </li> <li> <p>Otherwise, <code>convert</code> requests <code>HiveMetastoreCatalog</code> to HiveMetastoreCatalog.md#convertToLogicalRelation[convert a HiveTableRelation to a LogicalRelation over a HadoopFsRelation] (with <code>org.apache.spark.sql.hive.orc.OrcFileFormat</code> as <code>fileFormatClass</code>).</p> </li> </ul> <p>NOTE: <code>convert</code> uses the &lt;&gt; to access the HiveMetastoreCatalog."},{"location":"hive/RelationConversions/#note","title":"[NOTE]","text":"<p><code>convert</code> is used when <code>RelationConversions</code> logical evaluation rule is &lt;&gt; and does the following transformations: <ul> <li>Transforms an ../InsertIntoTable.md[InsertIntoTable] over a <code>HiveTableRelation</code> with a Hive table (i.e. with <code>hive</code> provider) that is not partitioned and uses <code>parquet</code> or <code>orc</code> data storage format</li> </ul>"},{"location":"hive/RelationConversions/#transforms-an-hivetablerelationmdhivetablerelation-with-a-hive-table-ie-with-hive-provider-that-uses-parquet-or-orc-data-storage-format","title":"* Transforms an HiveTableRelation.md[HiveTableRelation] with a Hive table (i.e. with <code>hive</code> provider) that uses <code>parquet</code> or <code>orc</code> data storage format","text":""},{"location":"hive/ResolveHiveSerdeTable/","title":"ResolveHiveSerdeTable Logical Resolution Rule","text":"<p><code>ResolveHiveSerdeTable</code> is a logical resolution rule (i.e. <code>Rule[LogicalPlan]</code>) that the Hive-specific logical query plan analyzer uses to &lt;&gt;. <p><code>ResolveHiveSerdeTable</code> is part of additional rules in Resolution fixed-point batch of rules.</p> <p>=== [[apply]] Applying ResolveHiveSerdeTable Rule to Logical Plan -- <code>apply</code> Method</p>"},{"location":"hive/ResolveHiveSerdeTable/#source-scala","title":"[source, scala]","text":"<p>apply(   plan: LogicalPlan): LogicalPlan</p> <p>NOTE: <code>apply</code> is part of ../catalyst/Rule.md#apply[Rule Contract] to apply a rule to a ../spark-sql-LogicalPlan.md[logical plan].</p> <p><code>apply</code>...FIXME</p>"},{"location":"hive/SaveAsHiveFile/","title":"SaveAsHiveFile","text":"<p>:spark-version: 2.4.5 :hive-version: 2.3.6 :hadoop-version: 2.10.0 :url-hive-javadoc: https://hive.apache.org/javadocs/r{hive-version}/api :url-hadoop-docs: https://hadoop.apache.org/docs/r{hadoop-version} :url-hadoop-javadoc: {url-hadoop-docs}/api</p> <p><code>SaveAsHiveFile</code> is an extension of the ../DataWritingCommand.md[DataWritingCommand] contract for &lt;&gt; that can &lt;&gt; (and &lt;&gt;)."},{"location":"hive/SaveAsHiveFile/#note","title":"[NOTE]","text":"<p><code>SaveAsHiveFile</code> supports <code>viewfs://</code> URI scheme for &lt;&gt;."},{"location":"hive/SaveAsHiveFile/#read-up-on-viewfs-in-the-url-hadoop-docshadoop-project-disthadoop-hdfsviewfshtmlhadoop-official-documentation","title":"Read up on <code>ViewFs</code> in the {url-hadoop-docs}/hadoop-project-dist/hadoop-hdfs/ViewFs.html[Hadoop official documentation].","text":"<p>[[implementations]] .SaveAsHiveFiles [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | SaveAsHiveFile | Description</p> <p>| InsertIntoHiveDirCommand.md[InsertIntoHiveDirCommand] | [[InsertIntoHiveDirCommand]]</p> <p>| InsertIntoHiveTable.md[InsertIntoHiveTable] | [[InsertIntoHiveTable]]</p> <p>|===</p>"},{"location":"hive/SaveAsHiveFile/#saveashivefile_1","title":"saveAsHiveFile <pre><code>saveAsHiveFile(\n  sparkSession: SparkSession,\n  plan: SparkPlan,\n  hadoopConf: Configuration,\n  fileSinkConf: FileSinkDesc,\n  outputLocation: String,\n  customPartitionLocations: Map[TablePartitionSpec, String] = Map.empty,\n  partitionAttributes: Seq[Attribute] = Nil): Set[String]\n</code></pre> <p><code>saveAsHiveFile</code> sets Hadoop configuration properties when a compressed file output format is used (based on hive.exec.compress.output configuration property).</p> <p><code>saveAsHiveFile</code> uses <code>FileCommitProtocol</code> utility to instantiate a committer for the input <code>outputLocation</code> based on the spark.sql.sources.commitProtocolClass configuration property.</p> <p><code>saveAsHiveFile</code> uses <code>FileFormatWriter</code> utility to write out the result of executing the input physical operator (with a HiveFileFormat for the input <code>FileSinkDesc</code>, the new <code>FileCommitProtocol</code> committer, and the input arguments).</p> <p><code>saveAsHiveFile</code> is used when InsertIntoHiveDirCommand and InsertIntoHiveTable logical commands are executed.</p> <p>=== [[getExternalTmpPath]] <code>getExternalTmpPath</code> Method</p>","text":""},{"location":"hive/SaveAsHiveFile/#source-scala","title":"[source, scala] <p>getExternalTmpPath(   sparkSession: SparkSession,   hadoopConf: Configuration,   path: Path): Path</p>  <p><code>getExternalTmpPath</code> finds the Hive version used. <code>getExternalTmpPath</code> requests the input ../SparkSession.md[SparkSession] for the ../SharedState.md#externalCatalog[ExternalCatalog] (that is expected to be a HiveExternalCatalog). <code>getExternalTmpPath</code> requests it for the underlying HiveClient that is in turn requested for the HiveClient.md#version[Hive version].</p> <p><code>getExternalTmpPath</code> divides (splits) the supported Hive versions into the ones (old versions) that use index.md#hive.exec.scratchdir[hive.exec.scratchdir] directory (<code>0.12.0</code> to <code>1.0.0</code>) and the ones (new versions) that use index.md#hive.exec.stagingdir[hive.exec.stagingdir] directory (<code>1.1.0</code> to <code>2.3.3</code>).</p> <p><code>getExternalTmpPath</code> &lt;&gt; for the old Hive versions and &lt;&gt; for the new Hive versions. <p><code>getExternalTmpPath</code> throws an <code>IllegalStateException</code> for unsupported Hive version:</p> <pre><code>Unsupported hive version: [hiveVersion]\n</code></pre> <p>NOTE: <code>getExternalTmpPath</code> is used when InsertIntoHiveDirCommand.md[InsertIntoHiveDirCommand] and InsertIntoHiveTable.md[InsertIntoHiveTable] logical commands are executed.</p> <p>=== [[deleteExternalTmpPath]] <code>deleteExternalTmpPath</code> Method</p>","text":""},{"location":"hive/SaveAsHiveFile/#source-scala_1","title":"[source, scala] <p>deleteExternalTmpPath(   hadoopConf: Configuration): Unit</p>  <p><code>deleteExternalTmpPath</code>...FIXME</p> <p>NOTE: <code>deleteExternalTmpPath</code> is used when...FIXME</p> <p>=== [[oldVersionExternalTempPath]] <code>oldVersionExternalTempPath</code> Internal Method</p>","text":""},{"location":"hive/SaveAsHiveFile/#source-scala_2","title":"[source, scala] <p>oldVersionExternalTempPath(   path: Path,   hadoopConf: Configuration,   scratchDir: String): Path</p>  <p><code>oldVersionExternalTempPath</code>...FIXME</p> <p>NOTE: <code>oldVersionExternalTempPath</code> is used when <code>SaveAsHiveFile</code> is requested to &lt;&gt;. <p>=== [[newVersionExternalTempPath]] <code>newVersionExternalTempPath</code> Internal Method</p>","text":""},{"location":"hive/SaveAsHiveFile/#source-scala_3","title":"[source, scala] <p>newVersionExternalTempPath(   path: Path,   hadoopConf: Configuration,   stagingDir: String): Path</p>  <p><code>newVersionExternalTempPath</code>...FIXME</p> <p>NOTE: <code>newVersionExternalTempPath</code> is used when <code>SaveAsHiveFile</code> is requested to &lt;&gt;. <p>=== [[getExtTmpPathRelTo]] <code>getExtTmpPathRelTo</code> Internal Method</p>","text":""},{"location":"hive/SaveAsHiveFile/#source-scala_4","title":"[source, scala] <p>getExtTmpPathRelTo(   path: Path,   hadoopConf: Configuration,   stagingDir: String): Path</p>  <p><code>getExtTmpPathRelTo</code>...FIXME</p> <p>NOTE: <code>getExtTmpPathRelTo</code> is used when <code>SaveAsHiveFile</code> is requested to &lt;&gt;. <p>=== [[getExternalScratchDir]] <code>getExternalScratchDir</code> Internal Method</p>","text":""},{"location":"hive/SaveAsHiveFile/#source-scala_5","title":"[source, scala] <p>getExternalScratchDir(   extURI: URI,   hadoopConf: Configuration,   stagingDir: String): Path</p>  <p><code>getExternalScratchDir</code>...FIXME</p> <p>NOTE: <code>getExternalScratchDir</code> is used when <code>SaveAsHiveFile</code> is requested to &lt;&gt;. <p>=== [[getStagingDir]] <code>getStagingDir</code> Internal Method</p>","text":""},{"location":"hive/SaveAsHiveFile/#source-scala_6","title":"[source, scala] <p>getStagingDir(   inputPath: Path,   hadoopConf: Configuration,   stagingDir: String): Path</p>  <p><code>getStagingDir</code>...FIXME</p> <p>NOTE: <code>getStagingDir</code> is used when <code>SaveAsHiveFile</code> is requested to &lt;&gt; and &lt;&gt;. <p>=== [[executionId]] <code>executionId</code> Internal Method</p>","text":""},{"location":"hive/SaveAsHiveFile/#source-scala_7","title":"[source, scala]","text":""},{"location":"hive/SaveAsHiveFile/#executionid-string","title":"executionId: String <p><code>executionId</code>...FIXME</p> <p>NOTE: <code>executionId</code> is used when...FIXME</p> <p>=== [[createdTempDir]] <code>createdTempDir</code> Internal Registry</p>","text":""},{"location":"hive/SaveAsHiveFile/#source-scala_8","title":"[source, scala]","text":""},{"location":"hive/SaveAsHiveFile/#createdtempdir-optionpath-none","title":"createdTempDir: Option[Path] = None <p><code>createdTempDir</code> is a Hadoop {url-hadoop-javadoc}/org/apache/hadoop/fs/Path.html[Path] of a staging directory.</p> <p><code>createdTempDir</code> is initialized when <code>SaveAsHiveFile</code> is requested to &lt;&gt; and &lt;&gt;. <p><code>createdTempDir</code> is the index.md#hive.exec.stagingdir[hive.exec.stagingdir] configuration property.</p> <p><code>createdTempDir</code> is deleted when <code>SaveAsHiveFile</code> is requested to &lt;&gt; and at the normal termination of VM (since <code>deleteOnExit</code> is used).","text":""},{"location":"hive/TableReader/","title":"TableReader","text":"<p><code>TableReader</code> is...FIXME</p>"},{"location":"hive/configuration-properties/","title":"Configuration Properties","text":"<p>This page contains the configuration properties of the Hive data source only.</p> <p>[[properties]] .Hive-Specific Spark SQL Configuration Properties [cols=\"1a\",options=\"header\",width=\"100%\"] |=== | Configuration Property</p> <p>| [[spark.sql.hive.convertMetastoreOrc]] spark.sql.hive.convertMetastoreOrc</p> <p>Controls whether to use the built-in ORC reader and writer for Hive tables with the ORC storage format (instead of Hive SerDe).</p> <p>Default: <code>true</code></p> <p>| [[spark.sql.hive.convertMetastoreParquet]] spark.sql.hive.convertMetastoreParquet</p> <p>Controls whether to use the built-in Parquet reader and writer for Hive tables with the parquet storage format (instead of Hive SerDe).</p> <p>Default: <code>true</code></p> <p>Internally, this property enables RelationConversions.md[RelationConversions] logical rule to RelationConversions.md#convert[convert HiveTableRelations to HadoopFsRelation]</p> <p>| [[spark.sql.hive.convertMetastoreParquet.mergeSchema]] spark.sql.hive.convertMetastoreParquet.mergeSchema</p> <p>Enables trying to merge possibly different but compatible Parquet schemas in different Parquet data files.</p> <p>Default: <code>false</code></p> <p>This configuration is only effective when &lt;&gt; is enabled. <p>| [[spark.sql.hive.manageFilesourcePartitions]] spark.sql.hive.manageFilesourcePartitions</p> <p>Enables metastore partition management for file source tables (filesource partition management). This includes both datasource and converted Hive tables.</p> <p>Default: <code>true</code></p> <p>When enabled (<code>true</code>), datasource tables store partition metadata in the Hive metastore, and use the metastore to prune partitions during query planning.</p> <p>Use SQLConf.manageFilesourcePartitions method to access the current value.</p> <p>| [[spark.sql.hive.metastore.barrierPrefixes]] spark.sql.hive.metastore.barrierPrefixes</p> <p>Comma-separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with, e.g. Hive UDFs that are declared in a prefix that typically would be shared (i.e. <code>org.apache.spark.*</code>)</p> <p>Default: <code>(empty)</code></p> <p>| [[spark.sql.hive.metastore.jars]] spark.sql.hive.metastore.jars</p> <p>Location of the jars that should be used to HiveUtils.md#newClientForMetadata[create a HiveClientImpl].</p> <p>Default: <code>builtin</code></p> <p>Supported locations:</p> <ul> <li> <p><code>builtin</code> - the jars that were used to load Spark SQL (aka Spark classes). Valid only when using the execution version of Hive, i.e. &lt;&gt; <li> <p><code>maven</code> - download the Hive jars from Maven repositories</p> </li> <li> <p>Classpath in the standard format for both Hive and Hadoop</p> </li> <p>| [[spark.sql.hive.metastore.sharedPrefixes]] spark.sql.hive.metastore.sharedPrefixes</p> <p>Comma-separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive.</p> <p>Default: <code>\"com.mysql.jdbc\", \"org.postgresql\", \"com.microsoft.sqlserver\", \"oracle.jdbc\"</code></p> <p>An example of classes that should be shared are:</p> <ul> <li> <p>JDBC drivers that are needed to talk to the metastore</p> </li> <li> <p>Other classes that interact with classes that are already shared, e.g. custom appenders that are used by log4j</p> </li> </ul> <p>| [[spark.sql.hive.metastore.version]] spark.sql.hive.metastore.version</p> <p>Version of the Hive metastore (and the HiveUtils.md#newClientForMetadata[client classes and jars]).</p> <p>Default: HiveUtils.md#builtinHiveVersion[1.2.1]</p> <p>Supported versions IsolatedClientLoader.md#hiveVersion[range from 0.12.0 up to and including 2.3.3]</p> <p>| [[spark.sql.hive.verifyPartitionPath]] spark.sql.hive.verifyPartitionPath</p> <p>When enabled (<code>true</code>), check all the partition paths under the table's root directory when reading data stored in HDFS. This configuration will be deprecated in the future releases and replaced by spark.files.ignoreMissingFiles.</p> <p>Default: <code>false</code></p> <p>| [[spark.sql.hive.metastorePartitionPruning]] spark.sql.hive.metastorePartitionPruning</p> <p>When enabled (<code>true</code>), some predicates will be pushed down into the Hive metastore so that unmatching partitions can be eliminated earlier.</p> <p>Default: <code>true</code></p> <p>This only affects Hive tables that are not converted to filesource relations (based on &lt;&gt; and &lt;&gt; properties). <p>Use SQLConf.metastorePartitionPruning method to access the current value.</p> <p>| [[spark.sql.hive.filesourcePartitionFileCacheSize]] spark.sql.hive.filesourcePartitionFileCacheSize</p> <p>| [[spark.sql.hive.caseSensitiveInferenceMode]] spark.sql.hive.caseSensitiveInferenceMode</p> <p>| [[spark.sql.hive.convertCTAS]] spark.sql.hive.convertCTAS</p> <p>| [[spark.sql.hive.gatherFastStats]] spark.sql.hive.gatherFastStats</p> <p>| [[spark.sql.hive.advancedPartitionPredicatePushdown.enabled]] spark.sql.hive.advancedPartitionPredicatePushdown.enabled</p> <p>|===</p>"},{"location":"hive/spark-sql-hive-metastore/","title":"Hive Metastore","text":"<p>Spark SQL uses a Hive metastore to manage the metadata of persistent relational entities (e.g. databases, tables, columns, partitions) in a relational database (for fast access).</p> <p>A Hive metastore warehouse (aka &lt;&gt;) is the directory where Spark SQL persists tables whereas a Hive metastore (aka &lt;&gt;) is a relational database to manage the metadata of the persistent relational entities, e.g. databases, tables, columns, partitions. <p>By default, Spark SQL uses the embedded deployment mode of a Hive metastore with a Apache Derby database.</p>"},{"location":"hive/spark-sql-hive-metastore/#important","title":"[IMPORTANT]","text":"<p>The default embedded deployment mode is not recommended for production use due to limitation of only one active SparkSession.md[SparkSession] at a time.</p>"},{"location":"hive/spark-sql-hive-metastore/#read-clouderas-httpswwwclouderacomdocumentationenterpriselatesttopicscdh_ig_hive_metastore_configurehtmlconfiguring-the-hive-metastore-for-cdh-document-that-explains-the-available-deployment-modes-of-a-hive-metastore","title":"Read Cloudera's https://www.cloudera.com/documentation/enterprise/latest/topics/cdh_ig_hive_metastore_configure.html[Configuring the Hive Metastore for CDH] document that explains the available deployment modes of a Hive metastore.","text":"<p>When <code>SparkSession</code> is SparkSession-Builder.md#enableHiveSupport[created with Hive support] the external catalog (aka metastore) is HiveExternalCatalog. <code>HiveExternalCatalog</code> uses &lt;&gt; directory for the location of the databases and &lt;&gt; for the connection to the Hive metastore database."},{"location":"hive/spark-sql-hive-metastore/#note","title":"[NOTE]","text":"<p>The metadata of relational entities is persisted in a metastore database over JDBC and http://www.datanucleus.org/[DataNucleus AccessPlatform] that uses &lt;&gt; properties."},{"location":"hive/spark-sql-hive-metastore/#read-httpscwikiapacheorgconfluencedisplayhiveadminmanualmetastoreadminhive-metastore-administration-to-learn-how-to-manage-a-hive-metastore","title":"Read https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin[Hive Metastore Administration] to learn how to manage a Hive Metastore.","text":"<p>[[javax.jdo.option]] [[hive-metastore-database-connection-properties]] .Hive Metastore Database Connection Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| [[javax.jdo.option.ConnectionURL]] <code>javax.jdo.option.ConnectionURL</code> a| The JDBC connection URL of a Hive metastore database to use</p> <pre><code>// the default setting in Spark SQL\njdbc:derby:;databaseName=metastore_db;create=true\n\n// Example: memory only and so volatile and not for production use\njdbc:derby:memory:;databaseName=${metastoreLocation.getAbsolutePath};create=true\n\njdbc:mysql://192.168.175.160:3306/metastore?useSSL=false\n</code></pre> <p>| [[javax.jdo.option.ConnectionDriverName]] <code>javax.jdo.option.ConnectionDriverName</code> a| The JDBC driver of a Hive metastore database to use</p> <pre><code>org.apache.derby.jdbc.EmbeddedDriver\n</code></pre> <p>| [[javax.jdo.option.ConnectionUserName]] <code>javax.jdo.option.ConnectionUserName</code> | The user name to use to connect to a Hive metastore database</p> <p>| [[javax.jdo.option.ConnectionPassword]] <code>javax.jdo.option.ConnectionPassword</code> | The password to use to connect to a Hive metastore database |===</p> <p>You can configure &lt;&gt; properties in &lt;&gt; or using options with &lt;&gt; prefix. <p>You can access the current connection properties for a Hive metastore in a Spark SQL application using the Spark internal classes.</p>"},{"location":"hive/spark-sql-hive-metastore/#source-scala","title":"[source, scala]","text":"<p>scala&gt; :type spark org.apache.spark.sql.SparkSession</p> <p>scala&gt; spark.sharedState.externalCatalog res1: org.apache.spark.sql.catalyst.catalog.ExternalCatalog = org.apache.spark.sql.hive.HiveExternalCatalog@79dd79eb</p> <p>// Use <code>:paste -raw</code> to paste the following code // This is to pass the private[spark] \"gate\" // BEGIN package org.apache.spark import org.apache.spark.sql.SparkSession object jacek {   def open(spark: SparkSession) = {     import org.apache.spark.sql.hive.HiveExternalCatalog     spark.sharedState.externalCatalog.asInstanceOf[HiveExternalCatalog].client   } } // END import org.apache.spark.jacek val hiveClient = jacek.open(spark) scala&gt; hiveClient.getConf(\"javax.jdo.option.ConnectionURL\", \"\") res2: String = jdbc:derby:;databaseName=metastore_db;create=true</p> <p>The benefits of using an external Hive metastore:</p> <p>. Allow multiple Spark applications (sessions) to access it concurrently</p> <p>. Allow a single Spark application to use table statistics without running \"ANALYZE TABLE\" every execution</p> <p>NOTE: As of Spark 2.2 (see https://issues.apache.org/jira/browse/SPARK-18112[SPARK-18112 Spark2.x does not support read data from Hive 2.x metastore]) Spark SQL supports reading data from Hive 2.1.1 metastore.</p> <p>CAUTION: FIXME Describe &lt;&gt; vs <code>config</code> method vs <code>--conf</code> with &lt;&gt; prefix. <p>Spark SQL uses the Hive-specific configuration properties that further fine-tune the Hive integration, e.g. spark.sql.hive.metastore.version or spark.sql.hive.metastore.jars.</p>"},{"location":"hive/spark-sql-hive-metastore/#sparksqlwarehousedir-configuration-property","title":"spark.sql.warehouse.dir Configuration Property <p>spark.sql.warehouse.dir is a static configuration property that sets Hive's <code>hive.metastore.warehouse.dir</code> property, i.e. the location of default database for the Hive warehouse.</p>  <p>Tip</p> <p>Refer to SharedState to learn about (the low-level details of) Spark SQL support for Apache Hive.</p> <p>See also the official Hive Metastore Administration document.</p>","text":""},{"location":"hive/spark-sql-hive-metastore/#hive-metastore-deployment-modes","title":"Hive Metastore Deployment Modes","text":""},{"location":"hive/spark-sql-hive-metastore/#configuring-external-hive-metastore-in-spark-sql","title":"Configuring External Hive Metastore in Spark SQL <p>In order to use an external Hive metastore you should do the following:</p> <ul> <li> <p>Enable Hive support in SparkSession-Builder.md#enableHiveSupport[SparkSession] (that makes sure that the Hive classes are on CLASSPATH and sets spark.sql.catalogImplementation internal configuration property to <code>hive</code>)</p> </li> <li> <p>spark.sql.warehouse.dir required?</p> </li> <li> <p>Define hive.metastore.warehouse.dir in hive-site.xml configuration resource</p> </li> <li> <p>Check out warehousePath</p> </li> <li> <p>Execute <code>./bin/run-example sql.hive.SparkHiveExample</code> to verify Hive configuration</p> </li> </ul> <p>When not configured by the &lt;&gt;, <code>SparkSession</code> automatically creates <code>metastore_db</code> in the current directory and creates a directory configured by &lt;&gt;, which defaults to the directory <code>spark-warehouse</code> in the current directory that the Spark application is started.","text":""},{"location":"hive/spark-sql-hive-metastore/#note_1","title":"[NOTE]","text":"<p><code>hive.metastore.warehouse.dir</code> property in <code>hive-site.xml</code> is deprecated since Spark 2.0.0. Use &lt;&gt; to specify the default location of the databases in a Hive warehouse."},{"location":"hive/spark-sql-hive-metastore/#you-may-need-to-grant-write-privilege-to-the-user-who-starts-the-spark-application","title":"You may need to grant write privilege to the user who starts the Spark application.","text":"<p>=== Hadoop Configuration Properties for Hive</p> <p>[[hadoop-configuration-properties]] .Hadoop Configuration Properties for Hive [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| [[hive.metastore.uris]] <code>hive.metastore.uris</code> a| The Thrift URI of a remote Hive metastore, i.e. one that is in a separate JVM process or on a remote node</p> <pre><code>config(\"hive.metastore.uris\", \"thrift://192.168.175.160:9083\")\n</code></pre> <p>| [[hive.metastore.warehouse.dir]] <code>hive.metastore.warehouse.dir</code> a| <code>SharedState</code> uses SharedState.md#hive.metastore.warehouse.dir[hive.metastore.warehouse.dir] to set spark.sql.warehouse.dir if the latter is undefined.</p> <p>CAUTION: FIXME How is <code>hive.metastore.warehouse.dir</code> related to <code>spark.sql.warehouse.dir</code>? <code>SharedState.warehousePath</code>? Review https://github.com/apache/spark/pull/16996/files</p> <p>| [[hive.metastore.schema.verification]] <code>hive.metastore.schema.verification</code> | Set to <code>false</code> (as seems to cause exceptions with an empty metastore database as of Hive 2.1) |===</p> <p>You may also want to use the following Hive configuration properties that (seem to) cause exceptions with an empty metastore database as of Hive 2.1.</p> <ul> <li><code>datanucleus.schema.autoCreateAll</code> set to <code>true</code></li> </ul> <p>=== [[spark.hadoop]] spark.hadoop Configuration Properties</p> <p>CAUTION: FIXME Describe the purpose of <code>spark.hadoop.*</code> properties</p> <p>You can specify any of the Hadoop configuration properties, e.g. &lt;&gt; with spark.hadoop prefix. <pre><code>$ spark-shell --conf spark.hadoop.hive.metastore.warehouse.dir=/tmp/hive-warehouse\n...\nscala&gt; spark.sharedState\n18/01/08 10:46:19 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/tmp/hive-warehouse').\n18/01/08 10:46:19 INFO SharedState: Warehouse path is '/tmp/hive-warehouse'.\nres1: org.apache.spark.sql.internal.SharedState = org.apache.spark.sql.internal.SharedState@5a69b3cf\n</code></pre> <p>=== [[hive-site.xml]] hive-site.xml Configuration Resource</p> <p><code>hive-site.xml</code> configures Hive clients (e.g. Spark SQL) with the Hive Metastore configuration.</p> <p><code>hive-site.xml</code> is loaded when SharedState.md#warehousePath[SharedState] is created (which is...FIXME).</p> <p>Configuration of Hive is done by placing your <code>hive-site.xml</code>, <code>core-site.xml</code> (for security configuration), and <code>hdfs-site.xml</code> (for HDFS configuration) file in <code>conf/</code> (that is automatically added to the CLASSPATH of a Spark application).</p> <p>TIP: You can use <code>--driver-class-path</code> or <code>spark.driver.extraClassPath</code> to point to the directory with configuration resources, e.g. <code>hive-site.xml</code>.</p>"},{"location":"hive/spark-sql-hive-metastore/#source-xml","title":"[source, xml] <p>  hive.metastore.warehouse.dir /tmp/hive-warehouse Hive Metastore location  </p>  <p>TIP: Read Resources section in Hadoop's http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration] javadoc to learn more about configuration resources.</p>","text":""},{"location":"hive/spark-sql-hive-metastore/#tip","title":"[TIP]","text":"<p>Use <code>SparkContext.hadoopConfiguration</code> to know which configuration resources have already been registered.</p>"},{"location":"hive/spark-sql-hive-metastore/#source-scala_1","title":"[source, scala] <p>scala&gt; sc.hadoopConfiguration res1: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml</p> <p>// Initialize warehousePath scala&gt; spark.sharedState.warehousePath res2: String = file:/Users/jacek/dev/oss/spark/spark-warehouse/</p> <p>// Note file:/Users/jacek/dev/oss/spark/spark-warehouse/ is added to configuration resources scala&gt; sc.hadoopConfiguration res3: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, file:/Users/jacek/dev/oss/spark/conf/hive-site.xml</p>  <p>Enable <code>org.apache.spark.sql.internal.SharedState</code> logger to <code>INFO</code> logging level to know where <code>hive-site.xml</code> comes from.</p>","text":""},{"location":"hive/spark-sql-hive-metastore/#scala-sparksharedstatewarehousepath-180108-094933-info-sharedstate-loading-hive-config-file-fileusersjacekdevosssparkconfhive-sitexml-180108-094933-info-sharedstate-setting-hivemetastorewarehousedir-null-to-the-value-of-sparksqlwarehousedir-fileusersjacekdevosssparkspark-warehouse-180108-094933-info-sharedstate-warehouse-path-is-fileusersjacekdevosssparkspark-warehouse-res2-string-fileusersjacekdevosssparkspark-warehouse","title":"<pre><code>scala&gt; spark.sharedState.warehousePath\n18/01/08 09:49:33 INFO SharedState: loading hive config file: file:/Users/jacek/dev/oss/spark/conf/hive-site.xml\n18/01/08 09:49:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/jacek/dev/oss/spark/spark-warehouse/').\n18/01/08 09:49:33 INFO SharedState: Warehouse path is 'file:/Users/jacek/dev/oss/spark/spark-warehouse/'.\nres2: String = file:/Users/jacek/dev/oss/spark/spark-warehouse/\n</code></pre>","text":""},{"location":"jdbc/","title":"JDBC Connector","text":"<p>Spark SQL supports loading data from tables using JDBC.</p> <p>Spark developers use DataFrameReader.jdbc to load data from an external table using JDBC.</p> <pre><code>val table = spark.read.jdbc(url, table, properties)\n\n// Alternatively\nval table = spark.read.format(\"jdbc\").options(...).load(...)\n</code></pre> <p>These one-liners create a DataFrame that represents the distributed process of loading data from a database and a table (with additional properties).</p>"},{"location":"jdbc/JDBCOptions/","title":"JDBCOptions","text":"<p><code>JDBCOptions</code> is the options of the JDBC data source.</p>"},{"location":"jdbc/JDBCOptions/#batchsize","title":"batchsize","text":""},{"location":"jdbc/JDBCOptions/#cascadetruncate","title":"cascadeTruncate","text":""},{"location":"jdbc/JDBCOptions/#createtablecolumntypes","title":"createTableColumnTypes","text":""},{"location":"jdbc/JDBCOptions/#createtableoptions","title":"createTableOptions","text":""},{"location":"jdbc/JDBCOptions/#customschema","title":"customSchema","text":""},{"location":"jdbc/JDBCOptions/#dbtable","title":"dbtable","text":""},{"location":"jdbc/JDBCOptions/#driver","title":"driver","text":""},{"location":"jdbc/JDBCOptions/#fetchsize","title":"fetchsize","text":""},{"location":"jdbc/JDBCOptions/#isolationlevel","title":"isolationLevel","text":""},{"location":"jdbc/JDBCOptions/#keytab","title":"keytab","text":""},{"location":"jdbc/JDBCOptions/#lowerbound","title":"lowerBound <p>(only for reading) The lower bound of the partitionColumn</p> <p>Must be smaller than the upperBound</p> <p>Required with the partitionColumn defined</p> <p>Used when:</p> <ul> <li><code>DataFrameReader</code> is requested to jdbc (and <code>JDBCRelation</code> utility is used to determine partitions)</li> </ul>","text":""},{"location":"jdbc/JDBCOptions/#numpartitions","title":"numPartitions <p>The number of partitions for loading or saving data</p> <p>Required with the partitionColumn defined</p> <p>Only used when the difference between upperBound and lowerBound is greater than or equal to this <code>numPartitions</code> option. JDBCRelation prints out the following WARN message to the logs when it happens:</p> <pre><code>The number of partitions is reduced because the specified number of partitions is less\nthan the difference between upper bound and lower bound.\nUpdated number of partitions: [difference];\nInput number of partitions: [numPartitions];\nLower bound: [lowerBound];\nUpper bound: [upperBound].\n</code></pre>  <p>Tip</p> <p>Enable <code>INFO</code> logging level of the JDBCRelation logger to see what happens under the covers.</p>  <p>Used when:</p> <ul> <li><code>DataFrameReader</code> is requested to jdbc (and <code>JDBCRelation</code> utility is used to determine partitions)</li> <li><code>JdbcUtils</code> is requested to <code>saveTable</code></li> </ul>","text":""},{"location":"jdbc/JDBCOptions/#partitioncolumn","title":"partitionColumn <p>The name of the column used to partition dataset (using a <code>JDBCPartitioningInfo</code>). The type of the column should be one of the following (or an <code>AnalysisException</code> is thrown):</p> <ul> <li>DateType</li> <li>TimestampType</li> <li>NumericType</li> </ul> <p>When specified, the other partitioned-reading properties are required:</p> <ul> <li>lowerBound</li> <li>upperBound</li> <li>numPartitions</li> </ul> <p>Cannot be used with query option</p> <p>When undefined, the other partitioned-reading properties should not be defined:</p> <ul> <li>lowerBound</li> <li>upperBound</li> </ul> <p>Used when:</p> <ul> <li><code>DataFrameReader</code> is requested to jdbc</li> </ul>","text":""},{"location":"jdbc/JDBCOptions/#principal","title":"principal","text":""},{"location":"jdbc/JDBCOptions/#pushdownaggregate","title":"pushDownAggregate","text":""},{"location":"jdbc/JDBCOptions/#pushdownpredicate","title":"pushDownPredicate","text":""},{"location":"jdbc/JDBCOptions/#query","title":"query","text":""},{"location":"jdbc/JDBCOptions/#querytimeout","title":"queryTimeout","text":""},{"location":"jdbc/JDBCOptions/#refreshkrb5config","title":"refreshKrb5Config","text":""},{"location":"jdbc/JDBCOptions/#sessioninitstatement","title":"sessionInitStatement","text":""},{"location":"jdbc/JDBCOptions/#tablecomment","title":"tableComment","text":""},{"location":"jdbc/JDBCOptions/#truncate","title":"truncate","text":""},{"location":"jdbc/JDBCOptions/#upperbound","title":"upperBound <p>(only for reading) The upper bound of the partitionColumn</p> <p>Required when the other properties are defined:</p> <ul> <li>partitionColumn</li> <li>lowerBound</li> <li>numPartitions</li> </ul> <p>Must be larger than the lowerBound</p> <p>Used when:</p> <ul> <li><code>DataFrameReader</code> is requested to jdbc (and <code>JDBCRelation</code> utility is used to determine partitions)</li> </ul>","text":""},{"location":"jdbc/JDBCOptions/#url","title":"url <p>(required) A JDBC URL to use to connect to a database</p>","text":""},{"location":"jdbc/JDBCOptions/#creating-instance","title":"Creating Instance <p><code>JDBCOptions</code> takes the following to be created:</p> <ul> <li> URL <li> Table (corresponds to the dbtable option) <li> Configuration Parameters","text":""},{"location":"jdbc/JDBCRDD/","title":"JDBCRDD","text":"<p><code>JDBCRDD</code> is a <code>RDD</code> of InternalRows that represents a structured query over a table in a database accessed via JDBC.</p> <p>Note</p> <p><code>JDBCRDD</code> represents a <code>SELECT requiredColumns FROM table</code> query.</p> <p><code>JDBCRDD</code> is &lt;&gt; exclusively when <code>JDBCRDD</code> is requested to &lt;&gt; (when <code>JDBCRelation</code> is requested to build a scan). <p>[[internal-registries]] .JDBCRDD's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| <code>columnList</code> | [[columnList]] Column names</p> <p>Used when...FIXME</p> <p>| <code>filterWhereClause</code> | [[filterWhereClause]] &lt;&gt; as a SQL <code>WHERE</code> clause <p>Used when...FIXME |===</p> <p>=== [[compute]] Computing Partition (in TaskContext) -- <code>compute</code> Method</p>"},{"location":"jdbc/JDBCRDD/#source-scala","title":"[source, scala]","text":""},{"location":"jdbc/JDBCRDD/#computethepart-partition-context-taskcontext-iteratorinternalrow","title":"compute(thePart: Partition, context: TaskContext): Iterator[InternalRow]","text":"<p>NOTE: <code>compute</code> is part of Spark Core's <code>RDD</code> Contract to compute a partition (in a <code>TaskContext</code>).</p> <p><code>compute</code>...FIXME</p> <p>=== [[resolveTable]] <code>resolveTable</code> Method</p>"},{"location":"jdbc/JDBCRDD/#source-scala_1","title":"[source, scala]","text":""},{"location":"jdbc/JDBCRDD/#resolvetableoptions-jdbcoptions-structtype","title":"resolveTable(options: JDBCOptions): StructType","text":"<p><code>resolveTable</code>...FIXME</p> <p>NOTE: <code>resolveTable</code> is used exclusively when <code>JDBCRelation</code> is requested for the &lt;&gt;. <p>=== [[scanTable]] Creating RDD for Distributed Data Scan -- <code>scanTable</code> Object Method</p>"},{"location":"jdbc/JDBCRDD/#source-scala_2","title":"[source, scala]","text":"<p>scanTable(   sc: SparkContext,   schema: StructType,   requiredColumns: Array[String],   filters: Array[Filter],   parts: Array[Partition],   options: JDBCOptions): RDD[InternalRow]</p> <p><code>scanTable</code> takes the &lt;&gt; option. <p><code>scanTable</code> finds the corresponding JDBC dialect (per the <code>url</code> option) and requests it to quote the column identifiers in the input <code>requiredColumns</code>.</p> <p><code>scanTable</code> uses the <code>JdbcUtils</code> object to <code>createConnectionFactory</code> and &lt;&gt; from the input <code>schema</code> to include the input <code>requiredColumns</code> only. <p>In the end, <code>scanTable</code> creates a new &lt;&gt;. <p>NOTE: <code>scanTable</code> is used exclusively when <code>JDBCRelation</code> is requested to &lt;&gt;."},{"location":"jdbc/JDBCRDD/#creating-instance","title":"Creating Instance","text":"<p><code>JDBCRDD</code> takes the following to be created:</p> <ul> <li>[[sc]] <code>SparkContext</code></li> <li>[[getConnection]] Function to create a <code>Connection</code> (<code>() =&gt; Connection</code>)</li> <li>[[schema]] Schema</li> <li>[[columns]] Array of column names</li> <li>[[filters]] Array of Filter predicates</li> <li>[[partitions]] Array of Spark Core's <code>Partitions</code></li> <li>[[url]] Connection URL</li> <li>[[options]] JDBCOptions</li> </ul> <p>=== [[getPartitions]] <code>getPartitions</code> Method</p>"},{"location":"jdbc/JDBCRDD/#source-scala_3","title":"[source, scala]","text":""},{"location":"jdbc/JDBCRDD/#getpartitions-arraypartition","title":"getPartitions: Array[Partition]","text":"<p>NOTE: <code>getPartitions</code> is part of Spark Core's <code>RDD</code> Contract to...FIXME</p> <p><code>getPartitions</code> simply returns the &lt;&gt; (this <code>JDBCRDD</code> was created with). <p>=== [[pruneSchema]] <code>pruneSchema</code> Internal Method</p>"},{"location":"jdbc/JDBCRDD/#source-scala_4","title":"[source, scala]","text":""},{"location":"jdbc/JDBCRDD/#pruneschemaschema-structtype-columns-arraystring-structtype","title":"pruneSchema(schema: StructType, columns: Array[String]): StructType","text":"<p><code>pruneSchema</code>...FIXME</p> <p>NOTE: <code>pruneSchema</code> is used when...FIXME</p> <p>=== [[compileFilter]] Converting Filter Predicate to SQL Expression -- <code>compileFilter</code> Object Method</p>"},{"location":"jdbc/JDBCRDD/#source-scala_5","title":"[source, scala]","text":""},{"location":"jdbc/JDBCRDD/#compilefilterf-filter-dialect-jdbcdialect-optionstring","title":"compileFilter(f: Filter, dialect: JdbcDialect): Option[String]","text":"<p><code>compileFilter</code>...FIXME</p>"},{"location":"jdbc/JDBCRDD/#note","title":"[NOTE]","text":"<p><code>compileFilter</code> is used when:</p> <ul> <li><code>JDBCRelation</code> is requested to &lt;&gt;"},{"location":"jdbc/JDBCRDD/#jdbcrdd-is","title":"* <code>JDBCRDD</code> is &lt;&gt;","text":""},{"location":"jdbc/JDBCRelation/","title":"JDBCRelation","text":"<p><code>JDBCRelation</code> is a BaseRelation with support for column pruning with filter pushdown and inserting or overwriting data.</p> <p></p>"},{"location":"jdbc/JDBCRelation/#prunedfilteredscan","title":"PrunedFilteredScan <p><code>JDBCRelation</code> is an PrunedFilteredScan and supports supports column pruning with filter pushdown.</p>","text":""},{"location":"jdbc/JDBCRelation/#insertablerelation","title":"InsertableRelation <p><code>JDBCRelation</code> is an InsertableRelation and supports inserting or overwriting data.</p>","text":""},{"location":"jdbc/JDBCRelation/#creating-instance","title":"Creating Instance <p><code>JDBCRelation</code> takes the following to be created:</p> <ul> <li> StructType <li> <code>Partition</code>s <li> JDBCOptions <li> SparkSession  <p><code>JDBCRelation</code> is created (possibly using apply) when:</p> <ul> <li><code>DataFrameReader</code> is requested to jdbc</li> <li><code>JDBCRelation</code> utility is used to apply</li> <li><code>JdbcRelationProvider</code> is requested to create a BaseRelation (for reading)</li> <li><code>JDBCScanBuilder</code> is requested to build a Scan</li> </ul>","text":""},{"location":"jdbc/JDBCRelation/#creating-jdbcrelation","title":"Creating JDBCRelation <pre><code>apply(\n  parts: Array[Partition],\n  jdbcOptions: JDBCOptions)(\n  sparkSession: SparkSession): JDBCRelation\n</code></pre> <p><code>apply</code> gets the schema (establishing a connection to the database system directly) and creates a JDBCRelation.</p>","text":""},{"location":"jdbc/JDBCRelation/#getschema","title":"getSchema <pre><code>getSchema(\n  resolver: Resolver,\n  jdbcOptions: JDBCOptions): StructType\n</code></pre> <p><code>getSchema</code> resolves the table (from the given JDBCOptions).</p> <p>With the customSchema option specified, <code>getSchema</code> gets the custom schema (based on the table schema from the database system). Otherwise, <code>getSchema</code> returns the table schema from the database system.</p> <p><code>getSchema</code>\u00a0is used when:</p> <ul> <li><code>JDBCRelation</code> utility is used to create a JDBCRelation</li> <li><code>JdbcRelationProvider</code> is requested to create a BaseRelation (for reading)</li> </ul>","text":""},{"location":"jdbc/JDBCRelation/#columnpartition","title":"columnPartition <pre><code>columnPartition(\n  schema: StructType,\n  resolver: Resolver,\n  timeZoneId: String,\n  jdbcOptions: JDBCOptions): Array[Partition]\n</code></pre> <p><code>columnPartition</code>...FIXME</p> <p>In the end, <code>columnPartition</code> prints out the following INFO message to the logs:</p> <pre><code>Number of partitions: [numPartitions], WHERE clauses of these partitions:\n[whereClause]\n</code></pre> <p><code>columnPartition</code>\u00a0is used when:</p> <ul> <li><code>JdbcRelationProvider</code> is requested to create a BaseRelation (for reading)</li> <li><code>JDBCScanBuilder</code> is requested to build a Scan</li> </ul>","text":""},{"location":"jdbc/JDBCRelation/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"jdbc/JDBCRelation/#review-me","title":"Review Me <p>[[toString]] When requested for a human-friendly text representation, <code>JDBCRelation</code> requests the &lt;&gt; for the name of the table and the &lt;&gt; (if defined). <pre><code>JDBCRelation([table]) [numPartitions=[number]]\n</code></pre> <pre><code>scala&gt; df.explain\n== Physical Plan ==\n*Scan JDBCRelation(projects) [numPartitions=1] [id#0,name#1,website#2] ReadSchema: struct&lt;id:int,name:string,website:string&gt;\n</code></pre> <p>[[needConversion]] <code>JDBCRelation</code> turns the needConversion flag off (to announce that &lt;&gt; returns an <code>RDD[InternalRow]</code> already and <code>DataSourceStrategy</code> execution planning strategy does not have to do the RDD conversion). <p>=== [[unhandledFilters]] Finding Unhandled Filter Predicates -- <code>unhandledFilters</code> Method</p>","text":""},{"location":"jdbc/JDBCRelation/#source-scala","title":"[source, scala]","text":""},{"location":"jdbc/JDBCRelation/#unhandledfiltersfilters-arrayfilter-arrayfilter","title":"unhandledFilters(filters: Array[Filter]): Array[Filter] <p><code>unhandledFilters</code> is part of BaseRelation abstraction.</p> <p><code>unhandledFilters</code> returns the Filter predicates in the input <code>filters</code> that could not be converted to a SQL expression (and are therefore unhandled by the JDBC data source natively).</p> <p>=== [[schema]] Schema of Tuples (Data) -- <code>schema</code> Property</p>","text":""},{"location":"jdbc/JDBCRelation/#source-scala_1","title":"[source, scala]","text":""},{"location":"jdbc/JDBCRelation/#schema-structtype","title":"schema: StructType <p><code>schema</code> uses <code>JDBCRDD</code> to resolveTable given the JDBCOptions (that simply returns the schema of the table, also known as the default table schema).</p> <p>If customSchema JDBC option was defined, <code>schema</code> uses <code>JdbcUtils</code> to replace the data types in the default table schema.</p> <p><code>schema</code> is part of BaseRelation abstraction.</p> <p>=== [[insert]] Inserting or Overwriting Data to JDBC Table -- <code>insert</code> Method</p>","text":""},{"location":"jdbc/JDBCRelation/#source-scala_2","title":"[source, scala]","text":""},{"location":"jdbc/JDBCRelation/#insertdata-dataframe-overwrite-boolean-unit","title":"insert(data: DataFrame, overwrite: Boolean): Unit <p><code>insert</code> is part of the InsertableRelation abstraction.</p> <p><code>insert</code> simply requests the input <code>DataFrame</code> for a &lt;&gt; that in turn is requested to save the data to a table using the JDBC data source (itself!) with the url, table and all options. <p><code>insert</code> also requests the <code>DataFrameWriter</code> to set the save mode as Overwrite or Append per the input <code>overwrite</code> flag.</p>  <p>Note</p> <p><code>insert</code> uses a \"trick\" to reuse a code that is responsible for saving data to a JDBC table.</p>  <p>=== [[buildScan]] Building Distributed Data Scan with Column Pruning and Filter Pushdown -- <code>buildScan</code> Method</p>","text":""},{"location":"jdbc/JDBCRelation/#source-scala_3","title":"[source, scala]","text":""},{"location":"jdbc/JDBCRelation/#buildscanrequiredcolumns-arraystring-filters-arrayfilter-rddrow","title":"buildScan(requiredColumns: Array[String], filters: Array[Filter]): RDD[Row] <p><code>buildScan</code> is part of the PrunedFilteredScan abstraction.</p> <p><code>buildScan</code> uses the <code>JDBCRDD</code> object to create a RDD[Row] for a distributed data scan.</p>","text":""},{"location":"jdbc/JDBCScan/","title":"JDBCScan","text":"<p><code>JDBCScan</code> is...FIXME</p>"},{"location":"jdbc/JDBCScanBuilder/","title":"JDBCScanBuilder","text":"<p><code>JDBCScanBuilder</code> is...FIXME</p>"},{"location":"jdbc/JdbcDialect/","title":"JdbcDialect","text":"<p><code>JdbcDialect</code> is the &lt;&gt; of &lt;&gt; that &lt;&gt; (and handle necessary type-related conversions to properly load a data from a table into a <code>DataFrame</code>). <p>[[contract]] [source, scala]</p> <p>package org.apache.spark.sql.jdbc</p> <p>abstract class JdbcDialect extends Serializable {   // only required properties (vals and methods) that have no implementation   // the others follow   def canHandle(url : String): Boolean }</p> <p>.(Subset of) JdbcDialect Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Property | Description</p> <p>| <code>canHandle</code> | [[canHandle]] Used when...FIXME |===</p> <p>[[extensions]] .JdbcDialects [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | JdbcDialect | Description</p> <p>| <code>AggregatedDialect</code> | [[AggregatedDialect]]</p> <p>| <code>DB2Dialect</code> | [[DB2Dialect]]</p> <p>| <code>DerbyDialect</code> | [[DerbyDialect]]</p> <p>| <code>MsSqlServerDialect</code> | [[MsSqlServerDialect]]</p> <p>| <code>MySQLDialect</code> | [[MySQLDialect]]</p> <p>| <code>NoopDialect</code> | [[NoopDialect]]</p> <p>| <code>OracleDialect</code> | [[OracleDialect]]</p> <p>| <code>PostgresDialect</code> | [[PostgresDialect]]</p> <p>| <code>TeradataDialect</code> | [[TeradataDialect]] |===</p> <p>=== [[getCatalystType]] <code>getCatalystType</code> Method</p>"},{"location":"jdbc/JdbcDialect/#source-scala","title":"[source, scala]","text":"<p>getCatalystType(   sqlType: Int,   typeName: String,   size: Int,   md: MetadataBuilder): Option[DataType]</p> <p><code>getCatalystType</code>...FIXME</p> <p>NOTE: <code>getCatalystType</code> is used when...FIXME</p> <p>=== [[getJDBCType]] <code>getJDBCType</code> Method</p>"},{"location":"jdbc/JdbcDialect/#source-scala_1","title":"[source, scala]","text":""},{"location":"jdbc/JdbcDialect/#getjdbctypedt-datatype-optionjdbctype","title":"getJDBCType(dt: DataType): Option[JdbcType]","text":"<p><code>getJDBCType</code>...FIXME</p> <p>NOTE: <code>getJDBCType</code> is used when...FIXME</p> <p>=== [[quoteIdentifier]] <code>quoteIdentifier</code> Method</p>"},{"location":"jdbc/JdbcDialect/#source-scala_2","title":"[source, scala]","text":""},{"location":"jdbc/JdbcDialect/#quoteidentifiercolname-string-string","title":"quoteIdentifier(colName: String): String","text":"<p><code>quoteIdentifier</code>...FIXME</p> <p>NOTE: <code>quoteIdentifier</code> is used when...FIXME</p> <p>=== [[getTableExistsQuery]] <code>getTableExistsQuery</code> Method</p>"},{"location":"jdbc/JdbcDialect/#source-scala_3","title":"[source, scala]","text":""},{"location":"jdbc/JdbcDialect/#gettableexistsquerytable-string-string","title":"getTableExistsQuery(table: String): String","text":"<p><code>getTableExistsQuery</code>...FIXME</p> <p>NOTE: <code>getTableExistsQuery</code> is used when...FIXME</p> <p>=== [[getSchemaQuery]] <code>getSchemaQuery</code> Method</p>"},{"location":"jdbc/JdbcDialect/#source-scala_4","title":"[source, scala]","text":""},{"location":"jdbc/JdbcDialect/#getschemaquerytable-string-string","title":"getSchemaQuery(table: String): String","text":"<p><code>getSchemaQuery</code>...FIXME</p> <p>NOTE: <code>getSchemaQuery</code> is used when...FIXME</p> <p>=== [[getTruncateQuery]] <code>getTruncateQuery</code> Method</p>"},{"location":"jdbc/JdbcDialect/#source-scala_5","title":"[source, scala]","text":""},{"location":"jdbc/JdbcDialect/#gettruncatequerytable-string-string","title":"getTruncateQuery(table: String): String","text":"<p><code>getTruncateQuery</code>...FIXME</p> <p>NOTE: <code>getTruncateQuery</code> is used when...FIXME</p> <p>=== [[beforeFetch]] <code>beforeFetch</code> Method</p>"},{"location":"jdbc/JdbcDialect/#source-scala_6","title":"[source, scala]","text":""},{"location":"jdbc/JdbcDialect/#beforefetchconnection-connection-properties-mapstring-string-unit","title":"beforeFetch(connection: Connection, properties: Map[String, String]): Unit","text":"<p><code>beforeFetch</code>...FIXME</p> <p>NOTE: <code>beforeFetch</code> is used when...FIXME</p> <p>=== [[escapeSql]] <code>escapeSql</code> Internal Method</p>"},{"location":"jdbc/JdbcDialect/#source-scala_7","title":"[source, scala]","text":""},{"location":"jdbc/JdbcDialect/#escapesqlvalue-string-string","title":"escapeSql(value: String): String","text":"<p><code>escapeSql</code>...FIXME</p> <p>NOTE: <code>escapeSql</code> is used when...FIXME</p> <p>=== [[compileValue]] <code>compileValue</code> Method</p>"},{"location":"jdbc/JdbcDialect/#source-scala_8","title":"[source, scala]","text":""},{"location":"jdbc/JdbcDialect/#compilevaluevalue-any-any","title":"compileValue(value: Any): Any","text":"<p><code>compileValue</code>...FIXME</p> <p>NOTE: <code>compileValue</code> is used when...FIXME</p> <p>=== [[isCascadingTruncateTable]] <code>isCascadingTruncateTable</code> Method</p>"},{"location":"jdbc/JdbcDialect/#source-scala_9","title":"[source, scala]","text":""},{"location":"jdbc/JdbcDialect/#iscascadingtruncatetable-optionboolean","title":"isCascadingTruncateTable(): Option[Boolean]","text":"<p><code>isCascadingTruncateTable</code>...FIXME</p> <p>NOTE: <code>isCascadingTruncateTable</code> is used when...FIXME</p>"},{"location":"jdbc/JdbcRelationProvider/","title":"JdbcRelationProvider","text":"<p><code>JdbcRelationProvider</code> is used as a RelationProvider and a CreatableRelationProvider of the JDBC Data Source.</p>"},{"location":"jdbc/JdbcRelationProvider/#datasourceregister","title":"DataSourceRegister <p><code>JdbcRelationProvider</code> is the DataSourceRegister to handle jdbc data source format.</p>  <p>Note</p> <p><code>JdbcRelationProvider</code> uses <code>META-INF/services/org.apache.spark.sql.sources.DataSourceRegister</code> file for registration that is available in the source code of Apache Spark.</p>","text":""},{"location":"jdbc/JdbcRelationProvider/#creating-baserelation","title":"Creating BaseRelation <pre><code>createRelation(\n  sqlContext: SQLContext,\n  parameters: Map[String, String]): BaseRelation\n</code></pre> <p><code>createRelation</code> creates a JDBCOptions (with the given <code>parameters</code>).</p> <p><code>createRelation</code> gets the schema (by querying the database system).</p> <p><code>createRelation</code> creates column partitions.</p> <p>In the end, <code>createRelation</code> creates a JDBCRelation.</p> <p><code>createRelation</code> is part of the RelationProvider abstraction.</p>","text":""},{"location":"jdbc/JdbcRelationProvider/#writing-rows-of-structured-query-dataframe-to-table-using-jdbc","title":"Writing Rows of Structured Query (DataFrame) to Table Using JDBC <pre><code>createRelation(\n  sqlContext: SQLContext,\n  mode: SaveMode,\n  parameters: Map[String, String],\n  df: DataFrame): BaseRelation\n</code></pre> <p><code>createRelation</code> is part of the CreatableRelationProvider abstraction.</p> <p>Internally, <code>createRelation</code> creates a JDBCOptions (from the input <code>parameters</code>).</p> <p><code>createRelation</code>...FIXME</p> <p><code>createRelation</code> checks whether the table (given <code>dbtable</code> and <code>url</code> options in the input <code>parameters</code>) exists.</p> <p>NOTE: <code>createRelation</code> uses a database-specific <code>JdbcDialect</code> to check whether a table exists.</p> <p><code>createRelation</code> branches off per whether the table already exists in the database or not.</p> <p>If the table does not exist, <code>createRelation</code> creates the table (by executing <code>CREATE TABLE</code> with createTableColumnTypes and createTableOptions options from the input <code>parameters</code>) and writes the rows to the database in a single transaction.</p> <p>If however the table does exist, <code>createRelation</code> branches off per SaveMode (see the following createRelation and SaveMode).</p> <p>[[createRelation-CreatableRelationProvider-SaveMode]] .createRelation and SaveMode [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| Append | Saves the records to the table.</p> <p>| ErrorIfExists a| Throws a <code>AnalysisException</code> with the message:</p> <pre><code>Table or view '[table]' already exists. SaveMode: ErrorIfExists.\n</code></pre> <p>| Ignore | Does nothing.</p> <p>| Overwrite a| Truncates or drops the table</p> <p>NOTE: <code>createRelation</code> truncates the table only when truncate JDBC option is enabled and JdbcDialect.md#isCascadingTruncateTable[isCascadingTruncateTable] is disabled. |===</p> <p>In the end, <code>createRelation</code> closes the JDBC connection to the database and creates a JDBCRelation.</p>","text":""},{"location":"kafka/","title":"Kafka Connector","text":"<p>Kafka Connector allows Spark SQL (and Spark Structured Streaming) to read data from and write data to topics in Apache Kafka.</p> <p>Kafka Connector is available as kafka format alias.</p> <p>The entry point is KafkaSourceProvider.</p> <p>Note</p> <p>Apache Kafka is a storage of records in a format-independent and fault-tolerant durable way.</p> <p>Learn more about Apache Kafka in the official documentation or The Internals of Apache Kafka.</p> <p>Kafka Connector supports options to fine-tune structured queries.</p>"},{"location":"kafka/ConsumerStrategy/","title":"ConsumerStrategy \u2014 Kafka Consumer Providers","text":"<p><code>ConsumerStrategy</code> is the &lt;&gt; for &lt;&gt; that can &lt;&gt; given Kafka parameters. <p>[[contract]] [source, scala]</p> <p>package org.apache.spark.sql.kafka010</p> <p>sealed trait ConsumerStrategy {   def createConsumer(kafkaParams: ju.Map[String, Object]): Consumer[Array[Byte], Array[Byte]] }</p> <p>.ConsumerStrategy Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Property | Description</p> <p>| createConsumer | [[createConsumer]] Creates a Kafka https://kafka.apache.org/20/javadoc/org/apache/kafka/clients/consumer/Consumer.html[Consumer] (of keys and values of type <code>Array[Byte]</code>)</p> <p>Used exclusively when <code>KafkaOffsetReader</code> is requested to creating a Kafka Consumer |===</p> <p>[[implementations]] .ConsumerStrategies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ConsumerStrategy | createConsumer</p> <p>| <code>AssignStrategy</code> | [[AssignStrategy]] Uses ++http://kafka.apache.org/20/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#assign-java.util.Collection-++[KafkaConsumer.assign(Collection partitions)] <p>| <code>SubscribeStrategy</code> | [[SubscribeStrategy]] Uses ++http://kafka.apache.org/20/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe-java.util.Collection-++[KafkaConsumer.subscribe(Collection topics)] <p>| <code>SubscribePatternStrategy</code> a| [[SubscribePatternStrategy]] Uses ++http://kafka.apache.org/20/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe-java.util.regex.Pattern-org.apache.kafka.clients.consumer.ConsumerRebalanceListener-++[KafkaConsumer.subscribe(Pattern pattern, ConsumerRebalanceListener listener)] with <code>NoOpConsumerRebalanceListener</code>.</p> <p>TIP: Refer to http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[java.util.regex.Pattern] for the format of supported topic subscription regex patterns. |===</p> <p>NOTE: <code>ConsumerStrategy</code> is a Scala sealed trait which means that all the &lt;&gt; are in the same compilation unit (a single file)."},{"location":"kafka/InternalKafkaConsumer/","title":"InternalKafkaConsumer","text":"<p><code>InternalKafkaConsumer</code> is...FIXME</p> <p>=== [[get]] Getting Single Kafka ConsumerRecord -- <code>get</code> Method</p>"},{"location":"kafka/InternalKafkaConsumer/#source-scala","title":"[source, scala]","text":"<p>get(   offset: Long,   untilOffset: Long,   pollTimeoutMs: Long,   failOnDataLoss: Boolean): ConsumerRecord[Array[Byte], Array[Byte]]</p> <p><code>get</code>...FIXME</p> <p>NOTE: <code>get</code> is used when...FIXME</p> <p>=== [[getAvailableOffsetRange]] Getting Single AvailableOffsetRange -- <code>getAvailableOffsetRange</code> Method</p>"},{"location":"kafka/InternalKafkaConsumer/#source-scala_1","title":"[source, scala]","text":""},{"location":"kafka/InternalKafkaConsumer/#getavailableoffsetrange-availableoffsetrange","title":"getAvailableOffsetRange(): AvailableOffsetRange","text":"<p><code>getAvailableOffsetRange</code>...FIXME</p> <p>NOTE: <code>getAvailableOffsetRange</code> is used when...FIXME</p>"},{"location":"kafka/InternalKafkaProducerPool/","title":"InternalKafkaProducerPool","text":""},{"location":"kafka/InternalKafkaProducerPool/#sparkkafkaproducercachetimeout","title":"spark.kafka.producer.cache.timeout <p><code>InternalKafkaProducerPool</code> uses spark.kafka.producer.cache.timeout when requested to acquire a CachedKafkaProducer.</p>","text":""},{"location":"kafka/InternalKafkaProducerPool/#acquiring-cachedkafkaproducer","title":"Acquiring CachedKafkaProducer <pre><code>acquire(\n  kafkaParams: ju.Map[String, Object]): CachedKafkaProducer\n</code></pre> <p><code>acquire</code>...FIXME</p> <p><code>acquire</code>\u00a0is used when...FIXME</p>","text":""},{"location":"kafka/JsonUtils/","title":"JsonUtils Helper Object","text":"<p><code>JsonUtils</code> is a Scala object with &lt;&gt; for serializing and deserializing Kafka https://kafka.apache.org/20/javadoc/org/apache/kafka/common/TopicPartition.html[TopicPartitions] to and from a single JSON text. <p><code>JsonUtils</code> uses http://json4s.org/[json4s] library that provides a single AST with the Jackson parser for parsing to the AST (using <code>json4s-jackson</code> module).</p> <p>[[methods]] .JsonUtils API [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| &lt;&gt; a| Deserializing partition offsets (i.e. offsets per Kafka <code>TopicPartition</code>) from JSON, e.g. <code>{\"topicA\":{\"0\":23,\"1\":-1},\"topicB\":{\"0\":-2}}</code>"},{"location":"kafka/JsonUtils/#source-scala","title":"[source, scala]","text":""},{"location":"kafka/JsonUtils/#partitionoffsetsstr-string-maptopicpartition-long","title":"partitionOffsets(str: String): Map[TopicPartition, Long]","text":"<p>| &lt;&gt; a| Serializing partition offsets (i.e. offsets per Kafka <code>TopicPartition</code>) to JSON"},{"location":"kafka/JsonUtils/#source-scala_1","title":"[source, scala]","text":""},{"location":"kafka/JsonUtils/#partitionoffsetspartitionoffsets-maptopicpartition-long-string","title":"partitionOffsets(partitionOffsets: Map[TopicPartition, Long]): String","text":"<p>| &lt;&gt; a| Deserializing <code>TopicPartitions</code> from JSON, e.g. <code>{\"topicA\":[0,1],\"topicB\":[0,1]}</code>"},{"location":"kafka/JsonUtils/#source-scala_2","title":"[source, scala]","text":""},{"location":"kafka/JsonUtils/#partitionsstr-string-arraytopicpartition","title":"partitions(str: String): Array[TopicPartition]","text":"<p>| &lt;&gt; a| Serializing <code>TopicPartitions</code> to JSON"},{"location":"kafka/JsonUtils/#source-scala_3","title":"[source, scala]","text":""},{"location":"kafka/JsonUtils/#partitionspartitions-iterabletopicpartition-string","title":"partitions(partitions: Iterable[TopicPartition]): String","text":"<p>|===</p> <p>=== [[partitionOffsets-String-Map]] Deserializing Partition Offsets From JSON -- <code>partitionOffsets</code> Method</p>"},{"location":"kafka/JsonUtils/#source-scala_4","title":"[source, scala]","text":""},{"location":"kafka/JsonUtils/#partitionoffsetsstr-string-maptopicpartition-long_1","title":"partitionOffsets(str: String): Map[TopicPartition, Long]","text":"<p><code>partitionOffsets</code>...FIXME</p> <p><code>partitionOffsets</code> is used when:</p> <ul> <li> <p><code>KafkaSourceProvider</code> is requested to get the desired KafkaOffsetRangeLimit (for offset option)</p> </li> <li> <p>(Spark Structured Streaming) <code>KafkaContinuousReader</code> is requested to <code>deserializeOffset</code></p> </li> <li> <p>(Spark Structured Streaming) <code>KafkaSourceOffset</code> is created (from a <code>SerializedOffset</code>)</p> </li> </ul> <p>=== [[partitionOffsets-Map-String]] Serializing Partition Offsets to JSON -- <code>partitionOffsets</code> Method</p>"},{"location":"kafka/JsonUtils/#source-scala_5","title":"[source, scala]","text":""},{"location":"kafka/JsonUtils/#partitionoffsetspartitionoffsets-maptopicpartition-long-string_1","title":"partitionOffsets(partitionOffsets: Map[TopicPartition, Long]): String","text":"<p><code>partitionOffsets</code>...FIXME</p> <p>NOTE: <code>partitionOffsets</code> is used when...FIXME</p> <p>=== [[partitions-Iterable-String]] Serializing TopicPartitions to JSON -- <code>partitions</code> Method</p>"},{"location":"kafka/JsonUtils/#source-scala_6","title":"[source, scala]","text":""},{"location":"kafka/JsonUtils/#partitionspartitions-iterabletopicpartition-string_1","title":"partitions(partitions: Iterable[TopicPartition]): String","text":"<p><code>partitions</code>...FIXME</p> <p>NOTE: <code>partitions</code> seems not to be used.</p> <p>=== [[partitions-String-Array]] Deserializing TopicPartitions from JSON -- <code>partitions</code> Method</p>"},{"location":"kafka/JsonUtils/#source-scala_7","title":"[source, scala]","text":""},{"location":"kafka/JsonUtils/#partitionsstr-string-arraytopicpartition_1","title":"partitions(str: String): Array[TopicPartition]","text":"<p><code>partitions</code> uses json4s-jakson's <code>Serialization</code> object to read a <code>Map[String, Seq[Int]</code> from the input string that represents a <code>Map</code> of topics and partition numbers, e.g. <code>{\"topicA\":[0,1],\"topicB\":[0,1]}</code>.</p> <p>For every pair of topic and partition number, <code>partitions</code> creates a new Kafka https://kafka.apache.org/20/javadoc/org/apache/kafka/common/TopicPartition.html[TopicPartition].</p> <p>In case of any parsing issues, <code>partitions</code> throws a new <code>IllegalArgumentException</code>:</p> <pre><code>Expected e.g. {\"topicA\":[0,1],\"topicB\":[0,1]}, got [str]\n</code></pre> <p><code>partitions</code> is used when <code>KafkaSourceProvider</code> is requested for a ConsumerStrategy (given assign option).</p>"},{"location":"kafka/KafkaBatch/","title":"KafkaBatch","text":"<p><code>KafkaBatch</code> is...FIXME</p>"},{"location":"kafka/KafkaBatchWrite/","title":"KafkaBatchWrite","text":"<p><code>KafkaBatchWrite</code> is...FIXME</p>"},{"location":"kafka/KafkaBatchWriterFactory/","title":"KafkaBatchWriterFactory","text":"<p><code>KafkaBatchWriterFactory</code> is...FIXME</p>"},{"location":"kafka/KafkaDataConsumer/","title":"KafkaDataConsumer","text":"<p><code>KafkaDataConsumer</code> is the &lt;&gt; for &lt;&gt; that use an &lt;&gt; for the following: <ul> <li> <p>&lt;&gt; <li> <p>&lt;&gt; <p><code>KafkaDataConsumer</code> has to be &lt;&gt; explicitly. <p>[[contract]] [source, scala]</p> <p>package org.apache.spark.sql.kafka010</p> <p>sealed trait KafkaDataConsumer {   // only required properties (vals and methods) that have no implementation   // the others follow   def internalConsumer: InternalKafkaConsumer   def release(): Unit }</p> <p>.KafkaDataConsumer Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Property | Description</p> <p>| internalConsumer a| [[internalConsumer]] Used when:</p> <ul> <li> <p><code>KafkaDataConsumer</code> is requested to &lt;&gt; and &lt;&gt; <li> <p><code>CachedKafkaDataConsumer</code> and <code>NonCachedKafkaDataConsumer</code> are requested to &lt;&gt; the <code>InternalKafkaConsumer</code> <p>| release a| [[release]] Used when:</p> <ul> <li> <p><code>KafkaSourceRDD</code> is requested to compute a partition</p> </li> <li> <p>(Spark Structured Streaming) <code>KafkaContinuousDataReader</code> is requested to <code>close</code> |===</p> </li> </ul> <p>[[implementations]] .KafkaDataConsumers [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | KafkaDataConsumer | Description</p> <p>| <code>CachedKafkaDataConsumer</code> | [[CachedKafkaDataConsumer]]</p> <p>| <code>NonCachedKafkaDataConsumer</code> | [[NonCachedKafkaDataConsumer]] |===</p> <p>NOTE: <code>KafkaDataConsumer</code> is a Scala sealed trait which means that all the &lt;&gt; are in the same compilation unit (a single file). <p>=== [[get]] Getting Single Kafka ConsumerRecord -- <code>get</code> Method</p>"},{"location":"kafka/KafkaDataConsumer/#source-scala","title":"[source, scala]","text":"<p>get(   offset: Long,   untilOffset: Long,   pollTimeoutMs: Long,   failOnDataLoss: Boolean): ConsumerRecord[Array[Byte], Array[Byte]]</p> <p><code>get</code> simply requests the &lt;&gt; to get a single Kafka ConsumerRecord. <p><code>get</code> is used when:</p> <ul> <li> <p><code>KafkaSourceRDD</code> is requested to compute a partition</p> </li> <li> <p>(Spark Structured Streaming) <code>KafkaContinuousDataReader</code> is requested to <code>next</code></p> </li> </ul> <p>=== [[getAvailableOffsetRange]] Getting Single AvailableOffsetRange -- <code>getAvailableOffsetRange</code> Method</p>"},{"location":"kafka/KafkaDataConsumer/#source-scala_1","title":"[source, scala]","text":""},{"location":"kafka/KafkaDataConsumer/#getavailableoffsetrange-availableoffsetrange","title":"getAvailableOffsetRange(): AvailableOffsetRange","text":"<p><code>getAvailableOffsetRange</code> simply requests the InternalKafkaConsumer to get a single AvailableOffsetRange.</p> <p><code>getAvailableOffsetRange</code> is used when:</p> <ul> <li> <p><code>KafkaSourceRDD</code> is requested to compute a partition (through resolveRange)</p> </li> <li> <p>(Spark Structured Streaming) <code>KafkaContinuousDataReader</code> is requested to <code>next</code></p> </li> </ul>"},{"location":"kafka/KafkaDataWriter/","title":"KafkaDataWriter","text":"<p><code>KafkaDataWriter</code> is a DataWriter.</p>"},{"location":"kafka/KafkaDataWriter/#kafkarowwriter","title":"KafkaRowWriter <p><code>KafkaDataWriter</code> is a KafkaRowWriter (for the input schema and topic).</p>","text":""},{"location":"kafka/KafkaDataWriter/#creating-instance","title":"Creating Instance <p><code>KafkaDataWriter</code> takes the following to be created:</p> <ul> <li> Optional Topic Name (target topic) <li> Kafka Producer Parameters <li> Input Schema (Attributes)  <p><code>KafkaDataWriter</code> is created when:</p> <ul> <li><code>KafkaBatchWriterFactory</code> is requested for a DataWriter</li> <li><code>KafkaStreamWriterFactory</code> (Spark Structured Streaming) is requested for a <code>DataWriter</code></li> </ul>","text":""},{"location":"kafka/KafkaDataWriter/#cached-kafkaproducer","title":"Cached KafkaProducer <pre><code>producer: Option[CachedKafkaProducer] = None\n</code></pre> <p><code>KafkaDataWriter</code> defines <code>producer</code> internal registry for a <code>CachedKafkaProducer</code>:</p> <ul> <li><code>producer</code> is undefined when <code>KafkaDataWriter</code> is created</li> <li><code>CachedKafkaProducer</code> is created (aquired) when writing out a row</li> <li><code>producer</code> is cleared (dereferenced) in close</li> </ul> <p>Once defined, <code>KafkaDataWriter</code> uses the <code>KafkaProducer</code> to send a row (when writing out a row).</p> <p><code>KafkaDataWriter</code> requests the <code>KafkaProducer</code> to flush out rows in commit.</p>  <p>FIXME: Why is InternalKafkaProducerPool required?</p>","text":""},{"location":"kafka/KafkaOffsetRangeLimit/","title":"KafkaOffsetRangeLimit","text":"<p><code>KafkaOffsetRangeLimit</code> is the desired &lt;&gt; for starting, ending, and specific offsets. <p>[[extensions]] .KafkaOffsetRangeLimits [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | KafkaOffsetRangeLimit | Description</p> <p>| EarliestOffsetRangeLimit | [[EarliestOffsetRangeLimit]] Bind to the earliest offset</p> <p>| LatestOffsetRangeLimit | [[LatestOffsetRangeLimit]] Bind to the latest offset</p> <p>| SpecificOffsetRangeLimit | [[SpecificOffsetRangeLimit]] Bind to specific offsets</p> <p>Takes <code>partitionOffsets</code> (as <code>Map[TopicPartition, Long]</code>) when created. |===</p> <p><code>KafkaOffsetRangeLimit</code> is \"created\" (i.e. mapped to from a human-readable text representation) when <code>KafkaSourceProvider</code> is requested to getKafkaOffsetRangeLimit.</p> <p><code>KafkaOffsetRangeLimit</code> defines two constants to denote offset range limits that are resolved via Kafka:</p> <ul> <li> <p>[[LATEST]] <code>-1L</code> for the latest offset</p> </li> <li> <p>[[EARLIEST]] <code>-2L</code> for the earliest offset</p> </li> </ul> <p>NOTE: <code>KafkaOffsetRangeLimit</code> is a Scala sealed trait which means that all the &lt;&gt; are in the same compilation unit (a single file)."},{"location":"kafka/KafkaOffsetReader/","title":"KafkaOffsetReader","text":"<p><code>KafkaOffsetReader</code> is used to query a Kafka cluster for partition offsets.</p>"},{"location":"kafka/KafkaRecordToRowConverter/","title":"KafkaRecordToRowConverter","text":""},{"location":"kafka/KafkaRecordToRowConverter/#kafkaschema","title":"kafkaSchema <pre><code>kafkaSchema(\n  includeHeaders: Boolean): StructType\n</code></pre> <p><code>kafkaSchema</code> is schemaWithHeaders for the given <code>includeHeaders</code> enabled. Otherwise, <code>kafkaSchema</code> is schemaWithoutHeaders.</p>  <p><code>kafkaSchema</code> is used when:</p> <ul> <li><code>KafkaRelation</code> is requested for the schema</li> <li><code>KafkaScan</code> is requested for the readSchema</li> <li><code>KafkaSource</code> (Spark Structured Streaming) is requested for the <code>schema</code></li> <li><code>KafkaSourceProvider</code> is requested for the sourceSchema</li> <li><code>KafkaTable</code> is requested for the schema</li> </ul>","text":""},{"location":"kafka/KafkaRelation/","title":"KafkaRelation","text":"<p><code>KafkaRelation</code> is a BaseRelation with a TableScan.</p> <p><code>KafkaRelation</code> is &lt;&gt; exclusively when <code>KafkaSourceProvider</code> is requested to create a BaseRelation (as a RelationProvider). <p>[[schema]] <code>KafkaRelation</code> uses the fixed schema.</p> <p>[[schema]] .KafkaRelation's Schema (in the positional order) [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Field Name | Data Type</p> <p>| <code>key</code> | <code>BinaryType</code></p> <p>| <code>value</code> | <code>BinaryType</code></p> <p>| <code>topic</code> | <code>StringType</code></p> <p>| <code>partition</code> | <code>IntegerType</code></p> <p>| <code>offset</code> | <code>LongType</code></p> <p>| <code>timestamp</code> | <code>TimestampType</code></p> <p>| <code>timestampType</code> | <code>IntegerType</code> |===</p> <p>[[toString]] <code>KafkaRelation</code> uses the following human-readable text representation:</p> <pre><code>KafkaRelation(strategy=[strategy], start=[startingOffsets], end=[endingOffsets])\n</code></pre> <p>[[internal-registries]] .KafkaRelation's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| pollTimeoutMs a| [[pollTimeoutMs]] Timeout (in milliseconds) to poll data from Kafka (pollTimeoutMs for <code>KafkaSourceRDD</code>)</p> <p>Initialized with the value of the following configuration properties (in the order until one found):</p> <p>. <code>kafkaConsumer.pollTimeoutMs</code> in the &lt;&gt; <p>. <code>spark.network.timeout</code> in the <code>SparkConf</code></p> <p>If neither is set, defaults to <code>120s</code>.</p> <p>Used exclusively when <code>KafkaRelation</code> is requested to &lt;&gt; (and creates a KafkaSourceRDD). |=== <p>[[logging]] [TIP] ==== Enable <code>INFO</code> or <code>DEBUG</code> logging level for <code>org.apache.spark.sql.kafka010.KafkaRelation</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.kafka010.KafkaRelation=DEBUG\n</code></pre>"},{"location":"kafka/KafkaRelation/#refer-to-spark-loggingmdlogging","title":"Refer to spark-logging.md[Logging].","text":"<p>=== [[creating-instance]] Creating KafkaRelation Instance</p> <p><code>KafkaRelation</code> takes the following when created:</p> <ul> <li>[[sqlContext]] <code>SQLContext</code></li> <li>[[strategy]] <code>ConsumerStrategy</code></li> <li>[[sourceOptions]] Source options (as <code>Map[String, String]</code>) that directly correspond to the options of DataFrameReader</li> <li>[[specifiedKafkaParams]] User-defined Kafka parameters (as <code>Map[String, String]</code>)</li> <li>[[failOnDataLoss]] <code>failOnDataLoss</code> flag</li> <li>[[startingOffsets]] Starting offsets (as KafkaOffsetRangeLimit)</li> <li>[[endingOffsets]] Ending offsets (as KafkaOffsetRangeLimit)</li> </ul> <p>=== [[buildScan]] Building Distributed Data Scan with Column Pruning (as TableScan) -- <code>buildScan</code> Method</p>"},{"location":"kafka/KafkaRelation/#source-scala","title":"[source, scala]","text":""},{"location":"kafka/KafkaRelation/#buildscan-rddrow","title":"buildScan(): RDD[Row]","text":"<p><code>buildScan</code> is part of TableScan abstraction.</p> <p><code>buildScan</code> kafkaParamsForDriver from the &lt;&gt; and uses it to create a KafkaOffsetReader (together with the &lt;&gt;, the &lt;&gt; and a unique group ID of the format <code>spark-kafka-relation-[randomUUID]-driver</code>). <p><code>buildScan</code> then uses the <code>KafkaOffsetReader</code> to &lt;&gt; for the starting and ending offsets and closes it right after. <p><code>buildScan</code>...FIXME</p> <p><code>buildScan</code> prints out the following INFO message to the logs:</p> <pre><code>GetBatch generating RDD of offset range: [comma-separated offsetRanges]\n</code></pre> <p><code>buildScan</code> then kafkaParamsForExecutors and uses it to create a <code>KafkaSourceRDD</code> (with the &lt;&gt;) and maps over all the elements (using <code>RDD.map</code> operator that creates a <code>MapPartitionsRDD</code>). <p>TIP: Use <code>RDD.toDebugString</code> to see the two RDDs, i.e. <code>KafkaSourceRDD</code> and <code>MapPartitionsRDD</code>, in the RDD lineage.</p> <p>In the end, <code>buildScan</code> requests the &lt;&gt; to &lt;&gt; from the <code>KafkaSourceRDD</code> and the &lt;&gt;. <p><code>buildScan</code> throws an <code>IllegalStateException</code> when the topic partitions for starting offsets are different from the ending offsets topics:</p> <pre><code>different topic partitions for starting offsets topics[[fromTopics]] and ending offsets topics[[untilTopics]]\n</code></pre> <p>=== [[getPartitionOffsets]] <code>getPartitionOffsets</code> Internal Method</p>"},{"location":"kafka/KafkaRelation/#source-scala_1","title":"[source, scala]","text":"<p>getPartitionOffsets(   kafkaReader: KafkaOffsetReader,   kafkaOffsets: KafkaOffsetRangeLimit): Map[TopicPartition, Long]</p> <p><code>getPartitionOffsets</code> requests the input <code>KafkaOffsetReader</code> to fetchTopicPartitions.</p> <p><code>getPartitionOffsets</code> uses the input KafkaOffsetRangeLimit to return the mapping of offsets per Kafka <code>TopicPartition</code> fetched:</p> <p>. For <code>EarliestOffsetRangeLimit</code>, <code>getPartitionOffsets</code> returns a map with every <code>TopicPartition</code> and <code>-2L</code> (as the offset)</p> <p>. For <code>LatestOffsetRangeLimit</code>, <code>getPartitionOffsets</code> returns a map with every <code>TopicPartition</code> and <code>-1L</code> (as the offset)</p> <p>. For <code>SpecificOffsetRangeLimit</code>, <code>getPartitionOffsets</code> returns a map from &lt;&gt; <p>NOTE: <code>getPartitionOffsets</code> is used exclusively when <code>KafkaRelation</code> is requested to &lt;&gt; (as a TableScan). <p>==== [[getPartitionOffsets-validateTopicPartitions]] Validating TopicPartitions (Against Partition Offsets) -- <code>validateTopicPartitions</code> Inner Method</p>"},{"location":"kafka/KafkaRelation/#source-scala_2","title":"[source, scala]","text":"<p>validateTopicPartitions(   partitions: Set[TopicPartition],   partitionOffsets: Map[TopicPartition, Long]): Map[TopicPartition, Long]</p> <p>NOTE: <code>validateTopicPartitions</code> is a Scala inner method of &lt;&gt;, i.e. <code>validateTopicPartitions</code> is defined within the body of <code>getPartitionOffsets</code> and so is visible and can only be used in <code>getPartitionOffsets</code>. <p><code>validateTopicPartitions</code> asserts that the input set of Kafka <code>TopicPartitions</code> is exactly the set of the keys in the input <code>partitionOffsets</code>.</p> <p><code>validateTopicPartitions</code> prints out the following DEBUG message to the logs:</p> <pre><code>Partitions assigned to consumer: [partitions]. Seeking to [partitionOffsets]\n</code></pre> <p>In the end, <code>validateTopicPartitions</code> returns the input <code>partitionOffsets</code>.</p> <p>If the input set of Kafka <code>TopicPartitions</code> is not the set of the keys in the input <code>partitionOffsets</code>, <code>validateTopicPartitions</code> throws an <code>AssertionError</code>:</p> <pre><code>assertion failed: If startingOffsets contains specific offsets, you must specify all TopicPartitions.\nUse -1 for latest, -2 for earliest, if you don't care.\nSpecified: [partitionOffsets] Assigned: [partitions]\n</code></pre>"},{"location":"kafka/KafkaRowWriter/","title":"KafkaRowWriter","text":"<p><code>KafkaRowWriter</code> is...FIXME</p>"},{"location":"kafka/KafkaScan/","title":"KafkaScan","text":"<p><code>KafkaScan</code> is a Scan (a logical scan over data in Apache Kafka).</p>"},{"location":"kafka/KafkaScan/#creating-instance","title":"Creating Instance","text":"<p><code>KafkaScan</code> takes the following to be created:</p> <ul> <li> Case-Insensitive Options <p><code>KafkaScan</code> is created when:</p> <ul> <li><code>KafkaTable</code> is requested for a ScanBuilder</li> </ul>"},{"location":"kafka/KafkaScan/#read-schema","title":"Read Schema <pre><code>readSchema(): StructType\n</code></pre> <p><code>readSchema</code> is part of the Scan abstraction.</p>  <p><code>readSchema</code> builds the read schema (possibly with records headers based on includeHeaders option).</p>","text":""},{"location":"kafka/KafkaScan/#supported-custom-metrics","title":"Supported Custom Metrics <pre><code>supportedCustomMetrics(): Array[CustomMetric]\n</code></pre> <p><code>supportedCustomMetrics</code> is part of the Scan abstraction.</p>  <p><code>supportedCustomMetrics</code> gives the following CustomMetrics:</p> <ul> <li><code>OffsetOutOfRangeMetric</code></li> <li><code>DataLossMetric</code></li> </ul>","text":""},{"location":"kafka/KafkaScan/#tomicrobatchstream","title":"toMicroBatchStream <pre><code>toMicroBatchStream(\n  checkpointLocation: String): MicroBatchStream\n</code></pre> <p><code>toMicroBatchStream</code> is part of the Scan abstraction.</p>  <p><code>toMicroBatchStream</code> validateStreamOptions.</p> <p><code>toMicroBatchStream</code> streamingUniqueGroupId.</p> <p><code>toMicroBatchStream</code> convertToSpecifiedParams.</p> <p><code>toMicroBatchStream</code> determines KafkaOffsetRangeLimit based on the following options (with <code>LatestOffsetRangeLimit</code> as the default):</p> <ul> <li>startingTimestamp</li> <li>startingOffsetsByTimestamp</li> <li>startingOffsets</li> </ul> <p><code>toMicroBatchStream</code> builds a KafkaOffsetReader for the following:</p>    Argument Value     ConsumerStrategy strategy   <code>driverKafkaParams</code> Kafka Configuration Properties for Driver   <code>readerOptions</code> options   <code>driverGroupIdPrefix</code> streamingUniqueGroupId with <code>-driver</code> suffix    <p>In the end, <code>toMicroBatchStream</code> creates a <code>KafkaMicroBatchStream</code> (Spark Structured Streaming) with the following:</p> <ul> <li>KafkaOffsetReader</li> <li>kafkaParamsForExecutors</li> <li>KafkaOffsetRangeLimit</li> <li>failOnDataLoss</li> </ul>","text":""},{"location":"kafka/KafkaSourceProvider/","title":"KafkaSourceProvider","text":"<p><code>KafkaSourceProvider</code> is the entry point to the kafka data source.</p> <p><code>KafkaSourceProvider</code> is a SimpleTableProvider (and does not support custom table schema and partitioning).</p> <p>Note</p> <p><code>KafkaSourceProvider</code> is also a <code>StreamSourceProvider</code> and a <code>StreamSinkProvider</code> to be used in Spark Structured Streaming.</p> <p>Learn more on StreamSourceProvider and StreamSinkProvider in The Internals of Spark Structured Streaming online book.</p>"},{"location":"kafka/KafkaSourceProvider/#datasourceregister","title":"DataSourceRegister <p><code>KafkaSourceProvider</code> is a DataSourceRegister and registers itself as kafka format.</p> <p><code>KafkaSourceProvider</code> uses <code>META-INF/services/org.apache.spark.sql.sources.DataSourceRegister</code> file for the registration (available in the source code of Apache Spark).</p>","text":""},{"location":"kafka/KafkaSourceProvider/#kafkatable","title":"KafkaTable <pre><code>getTable(\n  options: CaseInsensitiveStringMap): KafkaTable\n</code></pre> <p><code>getTable</code> is part of the SimpleTableProvider abstraction.</p> <p><code>getTable</code> creates a KafkaTable with the <code>includeHeaders</code> flag based on includeHeaders option.</p>","text":""},{"location":"kafka/KafkaSourceProvider/#creating-relation-for-reading-relationprovider","title":"Creating Relation for Reading (RelationProvider) <pre><code>createRelation(\n  sqlContext: SQLContext,\n  parameters: Map[String, String]): BaseRelation\n</code></pre> <p><code>createRelation</code> is part of the RelationProvider abstraction.</p> <p><code>createRelation</code> starts by &lt;&gt; in the input <code>parameters</code>. <p><code>createRelation</code> collects all <code>kafka.</code>-prefixed key options (in the input <code>parameters</code>) and creates a local <code>specifiedKafkaParams</code> with the keys without the <code>kafka.</code> prefix (e.g. <code>kafka.whatever</code> is simply <code>whatever</code>).</p> <p><code>createRelation</code> &lt;&gt; with the <code>startingoffsets</code> offset option key (in the given <code>parameters</code>) and EarliestOffsetRangeLimit as the default offsets. <p><code>createRelation</code> makes sure that the KafkaOffsetRangeLimit is not EarliestOffsetRangeLimit or throws an <code>AssertionError</code>.</p> <p><code>createRelation</code> &lt;&gt;, but this time with the <code>endingoffsets</code> offset option key (in the given <code>parameters</code>) and LatestOffsetRangeLimit as the default offsets. <p><code>createRelation</code> makes sure that the KafkaOffsetRangeLimit is not EarliestOffsetRangeLimit or throws a <code>AssertionError</code>.</p> <p>In the end, <code>createRelation</code> creates a KafkaRelation with the &lt;&gt; (in the given <code>parameters</code>), &lt;&gt; option, and the starting and ending offsets.","text":""},{"location":"kafka/KafkaSourceProvider/#creating-relation-for-writing-creatablerelationprovider","title":"Creating Relation for Writing (CreatableRelationProvider) <pre><code>createRelation(\n  sqlContext: SQLContext,\n  mode: SaveMode,\n  parameters: Map[String, String],\n  df: DataFrame): BaseRelation\n</code></pre> <p><code>createRelation</code> is part of the CreatableRelationProvider abstraction.</p> <p><code>createRelation</code> gets the topic option from the input <code>parameters</code>.</p> <p><code>createRelation</code> gets the &lt;&gt; from the input <code>parameters</code>. <p><code>createRelation</code> then uses the <code>KafkaWriter</code> helper object to write the rows of the DataFrame to the Kafka topic.</p> <p>In the end, <code>createRelation</code> creates a fake BaseRelation that simply throws an <code>UnsupportedOperationException</code> for all its methods.</p> <p><code>createRelation</code> supports Append and ErrorIfExists only. <code>createRelation</code> throws an <code>AnalysisException</code> for the other save modes:</p> <pre><code>Save mode [mode] not allowed for Kafka. Allowed save modes are [Append] and [ErrorIfExists] (default).\n</code></pre>","text":""},{"location":"kafka/KafkaSourceProvider/#kafka-configuration-properties-for-driver","title":"Kafka Configuration Properties for Driver <pre><code>kafkaParamsForDriver(\n  specifiedKafkaParams: Map[String, String]): ju.Map[String, Object]\n</code></pre> <p><code>kafkaParamsForDriver</code> is a utility to define required Kafka configuration parameters for the driver.</p> <p><code>kafkaParamsForDriver</code> is used when:</p> <ul> <li><code>KafkaBatch</code> is requested to planInputPartitions</li> <li><code>KafkaRelation</code> is requested to buildScan</li> <li><code>KafkaSourceProvider</code> to <code>createSource</code> (for Spark Structured Streaming)</li> <li><code>KafkaScan</code> is requested to <code>toMicroBatchStream</code> and <code>toContinuousStream</code> (for Spark Structured Streaming)</li> </ul>","text":""},{"location":"kafka/KafkaSourceProvider/#autooffsetreset","title":"auto.offset.reset <p>What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted):</p> <ul> <li> <p><code>earliest</code> - automatically reset the offset to the earliest offset</p> </li> <li> <p><code>latest</code> - automatically reset the offset to the latest offset</p> </li> <li> <p><code>none</code> - throw an exception to the Kafka consumer if no previous offset is found for the consumer's group</p> </li> <li> <p>anything else - throw an exception to the Kafka consumer</p> </li> </ul> <p>Value: <code>earliest</code></p> <p>ConsumerConfig.AUTO_OFFSET_RESET_CONFIG</p>","text":""},{"location":"kafka/KafkaSourceProvider/#enableautocommit","title":"enable.auto.commit <p>If <code>true</code> the Kafka consumer's offset will be periodically committed in the background</p> <p>Value: <code>false</code></p> <p>ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG</p>","text":""},{"location":"kafka/KafkaSourceProvider/#keydeserializer","title":"key.deserializer <p>Deserializer class for keys that implements the Kafka <code>Deserializer</code> interface.</p> <p>Value: org.apache.kafka.common.deserialization.ByteArrayDeserializer</p> <p>ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG</p>","text":""},{"location":"kafka/KafkaSourceProvider/#maxpollrecords","title":"max.poll.records <p>The maximum number of records returned in a single call to <code>Consumer.poll()</code></p> <p>Value: <code>1</code></p> <p>ConsumerConfig.MAX_POLL_RECORDS_CONFIG</p>","text":""},{"location":"kafka/KafkaSourceProvider/#receivebufferbytes","title":"receive.buffer.bytes <p>Only set if not set already</p> <p>Value: <code>65536</code></p> <p>ConsumerConfig.MAX_POLL_RECORDS_CONFIG</p>","text":""},{"location":"kafka/KafkaSourceProvider/#valuedeserializer","title":"value.deserializer <p>Deserializer class for values that implements the Kafka <code>Deserializer</code> interface.</p> <p>Value: org.apache.kafka.common.serialization.ByteArrayDeserializer</p> <p>ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG</p>  <p>Tip</p> <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.kafka010.KafkaSourceProvider.ConfigUpdater</code> logger to see updates of Kafka configuration parameters.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.kafka010.KafkaSourceProvider.ConfigUpdater=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"kafka/KafkaSourceProvider/#kafkaparamsforexecutors","title":"kafkaParamsForExecutors <pre><code>kafkaParamsForExecutors(\n  specifiedKafkaParams: Map[String, String],\n  uniqueGroupId: String): ju.Map[String, Object]\n</code></pre> <p><code>kafkaParamsForExecutors</code>...FIXME</p> <p><code>kafkaParamsForExecutors</code> is used when:</p> <ul> <li><code>KafkaBatch</code> is requested to planInputPartitions</li> <li><code>KafkaRelation</code> is requested to buildScan</li> <li><code>KafkaSourceProvider</code> to <code>createSource</code> (for Spark Structured Streaming)</li> <li><code>KafkaScan</code> is requested to <code>toMicroBatchStream</code> and <code>toContinuousStream</code> (for Spark Structured Streaming)</li> </ul>","text":""},{"location":"kafka/KafkaSourceProvider/#kafkaparamsforproducer","title":"kafkaParamsForProducer <pre><code>kafkaParamsForProducer(\n  params: CaseInsensitiveMap[String]): ju.Map[String, Object]\n</code></pre> <p><code>kafkaParamsForProducer</code>...FIXME</p> <p><code>kafkaParamsForProducer</code> is used when:</p> <ul> <li>KafkaSourceProvider is requested to create a relation for writing (and <code>createSink</code> for Spark Structured Streaming)</li> <li><code>KafkaTable</code> is requested for a WriteBuilder</li> </ul>","text":""},{"location":"kafka/KafkaSourceProvider/#validating-kafka-options-for-batch-queries","title":"Validating Kafka Options for Batch Queries <pre><code>validateBatchOptions(\n  params: CaseInsensitiveMap[String]): Unit\n</code></pre> <p><code>validateBatchOptions</code> &lt;&gt; for the startingoffsets option in the input <code>caseInsensitiveParams</code> and with EarliestOffsetRangeLimit as the default <code>KafkaOffsetRangeLimit</code>. <p><code>validateBatchOptions</code> then matches the returned KafkaOffsetRangeLimit as follows:</p> <ul> <li> <p>EarliestOffsetRangeLimit is acceptable and <code>validateBatchOptions</code> simply does nothing</p> </li> <li> <p>LatestOffsetRangeLimit is not acceptable and <code>validateBatchOptions</code> throws an <code>IllegalArgumentException</code>:</p> <pre><code>starting offset can't be latest for batch queries on Kafka\n</code></pre> </li> <li> <p>SpecificOffsetRangeLimit is acceptable unless one of the offsets is -1L for which <code>validateBatchOptions</code> throws an <code>IllegalArgumentException</code>:</p> <pre><code>startingOffsets for [tp] can't be latest for batch queries on Kafka\n</code></pre> </li> </ul> <p><code>validateBatchOptions</code> is used when:</p> <ul> <li><code>KafkaSourceProvider</code> is requested for a relation for reading</li> <li><code>KafkaScan</code> is requested for a Batch</li> </ul>","text":""},{"location":"kafka/KafkaSourceProvider/#getting-desired-kafkaoffsetrangelimit-for-offset-option","title":"Getting Desired KafkaOffsetRangeLimit (for Offset Option) <pre><code>getKafkaOffsetRangeLimit(\n  params: Map[String, String],\n  offsetOptionKey: String,\n  defaultOffsets: KafkaOffsetRangeLimit): KafkaOffsetRangeLimit\n</code></pre> <p><code>getKafkaOffsetRangeLimit</code> tries to find the given <code>offsetOptionKey</code> in the input <code>params</code> and converts the value found to a KafkaOffsetRangeLimit as follows:</p> <ul> <li> <p><code>latest</code> becomes LatestOffsetRangeLimit</p> </li> <li> <p><code>earliest</code> becomes EarliestOffsetRangeLimit</p> </li> <li> <p>For a JSON text, <code>getKafkaOffsetRangeLimit</code> uses the <code>JsonUtils</code> helper object to read per-TopicPartition offsets from it and creates a SpecificOffsetRangeLimit</p> </li> </ul> <p>When the input <code>offsetOptionKey</code> was not found, <code>getKafkaOffsetRangeLimit</code> returns the input <code>defaultOffsets</code>.</p> <p><code>getKafkaOffsetRangeLimit</code> is used when:</p> <ul> <li><code>KafkaSourceProvider</code> is requested for a relation for reading and <code>createSource</code> (Spark Structured Streaming)</li> <li><code>KafkaScan</code> is requested for a Batch, <code>toMicroBatchStream</code> (Spark Structured Streaming) and <code>toContinuousStream</code> (Spark Structured Streaming)</li> </ul>","text":""},{"location":"kafka/KafkaSourceProvider/#consumerstrategy","title":"ConsumerStrategy <pre><code>strategy(\n  params: CaseInsensitiveMap[String]): ConsumerStrategy\n</code></pre> <p><code>strategy</code> finds one of the strategy options: subscribe, subscribepattern and assign.</p> <p>For assign, <code>strategy</code> uses the <code>JsonUtils</code> helper object to deserialize TopicPartitions from JSON (e.g. <code>{\"topicA\":[0,1],\"topicB\":[0,1]}</code>) and returns a new AssignStrategy.</p> <p>For subscribe, <code>strategy</code> splits the value by <code>,</code> (comma) and returns a new SubscribeStrategy.</p> <p>For subscribepattern, <code>strategy</code> returns a new SubscribePatternStrategy</p> <p><code>strategy</code> is used when:</p> <ul> <li><code>KafkaSourceProvider</code> is requested for a relation for reading and <code>createSource</code> (Spark Structured Streaming)</li> <li><code>KafkaScan</code> is requested for a Batch, <code>toMicroBatchStream</code> (Spark Structured Streaming) and <code>toContinuousStream</code> (Spark Structured Streaming)</li> </ul>","text":""},{"location":"kafka/KafkaSourceProvider/#failondataloss-option","title":"failOnDataLoss Option <pre><code>failOnDataLoss(\n  params: CaseInsensitiveMap[String]): Boolean\n</code></pre> <p><code>failOnDataLoss</code> is the value of <code>failOnDataLoss</code> key in the given case-insensitive parameters (options) if available or <code>true</code>.</p> <ul> <li><code>KafkaSourceProvider</code> is requested for a relation for reading (and <code>createSource</code> for Spark Structured Streaming)</li> <li><code>KafkaScan</code> is requested for a Batch (and <code>toMicroBatchStream</code> and <code>toContinuousStream</code> for Spark Structured Streaming)</li> </ul>","text":""},{"location":"kafka/KafkaSourceRDD/","title":"KafkaSourceRDD","text":"<p><code>KafkaSourceRDD</code> is an <code>RDD</code> of Kafka's ConsumerRecords (with keys and values being collections of bytes, i.e. <code>Array[Byte]</code>).</p> <p><code>KafkaSourceRDD</code> uses KafkaSourceRDDPartition for the &lt;&gt;. <p><code>KafkaSourceRDD</code> has a specialized API for the following RDD operators:</p> <ul> <li> <p>&lt;&gt; <li> <p>&lt;&gt; <li> <p>&lt;&gt; <li> <p>&lt;&gt; <li> <p>&lt;&gt; <p><code>KafkaSourceRDD</code> is &lt;&gt; when: <ul> <li> <p><code>KafkaRelation</code> is requested to build a distributed data scan with column pruning (as a TableScan)</p> </li> <li> <p>(Spark Structured Streaming) <code>KafkaSource</code> is requested to <code>getBatch</code></p> </li> </ul> <p>=== [[compute]] Computing Partition (in TaskContext) -- <code>compute</code> Method</p>"},{"location":"kafka/KafkaSourceRDD/#source-scala","title":"[source, scala]","text":"<p>compute(   thePart: Partition,   context: TaskContext): Iterator[ConsumerRecord[Array[Byte], Array[Byte]]]</p> <p>NOTE: <code>compute</code> is part of Spark Core's <code>RDD</code> Contract to compute a partition (in a <code>TaskContext</code>).</p> <p><code>compute</code>...FIXME</p> <p>=== [[count]] <code>count</code> Operator</p>"},{"location":"kafka/KafkaSourceRDD/#source-scala_1","title":"[source, scala]","text":""},{"location":"kafka/KafkaSourceRDD/#count-long","title":"count(): Long","text":"<p>NOTE: <code>count</code> is part of Spark Core's <code>RDD</code> Contract to...FIXME.</p> <p><code>count</code>...FIXME</p> <p>=== [[countApprox]] <code>countApprox</code> Operator</p>"},{"location":"kafka/KafkaSourceRDD/#source-scala_2","title":"[source, scala]","text":""},{"location":"kafka/KafkaSourceRDD/#countapproxtimeout-long-confidence-double-partialresultboundeddouble","title":"countApprox(timeout: Long, confidence: Double): PartialResult[BoundedDouble]","text":"<p>NOTE: <code>countApprox</code> is part of Spark Core's <code>RDD</code> Contract to...FIXME.</p> <p><code>countApprox</code>...FIXME</p> <p>=== [[isEmpty]] <code>isEmpty</code> Operator</p>"},{"location":"kafka/KafkaSourceRDD/#source-scala_3","title":"[source, scala]","text":""},{"location":"kafka/KafkaSourceRDD/#isempty-boolean","title":"isEmpty(): Boolean","text":"<p>NOTE: <code>isEmpty</code> is part of Spark Core's <code>RDD</code> Contract to...FIXME.</p> <p><code>isEmpty</code>...FIXME</p> <p>=== [[persist]] <code>persist</code> Operator</p>"},{"location":"kafka/KafkaSourceRDD/#source-scala_4","title":"[source, scala]","text":""},{"location":"kafka/KafkaSourceRDD/#persistnewlevel-storagelevel-thistype","title":"persist(newLevel: StorageLevel): this.type","text":"<p>NOTE: <code>persist</code> is part of Spark Core's <code>RDD</code> Contract to...FIXME.</p> <p><code>persist</code>...FIXME</p> <p>=== [[getPartitions]] <code>getPartitions</code> Method</p>"},{"location":"kafka/KafkaSourceRDD/#source-scala_5","title":"[source, scala]","text":""},{"location":"kafka/KafkaSourceRDD/#getpartitions-arraypartition","title":"getPartitions: Array[Partition]","text":"<p>NOTE: <code>getPartitions</code> is part of Spark Core's <code>RDD</code> Contract to...FIXME</p> <p>=== [[getPreferredLocations]] <code>getPreferredLocations</code> Method</p>"},{"location":"kafka/KafkaSourceRDD/#source-scala_6","title":"[source, scala]","text":""},{"location":"kafka/KafkaSourceRDD/#getpreferredlocationssplit-partition-seqstring","title":"getPreferredLocations(split: Partition): Seq[String]","text":"<p>NOTE: <code>getPreferredLocations</code> is part of the RDD Contract to...FIXME.</p> <p><code>getPreferredLocations</code>...FIXME</p> <p>=== [[resolveRange]] <code>resolveRange</code> Internal Method</p>"},{"location":"kafka/KafkaSourceRDD/#source-scala_7","title":"[source, scala]","text":"<p>resolveRange(   consumer: KafkaDataConsumer,   range: KafkaSourceRDDOffsetRange): KafkaSourceRDDOffsetRange</p> <p><code>resolveRange</code>...FIXME</p> <p>NOTE: <code>resolveRange</code> is used exclusively when <code>KafkaSourceRDD</code> is requested to &lt;&gt;."},{"location":"kafka/KafkaSourceRDDPartition/","title":"KafkaSourceRDDPartition","text":"<p><code>KafkaSourceRDDPartition</code> is...FIXME</p>"},{"location":"kafka/KafkaTable/","title":"KafkaTable","text":"<p><code>KafkaTable</code> is a Table with read and write support for Kafka Data Source.</p>"},{"location":"kafka/KafkaTable/#name","title":"Name","text":"<p><code>KafkaTable</code> uses KafkaTable name.</p>"},{"location":"kafka/KafkaTable/#capabilities","title":"Capabilities <pre><code>capabilities(): ju.Set[TableCapability]\n</code></pre> <p><code>capabilities</code> is part of the Table abstraction.</p>  <p><code>capabilities</code> is the following table capabilities:</p> <ul> <li>BATCH_READ</li> <li>BATCH_WRITE</li> <li>MICRO_BATCH_READ</li> <li>CONTINUOUS_READ</li> <li>STREAMING_WRITE</li> <li>ACCEPT_ANY_SCHEMA</li> </ul>","text":""},{"location":"kafka/KafkaTable/#creating-scanbuilder","title":"Creating ScanBuilder <pre><code>newScanBuilder(\n  options: CaseInsensitiveStringMap): ScanBuilder\n</code></pre> <p><code>newScanBuilder</code> is part of the SupportsRead abstraction.</p>  <p><code>newScanBuilder</code> creates a ScanBuilder that can create a KafkaScan.</p>","text":""},{"location":"kafka/KafkaTable/#creating-writebuilder","title":"Creating WriteBuilder <pre><code>newWriteBuilder(\n  info: LogicalWriteInfo): WriteBuilder\n</code></pre> <p><code>newWriteBuilder</code> is part of the SupportsWrite abstraction.</p>  <p><code>newWriteBuilder</code> creates a custom WriteBuilder with support for truncate and update.</p>","text":""},{"location":"kafka/KafkaTable/#buildforbatch","title":"buildForBatch <pre><code>buildForBatch(): BatchWrite\n</code></pre> <p><code>buildForBatch</code> is part of the WriteBuilder abstraction.</p>  <p><code>buildForBatch</code> creates a KafkaBatchWrite.</p>","text":""},{"location":"kafka/KafkaWrite/","title":"KafkaWrite","text":"<p><code>KafkaWrite</code> is a Write for Kafka Data Source.</p>"},{"location":"kafka/KafkaWrite/#creating-instance","title":"Creating Instance","text":"<p><code>KafkaWrite</code> takes the following to be created:</p> <ul> <li> Topic Name <li> Kafka Producer Parameters <li> Schema (StructType) <p><code>KafkaWrite</code> is created when:</p> <ul> <li><code>KafkaTable</code> is requested to create a WriteBuilder</li> </ul>"},{"location":"kafka/KafkaWrite/#description","title":"Description <pre><code>description(): String\n</code></pre> <p><code>description</code> is part of the Write abstraction.</p>  <p><code>description</code> is <code>Kafka</code>.</p>","text":""},{"location":"kafka/KafkaWrite/#creating-batchwrite","title":"Creating BatchWrite <pre><code>toBatch: BatchWrite\n</code></pre> <p><code>toBatch</code> is part of the Write abstraction.</p>  <p><code>toBatch</code> creates a KafkaBatchWrite.</p>","text":""},{"location":"kafka/KafkaWrite/#creating-streamingwrite","title":"Creating StreamingWrite <pre><code>toStreaming: StreamingWrite\n</code></pre> <p><code>toStreaming</code> is part of the Write abstraction.</p>  <p><code>toStreaming</code> creates a <code>KafkaStreamingWrite</code> (Spark Structured Streaming).</p>","text":""},{"location":"kafka/KafkaWriteTask/","title":"KafkaWriteTask","text":"<p><code>KafkaWriteTask</code> is used to &lt;&gt; (from a structured query) to Apache Kafka. <p><code>KafkaWriteTask</code> is &lt;&gt; exclusively when <code>KafkaWriter</code> is requested to write the rows of a structured query to a Kafka topic. <p><code>KafkaWriteTask</code> &lt;&gt; keys and values in their binary format (as JVM's bytes) and so uses the raw-memory unsafe row format only (i.e. <code>UnsafeRow</code>). That is supposed to save time for reconstructing the rows to very tiny JVM objects (i.e. byte arrays). <p>[[internal-properties]] .KafkaWriteTask's Internal Properties [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| callback | [[callback]]</p> <p>| failedWrite | [[failedWrite]]</p> <p>| projection | [[projection]] UnsafeProjection</p> <p>&lt;&gt; once when <code>KafkaWriteTask</code> is created. |=== <p>=== [[execute]] Writing Rows to Kafka Asynchronously -- <code>execute</code> Method</p>"},{"location":"kafka/KafkaWriteTask/#source-scala","title":"[source, scala]","text":""},{"location":"kafka/KafkaWriteTask/#executeiterator-iteratorinternalrow-unit","title":"execute(iterator: Iterator[InternalRow]): Unit","text":"<p><code>execute</code> uses Apache Kafka's Producer API to create a https://kafka.apache.org/0101/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[KafkaProducer] and https://kafka.apache.org/0101/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[ProducerRecord] for every row in <code>iterator</code>, and sends the rows to Kafka in batches asynchronously.</p> <p>Internally, <code>execute</code> creates a <code>KafkaProducer</code> using <code>Array[Byte]</code> for the keys and values, and <code>producerConfiguration</code> for the producer's configuration.</p> <p>NOTE: <code>execute</code> creates a single <code>KafkaProducer</code> for all rows.</p> <p>For every row in the <code>iterator</code>, <code>execute</code> uses the internal &lt;&gt; to project (aka convert) InternalRow to an UnsafeRow object and take 0<sup>th</sup>, 1<sup>st</sup> and 2<sup>nd</sup> fields for a topic, key and value, respectively. <p><code>execute</code> then creates a <code>ProducerRecord</code> and sends it to Kafka (using the <code>KafkaProducer</code>). <code>execute</code> registers a asynchronous <code>Callback</code> to monitor the writing.</p>"},{"location":"kafka/KafkaWriteTask/#note","title":"[NOTE]","text":"<p>From https://kafka.apache.org/0101/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[KafkaProducer's documentation]:</p>"},{"location":"kafka/KafkaWriteTask/#the-send-method-is-asynchronous-when-called-it-adds-the-record-to-a-buffer-of-pending-record-sends-and-immediately-returns-this-allows-the-producer-to-batch-together-individual-records-for-efficiency","title":"&gt; The <code>send()</code> method is asynchronous. When called it adds the record to a buffer of pending record sends and immediately returns. This allows the producer to batch together individual records for efficiency.","text":"<p>=== [[createProjection]] Creating UnsafeProjection -- <code>createProjection</code> Internal Method</p>"},{"location":"kafka/KafkaWriteTask/#source-scala_1","title":"[source, scala]","text":""},{"location":"kafka/KafkaWriteTask/#createprojection-unsafeprojection","title":"createProjection: UnsafeProjection","text":"<p><code>createProjection</code> creates a UnsafeProjection with <code>topic</code>, <code>key</code> and <code>value</code> expressions/Expression.md[expressions] and the <code>inputSchema</code>.</p> <p><code>createProjection</code> makes sure that the following holds (and reports an <code>IllegalStateException</code> otherwise):</p> <ul> <li><code>topic</code> was defined (either as the input <code>topic</code> or in <code>inputSchema</code>) and is of type <code>StringType</code></li> <li>Optional <code>key</code> is of type <code>StringType</code> or <code>BinaryType</code> if defined</li> <li><code>value</code> was defined (in <code>inputSchema</code>) and is of type <code>StringType</code> or <code>BinaryType</code></li> </ul> <p><code>createProjection</code> casts <code>key</code> and <code>value</code> expressions to <code>BinaryType</code> in UnsafeProjection.</p> <p>NOTE: <code>createProjection</code> is used exclusively when <code>KafkaWriteTask</code> is created (as &lt;&gt;). <p>=== [[close]] <code>close</code> Method</p>"},{"location":"kafka/KafkaWriteTask/#source-scala_2","title":"[source, scala]","text":""},{"location":"kafka/KafkaWriteTask/#close-unit","title":"close(): Unit","text":"<p><code>close</code>...FIXME</p> <p>NOTE: <code>close</code> is used when...FIXME</p> <p>=== [[creating-instance]] Creating KafkaWriteTask Instance</p> <p><code>KafkaWriteTask</code> takes the following when created:</p> <ul> <li>[[producerConfiguration]] Kafka Producer configuration (as <code>Map[String, Object]</code>)</li> <li>[[inputSchema]] Input schema (as <code>Seq[Attribute]</code>)</li> <li>[[topic]] Topic name</li> </ul> <p><code>KafkaWriteTask</code> initializes the &lt;&gt;."},{"location":"kafka/KafkaWriter/","title":"KafkaWriter Utility","text":"<p><code>KafkaWriter</code> is a Scala object that is used to write the rows of a batch (or a streaming) structured query to Apache Kafka.</p> <p></p> <p><code>KafkaWriter</code> validates the schema of a structured query that it contains the following columns (output schema attributes):</p> <ul> <li> <p>Either topic of type <code>StringType</code> or the topic option are defined</p> </li> <li> <p>Optional key of type <code>StringType</code> or <code>BinaryType</code></p> </li> <li> <p>Required value of type <code>StringType</code> or <code>BinaryType</code></p> </li> </ul> <pre><code>// KafkaWriter is a private `kafka010` package object\n// and so the code to use it should also be in the same package\n// BEGIN: Use `:paste -raw` in spark-shell\npackage org.apache.spark.sql.kafka010\n\nobject PublicKafkaWriter {\n  import org.apache.spark.sql.execution.QueryExecution\n  def validateQuery(\n      queryExecution: QueryExecution,\n      kafkaParameters: Map[String, Object],\n      topic: Option[String] = None): Unit = {\n    import scala.collection.JavaConversions.mapAsJavaMap\n    KafkaWriter.validateQuery(queryExecution, kafkaParameters, topic)\n  }\n}\n// END\n\nimport org.apache.spark.sql.kafka010.{PublicKafkaWriter =&gt; PKW}\n\nval spark: SparkSession = ...\nval q = spark.range(1).select('id)\nscala&gt; PKW.validateQuery(\n  queryExecution = q.queryExecution,\n  kafkaParameters = Map.empty[String, Object])\norg.apache.spark.sql.AnalysisException: topic option required when no 'topic' attribute is present. Use the topic option for setting a topic.;\n  at org.apache.spark.sql.kafka010.KafkaWriter$$anonfun$2.apply(KafkaWriter.scala:53)\n  at org.apache.spark.sql.kafka010.KafkaWriter$$anonfun$2.apply(KafkaWriter.scala:52)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.sql.kafka010.KafkaWriter$.validateQuery(KafkaWriter.scala:51)\n  at org.apache.spark.sql.kafka010.PublicKafkaWriter$.validateQuery(&lt;pastie&gt;:10)\n  ... 50 elided\n</code></pre>"},{"location":"kafka/KafkaWriter/#writing-rows-of-structured-query","title":"Writing Rows of Structured Query <pre><code>write(\n  sparkSession: SparkSession,\n  queryExecution: QueryExecution,\n  kafkaParameters: ju.Map[String, Object],\n  topic: Option[String] = None): Unit\n</code></pre> <p><code>write</code> gets the output schema of the analyzed logical plan of the input QueryExecution.</p> <p><code>write</code> then validates the schema of a structured query.</p> <p>In the end, <code>write</code> requests the <code>QueryExecution</code> for RDD[InternalRow] (that represents the structured query as an RDD) and executes the following function on every partition of the RDD (using <code>RDD.foreachPartition</code> operation):</p> <ol> <li> <p>Creates a KafkaWriteTask (for the input <code>kafkaParameters</code>, the schema and the input <code>topic</code>)</p> </li> <li> <p>Requests the <code>KafkaWriteTask</code> to write the rows (of the partition) to Kafka topic</p> </li> <li> <p>Requests the <code>KafkaWriteTask</code> to close</p> </li> </ol> <p><code>write</code> is used when:</p> <ul> <li> <p><code>KafkaSourceProvider</code> is requested to write a DataFrame to a Kafka topic</p> </li> <li> <p>(Spark Structured Streaming) <code>KafkaSink</code> is requested to <code>addBatch</code></p> </li> </ul>","text":""},{"location":"kafka/KafkaWriter/#validating-schema-attributes-of-structured-query-and-topic-option-availability","title":"Validating Schema (Attributes) of Structured Query and Topic Option Availability <pre><code>validateQuery(\n  schema: Seq[Attribute],\n  kafkaParameters: ju.Map[String, Object],\n  topic: Option[String] = None): Unit\n</code></pre> <p><code>validateQuery</code> makes sure that the following attributes are in the input schema (or their alternatives) and of the right data types:</p> <ul> <li> <p>Either <code>topic</code> attribute of type <code>StringType</code> or the topic option are defined</p> </li> <li> <p>If <code>key</code> attribute is defined it is of type <code>StringType</code> or <code>BinaryType</code></p> </li> <li> <p><code>value</code> attribute is of type <code>StringType</code> or <code>BinaryType</code></p> </li> </ul> <p>If any of the requirements are not met, <code>validateQuery</code> throws an <code>AnalysisException</code>.</p> <p><code>validateQuery</code> is used when:</p> <ul> <li><code>KafkaWriter</code> object is requested to write the rows of a structured query to a Kafka topic</li> <li>(Spark Structured Streaming) <code>KafkaStreamWriter</code> is created</li> <li>(Spark Structured Streaming) <code>KafkaSourceProvider</code> is requested to <code>createStreamWriter</code></li> </ul>","text":""},{"location":"kafka/configuration-properties/","title":"Configuration Properties","text":""},{"location":"kafka/configuration-properties/#sparkkafkaconsumercachecapacity","title":"spark.kafka.consumer.cache.capacity <p>The maximum number of consumers cached. Please note it's a soft limit (check Structured Streaming Kafka integration guide for further details).</p> <p>Default: <code>64</code></p> <p>Used in InternalKafkaConsumerPool</p>","text":""},{"location":"kafka/configuration-properties/#sparkkafkaconsumercacheevictorthreadruninterval","title":"spark.kafka.consumer.cache.evictorThreadRunInterval <p>The interval of time (in millis) between runs of the idle evictor thread for consumer pool. When non-positive, no idle evictor thread will be run</p> <p>Default: <code>1m</code></p>","text":""},{"location":"kafka/configuration-properties/#sparkkafkaconsumercachejmxenable","title":"spark.kafka.consumer.cache.jmx.enable <p>Enable or disable JMX for pools created with this configuration instance.</p> <p>Default: <code>false</code></p>","text":""},{"location":"kafka/configuration-properties/#sparkkafkaconsumercachetimeout","title":"spark.kafka.consumer.cache.timeout <p>The minimum amount of time (in millis) a consumer may sit idle in the pool before it is eligible for eviction by the evictor. When non-positive, no consumers will be evicted from the pool due to idle time alone.</p> <p>Default: <code>5m</code></p>","text":""},{"location":"kafka/configuration-properties/#sparkkafkaconsumercacheevictorthreadruninterval_1","title":"spark.kafka.consumer.cache.evictorThreadRunInterval <p>The interval of time (in millis) between runs of the idle evictor thread for consumer pool. When non-positive, no idle evictor thread will be run.</p> <p>Default: <code>1m</code></p>","text":""},{"location":"kafka/configuration-properties/#sparkkafkaconsumerfetcheddatacachetimeout","title":"spark.kafka.consumer.fetchedData.cache.timeout <p>The minimum amount of time (in millis) a fetched data may sit idle in the pool before it is eligible for eviction by the evictor. When non-positive, no fetched data will be evicted from the pool due to idle time alone.</p> <p>Default: <code>5m</code></p>","text":""},{"location":"kafka/configuration-properties/#sparkkafkaproducercacheevictorthreadruninterval","title":"spark.kafka.producer.cache.evictorThreadRunInterval <p>The interval of time (in millis) between runs of the idle evictor thread for producer pool. When non-positive, no idle evictor thread will be run.</p> <p>Default: <code>1m</code></p>","text":""},{"location":"kafka/configuration-properties/#sparkkafkaproducercachetimeout","title":"spark.kafka.producer.cache.timeout <p>The expire time (in millis) to remove the unused producers.</p> <p>Default: <code>10m</code></p> <p>Used when:</p> <ul> <li><code>InternalKafkaProducerPool</code> is requested to acquire a CachedKafkaProducer</li> </ul>","text":""},{"location":"kafka/options/","title":"Options","text":""},{"location":"kafka/options/#includeheaders","title":"includeHeaders <p>default: <code>false</code></p>","text":""},{"location":"kafka/options/#startingoffsets","title":"startingOffsets <p>Specifies where to start reading records</p> <p>Only applies to new streaming queries that start from scratch (resuming will always pick up from where a query left off)</p> <p>default: <code>false</code></p> <p>Supported values:</p> <ul> <li><code>earliest</code></li> <li><code>latest</code></li> </ul> <p>Used when:</p> <ul> <li><code>KafkaSourceProvider</code> is requested to createSource (to getKafkaOffsetRangeLimit), createRelation, validateBatchOptions</li> <li><code>KafkaScan</code> is requested to toBatch, toMicroBatchStream, toContinuousStream</li> </ul>","text":""},{"location":"logical-analysis-rules/AddMetadataColumns/","title":"AddMetadataColumns Logical Resolution Rule","text":"<p><code>AddMetadataColumns</code> adds metadata columns to logical operators with metadata columns defined</p> <p><code>AddMetadataColumns</code> is a Rule to transform a LogicalPlan (<code>Rule[LogicalPlan]</code>).</p> <p><code>AddMetadataColumns</code> is part of the Resolution batch of the Analyzer.</p>"},{"location":"logical-analysis-rules/AddMetadataColumns/#apply","title":"Executing Rule","text":"Signature <pre><code>apply(\nplan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>\u00a0is part of the Rule abstraction.</p> <p><code>apply</code> adds metadata columns to logical operators (with metadata columns defined).</p>"},{"location":"logical-analysis-rules/AddMetadataColumns/#addMetadataCol","title":"addMetadataCol","text":"<pre><code>addMetadataCol(\nplan: LogicalPlan,\nrequiredAttrIds: Set[ExprId]): LogicalPlan\n</code></pre> <p><code>addMetadataCol</code>...FIXME</p>"},{"location":"logical-analysis-rules/AddMetadataColumns/#hasMetadataCol","title":"hasMetadataCol","text":"<pre><code>hasMetadataCol(\nplan: LogicalPlan): Boolean\n</code></pre> <p><code>hasMetadataCol</code> is positive (<code>true</code>) when there is at least one Attribute expression (among the Expressions of the given LogicalPlan) for which either holds true:</p> <ul> <li>It is a metadata column</li> <li>ExprId of this <code>Attribute</code> is among the metadata output attributes of any of the children of the given LogicalPlan</li> </ul>"},{"location":"logical-analysis-rules/AliasViewChild/","title":"AliasViewChild Logical Analysis Rule","text":"<p><code>AliasViewChild</code> is a logical analysis rule that &lt;&gt; with &lt;&gt; unary logical operators and adds &lt;&gt; logical operator (possibly with &lt;&gt; expressions) when the outputs of a view and the underlying table do not match (and therefore require aliasing and projection). <p><code>AliasViewChild</code> is part of the View once-executed batch in the standard batches of the Logical Analyzer.</p> <p><code>AliasViewChild</code> is simply a Catalyst rule for transforming logical plans, i.e. <code>Rule[LogicalPlan]</code>.</p> <p>[[conf]] [[creating-instance]] <code>AliasViewChild</code> takes a SQLConf when created.</p>"},{"location":"logical-analysis-rules/AliasViewChild/#source-scala","title":"[source, scala]","text":"<p>// Sample view for the demo // The order of the column names do not match // In View: name, id // In VALUES: id, name sql(\"\"\"   CREATE OR REPLACE VIEW v (name COMMENT 'First name only', id COMMENT 'Identifier') COMMENT 'Permanent view'   AS VALUES (1, 'Jacek'), (2, 'Agata') AS t1(id, name)   \"\"\")</p> <p>scala&gt; :type spark org.apache.spark.sql.SparkSession</p> <p>val q = spark.table(\"v\")</p> <p>val plan = q.queryExecution.logical scala&gt; println(plan.numberedTreeString) 00 'UnresolvedRelation <code>v</code></p> <p>// Resolve UnresolvedRelation first // since AliasViewChild work with View operators only import spark.sessionState.analyzer.ResolveRelations val resolvedPlan = ResolveRelations(plan) scala&gt; println(resolvedPlan.numberedTreeString) 00 SubqueryAlias v 01 +- View (<code>default</code>.<code>v</code>, [name#32,id#33]) 02    +- SubqueryAlias t1 03       +- LocalRelation [id#34, name#35]</p> <p>scala&gt; :type spark.sessionState.conf org.apache.spark.sql.internal.SQLConf</p> <p>import org.apache.spark.sql.catalyst.analysis.AliasViewChild val rule = AliasViewChild(spark.sessionState.conf)</p> <p>// Notice that resolvedPlan is used (not plan) val planAfterAliasViewChild = rule(resolvedPlan) scala&gt; println(planAfterAliasViewChild.numberedTreeString) 00 SubqueryAlias v 01 +- View (<code>default</code>.<code>v</code>, [name#32,id#33]) 02    +- Project [cast(id#34 as int) AS name#32, cast(name#35 as string) AS id#33] 03       +- SubqueryAlias t1 04          +- LocalRelation [id#34, name#35]</p> <p>=== [[apply]] Executing Rule -- <code>apply</code> Method</p>"},{"location":"logical-analysis-rules/AliasViewChild/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/AliasViewChild/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>"},{"location":"logical-analysis-rules/CTESubstitution/","title":"CTESubstitution Logical Analysis Rule","text":"<p><code>CTESubstitution</code> is a logical analysis rule that transforms a logical query plan with...FIXME</p> <p><code>CTESubstitution</code> is part of the Substitution fixed-point batch in the standard batches of the Logical Analyzer.</p> <p><code>CTESubstitution</code> is a Catalyst rule for transforming logical plans (<code>Rule[LogicalPlan]</code>).</p>"},{"location":"logical-analysis-rules/CTESubstitution/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code>\u00a0is part of the Rule abstraction.</p>","text":""},{"location":"logical-analysis-rules/CleanupAliases/","title":"CleanupAliases Logical Analysis Rule","text":"<p><code>CleanupAliases</code> is a logical analysis rule that &lt;&gt; with...FIXME <p><code>CleanupAliases</code> is part of the Cleanup fixed-point batch in the standard batches of the Logical Analyzer.</p> <p><code>CleanupAliases</code> is simply a Catalyst rule for transforming logical plans, i.e. <code>Rule[LogicalPlan]</code>.</p> <p>=== [[apply]] Executing Rule -- <code>apply</code> Method</p>"},{"location":"logical-analysis-rules/CleanupAliases/#source-scala","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/CleanupAliases/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>"},{"location":"logical-analysis-rules/DataSourceAnalysis/","title":"DataSourceAnalysis PostHoc Logical Resolution Rule","text":"<p><code>DataSourceAnalysis</code> is a post-hoc logical resolution rule that the default and Hive-specific logical query plan analyzers use to &lt;&gt;. <p>[[resolutions]] .DataSourceAnalysis's Logical Resolutions (Conversions) [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Source Operator | Target Operator | Description</p> <p>| &lt;&gt; [small]#(isDatasourceTable + no query)# | &lt;&gt; | [[CreateTable-no-query]] <p>| &lt;&gt; [small]#(isDatasourceTable + a resolved query)# | &lt;&gt; | [[CreateTable-query]] <p>| &lt;&gt; with InsertableRelation | &lt;&gt; | [[InsertIntoTable-InsertableRelation]] <p>| InsertIntoDir.md[InsertIntoDir] [small]#(non-hive provider)# | InsertIntoDataSourceDirCommand | [[InsertIntoDir]]</p> <p>| InsertIntoTable with HadoopFsRelation | InsertIntoHadoopFsRelationCommand | [[InsertIntoTable-HadoopFsRelation]] |===</p> <p>Technically, <code>DataSourceAnalysis</code> is a catalyst/Rule.md[Catalyst rule] for transforming spark-sql-LogicalPlan.md[logical plans], i.e. <code>Rule[LogicalPlan]</code>.</p>"},{"location":"logical-analysis-rules/DataSourceAnalysis/#source-scala","title":"[source, scala]","text":"<p>// FIXME Example of DataSourceAnalysis import org.apache.spark.sql.execution.datasources.DataSourceAnalysis val rule = DataSourceAnalysis(spark.sessionState.conf)</p> <p>val plan = FIXME</p>"},{"location":"logical-analysis-rules/DataSourceAnalysis/#ruleplan","title":"rule(plan)","text":"<p>=== [[apply]] Executing Rule</p>"},{"location":"logical-analysis-rules/DataSourceAnalysis/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/DataSourceAnalysis/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>"},{"location":"logical-analysis-rules/ExtractWindowExpressions/","title":"ExtractWindowExpressions Logical Resolution Rule","text":"<p><code>ExtractWindowExpressions</code> is a logical resolution rule that &lt;&gt; and replaces (extracts) &lt;&gt; expressions with &lt;&gt; logical operators. <p><code>ExtractWindowExpressions</code> is part of the Resolution fixed-point batch in the standard batches of the Analyzer.</p> <p><code>ExtractWindowExpressions</code> is simply a Catalyst rule for transforming logical plans, i.e. <code>Rule[LogicalPlan]</code>.</p> <pre><code>import spark.sessionState.analyzer.ExtractWindowExpressions\n\n// Example 1: Filter + Aggregate with WindowExpressions in aggregateExprs\nval q = ???\nval plan = q.queryExecution.logical\nval afterExtractWindowExpressions = ExtractWindowExpressions(plan)\n\n// Example 2: Aggregate with WindowExpressions in aggregateExprs\nval q = ???\nval plan = q.queryExecution.logical\nval afterExtractWindowExpressions = ExtractWindowExpressions(plan)\n\n// Example 3: Project with WindowExpressions in projectList\nval q = ???\nval plan = q.queryExecution.logical\nval afterExtractWindowExpressions = ExtractWindowExpressions(plan)\n</code></pre> <p>=== [[apply]] Executing Rule</p>"},{"location":"logical-analysis-rules/ExtractWindowExpressions/#source-scala","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ExtractWindowExpressions/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p><code>apply</code> transforms the logical operators downwards in the input &lt;&gt; as follows: <ul> <li> <p>For <code>Filter</code> unary operators with Aggregate operator that &lt;&gt; in the &lt;&gt;, <code>apply</code>...FIXME <li> <p>For &lt;&gt; logical operators that &lt;&gt; in the &lt;&gt;, <code>apply</code>...FIXME <li> <p>For &lt;&gt; logical operators that &lt;&gt; in the &lt;&gt;, <code>apply</code>...FIXME <p><code>apply</code> is part of the Rule abstraction.</p> <p>=== [[hasWindowFunction]] <code>hasWindowFunction</code> Internal Method</p>"},{"location":"logical-analysis-rules/ExtractWindowExpressions/#source-scala_1","title":"[source, scala]","text":"<p>hasWindowFunction(projectList: Seq[NamedExpression]): Boolean // &lt;1&gt; hasWindowFunction(expr: NamedExpression): Boolean</p> <p>&lt;1&gt; Executes the other <code>hasWindowFunction</code> on every <code>NamedExpression</code> in the <code>projectList</code></p> <p><code>hasWindowFunction</code> is positive (<code>true</code>) when the input <code>expr</code> &lt;&gt; is a &lt;&gt; expression. Otherwise, <code>hasWindowFunction</code> is negative (<code>false</code>). <p>NOTE: <code>hasWindowFunction</code> is used when <code>ExtractWindowExpressions</code> logical resolution rule is requested to &lt;&gt; and &lt;&gt;. <p>=== [[extract]] <code>extract</code> Internal Method</p>"},{"location":"logical-analysis-rules/ExtractWindowExpressions/#source-scala_2","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ExtractWindowExpressions/#extractexpressions-seqnamedexpression-seqnamedexpression-seqnamedexpression","title":"extract(expressions: Seq[NamedExpression]): (Seq[NamedExpression], Seq[NamedExpression])","text":"<p><code>extract</code>...FIXME</p> <p>NOTE: <code>extract</code> is used exclusively when <code>ExtractWindowExpressions</code> logical resolution rule is &lt;&gt;. <p>=== [[addWindow]] Adding Project and Window Logical Operators to Logical Plan -- <code>addWindow</code> Internal Method</p>"},{"location":"logical-analysis-rules/ExtractWindowExpressions/#source-scala_3","title":"[source, scala]","text":"<p>addWindow(   expressionsWithWindowFunctions: Seq[NamedExpression],   child: LogicalPlan): LogicalPlan</p> <p><code>addWindow</code> adds a &lt;&gt; logical operator with one or more &lt;&gt; logical operators (for every &lt;&gt; in the input &lt;&gt;) to the input &lt;&gt;. <p>Internally, <code>addWindow</code>...FIXME</p> <p>NOTE: <code>addWindow</code> is used exclusively when <code>ExtractWindowExpressions</code> logical resolution rule is &lt;&gt;."},{"location":"logical-analysis-rules/FindDataSourceTable/","title":"FindDataSourceTable Logical Evaluation Rule","text":"<p><code>FindDataSourceTable</code> is a catalyst/Rule.md[Catalyst rule] for &lt;&gt; (of Spark and Hive tables) in a logical query plan. <p><code>FindDataSourceTable</code> is part of additional rules in <code>Resolution</code> fixed-point batch of rules.</p> <p>[[sparkSession]][[creating-instance]] <code>FindDataSourceTable</code> takes a single SparkSession to be created.</p> <pre><code>scala&gt; :type spark\norg.apache.spark.sql.SparkSession\n\n// Example: InsertIntoTable with UnresolvedCatalogRelation\n// Drop tables to make the example reproducible\nval db = spark.catalog.currentDatabase\nSeq(\"t1\", \"t2\").foreach { t =&gt;\n  spark.sharedState.externalCatalog.dropTable(db, t, ignoreIfNotExists = true, purge = true)\n}\n\n// Create tables\nsql(\"CREATE TABLE t1 (id LONG) USING parquet\")\nsql(\"CREATE TABLE t2 (id LONG) USING orc\")\n\nimport org.apache.spark.sql.catalyst.dsl.plans._\nval plan = table(\"t1\").insertInto(tableName = \"t2\", overwrite = true)\nscala&gt; println(plan.numberedTreeString)\n00 'InsertIntoTable 'UnresolvedRelation `t2`, true, false\n01 +- 'UnresolvedRelation `t1`\n\n// Transform the logical plan with ResolveRelations logical rule first\n// so UnresolvedRelations become UnresolvedCatalogRelations\nimport spark.sessionState.analyzer.ResolveRelations\nval planWithUnresolvedCatalogRelations = ResolveRelations(plan)\nscala&gt; println(planWithUnresolvedCatalogRelations.numberedTreeString)\n00 'InsertIntoTable 'UnresolvedRelation `t2`, true, false\n01 +- 'SubqueryAlias t1\n02    +- 'UnresolvedCatalogRelation `default`.`t1`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n\n// Let's resolve UnresolvedCatalogRelations then\nimport org.apache.spark.sql.execution.datasources.FindDataSourceTable\nval r = new FindDataSourceTable(spark)\nval tablesResolvedPlan = r(planWithUnresolvedCatalogRelations)\n// FIXME Why is t2 not resolved?!\nscala&gt; println(tablesResolvedPlan.numberedTreeString)\n00 'InsertIntoTable 'UnresolvedRelation `t2`, true, false\n01 +- SubqueryAlias t1\n02    +- Relation[id#10L] parquet\n</code></pre>"},{"location":"logical-analysis-rules/FindDataSourceTable/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> resolves <code>UnresolvedCatalogRelation</code>s for Spark (Data Source) and Hive tables:</p> <ul> <li> <p><code>apply</code> creates HiveTableRelation logical operators for <code>UnresolvedCatalogRelation</code>s of Spark tables (incl. <code>InsertIntoTable</code>s)</p> </li> <li> <p><code>apply</code> creates LogicalRelation logical operators for <code>InsertIntoTable</code>s with <code>UnresolvedCatalogRelation</code> of a Hive table or <code>UnresolvedCatalogRelation</code>s of a Hive table</p> </li> </ul> <p><code>apply</code> is part of Rule contract.</p> <p>=== [[readHiveTable]] Creating HiveTableRelation Logical Operator -- <code>readHiveTable</code> Internal Method</p>","text":""},{"location":"logical-analysis-rules/FindDataSourceTable/#source-scala","title":"[source, scala] <p>readHiveTable(   table: CatalogTable): LogicalPlan</p>  <p><code>readHiveTable</code> creates a hive/HiveTableRelation.md[HiveTableRelation] for the input CatalogTable.</p> <p>NOTE: <code>readHiveTable</code> is used when <code>FindDataSourceTable</code> is requested to &lt;&gt; (for hive tables). <p>=== [[readDataSourceTable]] Creating LogicalRelation Logical Operator for CatalogTable -- <code>readDataSourceTable</code> Internal Method</p>","text":""},{"location":"logical-analysis-rules/FindDataSourceTable/#source-scala_1","title":"[source, scala] <p>readDataSourceTable(   table: CatalogTable): LogicalPlan</p>  <p><code>readDataSourceTable</code> requests the &lt;&gt; for SessionState.md#catalog[SessionCatalog]. <p><code>readDataSourceTable</code> requests the <code>SessionCatalog</code> for the cached logical plan for the input CatalogTable.</p> <p>If not available, <code>readDataSourceTable</code> creates a new DataSource for the provider (of the input <code>CatalogTable</code>) with the extra <code>path</code> option (based on the <code>locationUri</code> of the storage of the input <code>CatalogTable</code>). <code>readDataSourceTable</code> requests the <code>DataSource</code> to resolve the relation and create a corresponding BaseRelation that is then used to create a LogicalRelation with the input CatalogTable.</p> <p>NOTE: <code>readDataSourceTable</code> is used when <code>FindDataSourceTable</code> is requested to &lt;&gt; (for data source tables).","text":""},{"location":"logical-analysis-rules/HandleNullInputsForUDF/","title":"HandleNullInputsForUDF Logical Evaluation Rule","text":"<p><code>HandleNullInputsForUDF</code> is a logical evaluation rule (i.e. <code>Rule[LogicalPlan]</code>) that Logical Analyzer uses to...FIXME</p>"},{"location":"logical-analysis-rules/LookupFunctions/","title":"LookupFunctions Logical Rule","text":"<p><code>LookupFunctions</code> is a logical rule that the logical query plan analyzer uses to make sure that UnresolvedFunction expressions can be resolved in a logical query plan.</p> <p>ResolveFunctions Logical Resolution Rule</p> <p><code>LookupFunctions</code> is similar to ResolveFunctions logical resolution rule, but it is <code>ResolveFunctions</code> to resolve <code>UnresolvedFunction</code> expressions while <code>LookupFunctions</code> is just a sanity check that a future resolution is possible if tried.</p> <p><code>LookupFunctions</code> is a Catalyst rule for transforming logical plans (<code>Rule[LogicalPlan]</code>).</p> <p>Note</p> <p><code>LookupFunctions</code> does not transform a logical plan, but simply assert that a query plan is valid with regards to functions used.</p> <p><code>LookupFunctions</code> is part of Simple Sanity Check one-off batch of rules.</p>"},{"location":"logical-analysis-rules/LookupFunctions/#demo","title":"Demo","text":"<p>Let's use Catalyst DSL to create a logical plan with an unregistered function.</p> <pre><code>import org.apache.spark.sql.catalyst.dsl.plans._\nval t1 = table(\"t1\")\n\nimport org.apache.spark.sql.catalyst.dsl.expressions._\nval f1 = 'f1.function()\n\nval plan = t1.select(f1)\n</code></pre> <pre><code>scala&gt; println(plan.numberedTreeString)\n00 'Project [unresolvedalias('f1(), None)]\n01 +- 'UnresolvedRelation `t1`\n</code></pre> <p>Make sure the function <code>f1</code> does not exist.</p> <pre><code>import org.apache.spark.sql.catalyst.FunctionIdentifier\nspark.sessionState.catalog.dropFunction(FunctionIdentifier(\"f1\"), ignoreIfNotExists = true)\n\nassert(spark.catalog.functionExists(\"f1\") == false)\n</code></pre> <p>Executing <code>LookupFunctions</code> rule should end up with an <code>AnalysisException</code> since there is a function (<code>f1</code>) in the query plan that is not available.</p> <pre><code>import spark.sessionState.analyzer.LookupFunctions\nscala&gt; LookupFunctions.apply(plan)\norg.apache.spark.sql.AnalysisException: Undefined function: 'f1'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.;\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.$anonfun$applyOrElse$100(Analyzer.scala:1893)\n  at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1893)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1884)\n  ...\n</code></pre>"},{"location":"logical-analysis-rules/LookupFunctions/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> finds all UnresolvedFunction expressions (in every logical operator in the input logical plan) and requests the SessionCatalog to check if their functions exist.</p> <p><code>apply</code> does nothing when a function exists, and reports a <code>NoSuchFunctionException</code> otherwise (that fails logical analysis).</p> <pre><code>Undefined function: '[func]'. This function is neither a registered temporary function nor a permanent function registered in the database '[db]'.\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-analysis-rules/PreWriteCheck/","title":"PreWriteCheck Extended Analysis Check","text":"<p><code>PreWriteCheck</code> is an extended analysis check that verifies correctness of a &lt;&gt; with regard to &lt;&gt; unary logical operator (right before analysis can be considered complete). <p><code>PreWriteCheck</code> is part of the &lt;&gt; of the logical Analyzer in BaseSessionStateBuilder and HiveSessionStateBuilder. <p><code>PreWriteCheck</code> is simply a &lt;&gt; of &lt;&gt; that...FIXME <p>=== [[apply]] Executing Function -- <code>apply</code> Method</p>"},{"location":"logical-analysis-rules/PreWriteCheck/#source-scala","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/PreWriteCheck/#applyplan-logicalplan-unit","title":"apply(plan: LogicalPlan): Unit","text":"<p>NOTE: <code>apply</code> is part of Scala's https://www.scala-lang.org/api/2.11.12/index.html#scala.Function1[scala.Function1] contract to create a function of one parameter.</p> <p><code>apply</code> traverses the input &lt;&gt; and finds &lt;&gt; unary logical operators. <ul> <li> <p>[[apply-InsertableRelation]] For an <code>InsertIntoTable</code> with a &lt;&gt;...FIXME <li> <p>For any <code>InsertIntoTable</code>, <code>apply</code> throws a <code>AnalysisException</code> if the &lt;&gt; is neither a &lt;&gt; nor one of the following leaf logical operators: <code>Range</code>, <code>OneRowRelation</code>, &lt;&gt;. + <pre><code>Inserting into an RDD-based table is not allowed.\n</code></pre>"},{"location":"logical-analysis-rules/PreprocessTableCreation/","title":"PreprocessTableCreation PostHoc Logical Resolution Rule","text":"<p><code>PreprocessTableCreation</code> is a posthoc logical resolution rule that &lt;&gt; with &lt;&gt; logical operators. <p><code>PreprocessTableCreation</code> is part of the Post-Hoc Resolution once-executed batch of the Hive-specific and the default logical analyzers.</p> <p><code>PreprocessTableCreation</code> is simply a Catalyst rule for transforming logical plans, i.e. <code>Rule[LogicalPlan]</code>.</p> <p>[[sparkSession]] [[creating-instance]] <code>PreprocessTableCreation</code> takes a &lt;&gt; when created. <p>=== [[apply]] Executing Rule -- <code>apply</code> Method</p>"},{"location":"logical-analysis-rules/PreprocessTableCreation/#source-scala","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/PreprocessTableCreation/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>"},{"location":"logical-analysis-rules/RemoveAllHints/","title":"RemoveAllHints","text":"<p><code>RemoveAllHints</code> is...FIXME</p>"},{"location":"logical-analysis-rules/ResolveAggregateFunctions/","title":"ResolveAggregateFunctions Logical Analysis Rule","text":"<p><code>ResolveAggregateFunctions</code> is a logical rule (<code>Rule[LogicalPlan]</code>).</p> <p><code>ResolveAggregateFunctions</code> is part of Resolution batch of Logical Analyzer.</p>"},{"location":"logical-analysis-rules/ResolveAggregateFunctions/#creating-instance","title":"Creating Instance","text":"<p><code>ResolveAggregateFunctions</code> takes no arguments to be created.</p> <p><code>ResolveAggregateFunctions</code> is created when:</p> <ul> <li><code>Analyzer</code> is requested for batches</li> </ul>"},{"location":"logical-analysis-rules/ResolveAggregateFunctions/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> resolves the following operators in the input LogicalPlan:</p> <ul> <li>UnresolvedHaving with Aggregate resolved</li> <li><code>Filter</code> with Aggregate resolved</li> <li>Sort with Aggregate resolved</li> </ul>","text":""},{"location":"logical-analysis-rules/ResolveAliases/","title":"ResolveAliases Logical Resolution Rule","text":"<p><code>ResolveAliases</code> is a logical resolution rule that the logical query plan analyzer uses to &lt;&gt; in an entire logical query plan. <p>Technically, <code>ResolveAliases</code> is just a catalyst/Rule.md[Catalyst rule] for transforming spark-sql-LogicalPlan.md[logical plans], i.e. <code>Rule[LogicalPlan]</code>.</p> <p><code>ResolveAliases</code> is part of Resolution fixed-point batch of rules.</p>"},{"location":"logical-analysis-rules/ResolveAliases/#example","title":"Example","text":"<pre><code>import spark.sessionState.analyzer.ResolveAliases\n\n// FIXME Using ResolveAliases rule\n</code></pre> <p>=== [[apply]] Applying ResolveAliases to Logical Plan -- <code>apply</code> Method</p>"},{"location":"logical-analysis-rules/ResolveAliases/#source-scala","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ResolveAliases/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p>NOTE: <code>apply</code> is part of catalyst/Rule.md#apply[Rule Contract] to apply a rule to a spark-sql-LogicalPlan.md[logical plan].</p> <p><code>apply</code>...FIXME</p> <p>=== [[assignAliases]] <code>assignAliases</code> Internal Method</p>"},{"location":"logical-analysis-rules/ResolveAliases/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ResolveAliases/#assignaliasesexprs-seqnamedexpression-seqnamedexpression","title":"assignAliases(exprs: Seq[NamedExpression]): Seq[NamedExpression]","text":"<p><code>assignAliases</code>...FIXME</p> <p>NOTE: <code>assignAliases</code> is used when...FIXME</p>"},{"location":"logical-analysis-rules/ResolveCatalogs/","title":"ResolveCatalogs Logical Resolution Rule","text":"<p><code>ResolveCatalogs</code> is a logical rule (<code>Rule[LogicalPlan]</code>).</p> <p><code>ResolveCatalogs</code> is part of Resolution batch of Logical Analyzer.</p>"},{"location":"logical-analysis-rules/ResolveCatalogs/#creating-instance","title":"Creating Instance","text":"<p><code>ResolveCatalogs</code> takes the following to be created:</p> <ul> <li> CatalogManager <p><code>ResolveCatalogs</code> is created when:</p> <ul> <li><code>Analyzer</code> is requested for batches</li> </ul>"},{"location":"logical-analysis-rules/ResolveCatalogs/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p>  <p><code>apply</code>...FIXME</p>","text":""},{"location":"logical-analysis-rules/ResolveCoalesceHints/","title":"ResolveCoalesceHints Logical Resolution Rule","text":"<p><code>ResolveCoalesceHints</code> is a logical resolution rule to resolve UnresolvedHint logical operators.</p> Hint Name Arguments Logical Operator <code>COALESCE</code> Number of partitions Repartition (with <code>shuffle</code> off / <code>false</code>) <code>REBALANCE</code> RebalancePartitions <code>REPARTITION</code> Number of partitions alone or like <code>REPARTITION_BY_RANGE</code> Repartition (with <code>shuffle</code> on / <code>true</code>) <code>REPARTITION_BY_RANGE</code> Column names with an optional number of partitions (default: spark.sql.shuffle.partitions configuration property) RepartitionByExpression <p><code>ResolveCoalesceHints</code> is a Catalyst rule for transforming logical plans (<code>Rule[LogicalPlan]</code>).</p> <p><code>ResolveCoalesceHints</code> is part of Hints batch of rules of Logical Analyzer.</p>"},{"location":"logical-analysis-rules/ResolveCoalesceHints/#creating-instance","title":"Creating Instance","text":"<p><code>ResolveCoalesceHints</code> takes the following to be created:</p> <ul> <li> SQLConf <p><code>ResolveCoalesceHints</code> is created when Logical Analyzer is requested for the batches of rules.</p>"},{"location":"logical-analysis-rules/ResolveCoalesceHints/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> resolves UnresolvedHint logical operators with the following hint names (case-insensitive).</p>    Hint Name Trigger     <code>COALESCE</code> createRepartition (with <code>shuffle</code> off)   <code>REBALANCE</code> createRebalance   <code>REPARTITION</code> createRepartition (with <code>shuffle</code> on)   <code>REPARTITION_BY_RANGE</code> createRepartitionByRange    <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-analysis-rules/ResolveCoalesceHints/#createrebalance","title":"createRebalance <pre><code>createRebalance(\n  hint: UnresolvedHint): LogicalPlan\n</code></pre> <p><code>createRebalance</code> handles a <code>REBALANCE</code> hint and creates a Repartition logical operator.</p>","text":""},{"location":"logical-analysis-rules/ResolveCoalesceHints/#createrepartition","title":"createRepartition <pre><code>createRepartition(\n  shuffle: Boolean,\n  hint: UnresolvedHint): LogicalPlan\n</code></pre> <p><code>createRepartition</code> handles <code>COALESCE</code> and <code>REPARTITION</code> hints (and creates Repartition or RepartitionByExpression logical operators).</p>","text":""},{"location":"logical-analysis-rules/ResolveCoalesceHints/#createrepartitionbyrange","title":"createRepartitionByRange <pre><code>createRepartitionByRange(\n  hint: UnresolvedHint): RepartitionByExpression\n</code></pre> <p><code>createRepartitionByRange</code> creates a RepartitionByExpression logical operator.</p>","text":""},{"location":"logical-analysis-rules/ResolveCoalesceHints/#examples","title":"Examples","text":""},{"location":"logical-analysis-rules/ResolveCoalesceHints/#using-coalesce-hint","title":"Using COALESCE Hint <pre><code>// Use Catalyst DSL to create a logical plan\nimport org.apache.spark.sql.catalyst.dsl.plans._\nval plan = table(\"t1\").hint(name = \"COALESCE\", 3)\nscala&gt; println(plan.numberedTreeString)\n00 'UnresolvedHint COALESCE, [3]\n01 +- 'UnresolvedRelation `t1`\n\nimport org.apache.spark.sql.catalyst.analysis.ResolveHints.ResolveCoalesceHints\nval analyzedPlan = ResolveCoalesceHints(plan)\nscala&gt; println(analyzedPlan.numberedTreeString)\n00 'Repartition 3, false\n01 +- 'UnresolvedRelation `t1`\n</code></pre>","text":""},{"location":"logical-analysis-rules/ResolveCoalesceHints/#using-repartition-hint","title":"Using REPARTITION Hint <pre><code>// Use Catalyst DSL to create a logical plan\nimport org.apache.spark.sql.catalyst.dsl.plans._\nval plan = table(\"t1\").hint(name = \"REPARTITION\", 3)\nscala&gt; println(plan.numberedTreeString)\n00 'UnresolvedHint REPARTITION, [3]\n01 +- 'UnresolvedRelation `t1`\n\nimport org.apache.spark.sql.catalyst.analysis.ResolveHints.ResolveCoalesceHints\nval analyzedPlan = ResolveCoalesceHints(plan)\nscala&gt; println(analyzedPlan.numberedTreeString)\n00 'Repartition 3, true\n01 +- 'UnresolvedRelation `t1`\n</code></pre>","text":""},{"location":"logical-analysis-rules/ResolveCoalesceHints/#using-coalesce-hint-in-sql","title":"Using COALESCE Hint in SQL <pre><code>val q = sql(\"SELECT /*+ COALESCE(10) */ * FROM VALUES 1 t(id)\")\nval plan = q.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'UnresolvedHint COALESCE, [10]\n01 +- 'Project [*]\n02    +- 'SubqueryAlias `t`\n03       +- 'UnresolvedInlineTable [id], [List(1)]\n\nimport org.apache.spark.sql.catalyst.analysis.ResolveHints.ResolveCoalesceHints\nval analyzedPlan = ResolveCoalesceHints(plan)\nscala&gt; println(analyzedPlan.numberedTreeString)\n00 'Repartition 10, false\n01 +- 'Project [*]\n02    +- 'SubqueryAlias `t`\n03       +- 'UnresolvedInlineTable [id], [List(1)]\n</code></pre>","text":""},{"location":"logical-analysis-rules/ResolveCreateNamedStruct/","title":"ResolveCreateNamedStruct Logical Resolution Rule -- Resolving NamePlaceholders In CreateNamedStruct Expressions","text":"<p><code>ResolveCreateNamedStruct</code> is a logical resolution rule that &lt;&gt; in an entire logical query plan. <p><code>ResolveCreateNamedStruct</code> is part of the Resolution fixed-point batch in the standard batches of the Analyzer.</p> <p><code>ResolveCreateNamedStruct</code> is simply a Catalyst rule for transforming logical plans, i.e. <code>Rule[LogicalPlan]</code>.</p> <pre><code>scala&gt; :type spark\norg.apache.spark.sql.SparkSession\n\nval q = spark.range(1).select(struct($\"id\"))\nval logicalPlan = q.queryExecution.logical\nscala&gt; println(logicalPlan.numberedTreeString)\n00 'Project [unresolvedalias(named_struct(NamePlaceholder, 'id), None)]\n01 +- AnalysisBarrier\n02       +- Range (0, 1, step=1, splits=Some(8))\n\n// Let's resolve references first\nimport spark.sessionState.analyzer.ResolveReferences\nval planWithRefsResolved = ResolveReferences(logicalPlan)\n\nimport org.apache.spark.sql.catalyst.analysis.ResolveCreateNamedStruct\nval afterResolveCreateNamedStruct = ResolveCreateNamedStruct(planWithRefsResolved)\nscala&gt; println(afterResolveCreateNamedStruct.numberedTreeString)\n00 'Project [unresolvedalias(named_struct(id, id#4L), None)]\n01 +- AnalysisBarrier\n02       +- Range (0, 1, step=1, splits=Some(8))\n</code></pre> <p>=== [[apply]] Executing Rule</p>"},{"location":"logical-analysis-rules/ResolveCreateNamedStruct/#source-scala","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ResolveCreateNamedStruct/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p><code>apply</code> &lt;&gt; (in the input &lt;&gt;) that are &lt;&gt; expressions which are not &lt;&gt; yet and replaces <code>NamePlaceholders</code> with &lt;&gt; expressions. <p>In other words, <code>apply</code> finds unresolved &lt;&gt; expressions with <code>NamePlaceholder</code> expressions in the &lt;&gt; and replaces them with the &lt;&gt; of corresponding &lt;&gt;, but only if the <code>NamedExpression</code> is resolved. <p>In the end, <code>apply</code> creates a &lt;&gt; with new children. <p><code>apply</code> is part of the Rule abstraction.</p>"},{"location":"logical-analysis-rules/ResolveDefaultColumns/","title":"ResolveDefaultColumns Logical Resolution Rule","text":"<p><code>ResolveDefaultColumns</code> is...FIXME</p>"},{"location":"logical-analysis-rules/ResolveFunctions/","title":"ResolveFunctions Logical Resolution Rule","text":"<p><code>ResolveFunctions</code> is a logical resolution rule that the logical query plan analyzer uses to resolve the following logical operators and expressions in a logical query plan:</p> Logical Operator Expression <code>UnresolvedFunctionName</code> UnresolvedFunction UnresolvedTableValuedFunction UnresolvedGenerator <code>UnresolvedTVFAliases</code> <p><code>ResolveReferences</code> is a Catalyst rule for transforming logical plans (<code>Rule[LogicalPlan]</code>).</p> <p><code>ResolveFunctions</code> is part of Resolution fixed-point batch of rules.</p>"},{"location":"logical-analysis-rules/ResolveFunctions/#executing-rule","title":"Executing Rule  Signature <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p>   <p><code>apply</code> resolves logical operators up (post-order, bottom-up) in trees with the following TreePatterns:</p> <ul> <li><code>GENERATOR</code></li> <li><code>UNRESOLVED_FUNC</code></li> <li><code>UNRESOLVED_FUNCTION</code></li> <li><code>UNRESOLVED_TABLE_VALUED_FUNCTION</code></li> <li><code>UNRESOLVED_TVF_ALIASES</code></li> </ul>","text":""},{"location":"logical-analysis-rules/ResolveFunctions/#unresolvedfunctionname","title":"UnresolvedFunctionName <p>For an <code>UnresolvedFunctionName</code>, <code>apply</code>...FIXME</p>","text":""},{"location":"logical-analysis-rules/ResolveFunctions/#unresolvedtablevaluedfunction","title":"UnresolvedTableValuedFunction <p>For an UnresolvedTableValuedFunction, <code>apply</code> first attempts to resolveBuiltinOrTempTableFunction, if available.</p> <p>Otherwise, <code>apply</code> attempts to resolve a CatalogPlugin and, for the spark_catalog only, requests the SessionCatalog to resolvePersistentTableFunction.</p>","text":""},{"location":"logical-analysis-rules/ResolveFunctions/#others","title":"Others <p><code>apply</code>...FIXME</p>","text":""},{"location":"logical-analysis-rules/ResolveFunctions/#resolvebuiltinortemptablefunction","title":"resolveBuiltinOrTempTableFunction <pre><code>resolveBuiltinOrTempTableFunction(\n  name: Seq[String],\n  arguments: Seq[Expression]): Option[LogicalPlan]\n</code></pre> <p>Only for a one-element <code>name</code>, <code>resolveBuiltinOrTempTableFunction</code> requests the SessionCatalog to resolveBuiltinOrTempTableFunction.</p> <p>Otherwise, <code>resolveBuiltinOrTempTableFunction</code> returns <code>None</code> (an undefined value).</p>","text":""},{"location":"logical-analysis-rules/ResolveGroupingAnalytics/","title":"ResolveGroupingAnalytics Logical Resolution Rule","text":"<p><code>ResolveGroupingAnalytics</code> is a logical rule (<code>Rule[LogicalPlan]</code>).</p> <p><code>ResolveGroupingAnalytics</code> is part of Resolution batch of Logical Analyzer.</p>"},{"location":"logical-analysis-rules/ResolveGroupingAnalytics/#creating-instance","title":"Creating Instance","text":"<p><code>ResolveGroupingAnalytics</code> takes no arguments to be created.</p> <p><code>ResolveGroupingAnalytics</code> is created when:</p> <ul> <li><code>Analyzer</code> is requested for batches</li> </ul>"},{"location":"logical-analysis-rules/ResolveGroupingAnalytics/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> resolves the following operators in the input LogicalPlan:</p> <ul> <li>UnresolvedHaving with Aggregate with <code>Cube</code></li> <li>UnresolvedHaving with Aggregate with <code>Rollup</code></li> <li>UnresolvedHaving with GroupingSets</li> <li>Aggregate with <code>Cube</code></li> <li>Aggregate with <code>Rollup</code></li> <li>GroupingSets</li> <li><code>Filter</code> with <code>Grouping</code> or <code>GroupingID</code> expressions</li> <li>Sort with <code>Grouping</code> or <code>GroupingID</code> expressions</li> </ul>","text":""},{"location":"logical-analysis-rules/ResolveGroupingAnalytics/#tryresolvehavingcondition","title":"tryResolveHavingCondition <pre><code>tryResolveHavingCondition(\n  h: UnresolvedHaving): LogicalPlan\n</code></pre> <p><code>tryResolveHavingCondition</code>...FIXME</p>","text":""},{"location":"logical-analysis-rules/ResolveInlineTables/","title":"ResolveInlineTables Logical Resolution Rule","text":"<p><code>ResolveInlineTables</code> is a logical resolution rule that &lt;&gt; in a logical query plan. <p><code>ResolveInlineTables</code> is part of the Resolution fixed-point batch in the standard batches of the Analyzer.</p> <p><code>ResolveInlineTables</code> is a Catalyst rule for transforming logical plans, i.e. <code>Rule[LogicalPlan]</code>.</p> <p>[[conf]] [[creating-instance]] <code>ResolveInlineTables</code> takes a SQLConf when created.</p>"},{"location":"logical-analysis-rules/ResolveInlineTables/#example","title":"Example","text":"<pre><code>val q = sql(\"VALUES 1, 2, 3\")\nval plan = q.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'UnresolvedInlineTable [col1], [List(1), List(2), List(3)]\n\nscala&gt; :type spark\norg.apache.spark.sql.SparkSession\n\nscala&gt; :type spark.sessionState.conf\norg.apache.spark.sql.internal.SQLConf\n\nimport org.apache.spark.sql.catalyst.analysis.ResolveInlineTables\nval rule = ResolveInlineTables(spark.sessionState.conf)\n\nval planAfterResolveInlineTables = rule(plan)\nscala&gt; println(planAfterResolveInlineTables.numberedTreeString)\n00 LocalRelation [col1#2]\n</code></pre>"},{"location":"logical-analysis-rules/ResolveInlineTables/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> simply searches the input plan upwards to find <code>UnresolvedInlineTable</code> logical operators with rows expressions resolved.</p> <p>For such a <code>UnresolvedInlineTable</code> logical operator, <code>apply</code> validateInputDimension and validateInputEvaluable.</p> <p>In the end, <code>apply</code> converts the UnresolvedInlineTable to a LocalRelation.</p> <p><code>apply</code> is part of the Rule abstraction.</p> <p>=== [[validateInputDimension]] <code>validateInputDimension</code> Internal Method</p>","text":""},{"location":"logical-analysis-rules/ResolveInlineTables/#source-scala","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ResolveInlineTables/#validateinputdimensiontable-unresolvedinlinetable-unit","title":"validateInputDimension(table: UnresolvedInlineTable): Unit <p><code>validateInputDimension</code>...FIXME</p> <p>NOTE: <code>validateInputDimension</code> is used exclusively when <code>ResolveInlineTables</code> logical resolution rule is &lt;&gt;. <p>=== [[validateInputEvaluable]] <code>validateInputEvaluable</code> Internal Method</p>","text":""},{"location":"logical-analysis-rules/ResolveInlineTables/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ResolveInlineTables/#validateinputevaluabletable-unresolvedinlinetable-unit","title":"validateInputEvaluable(table: UnresolvedInlineTable): Unit <p><code>validateInputEvaluable</code>...FIXME</p> <p>NOTE: <code>validateInputEvaluable</code> is used exclusively when <code>ResolveInlineTables</code> logical resolution rule is &lt;&gt;. <p>=== [[convert]] Converting UnresolvedInlineTable to LocalRelation -- <code>convert</code> Internal Method</p>","text":""},{"location":"logical-analysis-rules/ResolveInlineTables/#source-scala_2","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ResolveInlineTables/#converttable-unresolvedinlinetable-localrelation","title":"convert(table: UnresolvedInlineTable): LocalRelation <p><code>convert</code>...FIXME</p> <p>NOTE: <code>convert</code> is used exclusively when <code>ResolveInlineTables</code> logical resolution rule is &lt;&gt;.","text":""},{"location":"logical-analysis-rules/ResolveInsertInto/","title":"ResolveInsertInto Logical Resolution Rule","text":"<p><code>ResolveInsertInto</code> is a logical resolution rule (<code>Rule[LogicalPlan]</code>).</p> <p><code>ResolveInsertInto</code> is part of Resolution batch of Logical Analyzer.</p>"},{"location":"logical-analysis-rules/ResolveInsertInto/#creating-instance","title":"Creating Instance","text":"<p><code>ResolveInsertInto</code> takes no parameters to be created.</p> <p><code>ResolveInsertInto</code> is created when <code>Analyzer</code> is requested for the batches.</p>"},{"location":"logical-analysis-rules/ResolveInsertInto/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> resolves InsertIntoStatement logical operators with DataSourceV2Relation tables to the following operators:</p> <ul> <li>AppendData</li> <li>OverwritePartitionsDynamic</li> <li>OverwriteByExpression</li> </ul> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-analysis-rules/ResolveJoinStrategyHints/","title":"ResolveJoinStrategyHints Logical Resolution Rule","text":"<p><code>ResolveJoinStrategyHints</code> is a logical resolution rule to resolve UnresolvedHint logical operators with JoinStrategyHints.</p> <p><code>ResolveJoinStrategyHints</code> is a Catalyst rule for transforming logical plans (<code>Rule[LogicalPlan]</code>).</p> <p><code>ResolveJoinStrategyHints</code> is part of Hints batch of rules of Logical Analyzer.</p>"},{"location":"logical-analysis-rules/ResolveJoinStrategyHints/#creating-instance","title":"Creating Instance","text":"<p><code>ResolveJoinStrategyHints</code> takes the following to be created:</p> <ul> <li> SQLConf <p><code>ResolveJoinStrategyHints</code> is created when:</p> <ul> <li>Logical Analyzer is requested for the batches of rules</li> </ul>"},{"location":"logical-analysis-rules/ResolveJoinStrategyHints/#hint-names","title":"Hint Names <p><code>ResolveJoinStrategyHints</code> takes the hintAliases of the strategies when created.</p> <p>The hint aliases are the only hints (of UnresolvedHints) that are going to be resolved when <code>ResolveJoinStrategyHints</code> is executed.</p>","text":""},{"location":"logical-analysis-rules/ResolveJoinStrategyHints/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> works with LogicalPlans that contain the UNRESOLVED_HINT tree pattern (that happens to be UnresolvedHints only).</p>  <p><code>apply</code> traverses the given logical query plan to find UnresolvedHints with names that are among the supported hint names.</p> <p>For <code>UnresolvedHint</code>s with no parameters, <code>apply</code> creates a ResolvedHint (with a HintInfo with the corresponding JoinStrategyHint).</p> <p>For <code>UnresolvedHint</code>s with parameters, <code>apply</code> accepts two types of parameters:</p> <ul> <li>Table Name (as a <code>String</code>)</li> <li>Table Identifier (as a UnresolvedAttribute)</li> </ul> <p><code>apply</code> applyJoinStrategyHint to create a ResolvedHint.</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-analysis-rules/ResolveJoinStrategyHints/#applyjoinstrategyhint","title":"applyJoinStrategyHint <pre><code>applyJoinStrategyHint(\n  plan: LogicalPlan,\n  relationsInHint: Set[Seq[String]],\n  relationsInHintWithMatch: mutable.HashSet[Seq[String]],\n  hintName: String): LogicalPlan\n</code></pre> <p><code>applyJoinStrategyHint</code>...FIXME</p>","text":""},{"location":"logical-analysis-rules/ResolveJoinStrategyHints/#demo","title":"Demo  <p>Fixme</p> <p>Review the example to use <code>ResolveJoinStrategyHints</code> and other hints</p>  <pre><code>// Use Catalyst DSL to create a logical plan\nimport org.apache.spark.sql.catalyst.dsl.plans._\nval plan = table(\"t1\").join(table(\"t2\")).hint(name = \"broadcast\", \"t1\", \"table2\")\n</code></pre> <pre><code>scala&gt; println(plan.numberedTreeString)\n00 'UnresolvedHint broadcast, [t1, table2]\n01 +- 'Join Inner\n02    :- 'UnresolvedRelation [t1], [], false\n03    +- 'UnresolvedRelation [t2], [], false\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.analysis.ResolveHints.ResolveJoinStrategyHints\nval analyzedPlan = ResolveJoinStrategyHints(plan)\n</code></pre> <pre><code>scala&gt; println(analyzedPlan.numberedTreeString)\n00 'Join Inner\n01 :- 'ResolvedHint (strategy=broadcast)\n02 :  +- 'UnresolvedRelation [t1], [], false\n03 +- 'UnresolvedRelation [t2], [], false\n</code></pre>","text":""},{"location":"logical-analysis-rules/ResolveMissingReferences/","title":"ResolveMissingReferences","text":"<p><code>ResolveMissingReferences</code> is...FIXME</p> <p>=== [[resolveExprsAndAddMissingAttrs]] <code>resolveExprsAndAddMissingAttrs</code> Internal Method</p>"},{"location":"logical-analysis-rules/ResolveMissingReferences/#source-scala","title":"[source, scala]","text":"<p>resolveExprsAndAddMissingAttrs(   exprs: Seq[Expression],   plan: LogicalPlan): (Seq[Expression], LogicalPlan)</p> <p><code>resolveExprsAndAddMissingAttrs</code>...FIXME</p> <p>NOTE: <code>resolveExprsAndAddMissingAttrs</code> is used when...FIXME</p>"},{"location":"logical-analysis-rules/ResolveOrdinalInOrderByAndGroupBy/","title":"ResolveOrdinalInOrderByAndGroupBy Logical Resolution Rule","text":"<p><code>ResolveOrdinalInOrderByAndGroupBy</code> is a logical resolution rule that &lt;&gt;  in a logical query plan. <p><code>ResolveOrdinalInOrderByAndGroupBy</code> is part of the Resolution fixed-point batch in the standard batches of the Analyzer.</p> <p><code>ResolveOrdinalInOrderByAndGroupBy</code> is a Catalyst rule for transforming logical plans, i.e. <code>Rule[LogicalPlan]</code>.</p> <p>[[creating-instance]] <code>ResolveOrdinalInOrderByAndGroupBy</code> takes no arguments when created.</p>"},{"location":"logical-analysis-rules/ResolveOrdinalInOrderByAndGroupBy/#source-scala","title":"[source, scala]","text":"<p>// FIXME: DEMO val rule = spark.sessionState.analyzer.ResolveOrdinalInOrderByAndGroupBy</p> <p>val plan = ??? val planResolved = rule(plan) scala&gt; println(planResolved.numberedTreeString) 00 'UnresolvedRelation <code>t1</code></p> <p>=== [[apply]] Executing Rule -- <code>apply</code> Method</p>"},{"location":"logical-analysis-rules/ResolveOrdinalInOrderByAndGroupBy/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ResolveOrdinalInOrderByAndGroupBy/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p><code>apply</code> walks the logical plan from children up the tree and looks for &lt;&gt; and &lt;&gt; logical operators with &lt;&gt; leaf expressions (in &lt;&gt; and &lt;&gt; expressions, respectively). <p>For a &lt;&gt; logical operator with &lt;&gt; expressions, <code>apply</code> replaces all the &lt;&gt; expressions (with &lt;&gt; child expressions) with <code>SortOrder</code> expressions and the expression at the <code>index - 1</code> position in the output schema of the &lt;&gt; logical operator. <p>For a &lt;&gt; logical operator with &lt;&gt; expressions, <code>apply</code> replaces all the expressions (with &lt;&gt; child expressions) with the expression at the <code>index - 1</code> position in the &lt;&gt; of the current <code>Aggregate</code> logical operator. <p><code>apply</code> throws a <code>AnalysisException</code> (and hence fails an analysis) if the ordinal is outside the range:</p> <pre><code>ORDER BY position [index] is not in select list (valid range is [1, [output.size]])\nGROUP BY position [index] is not in select list (valid range is [1, [aggs.size]])\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p>"},{"location":"logical-analysis-rules/ResolveOutputRelation/","title":"ResolveOutputRelation Logical Resolution Rule","text":"<p><code>ResolveOutputRelation</code> is...FIXME</p>"},{"location":"logical-analysis-rules/ResolveReferences/","title":"ResolveReferences Logical Resolution Rule","text":"<p><code>ResolveReferences</code> is a logical resolution rule that the logical query plan analyzer uses to &lt;&gt; in a logical query plan, i.e. <ol> <li>Resolves...FIXME</li> </ol> <p>Technically, <code>ResolveReferences</code> is just a catalyst/Rule.md[Catalyst rule] for transforming spark-sql-LogicalPlan.md[logical plans], i.e. <code>Rule[LogicalPlan]</code>.</p> <p><code>ResolveReferences</code> is part of Resolution fixed-point batch of rules.</p> <p>[[example]] [source, scala]</p> <p>// FIXME: Use Catalyst DSL to create a logical plan import org.apache.spark.sql.catalyst.dsl.plans._ val t1 = table(\"t1\")</p> <p>import org.apache.spark.sql.catalyst.dsl.expressions._ val logicalPlan = t1   .select(\"a\".attr, star())   .groupBy(groupingExprs = \"b\".attr)(aggregateExprs = star()) scala&gt; println(logicalPlan.numberedTreeString) 00 'Aggregate ['b], [*] 01 +- 'Project ['a, *] 02    +- 'UnresolvedRelation <code>t1</code> // END FIXME</p> <p>// Make the example repeatable val table = \"t1\" import org.apache.spark.sql.catalyst.TableIdentifier spark.sessionState.catalog.dropTable(TableIdentifier(table), ignoreIfNotExists = true, purge = true)</p> <p>Seq((0, \"zero\"), (1, \"one\")).toDF(\"id\", \"name\").createOrReplaceTempView(table) val query = spark.table(\"t1\").select(\"id\", \"\").groupBy(\"id\", \"name\").agg(col(\"\")) val logicalPlan = query.queryExecution.logical scala&gt; println(logicalPlan.numberedTreeString) 00 'Aggregate [id#28, name#29], [id#28, name#29, *] 01 +- Project [id#28, id#28, name#29] 02    +- SubqueryAlias t1 03       +- Project [_1#25 AS id#28, _2#26 AS name#29] 04          +- LocalRelation [_1#25, _2#26]</p> <p>import spark.sessionState.analyzer.ResolveReferences val planWithRefsResolved = ResolveReferences(logicalPlan) scala&gt; println(planWithRefsResolved.numberedTreeString) 00 Aggregate [id#28, name#29], [id#28, name#29, id#28, id#28, name#29] 01 +- Project [id#28, id#28, name#29] 02    +- SubqueryAlias t1 03       +- Project [_1#25 AS id#28, _2#26 AS name#29] 04          +- LocalRelation [_1#25, _2#26]</p> <p>=== [[resolve]] Resolving Expressions of Logical Plan -- <code>resolve</code> Internal Method</p>"},{"location":"logical-analysis-rules/ResolveReferences/#source-scala","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ResolveReferences/#resolvee-expression-q-logicalplan-expression","title":"resolve(e: Expression, q: LogicalPlan): Expression","text":"<p><code>resolve</code> resolves the input expression per type:</p> <p>. <code>UnresolvedAttribute</code> expressions</p> <p>. <code>UnresolvedExtractValue</code> expressions</p> <p>. All other expressions</p> <p>NOTE: <code>resolve</code> is used exclusively when <code>ResolveReferences</code> is requested to &lt;&gt;. <p>=== [[apply]] Resolving Reference Expressions In Logical Query Plan (Applying ResolveReferences to Logical Plan) -- <code>apply</code> Method</p>"},{"location":"logical-analysis-rules/ResolveReferences/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ResolveReferences/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p>NOTE: <code>apply</code> is part of catalyst/Rule.md#apply[Rule Contract] to apply a rule to a spark-sql-LogicalPlan.md[logical plan].</p> <p><code>apply</code> resolves the following logical operators:</p> <ul> <li> <p>Project.md[Project] logical operator with a <code>Star</code> expression to...FIXME</p> </li> <li> <p>Aggregate.md[Aggregate] logical operator with a <code>Star</code> expression to...FIXME</p> </li> <li> <p><code>ScriptTransformation</code> logical operator with a <code>Star</code> expression to...FIXME</p> </li> <li> <p>Generate.md[Generate] logical operator with a <code>Star</code> expression to...FIXME</p> </li> <li> <p>Join.md[Join] logical operator with <code>duplicateResolved</code>...FIXME</p> </li> <li> <p>Intersect logical operator with <code>duplicateResolved</code>...FIXME</p> </li> <li> <p>Except.md[Except] logical operator with <code>duplicateResolved</code>...FIXME</p> </li> <li> <p>Sort.md[Sort] logical operator unresolved with child operators resolved...FIXME</p> </li> <li> <p>Generate.md[Generate] logical operator resolved...FIXME</p> </li> <li> <p>Generate.md[Generate] logical operator unresolved...FIXME</p> </li> </ul> <p>In the end, <code>apply</code> &lt;&gt; the expressions of the input logical operator. <p><code>apply</code> skips logical operators that:</p> <ul> <li> <p>Use <code>UnresolvedDeserializer</code> expressions</p> </li> <li> <p>Have child operators spark-sql-LogicalPlan.md#childrenResolved[unresolved]</p> </li> </ul> <p>=== [[buildExpandedProjectList]] Expanding Star Expressions -- <code>buildExpandedProjectList</code> Internal Method</p>"},{"location":"logical-analysis-rules/ResolveReferences/#source-scala_2","title":"[source, scala]","text":"<p>buildExpandedProjectList(   exprs: Seq[NamedExpression],   child: LogicalPlan): Seq[NamedExpression]</p> <p><code>buildExpandedProjectList</code> expands (converts) expressions/Star.md[Star] expressions in the input expressions/NamedExpression.md[named expressions] recursively (down the expression tree) per expression:</p> <ul> <li> <p>For a <code>Star</code> expression, <code>buildExpandedProjectList</code> requests it to expressions/Star.md#expand[expand] given the input <code>child</code> logical plan</p> </li> <li> <p>For a <code>UnresolvedAlias</code> expression with a <code>Star</code> child expression, <code>buildExpandedProjectList</code> requests it to expressions/Star.md#expand[expand] given the input <code>child</code> logical plan (similarly to a <code>Star</code> expression alone in the above case)</p> </li> <li> <p>For <code>exprs</code> with <code>Star</code> expressions down the expression tree, <code>buildExpandedProjectList</code> &lt;&gt; passing the input <code>exprs</code> and <code>child</code> <p>NOTE: <code>buildExpandedProjectList</code> is used when <code>ResolveReferences</code> is requested to &lt;&gt; (in <code>Project</code> and <code>Aggregate</code> operators with <code>Star</code> expressions). <p>=== [[expandStarExpression]] <code>expandStarExpression</code> Method</p>"},{"location":"logical-analysis-rules/ResolveReferences/#source-scala_3","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ResolveReferences/#expandstarexpressionexpr-expression-child-logicalplan-expression","title":"expandStarExpression(expr: Expression, child: LogicalPlan): Expression","text":"<p><code>expandStarExpression</code> expands (transforms) the following expressions in the input <code>expr</code> expressions/Expression.md[expression]:</p> <ol> <li> <p>For spark-sql-Expression-UnresolvedFunction.md[UnresolvedFunction] expressions with expressions/Star.md[Star] child expressions, <code>expandStarExpression</code> requests the <code>Star</code> expressions to expressions/Star.md#expand[expand] given the input <code>child</code> logical plan and the resolver. + <pre><code>// Using Catalyst DSL to create a logical plan with a function with Star child expression\nimport org.apache.spark.sql.catalyst.dsl.plans._\nval t1 = table(\"t1\")\n\nimport org.apache.spark.sql.catalyst.dsl.expressions._\nval f1 = 'f1.function(star())\n\nval plan = t1.select(f1)\nscala&gt; println(plan.numberedTreeString)\n00 'Project [unresolvedalias('f1(*), None)]\n01 +- 'UnresolvedRelation `t1`\n\n// CAUTION: FIXME How to demo that the plan gets resolved using ResolveReferences.expandStarExpression?\n</code></pre></p> </li> <li> <p>For &lt;&gt; expressions with expressions/Star.md[Star] child expressions among the values, <code>expandStarExpression</code>...FIXME <li> <p><code>CreateArray</code> expressions with expressions/Star.md[Star] child expressions, <code>expandStarExpression</code>...FIXME</p> </li> <li> <p>For spark-sql-Expression-Murmur3Hash.md[Murmur3Hash] expressions with expressions/Star.md[Star] child expressions, <code>expandStarExpression</code>...FIXME</p> </li> <p>For any other uses of expressions/Star.md[Star] expressions, <code>expandStarExpression</code> fails analysis with a <code>AnalysisException</code>:</p> <pre><code>Invalid usage of '*' in expression '[exprName]'\n</code></pre> <p>NOTE: <code>expandStarExpression</code> is used exclusively when <code>ResolveReferences</code> is requested to &lt;&gt; (in <code>Project</code> and <code>Aggregate</code> operators). <p>=== [[dedupRight]] <code>dedupRight</code> Internal Method</p>"},{"location":"logical-analysis-rules/ResolveReferences/#source-scala_4","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ResolveReferences/#deduprightleft-logicalplan-right-logicalplan-logicalplan","title":"dedupRight(left: LogicalPlan, right: LogicalPlan): LogicalPlan","text":"<p><code>dedupRight</code>...FIXME</p> <p>NOTE: <code>dedupRight</code> is used when...FIXME</p> <p>=== [[dedupOuterReferencesInSubquery]] <code>dedupOuterReferencesInSubquery</code> Internal Method</p>"},{"location":"logical-analysis-rules/ResolveReferences/#source-scala_5","title":"[source, scala]","text":"<p>dedupOuterReferencesInSubquery(   plan: LogicalPlan,   attrMap: AttributeMap[Attribute]): LogicalPlan</p> <p><code>dedupOuterReferencesInSubquery</code>...FIXME</p> <p>NOTE: <code>dedupOuterReferencesInSubquery</code> is used when...FIXME</p>"},{"location":"logical-analysis-rules/ResolveRelations/","title":"ResolveRelations Logical Resolution Rule","text":"<p><code>ResolveRelations</code> is a logical resolution rule that the logical query plan analyzer uses to replace (resolve) UnresolvedRelations.</p> <p><code>ResolveRelations</code> is a Catalyst rule for transforming logical plans (<code>Rule[LogicalPlan]</code>).</p> <p><code>ResolveRelations</code> is part of Resolution fixed-point batch of rules.</p>"},{"location":"logical-analysis-rules/ResolveRelations/#creating-instance","title":"Creating Instance","text":"<p><code>ResolveRelations</code> takes no arguments to be created.</p> <p><code>ResolveRelations</code> is created when:</p> <ul> <li><code>Analyzer</code> is requested for the batches</li> </ul>"},{"location":"logical-analysis-rules/ResolveRelations/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p>  <p><code>apply</code> resolves the following operators in the given LogicalPlan (from bottom to the top of the tree):</p> <ul> <li>InsertIntoStatements</li> <li>V2WriteCommands</li> <li><code>UnresolvedRelation</code>s</li> <li><code>RelationTimeTravel</code>s</li> <li>UnresolvedTables</li> <li><code>UnresolvedView</code>s</li> <li>UnresolvedTableOrViews</li> </ul>","text":""},{"location":"logical-analysis-rules/ResolveRelations/#demo","title":"Demo <pre><code>// Example: InsertIntoTable with UnresolvedRelation\nimport org.apache.spark.sql.catalyst.dsl.plans._\nval plan = table(\"t1\").insertInto(tableName = \"t2\", overwrite = true)\nscala&gt; println(plan.numberedTreeString)\n00 'InsertIntoTable 'UnresolvedRelation `t2`, true, false\n01 +- 'UnresolvedRelation `t1`\n\n// Register the tables so the following resolution works\nsql(\"CREATE TABLE IF NOT EXISTS t1(id long)\")\nsql(\"CREATE TABLE IF NOT EXISTS t2(id long)\")\n\n// ResolveRelations is a Scala object of the Analyzer class\n// We need an instance of the Analyzer class to access it\nimport spark.sessionState.analyzer.ResolveRelations\nval resolvedPlan = ResolveRelations(plan)\nscala&gt; println(resolvedPlan.numberedTreeString)\n00 'InsertIntoTable 'UnresolvedRelation `t2`, true, false\n01 +- 'SubqueryAlias t1\n02    +- 'UnresolvedCatalogRelation `default`.`t1`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n\n// Example: Other uses of UnresolvedRelation\n// Use a temporary view\nval v1 = spark.range(1).createOrReplaceTempView(\"v1\")\nscala&gt; spark.catalog.listTables.filter($\"name\" === \"v1\").show\n+----+--------+-----------+---------+-----------+\n|name|database|description|tableType|isTemporary|\n+----+--------+-----------+---------+-----------+\n|  v1|    null|       null|TEMPORARY|       true|\n+----+--------+-----------+---------+-----------+\n\nimport org.apache.spark.sql.catalyst.dsl.expressions._\nval plan = table(\"v1\").select(star())\nscala&gt; println(plan.numberedTreeString)\n00 'Project [*]\n01 +- 'UnresolvedRelation `v1`\n\nval resolvedPlan = ResolveRelations(plan)\nscala&gt; println(resolvedPlan.numberedTreeString)\n00 'Project [*]\n01 +- SubqueryAlias v1\n02    +- Range (0, 1, step=1, splits=Some(8))\n\n// Example\nimport org.apache.spark.sql.catalyst.dsl.plans._\nval plan = table(db = \"db1\", ref = \"t1\")\nscala&gt; println(plan.numberedTreeString)\n00 'UnresolvedRelation `db1`.`t1`\n\n// Register the database so the following resolution works\nsql(\"CREATE DATABASE IF NOT EXISTS db1\")\n\nval resolvedPlan = ResolveRelations(plan)\nscala&gt; println(resolvedPlan.numberedTreeString)\n00 'SubqueryAlias t1\n01 +- 'UnresolvedCatalogRelation `db1`.`t1`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n</code></pre>","text":""},{"location":"logical-analysis-rules/ResolveRelations/#lookuprelation","title":"lookupRelation <pre><code>lookupRelation(\n  u: UnresolvedRelation,\n  timeTravelSpec: Option[TimeTravelSpec] = None): Option[LogicalPlan]\n</code></pre> <p><code>lookupRelation</code>...FIXME</p>","text":""},{"location":"logical-analysis-rules/ResolveRelations/#lookuptableorview","title":"lookupTableOrView <pre><code>lookupTableOrView(\n  identifier: Seq[String]): Option[LogicalPlan]\n</code></pre> <p><code>lookupTableOrView</code>...FIXME</p>","text":""},{"location":"logical-analysis-rules/ResolveRelations/#resolveviews","title":"resolveViews <pre><code>resolveViews(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>resolveViews</code> resolves the unresolved child of the given LogicalPlan if it is one of the following:</p> <ul> <li>View</li> <li>SubqueryAlias over a View</li> </ul> <p>Otherwise, <code>resolveViews</code> returns the given <code>plan</code> unmodified.</p> <p><code>resolveViews</code> uses the CatalogTable (of the View) to resolve the view (as the <code>CatalogTable</code> provides necessary information to resolve it).</p>","text":""},{"location":"logical-analysis-rules/ResolveSQLOnFile/","title":"ResolveSQLOnFile Logical Evaluation Rule for...FIXME","text":"<p><code>ResolveSQLOnFile</code> is...FIXME</p> <p>=== [[maybeSQLFile]] <code>maybeSQLFile</code> Internal Method</p>"},{"location":"logical-analysis-rules/ResolveSQLOnFile/#source-scala","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ResolveSQLOnFile/#maybesqlfileu-unresolvedrelation-boolean","title":"maybeSQLFile(u: UnresolvedRelation): Boolean","text":"<p><code>maybeSQLFile</code> is enabled (i.e. <code>true</code>) where the following all hold:</p> <ol> <li>FIXME</li> </ol> <p>NOTE: <code>maybeSQLFile</code> is used exclusively when...FIXME</p> <p>=== [[apply]] Applying Rule to Logical Plan -- <code>apply</code> Method</p>"},{"location":"logical-analysis-rules/ResolveSQLOnFile/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ResolveSQLOnFile/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p>NOTE: <code>apply</code> is part of catalyst/Rule.md#apply[Rule Contract] to apply a rule to a spark-sql-LogicalPlan.md[logical plan].</p> <p><code>apply</code>...FIXME</p>"},{"location":"logical-analysis-rules/ResolveSessionCatalog/","title":"ResolveSessionCatalog Logical Resolution Rule","text":"<p><code>ResolveSessionCatalog</code> is a logical resolution rule (<code>Rule[LogicalPlan]</code>).</p>"},{"location":"logical-analysis-rules/ResolveSessionCatalog/#creating-instance","title":"Creating Instance","text":"<p><code>ResolveSessionCatalog</code> takes the following to be created:</p> <ul> <li> CatalogManager <li> SQLConf <li> <code>isTempView</code> Function (<code>Seq[String] =&gt; Boolean</code>) <li> <code>isTempFunction</code> Function (<code>String =&gt; Boolean</code>) <p><code>ResolveSessionCatalog</code> is created as an extended resolution rule when HiveSessionStateBuilder and BaseSessionStateBuilder are requested for the analyzer.</p>"},{"location":"logical-analysis-rules/ResolveSessionCatalog/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Catalyst Rule abstraction.</p>","text":""},{"location":"logical-analysis-rules/ResolveSubquery/","title":"ResolveSubquery Logical Resolution Rule","text":"<p><code>ResolveSubquery</code> is a logical resolution rule that resolves SubqueryExpressions in the following logical operators:</p> <ul> <li><code>Filter</code>s with a child Aggregate</li> <li>Unary operators</li> <li>Join</li> <li>SupportsSubquery</li> </ul> <p><code>ResolveSubquery</code> resolves subqueries (logical query plans) in the following SubqueryExpressions:</p> <ul> <li>ScalarSubquery</li> <li>Exists</li> <li>ListQuery (in <code>InSubquery</code> expressions)</li> </ul> <p><code>ResolveSubquery</code> is part of Resolution rule batch of the Logical Analyzer.</p> <p><code>ResolveSubquery</code> is a Catalyst rule for transforming logical plans (<code>Rule[LogicalPlan]</code>).</p>"},{"location":"logical-analysis-rules/ResolveSubquery/#example","title":"Example","text":"<pre><code>// Use Catalyst DSL\nimport org.apache.spark.sql.catalyst.expressions._\nval a = 'a.int\n\nimport org.apache.spark.sql.catalyst.plans.logical.LocalRelation\nval rel = LocalRelation(a)\n\nimport org.apache.spark.sql.catalyst.expressions.Literal\nval list = Seq[Literal](1)\n\n// FIXME Use a correct query to demo ResolveSubquery\nimport org.apache.spark.sql.catalyst.plans.logical.Filter\nimport org.apache.spark.sql.catalyst.expressions.In\nval plan = Filter(condition = In(value = a, list), child = rel)\n\nscala&gt; println(plan.numberedTreeString)\n00 Filter a#9 IN (1)\n01 +- LocalRelation &lt;empty&gt;, [a#9]\n\nimport spark.sessionState.analyzer.ResolveSubquery\nval analyzedPlan = ResolveSubquery(plan)\nscala&gt; println(analyzedPlan.numberedTreeString)\n00 Filter a#9 IN (1)\n01 +- LocalRelation &lt;empty&gt;, [a#9]\n</code></pre>"},{"location":"logical-analysis-rules/ResolveSubquery/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> resolves SubqueryExpressions in the following logical operators in the given logical plan:</p> <ul> <li> <p><code>Filter</code>s with a child Aggregate</p> </li> <li> <p>Unary operators</p> </li> <li> <p>Join</p> </li> <li> <p>SupportsSubquery</p> </li> </ul> <p><code>apply</code> resolves logical operators that have their children all resolved already.</p> <p><code>apply</code> is part of Rule abstraction.</p>","text":""},{"location":"logical-analysis-rules/ResolveSubquery/#resolving-subqueryexpressions","title":"Resolving SubqueryExpressions <pre><code>resolveSubQueries(\n  plan: LogicalPlan,\n  plans: Seq[LogicalPlan]): LogicalPlan\n</code></pre> <p><code>resolveSubQueries</code> resolves subquery plans in the following SubqueryExpression expressions (by transforming expressions down the operator tree) in the given logical plan:</p> <ul> <li>ScalarSubquery</li> <li>Exists</li> <li>ListQuery (in <code>InSubquery</code> expressions)</li> </ul> <p><code>resolveSubQueries</code> is used when <code>ResolveSubquery</code> is executed.</p>","text":""},{"location":"logical-analysis-rules/ResolveSubquery/#resolving-subquery-plan","title":"Resolving Subquery Plan <pre><code>resolveSubQuery(\n  e: SubqueryExpression,\n  plans: Seq[LogicalPlan])(\n  f: (LogicalPlan, Seq[Expression]) =&gt; SubqueryExpression): SubqueryExpression\n</code></pre> <p><code>resolveSubQuery</code> resolves outer references in the logical plan of the given SubqueryExpression (and all sub-<code>SubqueryExpression</code>s).</p> <p>In the end, <code>resolveSubQuery</code> executes the given <code>f</code> function with the logical plan (of the <code>SubqueryExpression</code>) and all <code>OuterReference</code> leaf expressions when the logical plan has been fully resolved. Otherwise, <code>resolveSubQuery</code> requests the <code>SubqueryExpression</code> to withNewPlan.</p> <p><code>resolveSubQuery</code> is used when <code>ResolveSubquery</code> is requested to resolve subquery expressions.</p>","text":""},{"location":"logical-analysis-rules/ResolveSubquery/#resolving-outer-references-in-subquery-plan","title":"Resolving Outer References (in Subquery Plan) <pre><code>resolveOuterReferences(\n  plan: LogicalPlan,\n  outer: LogicalPlan): LogicalPlan\n</code></pre> <p><code>resolveOuterReferences</code> uses the <code>outer</code> logical plan to resolve UnresolvedAttribute expressions in the <code>plan</code> logical operator.</p> <p><code>resolveOuterReferences</code> translates resolved NamedExpression expressions to <code>OuterReference</code> leaf expressions.</p> <p><code>resolveOuterReferences</code> is used when <code>ResolveSubquery</code> is requested to resolve a SubqueryExpression.</p>","text":""},{"location":"logical-analysis-rules/ResolveTables/","title":"ResolveTables Logical Resolution Rule","text":"<p><code>ResolveTables</code> is...FIXME</p>"},{"location":"logical-analysis-rules/ResolveTempViews/","title":"ResolveTempViews Logical Resolution Rule","text":"<p><code>ResolveTempViews</code> is...FIXME</p>"},{"location":"logical-analysis-rules/ResolveTimeZone/","title":"ResolveTimeZone Logical Resolution Rule","text":"<p><code>ResolveTimeZone</code> is a logical resolution rule that the logical query plan analyzer uses to &lt;&gt;. <p><code>ResolveTimeZone</code> is a Catalyst rule for transforming logical plans, i.e. <code>Rule[LogicalPlan]</code>.</p> <p><code>ResolveTimeZone</code> is part of Resolution fixed-point batch of rules.</p> <p>=== [[apply]] Applying ResolveTimeZone to Logical Plan -- <code>apply</code> Method</p>"},{"location":"logical-analysis-rules/ResolveTimeZone/#source-scala","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ResolveTimeZone/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p>NOTE: <code>apply</code> is part of catalyst/Rule.md#apply[Rule Contract] to apply a rule to a spark-sql-LogicalPlan.md[logical plan].</p> <p><code>apply</code>...FIXME</p>"},{"location":"logical-analysis-rules/ResolveWindowFrame/","title":"ResolveWindowFrame Logical Resolution Rule","text":"<p><code>ResolveWindowFrame</code> is a logical resolution rule that the Logical Analyzer uses to &lt;&gt; in an entire logical query plan. <p><code>ResolveWindowFrame</code> is a Catalyst rule for transforming logical plans, i.e. <code>Rule[LogicalPlan]</code>.</p> <p><code>ResolveWindowFrame</code> is part of Resolution fixed-point batch of rules.</p> <p>[[transformations]] <code>ResolveWindowFrame</code> takes a spark-sql-LogicalPlan.md[logical plan] and does the following:</p> <p>. Makes sure that the window frame of a WindowFunction is unspecified or matches the <code>SpecifiedWindowFrame</code> of the spark-sql-Expression-WindowSpecDefinition.md[WindowSpecDefinition] expression. + Reports a <code>AnalysisException</code> when the frames do not match: + <pre><code>Window Frame [f] must match the required frame [frame]\n</code></pre></p> <p>. Copies the frame specification of <code>WindowFunction</code> to spark-sql-Expression-WindowSpecDefinition.md[WindowSpecDefinition]</p> <p>. Creates a new <code>SpecifiedWindowFrame</code> for <code>WindowExpression</code> with the resolved Catalyst expression and <code>UnspecifiedFrame</code></p>"},{"location":"logical-analysis-rules/ResolveWindowFrame/#example","title":"Example","text":"<pre><code>import import org.apache.spark.sql.expressions.Window\n// cume_dist requires ordered windows\nval q = spark.\n  range(5).\n  withColumn(\"cume_dist\", cume_dist() over Window.orderBy(\"id\"))\nimport org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\nval planBefore: LogicalPlan = q.queryExecution.logical\n\n// Before ResolveWindowFrame\nscala&gt; println(planBefore.numberedTreeString)\n00 'Project [*, cume_dist() windowspecdefinition('id ASC NULLS FIRST, UnspecifiedFrame) AS cume_dist#39]\n01 +- Range (0, 5, step=1, splits=Some(8))\n\nimport spark.sessionState.analyzer.ResolveWindowFrame\nval planAfter = ResolveWindowFrame.apply(plan)\n\n// After ResolveWindowFrame\nscala&gt; println(planAfter.numberedTreeString)\n00 'Project [*, cume_dist() windowspecdefinition('id ASC NULLS FIRST, RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cume_dist#31]\n01 +- Range (0, 5, step=1, splits=Some(8))\n</code></pre> <p>=== [[apply]] Applying ResolveWindowFrame to Logical Plan -- <code>apply</code> Method</p>"},{"location":"logical-analysis-rules/ResolveWindowFrame/#source-scala","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/ResolveWindowFrame/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p>NOTE: <code>apply</code> is part of catalyst/Rule.md#apply[Rule Contract] to apply a rule to a spark-sql-LogicalPlan.md[logical plan].</p> <p><code>apply</code>...FIXME</p>"},{"location":"logical-analysis-rules/ResolveWindowOrder/","title":"ResolveWindowOrder Logical Resolution Rule","text":"<p><code>ResolveWindowOrder</code> is...FIXME</p>"},{"location":"logical-analysis-rules/ResolveWithCTE/","title":"ResolveWithCTE Logical Resolution Rule","text":"<p><code>ResolveWithCTE</code> is a logical resolution rule that the Logical Analyzer uses to resolveWithCTE for CTE query plans.</p> <p><code>ResolveWithCTE</code> is a Catalyst rule for transforming logical plans (<code>Rule[LogicalPlan]</code>).</p> <p><code>ResolveWithCTE</code> is part of Resolution fixed-point batch of rules.</p>"},{"location":"logical-analysis-rules/ResolveWithCTE/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> does nothing and simply returns the given LogicalPlan when applied to a non-CTE query plan. Otherwise, <code>apply</code> resolveWithCTE.</p> <p><code>apply</code>\u00a0is part of the Rule abstraction.</p>","text":""},{"location":"logical-analysis-rules/ResolveWithCTE/#resolvewithcte","title":"resolveWithCTE <pre><code>resolveWithCTE(\n  plan: LogicalPlan,\n  cteDefMap: mutable.HashMap[Long, CTERelationDef]): LogicalPlan\n</code></pre> <p><code>resolveWithCTE</code> requests the given logical operator to <code>resolveOperatorsDownWithPruning</code> for CTE logical operators:</p> <ol> <li>WithCTE</li> <li>CTERelationRef</li> <li>Others with <code>CTE</code> and PLAN_EXPRESSION tree patterns</li> </ol> <p><code>resolveWithCTE</code> is a recursive function.</p>","text":""},{"location":"logical-analysis-rules/RewriteDeleteFromTable/","title":"RewriteDeleteFromTable Analysis Rule","text":""},{"location":"logical-analysis-rules/RewriteDeleteFromTable/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p>  <p><code>apply</code> rewrites the table in DeleteFromTable logical operators (in the given LogicalPlan):</p> <ul> <li> <p>For DataSourceV2Relations with TruncatableTable and <code>TrueLiteral</code> condition, skips rewriting (leaves them untouched).</p> </li> <li> <p>For DataSourceV2Relations with SupportsRowLevelOperations...FIXME</p> </li> <li> <p>For DataSourceV2Relations with SupportsDelete...FIXME</p> </li> </ul>","text":""},{"location":"logical-analysis-rules/RewriteDeleteFromTable/#buildreplacedataplan","title":"buildReplaceDataPlan <pre><code>buildReplaceDataPlan(\n  relation: DataSourceV2Relation,\n  operationTable: RowLevelOperationTable,\n  cond: Expression): ReplaceData\n</code></pre> <p><code>buildReplaceDataPlan</code>...FIXME</p>","text":""},{"location":"logical-analysis-rules/RewriteRowLevelCommand/","title":"RewriteRowLevelCommand Analysis Rule","text":"<p><code>RewriteRowLevelCommand</code> is...FIXME</p>"},{"location":"logical-analysis-rules/TableCapabilityCheck/","title":"TableCapabilityCheck Extended Analysis Check","text":"<p><code>TableCapabilityCheck</code> is...FIXME</p>"},{"location":"logical-analysis-rules/UpdateOuterReferences/","title":"UpdateOuterReferences Logical Rule","text":"<p><code>UpdateOuterReferences</code> is...FIXME</p> <p>=== [[apply]] Applying UpdateOuterReferences to Logical Plan -- <code>apply</code> Method</p>"},{"location":"logical-analysis-rules/UpdateOuterReferences/#source-scala","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/UpdateOuterReferences/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p>NOTE: <code>apply</code> is part of catalyst/Rule.md#apply[Rule Contract] to apply a rule to a spark-sql-LogicalPlan.md[logical plan].</p> <p><code>apply</code>...FIXME</p>"},{"location":"logical-analysis-rules/WidenSetOperationTypes/","title":"WidenSetOperationTypes","text":"<p><code>WidenSetOperationTypes</code> is...FIXME</p>"},{"location":"logical-analysis-rules/WindowFrameCoercion/","title":"WindowFrameCoercion Type Coercion Logical Rule","text":"<p><code>WindowFrameCoercion</code> is a <code>TypeCoercionRule</code> that cast the data types of the boundaries of a range window frame to the data type of the order specification in a WindowSpecDefinition in a logical plan.</p> <pre><code>import java.time.LocalDate\nimport java.sql.Timestamp\nval sales = Seq(\n  (Timestamp.valueOf(LocalDate.of(2018, 9, 1).atStartOfDay), 5),\n  (Timestamp.valueOf(LocalDate.of(2018, 9, 2).atStartOfDay), 10),\n  // Mind the 2-day gap\n  (Timestamp.valueOf(LocalDate.of(2018, 9, 5).atStartOfDay), 5)\n).toDF(\"time\", \"volume\")\nscala&gt; sales.show\n+-------------------+------+\n|               time|volume|\n+-------------------+------+\n|2018-09-01 00:00:00|     5|\n|2018-09-02 00:00:00|    10|\n|2018-09-05 00:00:00|     5|\n+-------------------+------+\n\nscala&gt; sales.printSchema\nroot\n |-- time: timestamp (nullable = true)\n |-- volume: integer (nullable = false)\n\n// FIXME Use Catalyst DSL\n// rangeBetween with column expressions\n// data type of orderBy expression is date\n// data types of range frame boundaries is interval\n// WindowSpecDefinition(_, Seq(order), SpecifiedWindowFrame(RangeFrame, lower, upper))\nimport org.apache.spark.unsafe.types.CalendarInterval\nval interval = lit(CalendarInterval.fromString(\"interval 1 days\"))\nimport org.apache.spark.sql.expressions.Window\nval windowSpec = Window.orderBy($\"time\").rangeBetween(currentRow(), interval)\n\nval q = sales.select(\n  $\"time\",\n  (sum($\"volume\") over windowSpec) as \"sum\",\n  (count($\"volume\") over windowSpec) as \"count\")\nval plan = q.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'Project [unresolvedalias('time, None), sum('volume) windowspecdefinition('time ASC NULLS FIRST, specifiedwindowframe(RangeFrame, currentrow$(), interval 1 days)) AS sum#156, count('volume) windowspecdefinition('time ASC NULLS FIRST, specifiedwindowframe(RangeFrame, currentrow$(), interval 1 days)) AS count#158]\n01 +- AnalysisBarrier\n02       +- Project [_1#129 AS time#132, _2#130 AS volume#133]\n03          +- LocalRelation [_1#129, _2#130]\n\nimport spark.sessionState.analyzer.ResolveReferences\nval planWithRefsResolved = ResolveReferences(plan)\n\nimport spark.sessionState.analyzer.ResolveAliases\nval planWithAliasesResolved = ResolveReferences(planWithRefsResolved)\n\n// FIXME Looks like nothing changes in the query plan with regard to WindowFrameCoercion\n\nimport org.apache.spark.sql.catalyst.analysis.TypeCoercion.WindowFrameCoercion\nval afterWindowFrameCoercion = WindowFrameCoercion(planWithRefsResolved)\nscala&gt; println(afterWindowFrameCoercion.numberedTreeString)\n00 'Project [unresolvedalias(time#132, None), sum(volume#133) windowspecdefinition(time#132 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, currentrow$(), interval 1 days)) AS sum#156L, count(volume#133) windowspecdefinition(time#132 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, currentrow$(), interval 1 days)) AS count#158L]\n01 +- AnalysisBarrier\n02       +- Project [_1#129 AS time#132, _2#130 AS volume#133]\n03          +- LocalRelation [_1#129, _2#130]\n</code></pre> <pre><code>import java.time.LocalDate\nimport java.sql.Date\nval sales = Seq(\n  (Date.valueOf(LocalDate.of(2018, 9, 1)), 5),\n  (Date.valueOf(LocalDate.of(2018, 9, 2)), 10),\n  // Mind the 2-day gap\n  (Date.valueOf(LocalDate.of(2018, 9, 5)), 5)\n).toDF(\"time\", \"volume\")\nscala&gt; sales.show\n+----------+------+\n|      time|volume|\n+----------+------+\n|2018-09-01|     5|\n|2018-09-02|    10|\n|2018-09-05|     5|\n+----------+------+\n\nscala&gt; sales.printSchema\nroot\n |-- time: date (nullable = true)\n |-- volume: integer (nullable = false)\n\n// FIXME Use Catalyst DSL\n// rangeBetween with column expressions\n// data type of orderBy expression is date\n// WindowSpecDefinition(_, Seq(order), SpecifiedWindowFrame(RangeFrame, lower, upper))\nimport org.apache.spark.sql.expressions.Window\nval windowSpec = Window.orderBy($\"time\").rangeBetween(currentRow(), lit(1))\n\nval q = sales.select(\n  $\"time\",\n  (sum($\"volume\") over windowSpec) as \"sum\")\nval plan = q.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'Project [unresolvedalias('time, None), sum('volume) windowspecdefinition('time ASC NULLS FIRST, specifiedwindowframe(RangeFrame, currentrow$(), 1)) AS sum#238]\n01 +- AnalysisBarrier\n02       +- Project [_1#222 AS time#225, _2#223 AS volume#226]\n03          +- LocalRelation [_1#222, _2#223]\n\nimport spark.sessionState.analyzer.ResolveReferences\nval planWithRefsResolved = ResolveReferences(plan)\n\nimport spark.sessionState.analyzer.ResolveAliases\nval planWithAliasesResolved = ResolveReferences(planWithRefsResolved)\n\n// FIXME Looks like nothing changes in the query plan with regard to WindowFrameCoercion\n\nimport org.apache.spark.sql.catalyst.analysis.TypeCoercion.WindowFrameCoercion\nval afterWindowFrameCoercion = WindowFrameCoercion(planWithAliasesResolved)\nscala&gt; println(afterWindowFrameCoercion.numberedTreeString)\n00 'Project [unresolvedalias(time#132, None), sum(volume#133) windowspecdefinition(time#132 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, currentrow$(), interval 1 days)) AS sum#156L, count(volume#133) windowspecdefinition(time#132 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, currentrow$(), interval 1 days)) AS count#158L]\n01 +- AnalysisBarrier\n02       +- Project [_1#129 AS time#132, _2#130 AS volume#133]\n03          +- LocalRelation [_1#129, _2#130]\n</code></pre> <p>=== [[coerceTypes]] Coercing Types in Logical Plan -- <code>coerceTypes</code> Method</p>"},{"location":"logical-analysis-rules/WindowFrameCoercion/#source-scala","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/WindowFrameCoercion/#coercetypesplan-logicalplan-logicalplan","title":"coerceTypes(plan: LogicalPlan): LogicalPlan","text":"<p><code>coerceTypes</code> is part of the <code>TypeCoercionRule</code> abstraction.</p> <p><code>coerceTypes</code> &lt;&gt; (in the input &lt;&gt;) and replaces the &lt;&gt; of every &lt;&gt; with a <code>RangeFrame</code> window frame and the single &lt;&gt; expression &lt;&gt; with the lower and upper window frame boundary expressions cast to the &lt;&gt; of the order specification expression. <p>=== [[createBoundaryCast]] <code>createBoundaryCast</code> Internal Method</p>"},{"location":"logical-analysis-rules/WindowFrameCoercion/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-analysis-rules/WindowFrameCoercion/#createboundarycastboundary-expression-dt-datatype-expression","title":"createBoundaryCast(boundary: Expression, dt: DataType): Expression","text":"<p><code>createBoundaryCast</code> returns a &lt;&gt; per the input <code>boundary</code> &lt;&gt; and the <code>dt</code> DataType (in the order of execution): <ul> <li> <p>The input <code>boundary</code> expression if it is a <code>SpecialFrameBoundary</code></p> </li> <li> <p>The input <code>boundary</code> expression if the <code>dt</code> data type is DateType or TimestampType</p> </li> <li> <p><code>Cast</code> unary operator with the input <code>boundary</code> expression and the <code>dt</code> data type if the &lt;&gt; of the <code>boundary</code> expression is not the <code>dt</code> data type, but the result type can be cast to the <code>dt</code> data type <li> <p>The input <code>boundary</code> expression</p> </li> <p>NOTE: <code>createBoundaryCast</code> is used exclusively when <code>WindowFrameCoercion</code> type coercion logical rule is requested to &lt;&gt;."},{"location":"logical-analysis-rules/WindowsSubstitution/","title":"WindowsSubstitution Logical Evaluation Rule","text":"<p><code>WindowsSubstitution</code> is a logical evaluation rule (<code>Rule[LogicalPlan]</code>) that the Logical Analyzer uses to resolve (aka substitute) WithWindowDefinition unary logical operators with <code>UnresolvedWindowExpression</code> to their corresponding spark-sql-Expression-WindowExpression.md[WindowExpression] with resolved spark-sql-Expression-WindowSpecDefinition.md[WindowSpecDefinition].</p> <p><code>WindowsSubstitution</code> is part of Substitution fixed-point batch of rules.</p> <p>NOTE: It appears that <code>WindowsSubstitution</code> is exclusively used for pure SQL queries because WithWindowDefinition.md[WithWindowDefinition] unary logical operator is created exclusively when <code>AstBuilder</code> WithWindowDefinition.md#creating-instance[parses window definitions].</p> <p>If a window specification is not found, <code>WindowsSubstitution</code> fails analysis with the following error:</p> <pre><code>Window specification [windowName] is not defined in the WINDOW clause.\n</code></pre> <p>NOTE: The analysis failure is unlikely to happen given <code>AstBuilder</code> sql/AstBuilder.md#withWindows[builds a lookup table of all the named window specifications] defined in a SQL text and reports a <code>ParseException</code> when a <code>WindowSpecReference</code> is not available earlier.</p> <p>For every <code>WithWindowDefinition</code>, <code>WindowsSubstitution</code> takes the <code>child</code> logical plan and transforms its <code>UnresolvedWindowExpression</code> expressions to be a spark-sql-Expression-WindowExpression.md[WindowExpression] with a window specification from the <code>WINDOW</code> clause (see spark-sql-Expression-WindowExpression.md#WithWindowDefinition-example[WithWindowDefinition Example]).</p>"},{"location":"logical-operators/","title":"Logical Operators","text":"<p>Logical Operators (Logical Relational Operators) are building blocks of logical query plans in Spark SQL.</p> <p>Logical Query Plan is a tree of nodes of logical operators that in turn can have (trees of) Catalyst expressions. In other words, there are at least two trees at every level (operator).</p> <p>The main abstraction is LogicalPlan that is a recursive data structure with zero, one, two or more child logical operators:</p> <ul> <li>LeafNode</li> <li>UnaryNode</li> <li>BinaryNode</li> </ul> <p>Among the logical operators are Commands.</p>"},{"location":"logical-operators/AddColumns/","title":"AddColumns Logical Operator","text":"<p><code>AddColumns</code> is an <code>AlterTableCommand</code> logical operator that represents ALTER TABLE ADD COLUMNS SQL statement (in a logical query plan).</p>"},{"location":"logical-operators/AddColumns/#creating-instance","title":"Creating Instance","text":"<p><code>AddColumns</code> takes the following to be created:</p> <ul> <li> Table (LogicalPlan) <li> Columns to Add <p><code>AddColumns</code> is created when:</p> <ul> <li><code>AstBuilder</code> is requested to parse ALTER TABLE ADD COLUMNS statement</li> </ul>"},{"location":"logical-operators/AddColumns/#altertableaddcolumnscommand","title":"AlterTableAddColumnsCommand <p><code>AddColumns</code> is resolved to a AlterTableAddColumnsCommand logical runnable command by ResolveSessionCatalog logical resolution rule.</p>","text":""},{"location":"logical-operators/Aggregate/","title":"Aggregate Logical Operator","text":"<p><code>Aggregate</code> is an unary logical operator that represents the following high-level operators in a logical query plan:</p> <ul> <li><code>AstBuilder</code> is requested to visitCommonSelectQueryClausePlan (<code>HAVING</code> clause without <code>GROUP BY</code>) and parse GROUP BY clause</li> <li><code>KeyValueGroupedDataset</code> is requested to agg (and aggUntyped)</li> <li><code>RelationalGroupedDataset</code> is requested to toDF</li> </ul>"},{"location":"logical-operators/Aggregate/#creating-instance","title":"Creating Instance","text":"<p><code>Aggregate</code> takes the following to be created:</p> <ul> <li> Grouping Expressions <li> Aggregate NamedExpressions <li> Child LogicalPlan <p><code>Aggregate</code> is created\u00a0when:</p> <ul> <li><code>AstBuilder</code> is requested to withSelectQuerySpecification and withAggregationClause</li> <li><code>DslLogicalPlan</code> is used to groupBy</li> <li><code>KeyValueGroupedDataset</code> is requested to aggUntyped</li> <li><code>RelationalGroupedDataset</code> is requested to toDF</li> <li>AnalyzeColumnCommand logical command (when <code>CommandUtils</code> is used to computeColumnStats and computePercentiles)</li> </ul>"},{"location":"logical-operators/Aggregate/#checking-requirements-for-hashaggregateexec","title":"Checking Requirements for HashAggregateExec <pre><code>supportsHashAggregate(\n  aggregateBufferAttributes: Seq[Attribute]): Boolean\n</code></pre> <p><code>supportsHashAggregate</code> builds a StructType for the given <code>aggregateBufferAttributes</code>.</p> <p>In the end, <code>supportsHashAggregate</code> isAggregateBufferMutable.</p>  <p><code>supportsHashAggregate</code> is used when:</p> <ul> <li><code>MergeScalarSubqueries</code> is requested to <code>supportedAggregateMerge</code></li> <li><code>AggUtils</code> is requested to create a physical operator for aggregation</li> <li><code>HashAggregateExec</code> physical operator is created (to assert that the aggregateBufferAttributes are supported)</li> </ul>","text":""},{"location":"logical-operators/Aggregate/#isaggregatebuffermutable","title":"isAggregateBufferMutable <pre><code>isAggregateBufferMutable(\n  schema: StructType): Boolean\n</code></pre> <p><code>isAggregateBufferMutable</code> is enabled (<code>true</code>) when the type of all the fields (in the given <code>schema</code>) are mutable.</p>  <p><code>isAggregateBufferMutable</code> is used when:</p> <ul> <li><code>Aggregate</code> is requested to check the requirements for HashAggregateExec</li> <li><code>UnsafeFixedWidthAggregationMap</code> is requested to supportsAggregationBufferSchema</li> </ul>","text":""},{"location":"logical-operators/Aggregate/#query-planning","title":"Query Planning <p><code>Aggregate</code> logical operator is planned to one of the physical operators in Aggregation execution planning strategy (using PhysicalAggregation utility):</p> <ul> <li>HashAggregateExec</li> <li>ObjectHashAggregateExec</li> <li>SortAggregateExec</li> </ul>","text":""},{"location":"logical-operators/Aggregate/#logical-optimization","title":"Logical Optimization <p>PushDownPredicate logical plan optimization applies so-called filter pushdown to a Pivot operator when under <code>Filter</code> operator and with all expressions deterministic.</p> <pre><code>import org.apache.spark.sql.catalyst.optimizer.PushDownPredicate\n\nval q = visits\n  .groupBy(\"city\")\n  .pivot(\"year\")\n  .count()\n  .where($\"city\" === \"Boston\")\n\nval pivotPlanAnalyzed = q.queryExecution.analyzed\nscala&gt; println(pivotPlanAnalyzed.numberedTreeString)\n00 Filter (city#8 = Boston)\n01 +- Project [city#8, __pivot_count(1) AS `count` AS `count(1) AS ``count```#142[0] AS 2015#143L, __pivot_count(1) AS `count` AS `count(1) AS ``count```#142[1] AS 2016#144L, __pivot_count(1) AS `count` AS `count(1) AS ``count```#142[2] AS 2017#145L]\n02    +- Aggregate [city#8], [city#8, pivotfirst(year#9, count(1) AS `count`#134L, 2015, 2016, 2017, 0, 0) AS __pivot_count(1) AS `count` AS `count(1) AS ``count```#142]\n03       +- Aggregate [city#8, year#9], [city#8, year#9, count(1) AS count(1) AS `count`#134L]\n04          +- Project [_1#3 AS id#7, _2#4 AS city#8, _3#5 AS year#9]\n05             +- LocalRelation [_1#3, _2#4, _3#5]\n\nval afterPushDown = PushDownPredicate(pivotPlanAnalyzed)\nscala&gt; println(afterPushDown.numberedTreeString)\n00 Project [city#8, __pivot_count(1) AS `count` AS `count(1) AS ``count```#142[0] AS 2015#143L, __pivot_count(1) AS `count` AS `count(1) AS ``count```#142[1] AS 2016#144L, __pivot_count(1) AS `count` AS `count(1) AS ``count```#142[2] AS 2017#145L]\n01 +- Aggregate [city#8], [city#8, pivotfirst(year#9, count(1) AS `count`#134L, 2015, 2016, 2017, 0, 0) AS __pivot_count(1) AS `count` AS `count(1) AS ``count```#142]\n02    +- Aggregate [city#8, year#9], [city#8, year#9, count(1) AS count(1) AS `count`#134L]\n03       +- Project [_1#3 AS id#7, _2#4 AS city#8, _3#5 AS year#9]\n04          +- Filter (_2#4 = Boston)\n05             +- LocalRelation [_1#3, _2#4, _3#5]\n</code></pre>","text":""},{"location":"logical-operators/AlterTable/","title":"AlterTable Logical Command","text":"<p><code>AlterTable</code> is a Command for <code>ALTER TABLE</code> SQL commands:</p> <ul> <li><code>ALTER TABLE ADD COLUMNS</code></li> <li><code>ALTER TABLE REPLACE COLUMNS</code></li> <li><code>ALTER TABLE CHANGE COLUMN</code></li> <li><code>ALTER TABLE RENAME COLUMN</code></li> <li><code>ALTER TABLE DROP COLUMNS</code></li> <li><code>ALTER TABLE SET TBLPROPERTIES</code></li> <li><code>ALTER TABLE UNSET TBLPROPERTIES</code></li> <li><code>ALTER TABLE SET LOCATION</code></li> </ul>"},{"location":"logical-operators/AlterTable/#creating-instance","title":"Creating Instance","text":"<p><code>AlterTable</code> takes the following to be created:</p> <ul> <li> TableCatalog <li> <code>Identifier</code> <li> NamedRelation for the table <li> TableChanges <p><code>AlterTable</code> is created\u00a0when:</p> <ul> <li><code>CatalogV2Util</code> is requested to createAlterTable</li> </ul>"},{"location":"logical-operators/AlterTable/#execution-planning","title":"Execution Planning","text":"<p><code>AlterTable</code> is resolved to AlterTableExec by DataSourceV2Strategy execution planning strategy.</p>"},{"location":"logical-operators/AlterTableAddColumnsCommand/","title":"AlterTableAddColumnsCommand Logical Runnable Command","text":"<p><code>AlterTableAddColumnsCommand</code> is a LeafRunnableCommand that represents AddColumns logical operator.</p>"},{"location":"logical-operators/AlterTableAddColumnsCommand/#creating-instance","title":"Creating Instance","text":"<p><code>AlterTableAddColumnsCommand</code> takes the following to be created:</p> <ul> <li> Table (<code>TableIdentifier</code>) <li> Columns to Add (StructFields) <p><code>AlterTableAddColumnsCommand</code> is created when:</p> <ul> <li>ResolveSessionCatalog logical resolution rule is executed (and resolves AddColumns logical operator)</li> </ul>"},{"location":"logical-operators/AlterTableAddColumnsCommand/#executing-command","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> verifyAlterTableAddColumn (with the SessionCatalog).</p> <p><code>run</code> uncaches the table.</p> <p><code>run</code> requests the <code>SessionCatalog</code> to refreshTable.</p> <p><code>run</code> checks the column names (against any duplications) and types, and re-constructs the original schema of columns from their column metadata (if there is any).</p> <p><code>run</code> requests the <code>SessionCatalog</code> to alterTableDataSchema.</p>  <p><code>run</code> is part of the RunnableCommand abstraction.</p>","text":""},{"location":"logical-operators/AlterTableAddColumnsCommand/#verifyaltertableaddcolumn","title":"verifyAlterTableAddColumn <pre><code>verifyAlterTableAddColumn(\n  conf: SQLConf,\n  catalog: SessionCatalog,\n  table: TableIdentifier): CatalogTable\n</code></pre> <p><code>verifyAlterTableAddColumn</code> asserts that the table is as follows:</p> <ol> <li>The table is not a view</li> <li>The table is a Hive table or one of the supported FileFormats</li> </ol>  <p><code>verifyAlterTableAddColumn</code> requests the given SessionCatalog for the getTempViewOrPermanentTableMetadata.</p> <p><code>verifyAlterTableAddColumn</code> throws an <code>AnalysisException</code> if the <code>table</code> is a <code>VIEW</code>:</p> <pre><code>ALTER ADD COLUMNS does not support views.\nYou must drop and re-create the views for adding the new columns. Views: [table]\n</code></pre> <p>For a Spark table (that is non-Hive), <code>verifyAlterTableAddColumn</code> finds the implementation of the table provider and makes sure that the table provider is one of the following supported file formats:</p> <ul> <li><code>CSVFileFormat</code> or <code>CSVDataSourceV2</code></li> <li><code>JsonFileFormat</code> or <code>JsonDataSourceV2</code></li> <li>ParquetFileFormat or <code>ParquetDataSourceV2</code></li> <li><code>OrcFileFormat</code> or <code>OrcDataSourceV2</code></li> </ul> <p>Otherwise, <code>verifyAlterTableAddColumn</code> throws an <code>AnalysisException</code>:</p> <pre><code>ALTER ADD COLUMNS does not support datasource table with type [tableType].\nYou must drop and re-create the table for adding the new columns. Tables: [table]\n</code></pre>","text":""},{"location":"logical-operators/AlterTableCommand/","title":"AlterTableCommand","text":"<p><code>AlterTableCommand</code> is an extension of the Command abstraction for unary logical commands to alter a table.</p>"},{"location":"logical-operators/AlterTableCommand/#contract","title":"Contract","text":""},{"location":"logical-operators/AlterTableCommand/#tablechanges","title":"TableChanges <pre><code>changes: Seq[TableChange]\n</code></pre> <p>TableChanges to apply to the table</p> <p>Used when:</p> <ul> <li>DataSourceV2Strategy execution planning strategy is executed (to plan an <code>AlterTableCommand</code> to AlterTableExec)</li> </ul>","text":""},{"location":"logical-operators/AlterTableCommand/#table","title":"Table <pre><code>table: LogicalPlan\n</code></pre> <p>LogicalPlan of the table to alter</p> <p>Used when:</p> <ul> <li><code>ResolveAlterTableCommands</code> analysis rule is executed</li> <li><code>AlterTableCommand</code> is requested for the child</li> <li>DataSourceV2Strategy execution planning strategy is executed (to plan an <code>AlterTableCommand</code> to AlterTableExec)</li> </ul>","text":""},{"location":"logical-operators/AlterTableCommand/#implementations","title":"Implementations","text":"<ul> <li><code>AddColumns</code></li> <li><code>AlterColumn</code></li> <li><code>CommentOnTable</code></li> <li><code>DropColumns</code></li> <li>RenameColumn</li> <li><code>ReplaceColumns</code></li> <li><code>SetTableLocation</code></li> <li><code>SetTableProperties</code></li> <li><code>UnsetTableProperties</code></li> </ul>"},{"location":"logical-operators/AlterTableCommand/#renamecolumn","title":"RenameColumn    AlterTableCommand TableChange SQL     <code>RenameColumn</code> RenameColumn ALTER TABLE RENAME COLUMN","text":""},{"location":"logical-operators/AlterTableCommand/#execution-planning","title":"Execution Planning","text":"<p><code>AlterTableCommand</code>s are planned as AlterTableExecs (by DataSourceV2Strategy execution planning strategy).</p>"},{"location":"logical-operators/AnalyzeColumn/","title":"AnalyzeColumn Logical Command","text":"<p><code>AnalyzeColumn</code> is a Command for ANALYZE TABLE FOR COLUMNS SQL statement.</p> <p><code>AnalyzeColumn</code> is resolved to AnalyzeColumnCommand logical command (by ResolveSessionCatalog logical resolution rule).</p>"},{"location":"logical-operators/AnalyzeColumn/#creating-instance","title":"Creating Instance","text":"<p><code>AnalyzeColumn</code> takes the following to be created:</p> <ul> <li> Child Logical Operator <li> Column Names <li> <code>allColumns</code> Flag <p><code>AnalyzeColumn</code> requires that either the column names or allColumns flag is defined (as mutually exclusive).</p> <p><code>AnalyzeColumn</code> is created\u00a0when:</p> <ul> <li><code>AstBuilder</code> is requested to parse ANALYZE TABLE FOR COLUMNS statement</li> </ul>"},{"location":"logical-operators/AnalyzeColumn/#query-planning","title":"Query Planning","text":"<p>DataSourceV2Strategy throws an <code>AnalysisException</code> for <code>AnalyzeColumn</code>s over <code>ResolvedTable</code>s:</p> <pre><code>ANALYZE TABLE is not supported for v2 tables.\n</code></pre>"},{"location":"logical-operators/AnalyzeColumnCommand/","title":"AnalyzeColumnCommand Logical Command","text":"<p><code>AnalyzeColumnCommand</code> is a logical command to represent AnalyzeColumn logical operator.</p> <p><code>AnalyzeColumnCommand</code> is not supported on views (unless they are cached).</p>"},{"location":"logical-operators/AnalyzeColumnCommand/#creating-instance","title":"Creating Instance","text":"<p><code>AnalyzeColumnCommand</code> takes the following to be created:</p> <ul> <li> Table <li> Column Names <li> <code>allColumns</code> Flag <p><code>AnalyzeColumnCommand</code> is created\u00a0when:</p> <ul> <li>ResolveSessionCatalog logical resolution rule is executed (to resolve an AnalyzeColumn)</li> </ul>"},{"location":"logical-operators/AnalyzeColumnCommand/#executing-logical-command","title":"Executing Logical Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> is part of RunnableCommand abstraction.</p> <p><code>run</code> calculates the following statistics:</p> <ul> <li>sizeInBytes</li> <li>stats for each column</li> </ul>","text":""},{"location":"logical-operators/AnalyzeColumnCommand/#computing-statistics-for-specified-columns","title":"Computing Statistics for Specified Columns <pre><code>computeColumnStats(\n  sparkSession: SparkSession,\n  tableIdent: TableIdentifier,\n  columnNames: Seq[String]): (Long, Map[String, ColumnStat])\n</code></pre> <p><code>computeColumnStats</code>...FIXME</p>","text":""},{"location":"logical-operators/AnalyzeColumnCommand/#computing-percentiles","title":"Computing Percentiles <pre><code>computePercentiles(\n  attributesToAnalyze: Seq[Attribute],\n  sparkSession: SparkSession,\n  relation: LogicalPlan): AttributeMap[ArrayData]\n</code></pre> <p><code>computePercentiles</code>...FIXME</p>","text":""},{"location":"logical-operators/AnalyzeColumnCommand/#analyzecolumnincatalog","title":"analyzeColumnInCatalog <pre><code>analyzeColumnInCatalog(\n  sparkSession: SparkSession): Unit\n</code></pre> <p><code>analyzeColumnInCatalog</code> requests the SessionCatalog for getTableMetadata of the table.</p> <p>For <code>VIEW</code> catalog tables, <code>analyzeColumnInCatalog</code> analyzes the columnNames if it's a cached view (or throws an AnalysisException).</p> <p>For <code>EXTERNAL</code> and <code>MANAGED</code> catalog tables, <code>analyzeColumnInCatalog</code> getColumnsToAnalyze for the columnNames.</p> <p><code>analyzeColumnInCatalog</code> computeColumnStats for the columnNames.</p> <p><code>analyzeColumnInCatalog</code> converts the column stats to CatalogColumnStat.</p> <p><code>analyzeColumnInCatalog</code> creates a CatalogStatistics with the following:</p>    Property Value     <code>sizeInBytes</code> calculateTotalSize   <code>rowCount</code> computeColumnStats   <code>colStats</code> CatalogStatistics with the new CatalogColumnStats applied    <p>In the end, <code>analyzeColumnInCatalog</code> requests the SessionCatalog to alter the table with the new CatalogStatistics.</p>","text":""},{"location":"logical-operators/AnalyzeColumnCommand/#analysisexception","title":"AnalysisException <p><code>analyzeColumnInCatalog</code> throws the following <code>AnalysisException</code> unless the catalog view is cached:</p> <pre><code>ANALYZE TABLE is not supported on views.\n</code></pre>","text":""},{"location":"logical-operators/AnalyzeColumnCommand/#demo","title":"Demo <p><code>AnalyzeColumnCommand</code> can generate column histograms when spark.sql.statistics.histogram.enabled configuration property is enabled. <code>AnalyzeColumnCommand</code> supports column histograms for the following data types:</p> <ul> <li><code>IntegralType</code></li> <li><code>DecimalType</code></li> <li><code>DoubleType</code></li> <li><code>FloatType</code></li> <li><code>DateType</code></li> <li><code>TimestampType</code></li> </ul> <pre><code>// ./bin/spark-shell --conf spark.sql.statistics.histogram.enabled=true\n// Use the above example to set up the environment\n// Make sure that ANALYZE TABLE COMPUTE STATISTICS FOR COLUMNS was run with histogram enabled\n\n// There are 254 bins by default\n// Use spark.sql.statistics.histogram.numBins to control the bins\nval descExtSQL = s\"DESC EXTENDED $tableName p1\"\nscala&gt; spark.sql(descExtSQL).show(truncate = false)\n+--------------+-----------------------------------------------------+\n|info_name     |info_value                                           |\n+--------------+-----------------------------------------------------+\n|col_name      |p1                                                   |\n|data_type     |double                                               |\n|comment       |NULL                                                 |\n|min           |0.0                                                  |\n|max           |1.4                                                  |\n|num_nulls     |0                                                    |\n|distinct_count|2                                                    |\n|avg_col_len   |8                                                    |\n|max_col_len   |8                                                    |\n|histogram     |height: 0.007874015748031496, num_of_bins: 254       |\n|bin_0         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_1         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_2         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_3         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_4         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_5         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_6         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_7         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_8         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n|bin_9         |lower_bound: 0.0, upper_bound: 0.0, distinct_count: 1|\n+--------------+-----------------------------------------------------+\nonly showing top 20 rows\n</code></pre>","text":""},{"location":"logical-operators/AnalyzeColumnCommand/#demo_1","title":"Demo <pre><code>// Make the example reproducible\nval tableName = \"t1\"\nimport org.apache.spark.sql.catalyst.TableIdentifier\nval tableId = TableIdentifier(tableName)\n\nval sessionCatalog = spark.sessionState.catalog\nsessionCatalog.dropTable(tableId, ignoreIfNotExists = true, purge = true)\n\nval df = Seq((0, 0.0, \"zero\"), (1, 1.4, \"one\")).toDF(\"id\", \"p1\", \"p2\")\ndf.write.saveAsTable(\"t1\")\n\n// AnalyzeColumnCommand represents ANALYZE TABLE...FOR COLUMNS SQL command\nval allCols = df.columns.mkString(\",\")\nval analyzeTableSQL = s\"ANALYZE TABLE $tableName COMPUTE STATISTICS FOR COLUMNS $allCols\"\nval plan = spark.sql(analyzeTableSQL).queryExecution.logical\nimport org.apache.spark.sql.execution.command.AnalyzeColumnCommand\nval cmd = plan.asInstanceOf[AnalyzeColumnCommand]\nscala&gt; println(cmd)\nAnalyzeColumnCommand `t1`, [id, p1, p2]\n\nspark.sql(analyzeTableSQL)\nval stats = sessionCatalog.getTableMetadata(tableId).stats.get\nscala&gt; println(stats.simpleString)\n1421 bytes, 2 rows\n\nscala&gt; stats.colStats.map { case (c, ss) =&gt; s\"$c: $ss\" }.foreach(println)\nid: ColumnStat(2,Some(0),Some(1),0,4,4,None)\np1: ColumnStat(2,Some(0.0),Some(1.4),0,8,8,None)\np2: ColumnStat(2,None,None,0,4,4,None)\n\n// Use DESC EXTENDED for friendlier output\nscala&gt; sql(s\"DESC EXTENDED $tableName id\").show\n+--------------+----------+\n|     info_name|info_value|\n+--------------+----------+\n|      col_name|        id|\n|     data_type|       int|\n|       comment|      NULL|\n|           min|         0|\n|           max|         1|\n|     num_nulls|         0|\n|distinct_count|         2|\n|   avg_col_len|         4|\n|   max_col_len|         4|\n|     histogram|      NULL|\n+--------------+----------+\n</code></pre>","text":""},{"location":"logical-operators/AnalyzePartitionCommand/","title":"AnalyzePartitionCommand Logical Command","text":"<p><code>AnalyzePartitionCommand</code> is a LeafRunnableCommand that represents ANALYZE TABLE PARTITION COMPUTE STATISTICS (AnalyzeTable logical command with partition specification) at analysis.</p> <p>AnalyzeTableCommand Logical Command</p> <p>AnalyzeTable logical command without partition specification is resolved to AnalyzeTableCommand logical command.</p>"},{"location":"logical-operators/AnalyzePartitionCommand/#creating-instance","title":"Creating Instance","text":"<p><code>AnalyzePartitionCommand</code> takes the following to be created:</p> <ul> <li> <code>TableIdentifier</code> <li> Partition Spec (<code>Map[String, Option[String]]</code>) <li> <code>noscan</code> flag (default: <code>true</code>) that indicates whether NOSCAN option was used or not <p><code>AnalyzePartitionCommand</code> is created when:</p> <ul> <li>ResolveSessionCatalog logical analysis rule is executed (to resolve an AnalyzeTable logical command)</li> </ul>"},{"location":"logical-operators/AnalyzePartitionCommand/#demo","title":"Demo","text":"<pre><code>// Seq((0, 0, \"zero\"), (1, 1, \"one\")).toDF(\"id\", \"p1\", \"p2\").write.partitionBy(\"p1\", \"p2\").saveAsTable(\"t1\")\nval analyzeTable = \"ANALYZE TABLE t1 PARTITION (p1, p2) COMPUTE STATISTICS\"\nval plan = spark.sql(analyzeTable).queryExecution.logical\nimport org.apache.spark.sql.execution.command.AnalyzePartitionCommand\nval cmd = plan.asInstanceOf[AnalyzePartitionCommand]\nscala&gt; println(cmd)\nAnalyzePartitionCommand `t1`, Map(p1 -&gt; None, p2 -&gt; None), false\n</code></pre>"},{"location":"logical-operators/AnalyzeTable/","title":"AnalyzeTable","text":"<p><code>AnalyzeTable</code> is a <code>UnaryCommand</code> (and a Logical Command) for ANALYZE TABLE SQL statement.</p>"},{"location":"logical-operators/AnalyzeTable/#creating-instance","title":"Creating Instance","text":"<p><code>AnalyzeTable</code> takes the following to be created:</p> <ul> <li> Child LogicalPlan <li> Partitions <li> <code>noScan</code> flag <p><code>AnalyzeTable</code> is created\u00a0when:</p> <ul> <li><code>AstBuilder</code> is requested to parse ANALYZE TABLE statement</li> </ul>"},{"location":"logical-operators/AnalyzeTable/#logical-analysis","title":"Logical Analysis","text":"<p><code>AnalyzeTable</code> is resolved to the following logical runnable commands (by ResolveSessionCatalog logical resolution rule):</p> <ul> <li>AnalyzePartitionCommand</li> <li>AnalyzeTableCommand (with no partitions)</li> </ul>"},{"location":"logical-operators/AnalyzeTable/#query-planning","title":"Query Planning","text":"<p>DataSourceV2Strategy throws an <code>AnalysisException</code> for <code>AnalyzeTable</code>s over <code>ResolvedTable</code>s:</p> <pre><code>ANALYZE TABLE is not supported for v2 tables.\n</code></pre>"},{"location":"logical-operators/AnalyzeTableCommand/","title":"AnalyzeTableCommand Logical Command","text":"<p><code>AnalyzeTableCommand</code> is a LeafRunnableCommand that computes statistics (and stores them in a metastore).</p>"},{"location":"logical-operators/AnalyzeTableCommand/#creating-instance","title":"Creating Instance","text":"<p><code>AnalyzeTableCommand</code> takes the following to be created:</p> <ul> <li> Multi-part table identifier <li> <code>noScan</code> flag (default: <code>true</code> that indicates whether NOSCAN option was used or not) <p><code>AnalyzeTableCommand</code> is created when:</p> <ul> <li>ResolveSessionCatalog logical resolution rule is executed (and resolves an AnalyzeTable logical operator with no <code>PARTITION</code>s)</li> </ul>"},{"location":"logical-operators/AnalyzeTableCommand/#run","title":"run <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> analyzes the given table.</p> <p><code>run</code> returns an empty collection.</p> <p><code>run</code> is part of the RunnableCommand abstraction.</p>","text":""},{"location":"logical-operators/AnalyzeTableCommand/#demo","title":"Demo","text":""},{"location":"logical-operators/AnalyzeTableCommand/#analyzetablecommand","title":"AnalyzeTableCommand","text":"<pre><code>// Seq((0, 0, \"zero\"), (1, 1, \"one\")).toDF(\"id\", \"p1\", \"p2\").write.partitionBy(\"p1\", \"p2\").saveAsTable(\"t1\")\nval sqlText = \"ANALYZE TABLE t1 COMPUTE STATISTICS NOSCAN\"\nval plan = spark.sql(sqlText).queryExecution.logical\nimport org.apache.spark.sql.execution.command.AnalyzeTableCommand\nval cmd = plan.asInstanceOf[AnalyzeTableCommand]\nscala&gt; println(cmd)\nAnalyzeTableCommand `t1`, false\n</code></pre>"},{"location":"logical-operators/AnalyzeTableCommand/#create-table","title":"CREATE TABLE","text":"<pre><code>sql(\"CREATE TABLE demo_cbo VALUES (0, 'zero'), (1, 'one') AS t1(id, name)\")\n</code></pre> <pre><code>scala&gt; sql(\"desc extended demo_cbo\").show(numRows = Integer.MAX_VALUE, truncate = false)\n+----------------------------+----------------------------------------------------------+-------+\n|col_name                    |data_type                                                 |comment|\n+----------------------------+----------------------------------------------------------+-------+\n|id                          |int                                                       |null   |\n|name                        |string                                                    |null   |\n|                            |                                                          |       |\n|# Detailed Table Information|                                                          |       |\n|Database                    |default                                                   |       |\n|Table                       |demo_cbo                                                  |       |\n|Owner                       |jacek                                                     |       |\n|Created Time                |Sat Feb 12 19:51:15 CET 2022                              |       |\n|Last Access                 |UNKNOWN                                                   |       |\n|Created By                  |Spark 3.2.1                                               |       |\n|Type                        |MANAGED                                                   |       |\n|Provider                    |hive                                                      |       |\n|Table Properties            |[transient_lastDdlTime=1644691875]                        |       |\n|Statistics                  |13 bytes                                                  |       |\n|Location                    |file:/Users/jacek/dev/oss/spark/spark-warehouse/demo_cbo  |       |\n|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe        |       |\n|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                  |       |\n|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat|       |\n|Storage Properties          |[serialization.format=1]                                  |       |\n|Partition Provider          |Catalog                                                   |       |\n+----------------------------+----------------------------------------------------------+-------+\n</code></pre>"},{"location":"logical-operators/AnalyzeTableCommand/#analyze-table-compute-statistics","title":"ANALYZE TABLE COMPUTE STATISTICS","text":"<pre><code>sql(\"ANALYZE TABLE demo_cbo COMPUTE STATISTICS\")\n</code></pre> <p>Note the extra <code>Statistics</code> row.</p> <pre><code>scala&gt; sql(\"desc extended demo_cbo\").show(numRows = Integer.MAX_VALUE, truncate = false)\n+----------------------------+----------------------------------------------------------+-------+\n|col_name                    |data_type                                                 |comment|\n+----------------------------+----------------------------------------------------------+-------+\n|id                          |int                                                       |null   |\n|name                        |string                                                    |null   |\n|                            |                                                          |       |\n|# Detailed Table Information|                                                          |       |\n|Database                    |default                                                   |       |\n|Table                       |demo_cbo                                                  |       |\n|Owner                       |jacek                                                     |       |\n|Created Time                |Sat Feb 12 19:51:15 CET 2022                              |       |\n|Last Access                 |UNKNOWN                                                   |       |\n|Created By                  |Spark 3.2.1                                               |       |\n|Type                        |MANAGED                                                   |       |\n|Provider                    |hive                                                      |       |\n|Table Properties            |[transient_lastDdlTime=1644691915]                        |       |\n|Statistics                  |13 bytes, 2 rows                                          |       |\n|Location                    |file:/Users/jacek/dev/oss/spark/spark-warehouse/demo_cbo  |       |\n|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe        |       |\n|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                  |       |\n|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat|       |\n|Storage Properties          |[serialization.format=1]                                  |       |\n|Partition Provider          |Catalog                                                   |       |\n+----------------------------+----------------------------------------------------------+-------+\n</code></pre>"},{"location":"logical-operators/AnalyzeTableCommand/#for-all-columns","title":"FOR ALL COLUMNS","text":"<pre><code>sql(\"ANALYZE TABLE demo_cbo COMPUTE STATISTICS FOR ALL COLUMNS\")\n</code></pre> <pre><code>// Use describeColName\nscala&gt; sql(\"desc extended demo_cbo id\").show(numRows = Integer.MAX_VALUE, truncate = false)\n+--------------+----------+\n|info_name     |info_value|\n+--------------+----------+\n|col_name      |id        |\n|data_type     |int       |\n|comment       |NULL      |\n|min           |0         |\n|max           |1         |\n|num_nulls     |0         |\n|distinct_count|2         |\n|avg_col_len   |4         |\n|max_col_len   |4         |\n|histogram     |NULL      |\n+--------------+----------+\n</code></pre> <pre><code>scala&gt; sql(\"desc extended demo_cbo name\").show(numRows = Integer.MAX_VALUE, truncate = false)\n+--------------+----------+\n|info_name     |info_value|\n+--------------+----------+\n|col_name      |name      |\n|data_type     |string    |\n|comment       |NULL      |\n|min           |NULL      |\n|max           |NULL      |\n|num_nulls     |0         |\n|distinct_count|2         |\n|avg_col_len   |4         |\n|max_col_len   |4         |\n|histogram     |NULL      |\n+--------------+----------+\n</code></pre>"},{"location":"logical-operators/AnalyzeTablesCommand/","title":"AnalyzeTablesCommand Logical Command","text":"<p><code>AnalyzeTablesCommand</code> is a LeafRunnableCommand that represents <code>ANALYZE TABLES COMPUTE STATISTICS</code> SQL statement (<code>AnalyzeTables</code> logical command) at query execution.</p>"},{"location":"logical-operators/AnalyzeTablesCommand/#creating-instance","title":"Creating Instance","text":"<p><code>AnalyzeTablesCommand</code> takes the following to be created:</p> <ul> <li> Database Name <li> <code>noScan</code> flag <p><code>AnalyzeTablesCommand</code> is created when:</p> <ul> <li>ResolveSessionCatalog analysis rule is executed (and resolves an <code>AnalyzeTables</code> logical command for <code>ANALYZE TABLES COMPUTE STATISTICS</code> SQL statement)</li> </ul>"},{"location":"logical-operators/AppendData/","title":"AppendData Logical Command","text":"<p><code>AppendData</code> is a V2WriteCommand that represents appending data (the result of executing a structured query) to a table (with the columns matching by name or position).</p>"},{"location":"logical-operators/AppendData/#creating-instance","title":"Creating Instance","text":"<p><code>AppendData</code> takes the following to be created:</p> <ul> <li> NamedRelation for the table (to append data to) <li> Query (LogicalPlan) <li> Write Options (<code>Map[String, String]</code>) <li>isByName flag</li> <li> Write <p><code>AppendData</code> is created using byName and byPosition operators.</p>"},{"location":"logical-operators/AppendData/#isbyname-flag","title":"isByName flag <p><code>AppendData</code> is given <code>isByName</code> flag when created:</p> <ul> <li>byName with the flag enabled (<code>true</code>)</li> <li>byPosition with the flag disabled (<code>false</code>)</li> </ul> <p><code>isByName</code> is part of the V2WriteCommand abstraction.</p>","text":""},{"location":"logical-operators/AppendData/#byname","title":"byName <pre><code>byName(\n  table: NamedRelation,\n  df: LogicalPlan,\n  writeOptions: Map[String, String] = Map.empty): AppendData\n</code></pre> <p><code>byName</code> creates a AppendData with the isByName flag enabled (<code>true</code>).</p> <p><code>byName</code> is used when:</p> <ul> <li><code>DataFrameWriter</code> is requested to saveInternal (with <code>SaveMode.Append</code> mode) and saveAsTable (with <code>SaveMode.Append</code> mode)</li> <li><code>DataFrameWriterV2</code> is requested to append</li> </ul>","text":""},{"location":"logical-operators/AppendData/#byposition","title":"byPosition <pre><code>byPosition(\n  table: NamedRelation,\n  query: LogicalPlan,\n  writeOptions: Map[String, String] = Map.empty): AppendData\n</code></pre> <p><code>byPosition</code> creates a AppendData with the isByName flag disabled (<code>false</code>).</p> <p><code>byPosition</code> is used when:</p> <ul> <li>ResolveInsertInto logical resolution rule is executed</li> <li><code>DataFrameWriter</code> is requested to insertInto</li> </ul>","text":""},{"location":"logical-operators/AppendData/#execution-planning","title":"Execution Planning <p><code>AppendData</code> is planned as one of the physical operators by DataSourceV2Strategy execution planning strategy:</p> <ul> <li><code>AppendDataExecV1</code></li> <li><code>AppendDataExec</code></li> </ul>","text":""},{"location":"logical-operators/CTERelationDef/","title":"CTERelationDef Unary Logical Operator","text":"<p><code>CTERelationDef</code> is a unary logical operator.</p>"},{"location":"logical-operators/CTERelationDef/#creating-instance","title":"Creating Instance","text":"<p><code>CTERelationDef</code> takes the following to be created:</p> <ul> <li> Child logical operator <li> ID (default: a new unique ID) <p><code>CTERelationDef</code> is created when:</p> <ul> <li>CTESubstitution logical analysis rule is executed</li> </ul>"},{"location":"logical-operators/CTERelationDef/#node-patterns","title":"Node Patterns <pre><code>nodePatterns: Seq[TreePattern]\n</code></pre> <p><code>nodePatterns</code> is CTE.</p> <p><code>nodePatterns</code> is part of the TreeNode abstraction.</p>","text":""},{"location":"logical-operators/CTERelationRef/","title":"CTERelationRef Leaf Logical Operator","text":"<p><code>CTERelationRef</code> is a leaf logical operator.</p>"},{"location":"logical-operators/CTERelationRef/#creating-instance","title":"Creating Instance","text":"<p><code>CTERelationRef</code> takes the following to be created:</p> <ul> <li> CTE Id <li> <code>_resolved</code> flag <li> Output Attributes <li> Optional Statistics (default: <code>None</code>) <p><code>CTERelationRef</code> is created when:</p> <ul> <li>CTESubstitution logical resolution rule is executed</li> <li>ResolveWithCTE logical resolution rule is executed</li> </ul>"},{"location":"logical-operators/CTERelationRef/#multiinstancerelation","title":"MultiInstanceRelation <p><code>CTERelationRef</code> is a MultiInstanceRelation.</p>","text":""},{"location":"logical-operators/CTERelationRef/#node-patterns","title":"Node Patterns <pre><code>nodePatterns: Seq[TreePattern]\n</code></pre> <p><code>nodePatterns</code> is CTE.</p> <p><code>nodePatterns</code> is part of the TreeNode abstraction.</p>","text":""},{"location":"logical-operators/CTERelationRef/#query-planning","title":"Query Planning <p><code>CTERelationRef</code> logical operators are planned by WithCTEStrategy execution planning strategy (to ShuffleExchangeExec physical operators).</p>","text":""},{"location":"logical-operators/CacheTableCommand/","title":"CacheTableCommand Runnable Logical Command","text":"<p>When &lt;&gt;, <code>CacheTableCommand</code> creates a DataFrame followed by registering a temporary view for the optional <code>query</code>. <pre><code>CACHE LAZY? TABLE [table] (AS? [query])?\n</code></pre> <p><code>CacheTableCommand</code> requests the session-specific <code>Catalog</code> to cache the table.</p> <p>Note</p> <p><code>CacheTableCommand</code> uses <code>SparkSession</code> to access the <code>Catalog</code>.</p> <p>If the caching is not <code>LAZY</code> (which is not by default), <code>CacheTableCommand</code> creates a DataFrame for the table and counts the rows (that will trigger the caching).</p> <p>Note</p> <p><code>CacheTableCommand</code> uses a Spark SQL pattern to trigger DataFrame caching by executing <code>count</code> operation.</p> <pre><code>val q = \"CACHE TABLE ids AS SELECT * from range(5)\"\nscala&gt; println(sql(q).queryExecution.logical.numberedTreeString)\n00 CacheTableCommand `ids`, false\n01    +- 'Project [*]\n02       +- 'UnresolvedTableValuedFunction range, [5]\n\n// ids table is already cached but let's use it anyway (and see what happens)\nval q2 = \"CACHE LAZY TABLE ids\"\nscala&gt; println(sql(q2).queryExecution.logical.numberedTreeString)\n17/05/17 06:16:39 WARN CacheManager: Asked to cache already cached data.\n00 CacheTableCommand `ids`, true\n</code></pre>"},{"location":"logical-operators/ClearCacheCommand/","title":"ClearCacheCommand Logical Command","text":"<p><code>ClearCacheCommand</code> is a logical command to remove all cached tables from the in-memory cache.</p> <p><code>ClearCacheCommand</code> corresponds to <code>CLEAR CACHE</code> SQL statement.</p>"},{"location":"logical-operators/ClearCacheCommand/#clearcache-labeled-alternative","title":"clearCache Labeled Alternative","text":"<p><code>ClearCacheCommand</code> is described by <code>clearCache</code> labeled alternative in <code>statement</code> expression in SqlBaseParser.g4 and parsed using SparkSqlParser.</p>"},{"location":"logical-operators/CollectMetrics/","title":"CollectMetrics Logical Operator","text":"<p><code>CollectMetrics</code> is a unary logical operator that represents Dataset.observe operator (in the logical query plan).</p>"},{"location":"logical-operators/CollectMetrics/#creating-instance","title":"Creating Instance","text":"<p><code>CollectMetrics</code> takes the following to be created:</p> <ul> <li> Name <li> Metric NamedExpressions <li> Child logical operator <p><code>CollectMetrics</code> is created when Dataset.observe operator is used.</p>"},{"location":"logical-operators/CollectMetrics/#execution-planning","title":"Execution Planning","text":"<p><code>CollectMetrics</code> is planned as CollectMetricsExec physical operator.</p>"},{"location":"logical-operators/CollectMetrics/#demo","title":"Demo","text":"<pre><code>// TIP: Use AggregateExpressions\nval q = spark\n  .range(0, 5, 1, numPartitions = 1)\n  .observe(name = \"myMetric\", expr = count('id))\nval plan = q.queryExecution.logical\n\nscala&gt; println(plan.numberedTreeString)\n00 'CollectMetrics myMetric, [count('id) AS count(id)#46]\n01 +- Range (0, 5, step=1, splits=Some(1))\n</code></pre>"},{"location":"logical-operators/Command/","title":"Command \u2014 Eagerly-Executed Logical Operators","text":"<p><code>Command</code>\u00a0is an extension of the LogicalPlan abstraction for logical operators that are executed early in the query plan lifecycle (unlike logical operators in general).</p> <p><code>Command</code>\u00a0is a marker interface for logical operators that are executed when a <code>Dataset</code> is requested for the logical plan (which is after the query has been analyzed).</p>"},{"location":"logical-operators/Command/#implementations","title":"Implementations","text":"<ul> <li>AlterTable</li> <li>CommentOnTable</li> <li>DataWritingCommand</li> <li>DeleteFromTable</li> <li>DescribeRelation</li> <li>MergeIntoTable</li> <li>RunnableCommand</li> <li>SetCatalogAndNamespace</li> <li>ShowTableProperties</li> <li>ShowTables</li> <li>UpdateTable</li> <li>V2WriteCommand</li> <li>others</li> </ul>"},{"location":"logical-operators/Command/#output-attributes","title":"Output Attributes <pre><code>output: Seq[Attribute]\n</code></pre> <p><code>Command</code> has no output attributes by default.</p> <p><code>output</code>\u00a0is part of the QueryPlan abstraction.</p>","text":""},{"location":"logical-operators/Command/#child-logical-operators","title":"Child Logical Operators <pre><code>children: Seq[LogicalPlan]\n</code></pre> <p><code>Command</code> has no child logical operators by default.</p> <p><code>children</code>\u00a0is part of the TreeNode abstraction.</p>","text":""},{"location":"logical-operators/Command/#statistics","title":"Statistics <pre><code>stats: Statistics\n</code></pre> <p><code>stats</code>\u00a0is part of the LogicalPlanStats abstraction.</p>  <p><code>Command</code> has no Statistics by default.</p>","text":""},{"location":"logical-operators/CommentOnTable/","title":"CommentOnTable Logical Command","text":"<p><code>CommentOnTable</code> is a Command that represents COMMENT ON TABLE SQL command.</p>"},{"location":"logical-operators/CommentOnTable/#creating-instance","title":"Creating Instance","text":"<p><code>CommentOnTable</code> takes the following to be created:</p> <ul> <li> LogicalPlan <li> Comment <p><code>CommentOnTable</code> is created\u00a0when:</p> <ul> <li><code>AstBuilder</code> is requested to parse COMMENT ON TABLE command</li> </ul>"},{"location":"logical-operators/CommentOnTable/#execution-planning","title":"Execution Planning","text":"<p><code>CommentOnTable</code> is resolved to AlterTableExec by DataSourceV2Strategy execution planning strategy.</p>"},{"location":"logical-operators/CreateDataSourceTableAsSelectCommand/","title":"CreateDataSourceTableAsSelectCommand Logical Command","text":"<p><code>CreateDataSourceTableAsSelectCommand</code> is a &lt;&gt; that &lt;&gt; with the data from a &lt;&gt; (AS query). <p>NOTE: A DataSource table is a Spark SQL native table that uses any data source but Hive (per <code>USING</code> clause).</p> <p><code>CreateDataSourceTableAsSelectCommand</code> is &lt;&gt; when DataSourceAnalysis post-hoc logical resolution rule is executed (and resolves a CreateTable.md[CreateTable] logical operator for a Spark table with a &lt;&gt;). <p>NOTE: CreateDataSourceTableCommand.md[CreateDataSourceTableCommand] is used instead when a CreateTable.md[CreateTable] logical operator is used with no &lt;&gt;."},{"location":"logical-operators/CreateDataSourceTableAsSelectCommand/#sourceplaintext","title":"[source,plaintext]","text":"<p>val ctas = \"\"\"   CREATE TABLE users   USING csv   COMMENT 'users table'   LOCATION '/tmp/users'   AS SELECT * FROM VALUES ((0, \"jacek\")) \"\"\" scala&gt; sql(ctas) ... WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table <code>default</code>.<code>users</code> into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.</p> <p>val plan = sql(ctas).queryExecution.logical.numberedTreeString org.apache.spark.sql.AnalysisException: Table default.users already exists. You need to drop it first.;   at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:159)   at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResultlzycompute(commands.scala:104)   at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)   at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:115)   at org.apache.spark.sql.Dataset.anonfunlogicalPlan1(Dataset.scala:194)   at org.apache.spark.sql.Dataset.anonfunwithAction2(Dataset.scala:3370)   at org.apache.spark.sql.execution.SQLExecution.anonfunwithNewExecutionId1(SQLExecution.scala:78)   at org.apache.spark.sql.execution.SQLExecution.withSQLConfPropagated(SQLExecution.scala:125)   at org.apache.spark.sql.execution.SQLExecution.withNewExecutionId(SQLExecution.scala:73)   at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3370)   at org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:194)   at org.apache.spark.sql.Dataset.ofRows(Dataset.scala:79)   at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)   ... 49 elided</p>"},{"location":"logical-operators/CreateDataSourceTableAsSelectCommand/#creating-instance","title":"Creating Instance","text":"<p><code>CreateDataSourceTableAsSelectCommand</code> takes the following to be created:</p> <ul> <li>[[table]] CatalogTable</li> <li>[[mode]] SaveMode</li> <li>[[query]] AS query (LogicalPlan)</li> <li>[[outputColumnNames]] Output column names (<code>Seq[String]</code>)</li> </ul> <p>=== [[run]] Executing Data-Writing Logical Command -- <code>run</code> Method</p>"},{"location":"logical-operators/CreateDataSourceTableAsSelectCommand/#source-scala","title":"[source, scala]","text":"<p>run(   sparkSession: SparkSession,   child: SparkPlan): Seq[Row]</p> <p>NOTE: <code>run</code> is part of DataWritingCommand.md#run[DataWritingCommand] contract.</p> <p><code>run</code>...FIXME</p> <p><code>run</code> throws an <code>AssertionError</code> when the tableType of the CatalogTable is <code>VIEW</code> or the provider is undefined.</p>"},{"location":"logical-operators/CreateDataSourceTableAsSelectCommand/#savedataintotable","title":"saveDataIntoTable <pre><code>saveDataIntoTable(\n  session: SparkSession,\n  table: CatalogTable,\n  tableLocation: Option[URI],\n  physicalPlan: SparkPlan,\n  mode: SaveMode,\n  tableExists: Boolean): BaseRelation\n</code></pre> <p><code>saveDataIntoTable</code> creates a BaseRelation for...FIXME</p> <p><code>saveDataIntoTable</code>...FIXME</p>","text":""},{"location":"logical-operators/CreateDataSourceTableCommand/","title":"CreateDataSourceTableCommand Logical Command","text":"<p><code>CreateDataSourceTableCommand</code> is a RunnableCommand.md[logical command] that &lt;&gt; (in a SessionCatalog). <p><code>CreateDataSourceTableCommand</code> is created when DataSourceAnalysis posthoc logical resolution rule resolves a CreateTable.md[CreateTable] logical operator for a non-Hive table provider with no query.</p>"},{"location":"logical-operators/CreateDataSourceTableCommand/#creating-instance","title":"Creating Instance","text":"<p><code>CreateDataSourceTableCommand</code> takes the following to be created:</p> <ul> <li>[[table]] CatalogTable</li> <li>[[ignoreIfExists]] <code>ignoreIfExists</code> Flag</li> </ul> <p>=== [[run]] Executing Logical Command -- <code>run</code> Method</p>"},{"location":"logical-operators/CreateDataSourceTableCommand/#source-scala","title":"[source, scala]","text":""},{"location":"logical-operators/CreateDataSourceTableCommand/#runsparksession-sparksession-seqrow","title":"run(sparkSession: SparkSession): Seq[Row]","text":"<p>NOTE: <code>run</code> is part of &lt;&gt; to execute (run) a logical command. <p><code>run</code> requests a session-scoped <code>SessionCatalog</code> to create a table.</p> <p>NOTE: <code>run</code> uses the input <code>SparkSession</code> to SparkSession.md#sessionState[access SessionState] that in turn is used to SessionState.md#catalog[access the current SessionCatalog].</p> <p>Internally, <code>run</code> creates a BaseRelation to access the table's schema.</p> <p>CAUTION: FIXME</p> <p>NOTE: <code>run</code> accepts tables only (not views) with the provider defined.</p>"},{"location":"logical-operators/CreateTable/","title":"CreateTable Logical Operator","text":"<p><code>CreateTable</code> is a logical operator that represents (is &lt;&gt; for) the following: <ul> <li> <p><code>DataFrameWriter</code> is requested to create a table (for DataFrameWriter.saveAsTable operator)</p> </li> <li> <p><code>SparkSqlAstBuilder</code> is requested to visitCreateTable (for <code>CREATE TABLE</code> SQL command) or visitCreateHiveTable (for <code>CREATE EXTERNAL TABLE</code> SQL command)</p> </li> <li> <p><code>CatalogImpl</code> is requested to create a table (for Catalog.createTable operator)</p> </li> </ul> <p><code>CreateTable</code> requires that the table provider of the CatalogTable is defined or throws an <code>AssertionError</code>:</p> <pre><code>assertion failed: The table to be created must have a provider.\n</code></pre> <p>The optional &lt;&gt; is defined when used for the following: <ul> <li> <p><code>DataFrameWriter</code> is requested to create a table (for DataFrameWriter.saveAsTable operator)</p> </li> <li> <p><code>SparkSqlAstBuilder</code> is requested to visitCreateTable (for <code>CREATE TABLE</code> SQL command) or visitCreateHiveTable (for <code>CREATE EXTERNAL TABLE</code> SQL command) with an AS clause</p> </li> </ul> <p>[[resolved]] <code>CreateTable</code> can never be resolved and is replaced (resolved) with a logical command at analysis phase in the following rules:</p> <ul> <li> <p>(for non-hive data source tables) DataSourceAnalysis posthoc logical resolution rule to a &lt;&gt; or a &lt;&gt; logical command (when the &lt;&gt; was defined or not, respectively) <li> <p>(for hive tables) hive/HiveAnalysis.md[HiveAnalysis] post-hoc logical resolution rule to a <code>CreateTableCommand</code> or a CreateHiveTableAsSelectCommand logical command (when &lt;&gt; was defined or not, respectively) <p>=== [[creating-instance]] Creating CreateTable Instance</p> <p><code>CreateTable</code> takes the following to be created:</p> <ul> <li>[[tableDesc]] Table metadata</li> <li>[[mode]] SaveMode</li> <li>[[query]] Optional AS query (Logical query plan)</li> </ul> <p>When created, <code>CreateTable</code> makes sure that the optional &lt;&gt; is undefined only when the &lt;&gt; is <code>ErrorIfExists</code> or <code>Ignore</code>. <code>CreateTable</code> throws an <code>AssertionError</code> otherwise: <pre><code>assertion failed: create table without data insertion can only use ErrorIfExists or Ignore as SaveMode.\n</code></pre>"},{"location":"logical-operators/CreateTableAsSelect/","title":"CreateTableAsSelect Logical Command","text":"<p><code>CreateTableAsSelect</code> is a <code>BinaryCommand</code> with V2CreateTablePlan that represents the following high-level operators in a logical query plan:</p> <ul> <li>CREATE TABLE AS SELECT SQL statement</li> <li>DataFrameWriter.save</li> <li>DataFrameWriter.saveAsTable</li> <li>DataFrameWriterV2.create</li> </ul> <p><code>CreateTableAsSelect</code> is planned as <code>AtomicCreateTableAsSelectExec</code> or CreateTableAsSelectExec physical command at execution.</p>"},{"location":"logical-operators/CreateTableAsSelect/#creating-instance","title":"Creating Instance","text":"<p><code>CreateTableAsSelect</code> takes the following to be created:</p> <ul> <li> Name LogicalPlan <li> Partitioning Transforms <li> Query LogicalPlan <li> <code>TableSpec</code> <li> Write Options <li> <code>ignoreIfExists</code> flag <p><code>CreateTableAsSelect</code> is created\u00a0when:</p> <ul> <li><code>AstBuilder</code> is requested to parse CREATE TABLE SQL statement (CREATE TABLE AS SELECT (CTAS))</li> <li><code>DataFrameWriter</code> is requested to save (and saveInternal) and saveAsTable</li> <li><code>DataFrameWriterV2</code> is requested to create a table</li> </ul>"},{"location":"logical-operators/CreateTableAsSelect/#execution-planning","title":"Execution Planning","text":"<p><code>CreateTableAsSelect</code> is planned to one of the following by DataSourceV2Strategy execution planning strategy (based on a CatalogPlugin this <code>CreateTableAsSelect</code> is executed against):</p> <ul> <li><code>AtomicCreateTableAsSelectExec</code> unary physical command on a StagingTableCatalog</li> <li>CreateTableAsSelectExec physical command, otherwise</li> </ul>"},{"location":"logical-operators/CreateTableAsSelect/#legacy-logical-resolution","title":"(Legacy) Logical Resolution","text":"<p><code>CreateTableAsSelect</code> can be resolved to a <code>CreateTableV1</code> logical operator by ResolveSessionCatalog logical resolution rule (for non-isV2Provider legacy providers).</p>"},{"location":"logical-operators/CreateTempViewUsing/","title":"CreateTempViewUsing Logical Command","text":"<p><code>CreateTempViewUsing</code> is a LeafRunnableCommand that represents the following SQL statement at execution:</p> <pre><code>CREATE (OR REPLACE)? GLOBAL? TEMPORARY VIEW\ntableIdentifier ('(' colTypeList ')')?\nUSING multipartIdentifier\n(OPTIONS propertyList)?\n</code></pre>"},{"location":"logical-operators/CreateTempViewUsing/#creating-instance","title":"Creating Instance","text":"<p><code>CreateTempViewUsing</code> takes the following to be created:</p> <ul> <li> <code>TableIdentifier</code> <li> User-Specified Schema <li> <code>replace</code> flag <li> <code>global</code> flag <li> Provider Name <li> Options <p><code>CreateTempViewUsing</code> is created when:</p> <ul> <li><code>SparkSqlAstBuilder</code> is requested to parse CREATE TEMPORARY TABLE USING (deprecated) and CREATE TEMPORARY VIEW statements</li> </ul>"},{"location":"logical-operators/CreateViewCommand/","title":"CreateViewCommand Logical Command","text":"<p><code>CreateViewCommand</code> is a &lt;&gt; for &lt;&gt;. <p><code>CreateViewCommand</code> is &lt;&gt; to represent the following: <ul> <li> <p>&lt;&gt; SQL statements <li> <p><code>Dataset</code> operators: &lt;&gt;, &lt;&gt;, &lt;&gt; and &lt;&gt; <p>CAUTION: FIXME What's the difference between <code>CreateTempViewUsing</code>?</p> <p><code>CreateViewCommand</code> works with different &lt;&gt;. <p>[[viewType]] .CreateViewCommand Behaviour Per View Type [options=\"header\",cols=\"1m,2\",width=\"100%\"] |=== | View Type | Description / Side Effect</p> <p>| LocalTempView | [[LocalTempView]] A session-scoped local temporary view that is available until the session, that has created it, is stopped.</p> <p>When executed, <code>CreateViewCommand</code> requests the current <code>SessionCatalog</code> to create a temporary view.</p> <p>| GlobalTempView | [[GlobalTempView]] A cross-session global temporary view that is available until the Spark application stops.</p> <p>When executed, <code>CreateViewCommand</code> requests the current <code>SessionCatalog</code> to create a global view.</p> <p>| PersistedView | [[PersistedView]] A cross-session persisted view that is available until dropped.</p> <p>When executed, <code>CreateViewCommand</code> checks if the table exists. If it does and replace is enabled <code>CreateViewCommand</code> requests the current <code>SessionCatalog</code> to alter a table. Otherwise, when the table does not exist, <code>CreateViewCommand</code> requests the current <code>SessionCatalog</code> to create it. |===</p> <pre><code>/* CREATE [OR REPLACE] [[GLOBAL] TEMPORARY]\nVIEW [IF NOT EXISTS] tableIdentifier\n[identifierCommentList] [COMMENT STRING]\n[PARTITIONED ON identifierList]\n[TBLPROPERTIES tablePropertyList] AS query */\n\n// Demo table for \"AS query\" part\nspark.range(10).write.mode(\"overwrite\").saveAsTable(\"t1\")\n\n// The \"AS\" query\nval asQuery = \"SELECT * FROM t1\"\n\n// The following queries should all work fine\nval q1 = \"CREATE VIEW v1 AS \" + asQuery\nsql(q1)\n\nval q2 = \"CREATE OR REPLACE VIEW v1 AS \" + asQuery\nsql(q2)\n\nval q3 = \"CREATE OR REPLACE TEMPORARY VIEW v1 \" + asQuery\nsql(q3)\n\nval q4 = \"CREATE OR REPLACE GLOBAL TEMPORARY VIEW v1 \" + asQuery\nsql(q4)\n\nval q5 = \"CREATE VIEW IF NOT EXISTS v1 AS \" + asQuery\nsql(q5)\n\n// The following queries should all fail\n// the number of user-specified columns does not match the schema of the AS query\nval qf1 = \"CREATE VIEW v1 (c1 COMMENT 'comment', c2) AS \" + asQuery\nscala&gt; sql(qf1)\norg.apache.spark.sql.AnalysisException: The number of columns produced by the SELECT clause (num: `1`) does not match the number of column names specified by CREATE VIEW (num: `2`).;\n  at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:134)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n  at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190)\n  at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190)\n  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n  at org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:190)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:75)\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:641)\n  ... 49 elided\n\n// CREATE VIEW ... PARTITIONED ON is not allowed\nval qf2 = \"CREATE VIEW v1 PARTITIONED ON (c1, c2) AS \" + asQuery\nscala&gt; sql(qf2)\norg.apache.spark.sql.catalyst.parser.ParseException:\nOperation not allowed: CREATE VIEW ... PARTITIONED ON(line 1, pos 0)\n\n// Use the same name of t1 for a new view\nval qf3 = \"CREATE VIEW t1 AS \" + asQuery\nscala&gt; sql(qf3)\norg.apache.spark.sql.AnalysisException: `t1` is not a view;\n  at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:156)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n  at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190)\n  at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190)\n  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n  at org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:190)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:75)\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:641)\n  ... 49 elided\n\n// View already exists\nval qf4 = \"CREATE VIEW v1 AS \" + asQuery\nscala&gt; sql(qf4)\norg.apache.spark.sql.AnalysisException: View `v1` already exists. If you want to update the view definition, please use ALTER VIEW AS or CREATE OR REPLACE VIEW AS;\n  at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:169)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n  at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190)\n  at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190)\n  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n  at org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:190)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:75)\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:641)\n  ... 49 elided\n</code></pre> <p>[[innerChildren]] <code>CreateViewCommand</code> returns the &lt;&gt; when requested for the inner nodes (that should be shown as an inner nested tree of this node)."},{"location":"logical-operators/CreateViewCommand/#source-scala","title":"[source, scala]","text":"<p>val sqlText = \"CREATE VIEW v1 AS \" + asQuery val plan = spark.sessionState.sqlParser.parsePlan(sqlText) scala&gt; println(plan.numberedTreeString) 00 CreateViewCommand <code>v1</code>, SELECT * FROM t1, false, false, PersistedView 01    +- 'Project [*] 02       +- 'UnresolvedRelation <code>t1</code></p> <p>=== [[prepareTable]] Creating CatalogTable -- <code>prepareTable</code> Internal Method</p>"},{"location":"logical-operators/CreateViewCommand/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-operators/CreateViewCommand/#preparetablesession-sparksession-analyzedplan-logicalplan-catalogtable","title":"prepareTable(session: SparkSession, analyzedPlan: LogicalPlan): CatalogTable","text":"<p><code>prepareTable</code>...FIXME</p> <p>NOTE: <code>prepareTable</code> is used exclusively when <code>CreateViewCommand</code> logical command is &lt;&gt;. <p>=== [[run]] Executing Logical Command -- <code>run</code> Method</p>"},{"location":"logical-operators/CreateViewCommand/#source-scala_2","title":"[source, scala]","text":""},{"location":"logical-operators/CreateViewCommand/#runsparksession-sparksession-seqrow","title":"run(sparkSession: SparkSession): Seq[Row]","text":"<p>NOTE: <code>run</code> is part of &lt;&gt; to execute (run) a logical command. <p><code>run</code> requests the input <code>SparkSession</code> for the &lt;&gt; that is in turn requested to \"execute\" the &lt;&gt; (which simply creates a QueryExecution)."},{"location":"logical-operators/CreateViewCommand/#note","title":"[NOTE]","text":"<p><code>run</code> uses a &lt;&gt; in Spark SQL to make sure that a logical plan can be analyzed, i.e."},{"location":"logical-operators/CreateViewCommand/#source-scala_3","title":"[source, scala]","text":"<p>val qe = sparkSession.sessionState.executePlan(child) qe.assertAnalyzed() val analyzedPlan = qe.analyzed</p> <p>====</p> <p><code>run</code> &lt;&gt;. <p><code>run</code> requests the input <code>SparkSession</code> for the &lt;&gt; that is in turn requested for the &lt;&gt;. <p><code>run</code> then branches off per the &lt;&gt;: <ul> <li> <p>For &lt;&gt;, <code>run</code> &lt;&gt; the analyzed plan and requests the <code>SessionCatalog</code> to create or replace a local temporary view <li> <p>For &lt;&gt;, <code>run</code> also &lt;&gt; the analyzed plan and requests the <code>SessionCatalog</code> to create or replace a global temporary view <li> <p>For &lt;&gt;, <code>run</code> asks the <code>SessionCatalog</code> whether the table exists or not (given &lt;&gt;). <p>** If the &lt;&gt; exists and the &lt;&gt; flag is on, <code>run</code> simply does nothing (and exits) <p>** If the &lt;&gt; exists and the &lt;&gt; flag is on, <code>run</code> requests the <code>SessionCatalog</code> for the table metadata and replaces the table, i.e. <code>run</code> requests the <code>SessionCatalog</code> to drop the table followed by re-creating it (with a &lt;&gt;) <p>** If however the &lt;&gt; does not exist, <code>run</code> simply requests the <code>SessionCatalog</code> to create it (with a &lt;&gt;) <p><code>run</code> throws an <code>AnalysisException</code> for &lt;&gt; when they already exist, the &lt;&gt; flag is off and the table type is not a view. <pre><code>[name] is not a view\n</code></pre> <p><code>run</code> throws an <code>AnalysisException</code> for &lt;&gt; when they already exist and the &lt;&gt; and &lt;&gt; flags are off. <pre><code>View [name] already exists. If you want to update the view definition, please use ALTER VIEW AS or CREATE OR REPLACE VIEW AS\n</code></pre> <p><code>run</code> throws an <code>AnalysisException</code> if the &lt;&gt; are defined and their numbers is different from the number of &lt;&gt; of the analyzed logical plan. <pre><code>The number of columns produced by the SELECT clause (num: `[output.length]`) does not match the number of column names specified by CREATE VIEW (num: `[userSpecifiedColumns.length]`).\n</code></pre> <p>=== [[creating-instance]] Creating CreateViewCommand Instance</p> <p><code>CreateViewCommand</code> takes the following when created:</p> <ul> <li>[[name]] <code>TableIdentifier</code></li> <li>[[userSpecifiedColumns]] User-defined columns (as <code>Seq[(String, Option[String])]</code>)</li> <li>[[comment]] Optional comment</li> <li>[[properties]] Properties (as <code>Map[String, String]</code>)</li> <li>[[originalText]] Optional DDL statement</li> <li>[[child]] Child &lt;&gt; <li>[[allowExisting]] <code>allowExisting</code> flag</li> <li>[[replace]] <code>replace</code> flag</li> <li>&lt;&gt; <p>=== [[verifyTemporaryObjectsNotExists]] <code>verifyTemporaryObjectsNotExists</code> Internal Method</p>"},{"location":"logical-operators/CreateViewCommand/#source-scala_4","title":"[source, scala]","text":""},{"location":"logical-operators/CreateViewCommand/#verifytemporaryobjectsnotexistssparksession-sparksession-unit","title":"verifyTemporaryObjectsNotExists(sparkSession: SparkSession): Unit","text":"<p><code>verifyTemporaryObjectsNotExists</code>...FIXME</p> <p>NOTE: <code>verifyTemporaryObjectsNotExists</code> is used exclusively when <code>CreateViewCommand</code> logical command is &lt;&gt;. <p>=== [[aliasPlan]] <code>aliasPlan</code> Internal Method</p>"},{"location":"logical-operators/CreateViewCommand/#source-scala_5","title":"[source, scala]","text":""},{"location":"logical-operators/CreateViewCommand/#aliasplansession-sparksession-analyzedplan-logicalplan-logicalplan","title":"aliasPlan(session: SparkSession, analyzedPlan: LogicalPlan): LogicalPlan","text":"<p><code>aliasPlan</code>...FIXME</p> <p>NOTE: <code>aliasPlan</code> is used when <code>CreateViewCommand</code> logical command is &lt;&gt; (and &lt;&gt;)."},{"location":"logical-operators/DataSourceV2Relation/","title":"DataSourceV2Relation Leaf Logical Operator","text":"<p><code>DataSourceV2Relation</code> is a leaf logical operator that represents a scan over tables with support for BATCH_READ (at the very least).</p> <p><code>DataSourceV2Relation</code> is a NamedRelation.</p> <p><code>DataSourceV2Relation</code> is an ExposesMetadataColumns.</p>"},{"location":"logical-operators/DataSourceV2Relation/#creating-instance","title":"Creating Instance","text":"<p><code>DataSourceV2Relation</code> takes the following to be created:</p> <ul> <li> Table <li> Output <code>AttributeReference</code>s <li>CatalogPlugin</li> <li> (optional) <code>Identifier</code> <li> Case-Insensitive Options <p><code>DataSourceV2Relation</code> is created (indirectly) using create utility and withMetadataColumns.</p>"},{"location":"logical-operators/DataSourceV2Relation/#catalog","title":"CatalogPlugin","text":"<p><code>DataSourceV2Relation</code> can be given a CatalogPlugin when created.</p> <p>The <code>CatalogPlugin</code> can be as follows:</p> <ul> <li>Current Catalog for a single-part table reference</li> <li>v2SessionCatalog for global temp views</li> <li>Custom Catalog by name</li> </ul>"},{"location":"logical-operators/DataSourceV2Relation/#creating-datasourcev2relation","title":"Creating DataSourceV2Relation <pre><code>create(\n  table: Table,\n  catalog: Option[CatalogPlugin],\n  identifier: Option[Identifier]): DataSourceV2Relation\ncreate(\n  table: Table,\n  catalog: Option[CatalogPlugin],\n  identifier: Option[Identifier],\n  options: CaseInsensitiveStringMap): DataSourceV2Relation\n</code></pre> <p><code>create</code> replaces <code>CharType</code> and <code>VarcharType</code> types in the schema of the given Table with \"annotated\" <code>StringType</code> (as the query engine doesn't support char/varchar).</p> <p>In the end, <code>create</code> uses the new schema to create a DataSourceV2Relation.</p>  <p><code>create</code>\u00a0is used when:</p> <ul> <li><code>CatalogV2Util</code> utility is used to loadRelation</li> <li><code>DataFrameWriter</code> is requested to insertInto, saveAsTable and saveInternal</li> <li><code>DataSourceV2Strategy</code> execution planning strategy is requested to invalidateCache</li> <li><code>RenameTableExec</code> physical command is executed</li> <li>ResolveTables logical resolution rule is executed (and requested to lookupV2Relation)</li> <li>ResolveRelations logical resolution rule is executed (and requested to lookupRelation)</li> <li><code>DataFrameReader</code> is requested to load data</li> </ul>","text":""},{"location":"logical-operators/DataSourceV2Relation/#multiinstancerelation","title":"MultiInstanceRelation <p><code>DataSourceV2Relation</code> is a MultiInstanceRelation.</p>","text":""},{"location":"logical-operators/DataSourceV2Relation/#metadata-columns","title":"Metadata Columns  Signature <pre><code>metadataOutput: Seq[AttributeReference]\n</code></pre> <p><code>metadataOutput</code>\u00a0is part of the LogicalPlan abstraction.</p>  <p><code>metadataOutput</code> requests the Table for the metadata columns (if it is a SupportsMetadataColumns).</p> <p><code>metadataOutput</code> filters out metadata columns with the same name as regular output columns.</p>","text":""},{"location":"logical-operators/DataSourceV2Relation/#creating-datasourcev2relation-with-metadata-columns","title":"Creating DataSourceV2Relation with Metadata Columns <pre><code>withMetadataColumns(): DataSourceV2Relation\n</code></pre> <p><code>withMetadataColumns</code> creates a DataSourceV2Relation with the extra metadataOutput (for the output attributes) if defined.</p> <p><code>withMetadataColumns</code>\u00a0is used when:</p> <ul> <li>AddMetadataColumns logical resolution rule is executed</li> </ul>","text":""},{"location":"logical-operators/DataSourceV2Relation/#required-table-capabilities","title":"Required Table Capabilities <p>TableCapabilityCheck is used to assert the following regarding <code>DataSourceV2Relation</code> and the Table:</p> <ol> <li>Table supports BATCH_READ</li> <li>Table supports BATCH_WRITE or V1_BATCH_WRITE for AppendData (append in batch mode)</li> <li>Table supports BATCH_WRITE with OVERWRITE_DYNAMIC for OverwritePartitionsDynamic (dynamic overwrite in batch mode)</li> <li>Table supports BATCH_WRITE, V1_BATCH_WRITE or OVERWRITE_BY_FILTER possibly with TRUNCATE for OverwriteByExpression (truncate in batch mode and overwrite by filter in batch mode)</li> </ol>","text":""},{"location":"logical-operators/DataSourceV2Relation/#name","title":"Name  Signature <pre><code>name: String\n</code></pre> <p><code>name</code>\u00a0is part of the NamedRelation abstraction.</p>  <p><code>name</code> requests the Table for the name</p>","text":""},{"location":"logical-operators/DataSourceV2Relation/#simple-node-description","title":"Simple Node Description  Signature <pre><code>simpleString(\n  maxFields: Int): String\n</code></pre> <p><code>simpleString</code>\u00a0is part of the TreeNode abstraction.</p>  <p><code>simpleString</code> gives the following (with the output and the name):</p> <pre><code>RelationV2[output] [name]\n</code></pre>","text":""},{"location":"logical-operators/DataSourceV2Relation/#skipschemaresolution","title":"skipSchemaResolution  Signature <pre><code>skipSchemaResolution: Boolean\n</code></pre> <p><code>skipSchemaResolution</code>\u00a0is part of the NamedRelation abstraction.</p>  <p><code>skipSchemaResolution</code> is enabled (<code>true</code>) when the Table supports ACCEPT_ANY_SCHEMA capability.</p>","text":""},{"location":"logical-operators/DataSourceV2ScanRelation/","title":"DataSourceV2ScanRelation Leaf Logical Operator","text":"<p><code>DataSourceV2ScanRelation</code> is a leaf logical operator and a NamedRelation.</p>"},{"location":"logical-operators/DataSourceV2ScanRelation/#creating-instance","title":"Creating Instance","text":"<p><code>DataSourceV2ScanRelation</code> takes the following to be created:</p> <ul> <li> DataSourceV2Relation <li> Scan <li> Output Schema (<code>AttributeReference</code>s) <p><code>DataSourceV2ScanRelation</code> is created when:</p> <ul> <li>V2ScanRelationPushDown logical optimization is executed (for a DataSourceV2Relation)</li> </ul>"},{"location":"logical-operators/DataSourceV2ScanRelation/#name","title":"Name <pre><code>name: String\n</code></pre> <p><code>name</code> is part of the NamedRelation abstraction.</p> <p><code>name</code> requests the DataSourceV2Relation for the Table that is in turn requested for the name.</p>","text":""},{"location":"logical-operators/DataSourceV2ScanRelation/#simple-node-description","title":"Simple Node Description <pre><code>simpleString(\n  maxFields: Int): String\n</code></pre> <p><code>simpleString</code> is part of the TreeNode abstraction.</p> <p><code>simpleString</code> is the following (with the output schema and the name):</p> <pre><code>RelationV2[output] [name]\n</code></pre>","text":""},{"location":"logical-operators/DataSourceV2ScanRelation/#statistics","title":"Statistics <pre><code>computeStats(): Statistics\n</code></pre> <p><code>computeStats</code> is part of the LeafNode abstraction.</p> <p><code>computeStats</code>...FIXME</p>","text":""},{"location":"logical-operators/DataSourceV2ScanRelation/#execution-planning","title":"Execution Planning <p><code>DataSourceV2ScanRelation</code> is planned by DataSourceV2Strategy execution planning strategy to the following physical operators:</p> <ul> <li>RowDataSourceScanExec</li> <li>BatchScanExec</li> </ul>","text":""},{"location":"logical-operators/DataWritingCommand/","title":"DataWritingCommand Logical Commands","text":"<p><code>DataWritingCommand</code> is an extension of the <code>UnaryCommand</code> abstraction for logical commands that write the result of executing query (query data) to a relation (when executed).</p>"},{"location":"logical-operators/DataWritingCommand/#contract","title":"Contract","text":""},{"location":"logical-operators/DataWritingCommand/#output-column-names","title":"Output Column Names <pre><code>outputColumnNames: Seq[String]\n</code></pre> <p>The names of the output columns of the analyzed input query plan</p> <p>Used when:</p> <ul> <li><code>DataWritingCommand</code> is requested for the output columns</li> </ul>","text":""},{"location":"logical-operators/DataWritingCommand/#query","title":"Query <pre><code>query: LogicalPlan\n</code></pre> <p>The analyzed LogicalPlan representing the data to write (i.e. whose result will be inserted into a relation)</p> <p>Used when:</p> <ul> <li>BasicOperators execution planning strategy is executed</li> <li><code>DataWritingCommand</code> is requested for the child logical operator and the output columns</li> </ul>","text":""},{"location":"logical-operators/DataWritingCommand/#executing","title":"Executing <pre><code>run(\n  sparkSession: SparkSession,\n  child: SparkPlan): Seq[Row]\n</code></pre> <p>Used when:</p> <ul> <li><code>CreateHiveTableAsSelectBase</code> is requested to <code>run</code></li> <li>DataWritingCommandExec physical operator is requested for the sideEffectResult</li> </ul>","text":""},{"location":"logical-operators/DataWritingCommand/#implementations","title":"Implementations","text":"<ul> <li>CreateDataSourceTableAsSelectCommand</li> <li><code>CreateHiveTableAsSelectBase</code></li> <li>InsertIntoHadoopFsRelationCommand</li> <li>SaveAsHiveFile</li> </ul>"},{"location":"logical-operators/DataWritingCommand/#performance-metrics","title":"Performance Metrics","text":""},{"location":"logical-operators/DataWritingCommand/#job-commit-time","title":"job commit time","text":""},{"location":"logical-operators/DataWritingCommand/#number-of-dynamic-part","title":"number of dynamic part <p>Number of partitions (when processing write job statistics)</p> <p>Corresponds to the number of times when newPartition of BasicWriteTaskStatsTracker was called (that is to announce the fact that a new partition is about to be written)</p>","text":""},{"location":"logical-operators/DataWritingCommand/#number-of-output-rows","title":"number of output rows","text":""},{"location":"logical-operators/DataWritingCommand/#number-of-written-files","title":"number of written files","text":""},{"location":"logical-operators/DataWritingCommand/#task-commit-time","title":"task commit time","text":""},{"location":"logical-operators/DataWritingCommand/#written-output","title":"written output","text":""},{"location":"logical-operators/DataWritingCommand/#execution-planning","title":"Execution Planning <p><code>DataWritingCommand</code> is resolved to a DataWritingCommandExec physical operator by BasicOperators execution planning strategy.</p>","text":""},{"location":"logical-operators/DataWritingCommand/#basicWriteJobStatsTracker","title":"BasicWriteJobStatsTracker <pre><code>basicWriteJobStatsTracker(\n  hadoopConf: Configuration): BasicWriteJobStatsTracker // (1)!\nbasicWriteJobStatsTracker(\n  metrics: Map[String, SQLMetric],\n  hadoopConf: Configuration): BasicWriteJobStatsTracker\n</code></pre> <ol> <li>Uses the metrics</li> </ol> <p><code>basicWriteJobStatsTracker</code> creates a new BasicWriteJobStatsTracker (with the given Hadoop Configuration and the metrics).</p>  <p><code>basicWriteJobStatsTracker</code> is used when:</p> <ul> <li><code>FileFormatWriter</code> is requested to write data out</li> <li>InsertIntoHadoopFsRelationCommand logical command is executed</li> <li>SaveAsHiveFile logical command is executed (and requested to saveAsHiveFile)</li> </ul>","text":""},{"location":"logical-operators/DeleteFromTable/","title":"DeleteFromTable Logical Command","text":"<p><code>DeleteFromTable</code> is a Command that represents DELETE FROM SQL statement.</p> <p><code>DeleteFromTable</code> is a SupportsSubquery.</p>"},{"location":"logical-operators/DeleteFromTable/#creating-instance","title":"Creating Instance","text":"<p><code>DeleteFromTable</code> takes the following to be created:</p> <ul> <li> LogicalPlan of the table <li> Condition Expression (optional) <p><code>DeleteFromTable</code> is created\u00a0when:</p> <ul> <li><code>AstBuilder</code> is requested to parse DELETE FROM SQL statement</li> </ul>"},{"location":"logical-operators/DeleteFromTable/#execution-planning","title":"Execution Planning","text":"<p><code>DeleteFromTable</code> command is resolved to DeleteFromTableExec physical operator by DataSourceV2Strategy execution planning strategy.</p> <p>It is only supported for <code>DeleteFromTable</code> command over DataSourceV2ScanRelation relations (v2 tables).</p>"},{"location":"logical-operators/DescribeColumnCommand/","title":"DescribeColumnCommand Logical Command","text":"<p><code>DescribeColumnCommand</code> is a RunnableCommand.md[logical command] for spark-sql-SparkSqlAstBuilder.md#DescribeColumnCommand[DESCRIBE TABLE] SQL command with a single column only (i.e. no <code>PARTITION</code> specification).</p> <pre><code>[DESC|DESCRIBE] TABLE? [EXTENDED|FORMATTED] table_name column_name\n</code></pre>"},{"location":"logical-operators/DescribeColumnCommand/#source-scala","title":"[source, scala]","text":"<p>// Make the example reproducible val tableName = \"t1\" import org.apache.spark.sql.catalyst.TableIdentifier val tableId = TableIdentifier(tableName)</p> <p>val sessionCatalog = spark.sessionState.catalog sessionCatalog.dropTable(tableId, ignoreIfNotExists = true, purge = true)</p> <p>val df = Seq((0, 0.0, \"zero\"), (1, 1.4, \"one\")).toDF(\"id\", \"p1\", \"p2\") df.write.saveAsTable(\"t1\")</p> <p>// DescribeColumnCommand represents DESC EXTENDED tableName colName SQL command val descExtSQL = \"DESC EXTENDED t1 p1\" val plan = spark.sql(descExtSQL).queryExecution.logical import org.apache.spark.sql.execution.command.DescribeColumnCommand val cmd = plan.asInstanceOf[DescribeColumnCommand] scala&gt; println(cmd) DescribeColumnCommand <code>t1</code>, [p1], true</p> <p>scala&gt; spark.sql(descExtSQL).show +--------------+----------+ |     info_name|info_value| +--------------+----------+ |      col_name|        p1| |     data_type|    double| |       comment|      NULL| |           min|      NULL| |           max|      NULL| |     num_nulls|      NULL| |distinct_count|      NULL| |   avg_col_len|      NULL| |   max_col_len|      NULL| |     histogram|      NULL| +--------------+----------+</p> <p>// Run ANALYZE TABLE...FOR COLUMNS SQL command to compute the column statistics val allCols = df.columns.mkString(\",\") val analyzeTableSQL = s\"ANALYZE TABLE $tableName COMPUTE STATISTICS FOR COLUMNS $allCols\" spark.sql(analyzeTableSQL)</p> <p>scala&gt; spark.sql(descExtSQL).show +--------------+----------+ |     info_name|info_value| +--------------+----------+ |      col_name|        p1| |     data_type|    double| |       comment|      NULL| |           min|       0.0| |           max|       1.4| |     num_nulls|         0| |distinct_count|         2| |   avg_col_len|         8| |   max_col_len|         8| |     histogram|      NULL| +--------------+----------+</p> <p>[[output]] <code>DescribeColumnCommand</code> defines the Command.md#output[output schema] with the following columns:</p> <ul> <li><code>info_name</code> with \"name of the column info\" comment</li> <li><code>info_value</code> with \"value of the column info\" comment</li> </ul>"},{"location":"logical-operators/DescribeColumnCommand/#describetable-labeled-alternative","title":"describeTable Labeled Alternative","text":"<p><code>DescribeColumnCommand</code> is described by <code>describeTable</code> labeled alternative in <code>statement</code> expression in SqlBaseParser.g4 and parsed using SparkSqlParser.</p> <p>=== [[run]] Executing Logical Command (Describing Column with Optional Statistics) -- <code>run</code> Method</p>"},{"location":"logical-operators/DescribeColumnCommand/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-operators/DescribeColumnCommand/#runsession-sparksession-seqrow","title":"run(session: SparkSession): Seq[Row]","text":"<p>NOTE: <code>run</code> is part of &lt;&gt; to execute (run) a logical command. <p><code>run</code> resolves the &lt;&gt; in &lt;&gt; and makes sure that it is a \"flat\" field (i.e. not of a nested data type). <p><code>run</code> requests the <code>SessionCatalog</code> for the table metadata.</p> <p>NOTE: <code>run</code> uses the input <code>SparkSession</code> to access SparkSession.md#sessionState[SessionState] that in turn is used to access the SessionState.md#catalog[SessionCatalog].</p> <p><code>run</code> takes the CatalogStatistics.md#colStats[column statistics] from the  table statistics if available.</p> <p>NOTE: CatalogStatistics.md#colStats[Column statistics] are available (in the table statistics) only after AnalyzeColumnCommand.md[ANALYZE TABLE FOR COLUMNS] SQL command was run.</p> <p><code>run</code> adds <code>comment</code> metadata if available for the &lt;&gt;. <p><code>run</code> gives the following rows (in that order):</p> <p>. <code>col_name</code> . <code>data_type</code> . <code>comment</code></p> <p>If <code>DescribeColumnCommand</code> command was executed with &lt;&gt;, <code>run</code> gives the following additional rows (in that order): <p>. <code>min</code> . <code>max</code> . <code>num_nulls</code> . <code>distinct_count</code> . <code>avg_col_len</code> . <code>max_col_len</code> . &lt;&gt; <p><code>run</code> gives <code>NULL</code> for the value of the comment and statistics if not available.</p> <p>=== [[histogramDescription]] <code>histogramDescription</code> Internal Method</p>"},{"location":"logical-operators/DescribeColumnCommand/#source-scala_2","title":"[source, scala]","text":""},{"location":"logical-operators/DescribeColumnCommand/#histogramdescriptionhistogram-histogram-seqrow","title":"histogramDescription(histogram: Histogram): Seq[Row]","text":"<p><code>histogramDescription</code>...FIXME</p> <p>NOTE: <code>histogramDescription</code> is used exclusively when <code>DescribeColumnCommand</code> is &lt;&gt; with <code>EXTENDED</code> or <code>FORMATTED</code> option turned on. <p>=== [[creating-instance]] Creating DescribeColumnCommand Instance</p> <p><code>DescribeColumnCommand</code> takes the following when created:</p> <ul> <li>[[table]] <code>TableIdentifier</code></li> <li>[[colNameParts]] Column name</li> <li>[[isExtended]] <code>isExtended</code> flag that indicates whether spark-sql-SparkSqlAstBuilder.md#DescribeColumnCommand[EXTENDED or FORMATTED option] was used or not</li> </ul>"},{"location":"logical-operators/DescribeRelation/","title":"DescribeRelation Logical Command","text":"<p><code>DescribeRelation</code> is a logical command that represents DESCRIBE TABLE SQL statement.</p>"},{"location":"logical-operators/DescribeRelation/#demo","title":"Demo","text":"<pre><code>val table = \"four\"\nspark.range(4).writeTo(table).create\nval stmt = s\"DESC TABLE EXTENDED $table\"\n</code></pre> <pre><code>scala&gt; sql(stmt).show(truncate = false)\n+----------------------------+--------------------------------------------------------------+-------+\n|col_name                    |data_type                                                     |comment|\n+----------------------------+--------------------------------------------------------------+-------+\n|id                          |bigint                                                        |null   |\n|                            |                                                              |       |\n|# Detailed Table Information|                                                              |       |\n|Database                    |default                                                       |       |\n|Table                       |four                                                          |       |\n|Owner                       |jacek                                                         |       |\n|Created Time                |Sun Nov 08 11:49:04 CET 2020                                  |       |\n|Last Access                 |UNKNOWN                                                       |       |\n|Created By                  |Spark 3.0.1                                                   |       |\n|Type                        |MANAGED                                                       |       |\n|Provider                    |parquet                                                       |       |\n|Statistics                  |2125 bytes                                                    |       |\n|Location                    |file:/Users/jacek/dev/oss/spark/spark-warehouse/four          |       |\n|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n+----------------------------+--------------------------------------------------------------+-------+\n</code></pre>"},{"location":"logical-operators/DescribeRelation/#creating-instance","title":"Creating Instance","text":"<p><code>DescribeRelation</code> takes the following to be created:</p> <ul> <li> LogicalPlan <li> Partition Specification (<code>Map[String, String]</code>) <li> <code>isExtended</code> flag <p><code>DescribeRelation</code> is created when <code>AstBuilder</code> is requested to parse DESCRIBE TABLE statement.</p>"},{"location":"logical-operators/DescribeRelation/#analysis","title":"Analysis","text":"<p><code>DescribeRelation</code> is resolved to a DescribeTableCommand logical operator with the following logical relations:</p> <ul> <li><code>ResolvedTable</code> leaf logical operators with V1Table tables</li> <li><code>ResolvedView</code> leaf logical operator</li> </ul> <p><code>DescribeRelation</code> is resolved by ResolveSessionCatalog logical analysis rule.</p>"},{"location":"logical-operators/DescribeRelation/#execution-planning","title":"Execution Planning","text":"<p><code>DescribeRelation</code> (with a <code>ResolvedTable</code> leaf logical operator) is planned to DescribeTableExec physical command.</p> <p><code>DescribeRelation</code> is planned by DataSourceV2Strategy execution planning strategy.</p>"},{"location":"logical-operators/DescribeTableCommand/","title":"DescribeTableCommand Logical Command","text":"<p><code>DescribeTableCommand</code> is a LeafRunnableCommand (indirectly as <code>DescribeCommandBase</code>) that represents a DescribeRelation at execution (and hence DESCRIBE TABLE SQL statement).</p>"},{"location":"logical-operators/DescribeTableCommand/#creating-instance","title":"Creating Instance","text":"<p><code>DescribeTableCommand</code> takes the following to be created:</p> <ul> <li> <code>TableIdentifier</code> <li> <code>TablePartitionSpec</code> <li> <code>isExtended</code> flag <li> Output Attributes <p><code>DescribeTableCommand</code> is created when:</p> <ul> <li>ResolveSessionCatalog logical analysis rule is executed (to resolve DescribeRelation logical operator)</li> </ul>"},{"location":"logical-operators/DescribeTableCommand/#detailed-table-information","title":"Detailed Table Information <pre><code>describeFormattedTableInfo(\n  table: CatalogTable,\n  buffer: ArrayBuffer[Row]): Unit\n</code></pre> <p><code>describeFormattedTableInfo</code>...FIXME</p>  <p><code>describeFormattedTableInfo</code> is used when:</p> <ul> <li><code>DescribeTableCommand</code> is executed (with a table and isExtended enabled)</li> </ul>","text":""},{"location":"logical-operators/DeserializeToObject/","title":"DeserializeToObject","text":"<p><code>DeserializeToObject</code> is a unary logical operator and a <code>ObjectProducer</code>.</p>"},{"location":"logical-operators/DeserializeToObject/#creating-instance","title":"Creating Instance","text":"<p><code>DeserializeToObject</code> takes the following to be created:</p> <ul> <li> Deserializer Expression <li>Attribute</li> <li> Child logical operator <p><code>DeserializeToObject</code> is created when:</p> <ul> <li><code>CatalystSerde</code> utility is used to create a deserializer (for a logical operator)</li> </ul>"},{"location":"logical-operators/DeserializeToObject/#node-patterns","title":"Node Patterns <pre><code>nodePatterns: Seq[TreePattern]\n</code></pre> <p><code>nodePatterns</code> is DESERIALIZE_TO_OBJECT.</p> <p><code>nodePatterns</code> is part of the TreeNode abstraction.</p>","text":""},{"location":"logical-operators/DeserializeToObject/#logical-optimization","title":"Logical Optimization <p><code>DeserializeToObject</code> is a target of the following logical optimizations:</p> <ul> <li>EliminateSerialization</li> <li>ColumnPruning</li> </ul>","text":""},{"location":"logical-operators/DeserializeToObject/#execution-planning","title":"Execution Planning <p><code>DeserializeToObject</code> is planned for execution as DeserializeToObjectExec physical operator (by BasicOperators execution planning strategy).</p>","text":""},{"location":"logical-operators/Except/","title":"Except Logical Operator","text":"<p><code>Except</code> is a spark-sql-LogicalPlan.md#BinaryNode[binary logical operator] that represents the following high-level operators in a logical plan:</p> <ul> <li> <p><code>EXCEPT [ DISTINCT | ALL ]</code> and <code>MINUS [ DISTINCT | ALL ]</code> SQL statements (cf. sql/AstBuilder.md#visitSetOperation[AstBuilder])</p> </li> <li> <p>spark-sql-dataset-operators.md#except[Dataset.except] and spark-sql-dataset-operators.md#exceptAll[Dataset.exceptAll]</p> </li> </ul> <p><code>Except</code> is supposed to be resolved (optimized) to &lt;&gt; at logical optimization phase (i.e. <code>Except</code> should not be part of a logical plan after logical optimization). BasicOperators execution planning strategy throws an <code>IllegalStateException</code> if conversions did not happen. <p>[[logical-conversions]] .Except's Logical Resolutions (Conversions) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Target Logical Operators | Optimization Rules and Demos</p> <p>| Left-Anti Join.md[Join] | <code>Except</code> (DISTINCT) in ReplaceExceptWithAntiJoin.md[ReplaceExceptWithAntiJoin] logical optimization rule</p> <p>Consult &lt;&gt; <p>| <code>Filter</code> | <code>Except</code> (DISTINCT) in ReplaceExceptWithFilter logical optimization rule</p> <p>Consult &lt;&gt; <p>| <code>Union</code>, Aggregate.md[Aggregate] and Generate.md[Generate] | <code>Except</code> (ALL) in RewriteExceptAll.md[RewriteExceptAll] logical optimization rule</p> <p>Consult &lt;&gt; <p>|===</p> <p>The types of the &lt;&gt; and &lt;&gt; logical (sub)operators can be widen in WidenSetOperationTypes logical analysis type-coercion rule. <p>=== [[creating-instance]] Creating Except Instance</p> <p><code>Except</code> takes the following to be created:</p> <ul> <li>[[left]] Left spark-sql-LogicalPlan.md[logical operator]</li> <li>[[right]] Right spark-sql-LogicalPlan.md[logical operator]</li> <li>[[isAll]] <code>isAll</code> flag for <code>DISTINCT</code> (<code>false</code>) or <code>ALL</code> (<code>true</code>)</li> </ul> <p>=== [[catalyst-dsl]] Catalyst DSL -- <code>except</code> Operator</p>"},{"location":"logical-operators/Except/#source-scala","title":"[source, scala]","text":"<p>except(   otherPlan: LogicalPlan,   isAll: Boolean): LogicalPlan</p> <p>Catalyst DSL defines except extension method to create an <code>Except</code> logical operator, e.g. for testing or Spark SQL internals exploration.</p>"},{"location":"logical-operators/Except/#source-plaintext","title":"[source, plaintext]","text":"<p>import org.apache.spark.sql.catalyst.dsl.plans._ val plan = table(\"a\").except(table(\"b\"), isAll = false) scala&gt; println(plan.numberedTreeString) 00 'Except false 01 :- 'UnresolvedRelation <code>a</code> 02 +- 'UnresolvedRelation <code>b</code></p> <p>import org.apache.spark.sql.catalyst.plans.logical.Except val op = plan.p(0) assert(op.isInstanceOf[Except])</p> <p>=== [[CheckAnalysis]] Except Only on Relations with Same Number of Columns</p> <p><code>Except</code> logical operator can only be performed on CheckAnalysis.md#checkAnalysis[tables with the same number of columns].</p> <pre><code>scala&gt; left.except(right)\norg.apache.spark.sql.AnalysisException: Except can only be performed on tables with the same number of columns, but the first table has 3 columns and the second table has 4 columns;;\n'Except false\n:- SubqueryAlias `default`.`except_left`\n:  +- Relation[id#16,name#17,triple#18] parquet\n+- SubqueryAlias `default`.`except_right`\n   +- Relation[id#181,name#182,triple#183,extra_column#184] parquet\n\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:43)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:42)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$16(CheckAnalysis.scala:288)\n  ...\n</code></pre> <p>=== [[demo-left-anti-join]] Demo: Except Operator Replaced with Left-Anti Join</p> <pre><code>Seq((0, \"zero\", \"000\"), (1, \"one\", \"111\"))\n  .toDF(\"id\", \"name\", \"triple\")\n  .write\n  .saveAsTable(\"except_left\")\nval left = spark.table(\"except_left\")\n\n// The number of rows differ\nSeq((1, \"one\", \"111\"))\n  .toDF(\"id\", \"name\", \"triple\")\n  .write\n  .mode(\"overwrite\")\n  .saveAsTable(\"except_right\")\nval right = spark.table(\"except_right\")\n\nval q = left.except(right)\n\n// SELECT a1, a2 FROM Tab1 EXCEPT SELECT b1, b2 FROM Tab2\n// ==&gt;  SELECT DISTINCT a1, a2 FROM Tab1 LEFT ANTI JOIN Tab2 ON a1&lt;=&gt;b1 AND a2&lt;=&gt;b2\n\n// Note the use of &lt;=&gt; null-safe comparison operator\nscala&gt; println(q.queryExecution.optimizedPlan.numberedTreeString)\n00 Aggregate [id#16, name#17, triple#18], [id#16, name#17, triple#18]\n01 +- Join LeftAnti, (((id#16 &lt;=&gt; id#209) &amp;&amp; (name#17 &lt;=&gt; name#210)) &amp;&amp; (triple#18 &lt;=&gt; triple#211))\n02    :- Relation[id#16,name#17,triple#18] parquet\n03    +- Relation[id#209,name#210,triple#211] parquet\n</code></pre> <p>=== [[demo-except-filter]] Demo: Except Operator Replaced with Filter Operator</p> <pre><code>Seq((0, \"zero\", \"000\"), (1, \"one\", \"111\"))\n  .toDF(\"id\", \"name\", \"triple\")\n  .write\n  .saveAsTable(\"except_left\")\nval t1 = spark.table(\"except_left\")\n\nval left = t1.where(length($\"name\") &gt; 3)\nval right = t1.where($\"id\" &gt; 0)\n\n// SELECT a1, a2 FROM Tab1 WHERE a2 = 12 EXCEPT SELECT a1, a2 FROM Tab1 WHERE a1 = 5\nval q = left.except(right)\n\nscala&gt; println(q.queryExecution.optimizedPlan.numberedTreeString)\n00 Aggregate [id#16, name#17, triple#18], [id#16, name#17, triple#18]\n01 +- Filter ((length(name#17) = 3) &amp;&amp; NOT coalesce((id#16 = 0), false))\n02    +- Relation[id#16,name#17,triple#18] parquet\n</code></pre> <p>=== [[demo-except-all]] Demo: Except (All) Operator Replaced with Union, Aggregate and Generate Operators</p> <pre><code>Seq((0, \"zero\", \"000\"), (1, \"one\", \"111\"))\n  .toDF(\"id\", \"name\", \"triple\")\n  .write\n  .saveAsTable(\"except_left\")\nval left = spark.table(\"except_left\")\n\n// The number of rows differ\nSeq((1, \"one\", \"111\"))\n  .toDF(\"id\", \"name\", \"triple\")\n  .write\n  .mode(\"overwrite\")\n  .saveAsTable(\"except_right\")\nval right = spark.table(\"except_right\")\n\n// SELECT c1 FROM ut1 EXCEPT ALL SELECT c1 FROM ut2\nval q = left.exceptAll(right)\n\nscala&gt; println(q.queryExecution.optimizedPlan.numberedTreeString)\n00 Project [id#16, name#17, triple#18]\n01 +- Generate replicaterows(sum#227L, id#16, name#17, triple#18), [3], false, [id#16, name#17, triple#18]\n02    +- Filter (isnotnull(sum#227L) &amp;&amp; (sum#227L &gt; 0))\n03       +- Aggregate [id#16, name#17, triple#18], [id#16, name#17, triple#18, sum(vcol#224L) AS sum#227L]\n04          +- Union\n05             :- Project [1 AS vcol#224L, id#16, name#17, triple#18]\n06             :  +- Relation[id#16,name#17,triple#18] parquet\n07             +- Project [-1 AS vcol#225L, id#209, name#210, triple#211]\n08                +- Relation[id#209,name#210,triple#211] parquet\n</code></pre>"},{"location":"logical-operators/Expand/","title":"Expand Unary Logical Operator","text":"<p><code>Expand</code> is a spark-sql-LogicalPlan.md#UnaryNode[unary logical operator] that represents <code>Cube</code>, <code>Rollup</code>, GroupingSets.md[GroupingSets] and expressions/TimeWindow.md[TimeWindow] logical operators after they have been resolved at &lt;&gt;. <pre><code>FIXME Examples for\n1. Cube\n2. Rollup\n3. GroupingSets\n4. See TimeWindow\n\nval q = ...\n\nscala&gt; println(q.queryExecution.logical.numberedTreeString)\n...\n</code></pre> <p>Note</p> <p><code>Expand</code> logical operator is resolved to <code>ExpandExec</code> physical operator in BasicOperators execution planning strategy.</p> <p>[[properties]] .Expand's Properties [width=\"100%\",cols=\"1,2\",options=\"header\"] |=== | Name | Description</p> <p>| <code>references</code> | <code>AttributeSet</code> from &lt;&gt; <p>| <code>validConstraints</code> | Empty set of expressions/Expression.md[expressions] |===</p>"},{"location":"logical-operators/Expand/#analysis-phase","title":"Analysis Phase <p><code>Expand</code> logical operator is resolved to at analysis phase in the following logical evaluation rules:</p> <ul> <li> <p>ResolveGroupingAnalytics (for <code>Cube</code>, <code>Rollup</code>, GroupingSets logical operators)</p> </li> <li> <p><code>TimeWindowing</code> (for TimeWindow logical operator)</p> </li> </ul> <p>NOTE: Aggregate -&gt; (Cube|Rollup|GroupingSets) -&gt; constructAggregate -&gt; constructExpand</p> <pre><code>val spark: SparkSession = ...\n// using q from the example above\nval plan = q.queryExecution.logical\n\nscala&gt; println(plan.numberedTreeString)\n...FIXME\n</code></pre> <p>=== [[optimizer]] Rule-Based Logical Query Optimization Phase</p> <ul> <li>ColumnPruning</li> <li>FoldablePropagation</li> <li>RewriteDistinctAggregates</li> </ul> <p>=== [[creating-instance]] Creating Expand Instance</p> <p><code>Expand</code> takes the following when created:</p> <ul> <li>[[projections]] Projection expressions</li> <li>[[output]] Output schema attributes</li> <li>[[child]] Child logical plan</li> </ul>","text":""},{"location":"logical-operators/ExplainCommand/","title":"ExplainCommand Logical Command","text":"<p><code>ExplainCommand</code> is a logical command to display logical and physical query plans (with optional details about codegen and cost statistics) that represents EXPLAIN SQL statement at execution.</p>"},{"location":"logical-operators/ExplainCommand/#creating-instance","title":"Creating Instance","text":"<p><code>ExplainCommand</code> takes the following to be created:</p> <ul> <li> LogicalPlan <li> <code>ExplainMode</code> <p><code>ExplainCommand</code> is created when:</p> <ul> <li><code>SparkSqlAstBuilder</code> is requested to parse EXPLAIN statement</li> </ul>"},{"location":"logical-operators/ExplainCommand/#output-attributes","title":"Output Attributes <p><code>ExplainCommand</code> uses the following output attributes:</p> <ul> <li><code>plan</code> (type: <code>StringType</code>)</li> </ul>","text":""},{"location":"logical-operators/ExplainCommand/#executing-logical-command","title":"Executing Logical Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> requests the given SparkSession for SessionState that is requested to execute the given LogicalPlan.</p> <p>The result <code>QueryExecution</code> is requested to explainString with the given ExplainMode that becomes the output.</p> <p>In case of a <code>TreeNodeException</code>, <code>run</code> gives the following output:</p> <pre><code>Error occurred during query planning:\n[cause]\n</code></pre> <p><code>run</code> is part of the RunnableCommand abstraction.</p>","text":""},{"location":"logical-operators/ExposesMetadataColumns/","title":"ExposesMetadataColumns Logical Operators","text":"<p><code>ExposesMetadataColumns</code> is an extension of the LogicalPlan abstraction for logical operators that can withMetadataColumns.</p>"},{"location":"logical-operators/ExposesMetadataColumns/#contract","title":"Contract","text":""},{"location":"logical-operators/ExposesMetadataColumns/#withmetadatacolumns","title":"withMetadataColumns <pre><code>withMetadataColumns(): LogicalPlan\n</code></pre> <p>See:</p> <ul> <li>DataSourceV2Relation</li> <li>LogicalRelation</li> </ul> <p>Used when:</p> <ul> <li>AddMetadataColumns logical analysis rule is executed (and addMetadataCol)</li> </ul>","text":""},{"location":"logical-operators/ExposesMetadataColumns/#implementations","title":"Implementations","text":"<ul> <li>DataSourceV2Relation</li> <li>LogicalRelation</li> <li><code>StreamingRelation</code> (Spark Structured Streaming)</li> </ul>"},{"location":"logical-operators/ExternalRDD/","title":"ExternalRDD Leaf Logical Operator","text":"<p><code>ExternalRDD</code> is a LeafNode.md[leaf logical operator] that is a logical representation of (the data from) an RDD in a logical query plan.</p> <p><code>ExternalRDD</code> is &lt;&gt; when: <ul> <li> <p><code>SparkSession</code> is requested to create a SparkSession.md#createDataFrame[DataFrame from RDD of product types] (e.g. Scala case classes, tuples) or SparkSession.md#createDataset[Dataset from RDD of a given type]</p> </li> <li> <p><code>ExternalRDD</code> is requested to &lt;&gt;"},{"location":"logical-operators/ExternalRDD/#source-scala","title":"[source, scala]","text":"<p>val pairsRDD = sc.parallelize((0, \"zero\") :: (1, \"one\") :: (2, \"two\") :: Nil)</p> <p>// A tuple of Int and String is a product type scala&gt; :type pairsRDD org.apache.spark.rdd.RDD[(Int, String)]</p> <p>val pairsDF = spark.createDataFrame(pairsRDD)</p> <p>// ExternalRDD represents the pairsRDD in the query plan val logicalPlan = pairsDF.queryExecution.logical scala&gt; println(logicalPlan.numberedTreeString) 00 SerializeFromObject [assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#10, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false) AS _2#11] 01 +- ExternalRDD [obj#9]</p> <p><code>ExternalRDD</code> is a &lt;&gt; and a <code>ObjectProducer</code>. <p><code>ExternalRDD</code> is resolved to ExternalRDDScanExec.md[ExternalRDDScanExec] when BasicOperators execution planning strategy is executed.</p> <p>=== [[newInstance]] <code>newInstance</code> Method</p>"},{"location":"logical-operators/ExternalRDD/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-operators/ExternalRDD/#newinstance-logicalrddthistype","title":"newInstance(): LogicalRDD.this.type","text":"<p><code>newInstance</code> is part of MultiInstanceRelation abstraction.</p> <p><code>newInstance</code>...FIXME</p> <p>=== [[creating-instance]] Creating ExternalRDD Instance</p> <p><code>ExternalRDD</code> takes the following when created:</p> <ul> <li>[[outputObjAttr]] Output schema spark-sql-Expression-Attribute.md[attribute]</li> <li>[[rdd]] <code>RDD</code> of <code>T</code></li> <li>[[session]] SparkSession.md[SparkSession]</li> </ul> <p>=== [[apply]] Creating ExternalRDD -- <code>apply</code> Factory Method</p>"},{"location":"logical-operators/ExternalRDD/#source-scala_2","title":"[source, scala]","text":""},{"location":"logical-operators/ExternalRDD/#applyt-encoder-logicalplan","title":"applyT: Encoder: LogicalPlan","text":"<p><code>apply</code>...FIXME</p> <p>NOTE: <code>apply</code> is used when <code>SparkSession</code> is requested to create a SparkSession.md#createDataFrame[DataFrame from RDD of product types] (e.g. Scala case classes, tuples) or SparkSession.md#createDataset[Dataset from RDD of a given type].</p>"},{"location":"logical-operators/FlatMapGroupsWithState/","title":"FlatMapGroupsWithState Logical Operator","text":"<p><code>FlatMapGroupsWithState</code> is a binary logical operator that represents the following KeyValueGroupedDataset high-level operators:</p> <ul> <li>mapGroupsWithState</li> <li>flatMapGroupsWithState</li> </ul>"},{"location":"logical-operators/FlatMapGroupsWithState/#binarynode","title":"BinaryNode <p><code>FlatMapGroupsWithState</code> is a binary logical operator with two child operators:</p> <ul> <li>The child as the left operator</li> <li>The initialState as the right operator</li> </ul>","text":""},{"location":"logical-operators/FlatMapGroupsWithState/#objectproducer","title":"ObjectProducer <p><code>FlatMapGroupsWithState</code> is an <code>ObjectProducer</code>.</p>","text":""},{"location":"logical-operators/FlatMapGroupsWithState/#creating-instance","title":"Creating Instance <p><code>FlatMapGroupsWithState</code> takes the following to be created:</p> <ul> <li> State Mapping Function (<code>(Any, Iterator[Any], LogicalGroupState[Any]) =&gt; Iterator[Any]</code>) <li> Key Deserializer Expression <li> Value Deserializer Expression <li> Grouping Attributes <li> Data Attributes <li> Output Object Attribute <li> State ExpressionEncoder <li> <code>OutputMode</code> <li> <code>isMapGroupsWithState</code> flag (default: <code>false</code>) <li> <code>GroupStateTimeout</code> <li> <code>hasInitialState</code> flag (default: <code>false</code>) <li> Initial State Group Attributes <li> Initial State Data Attributes <li> Initial State Deserializer Expression <li> Initial State LogicalPlan <li> Child LogicalPlan  <p><code>FlatMapGroupsWithState</code> is created using apply factory.</p>","text":""},{"location":"logical-operators/FlatMapGroupsWithState/#creating-flatmapgroupswithstate","title":"Creating FlatMapGroupsWithState <pre><code>apply[K: Encoder, V: Encoder, S: Encoder, U: Encoder](\n  func: (Any, Iterator[Any], LogicalGroupState[Any]) =&gt; Iterator[Any],\n  groupingAttributes: Seq[Attribute],\n  dataAttributes: Seq[Attribute],\n  outputMode: OutputMode,\n  isMapGroupsWithState: Boolean,\n  timeout: GroupStateTimeout,\n  child: LogicalPlan): LogicalPlan\napply[K: Encoder, V: Encoder, S: Encoder, U: Encoder](\n  func: (Any, Iterator[Any], LogicalGroupState[Any]) =&gt; Iterator[Any],\n  groupingAttributes: Seq[Attribute],\n  dataAttributes: Seq[Attribute],\n  outputMode: OutputMode,\n  isMapGroupsWithState: Boolean,\n  timeout: GroupStateTimeout,\n  child: LogicalPlan,\n  initialStateGroupAttrs: Seq[Attribute],\n  initialStateDataAttrs: Seq[Attribute],\n  initialState: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> creates a FlatMapGroupsWithState with the following:</p> <ul> <li><code>UnresolvedDeserializer</code>s for the keys, values and the initial state</li> <li>Generates the output object attribute</li> </ul> <p>In the end, <code>apply</code> creates a SerializeFromObject unary logical operator with the <code>FlatMapGroupsWithState</code> operator as the child.</p>  <p><code>apply</code> is used for the following high-level operators:</p> <ul> <li>KeyValueGroupedDataset.mapGroupsWithState</li> <li>KeyValueGroupedDataset.flatMapGroupsWithState</li> </ul>","text":""},{"location":"logical-operators/FlatMapGroupsWithState/#execution-planning","title":"Execution Planning <p><code>FlatMapGroupsWithState</code> is planed as follows:</p>    Physical Operator Execution Planning Strategy     <code>FlatMapGroupsWithStateExec</code> (Spark Structured Streaming) <code>FlatMapGroupsWithStateStrategy</code> (Spark Structured Streaming)   <code>CoGroupExec</code> or <code>MapGroupsExec</code> BasicOperators","text":""},{"location":"logical-operators/Generate/","title":"Generate Unary Logical Operator","text":"<p><code>Generate</code> is a spark-sql-LogicalPlan.md#UnaryNode[unary logical operator] that is &lt;&gt; to represent the following (after a logical plan is spark-sql-LogicalPlan.md#analyzed[analyzed]): <ul> <li> <p>expressions/Generator.md[Generator] or <code>GeneratorOuter</code> expressions (by ExtractGenerator logical evaluation rule)</p> </li> <li> <p>SQL's sql/AstBuilder.md#withGenerate[LATERAL VIEW] clause (in <code>SELECT</code> or <code>FROM</code> clauses)</p> </li> </ul> <p>[[resolved]] <code>resolved</code> flag is...FIXME</p> <p>NOTE: <code>resolved</code> is part of spark-sql-LogicalPlan.md#resolved[LogicalPlan Contract] to...FIXME.</p> <p>[[producedAttributes]] <code>producedAttributes</code>...FIXME</p> <p>[[output]] The catalyst/QueryPlan.md#output[output schema] of a <code>Generate</code> is...FIXME</p> <p>Note</p> <p><code>Generate</code> logical operator is resolved to GenerateExec.md[GenerateExec] unary physical operator in BasicOperators execution planning strategy.</p>"},{"location":"logical-operators/Generate/#tip","title":"[TIP]","text":"<p>Use <code>generate</code> operator from Catalyst DSL to create a <code>Generate</code> logical operator, e.g. for testing or Spark SQL internals exploration.</p>"},{"location":"logical-operators/Generate/#source-scala","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.plans.logical._ import org.apache.spark.sql.types._ val lr = LocalRelation('key.int, 'values.array(StringType))</p> <p>// JsonTuple generator import org.apache.spark.sql.catalyst.expressions.JsonTuple import org.apache.spark.sql.catalyst.dsl.expressions._ import org.apache.spark.sql.catalyst.expressions.Expression val children: Seq[Expression] = Seq(\"e\") val json_tuple = JsonTuple(children)</p> <p>import org.apache.spark.sql.catalyst.dsl.plans._  // \u2190 gives generate val plan = lr.generate(   generator = json_tuple,   join = true,   outer = true,   alias = Some(\"alias\"),   outputNames = Seq.empty) scala&gt; println(plan.numberedTreeString) 00 'Generate json_tuple(e), true, true, alias 01 +- LocalRelation , [key#0, values#1] <p>====</p> <p>=== [[creating-instance]] Creating Generate Instance</p> <p><code>Generate</code> takes the following when created:</p> <ul> <li>[[generator]] expressions/Generator.md[Generator] expression</li> <li>[[join]] <code>join</code> flag...FIXME</li> <li>[[outer]] <code>outer</code> flag...FIXME</li> <li>[[qualifier]] Optional qualifier</li> <li>[[generatorOutput]] Output spark-sql-Expression-Attribute.md[attributes]</li> <li>[[child]] Child spark-sql-LogicalPlan.md[logical plan]</li> </ul> <p><code>Generate</code> initializes the &lt;&gt;."},{"location":"logical-operators/GlobalLimit/","title":"GlobalLimit Logical Operator","text":"<p><code>GlobalLimit</code> is an order-preserving unary logical operator.</p>"},{"location":"logical-operators/GlobalLimit/#creating-instance","title":"Creating Instance","text":"<p><code>GlobalLimit</code> takes the following to be created:</p> <ul> <li> Limit Expression <li> Child query plan <p><code>GlobalLimit</code> is created using Limit.apply utility.</p>"},{"location":"logical-operators/GlobalLimit/#query-planning","title":"Query Planning","text":"<p><code>GlobalLimit</code> is planned to <code>GlobalLimitExec</code> physical operator (when BasicOperators execution planning strategy is executed).</p>"},{"location":"logical-operators/GlobalLimit/#query-physical-optimization","title":"Query Physical Optimization","text":"<ul> <li><code>CombineLimits</code> physical optimization</li> </ul>"},{"location":"logical-operators/GlobalLimit/#apply-utility","title":"apply Utility <pre><code>apply(\n  limitExpr: Expression,\n  child: LogicalPlan): UnaryNode\n</code></pre> <p><code>apply</code> creates a <code>GlobalLimit</code> for the given <code>limitExpr</code> expression and a <code>LocalLimit</code> logical operator.</p> <p><code>apply</code> is used when:</p> <ul> <li>AstBuilder is requested to withQueryResultClauses and withSample</li> <li><code>Dataset.limit</code> operator is used</li> <li><code>RewriteNonCorrelatedExists</code> logical optimization is executed</li> <li><code>CombineLimits</code> physical optimization is executed</li> <li>Catalyst DSL's <code>limit</code> operator is used</li> </ul>","text":""},{"location":"logical-operators/GlobalLimit/#example","title":"Example <pre><code>val q = spark.range(10).limit(3)\nscala&gt; q.explain(extended = true)\n== Parsed Logical Plan ==\nGlobalLimit 3\n+- LocalLimit 3\n   +- Range (0, 10, step=1, splits=Some(16))\n\n== Analyzed Logical Plan ==\nid: bigint\nGlobalLimit 3\n+- LocalLimit 3\n   +- Range (0, 10, step=1, splits=Some(16))\n\n== Optimized Logical Plan ==\nGlobalLimit 3\n+- LocalLimit 3\n   +- Range (0, 10, step=1, splits=Some(16))\n\n== Physical Plan ==\nCollectLimit 3\n+- *(1) Range (0, 10, step=1, splits=16)\n</code></pre>","text":""},{"location":"logical-operators/GroupingSets/","title":"GroupingSets Unary Logical Operator","text":"<p><code>GroupingSets</code> is a spark-sql-LogicalPlan.md#UnaryNode[unary logical operator] that represents SQL's sql/AstBuilder.md#withAggregation[GROUPING SETS] variant of <code>GROUP BY</code> clause.</p> <pre><code>val q = sql(\"\"\"\n  SELECT customer, year, SUM(sales)\n  FROM VALUES (\"abc\", 2017, 30) AS t1 (customer, year, sales)\n  GROUP BY customer, year\n  GROUPING SETS ((customer), (year))\n  \"\"\")\nscala&gt; println(q.queryExecution.logical.numberedTreeString)\n00 'GroupingSets [ArrayBuffer('customer), ArrayBuffer('year)], ['customer, 'year], ['customer, 'year, unresolvedalias('SUM('sales), None)]\n01 +- 'SubqueryAlias t1\n02    +- 'UnresolvedInlineTable [customer, year, sales], [List(abc, 2017, 30)]\n</code></pre> <p><code>GroupingSets</code> operator is resolved to an <code>Aggregate</code> logical operator at &lt;&gt;. <pre><code>scala&gt; println(q.queryExecution.analyzed.numberedTreeString)\n00 Aggregate [customer#8, year#9, spark_grouping_id#5], [customer#8, year#9, sum(cast(sales#2 as bigint)) AS sum(sales)#4L]\n01 +- Expand [List(customer#0, year#1, sales#2, customer#6, null, 1), List(customer#0, year#1, sales#2, null, year#7, 2)], [customer#0, year#1, sales#2, customer#8, year#9, spark_grouping_id#5]\n02    +- Project [customer#0, year#1, sales#2, customer#0 AS customer#6, year#1 AS year#7]\n03       +- SubqueryAlias t1\n04          +- LocalRelation [customer#0, year#1, sales#2]\n</code></pre> <p>NOTE: <code>GroupingSets</code> can only be created using SQL.</p> <p>NOTE: <code>GroupingSets</code> is not supported on Structured Streaming's spark-sql-LogicalPlan.md#isStreaming[streaming Datasets].</p> <p>[[resolved]] <code>GroupingSets</code> is never resolved (as it can only be converted to an <code>Aggregate</code> logical operator).</p> <p>[[output]] The catalyst/QueryPlan.md#output[output schema] of a <code>GroupingSets</code> are exactly the attributes of &lt;&gt;. <p>=== [[analyzer]] Analysis Phase</p> <p><code>GroupingSets</code> operator is resolved at analysis phase in the following logical evaluation rules:</p> <ul> <li> <p>ResolveAliases for unresolved aliases in &lt;&gt; <li> <p>ResolveGroupingAnalytics</p> </li> <p><code>GroupingSets</code> operator is resolved to an Aggregate.md[Aggregate] with Expand.md[Expand] logical operators.</p>"},{"location":"logical-operators/GroupingSets/#source-scala","title":"[source, scala]","text":"<p>val spark: SparkSession = ... // using q from the example above val plan = q.queryExecution.logical</p> <p>scala&gt; println(plan.numberedTreeString) 00 'GroupingSets [ArrayBuffer('customer), ArrayBuffer('year)], ['customer, 'year], ['customer, 'year, unresolvedalias('SUM('sales), None)] 01 +- 'SubqueryAlias t1 02    +- 'UnresolvedInlineTable [customer, year, sales], [List(abc, 2017, 30)]</p> <p>// Note unresolvedalias for SUM expression // Note UnresolvedInlineTable and SubqueryAlias</p>"},{"location":"logical-operators/GroupingSets/#fixme-show-the-evaluation-rules-to-get-rid-of-the-unresolvable-parts","title":"// FIXME Show the evaluation rules to get rid of the unresolvable parts","text":"<p>=== [[creating-instance]] Creating GroupingSets Instance</p> <p><code>GroupingSets</code> takes the following when created:</p> <ul> <li>[[selectedGroupByExprs]] expressions/Expression.md[Expressions] from <code>GROUPING SETS</code> clause</li> <li>[[groupByExprs]] Grouping expressions/Expression.md[expressions] from <code>GROUP BY</code> clause</li> <li>[[child]] Child spark-sql-LogicalPlan.md[logical plan]</li> <li>[[aggregations]] Aggregate expressions/NamedExpression.md[named expressions]</li> </ul>"},{"location":"logical-operators/IgnoreCachedData/","title":"IgnoreCachedData Logical Operators","text":"<p><code>IgnoreCachedData</code> is a marker interface for logical operators that should be skipped (ignored) by CacheManager (while replacing segments of a logical query with cached data).</p>"},{"location":"logical-operators/IgnoreCachedData/#implementations","title":"Implementations","text":"<ul> <li>ClearCacheCommand</li> <li><code>ResetCommand</code></li> </ul>"},{"location":"logical-operators/InMemoryRelation/","title":"InMemoryRelation Leaf Logical Operator","text":"<p><code>InMemoryRelation</code> is a leaf logical operator that represents a structured query that is cached in memory (when <code>CacheManager</code> is requested to cache it).</p> <p><code>InMemoryRelation</code> uses spark.sql.cache.serializer configuration property to create a CachedBatchSerializer.</p>"},{"location":"logical-operators/InMemoryRelation/#creating-instance","title":"Creating Instance","text":"<p><code>InMemoryRelation</code> takes the following to be created:</p> <ul> <li> Output (Attributes) <li>CachedRDDBuilder</li> <li> Output Ordering (<code>Seq[SortOrder]</code>) <p><code>InMemoryRelation</code> is created using apply factory methods.</p>"},{"location":"logical-operators/InMemoryRelation/#cachedrddbuilder","title":"CachedRDDBuilder <p><code>InMemoryRelation</code> can be given a CachedRDDBuilder when created (using apply).</p> <p><code>InMemoryRelation</code> is <code>@transient</code> (so it won't be preseved when this operator has been serialized).</p> <p>The <code>CachedRDDBuilder</code> is used by the following:</p> <ul> <li>CacheManager</li> <li>InMemoryTableScanExec physical operator</li> </ul> <p>The <code>CachedRDDBuilder</code> is used to access storageLevel when (when the <code>Dataset</code> is cached):</p> <ul> <li>Dataset.storageLevel operator is used</li> <li><code>AlterTableRenameCommand</code> is executed</li> <li><code>DataSourceV2Strategy</code> execution planning strategy is requested to invalidateTableCache (to plan a <code>RenameTable</code> unary logical command)</li> <li>PartitionPruning logical optimization is executed (and calculatePlanOverhead)</li> </ul>","text":""},{"location":"logical-operators/InMemoryRelation/#demo","title":"Demo","text":"<pre><code>// Cache sample table range5 using pure SQL\n// That registers range5 to contain the output of range(5) function\nspark.sql(\"CACHE TABLE range5 AS SELECT * FROM range(5)\")\nval q1 = spark.sql(\"SELECT * FROM range5\")\nscala&gt; q1.explain\n== Physical Plan ==\nInMemoryTableScan [id#0L]\n   +- InMemoryRelation [id#0L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas), `range5`\n         +- *Range (0, 5, step=1, splits=8)\n\n// you could also use optimizedPlan to see InMemoryRelation\nscala&gt; println(q1.queryExecution.optimizedPlan.numberedTreeString)\n00 InMemoryRelation [id#0L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas), `range5`\n01    +- *Range (0, 5, step=1, splits=8)\n\n// Use Dataset's cache\nval q2 = spark.range(10).groupBy('id % 5).count.cache\nscala&gt; println(q2.queryExecution.optimizedPlan.numberedTreeString)\n00 InMemoryRelation [(id % 5)#84L, count#83L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n01    +- *HashAggregate(keys=[(id#77L % 5)#88L], functions=[count(1)], output=[(id % 5)#84L, count#83L])\n02       +- Exchange hashpartitioning((id#77L % 5)#88L, 200)\n03          +- *HashAggregate(keys=[(id#77L % 5) AS (id#77L % 5)#88L], functions=[partial_count(1)], output=[(id#77L % 5)#88L, count#90L])\n04             +- *Range (0, 10, step=1, splits=8)\n</code></pre>"},{"location":"logical-operators/InMemoryRelation/#multiinstancerelation","title":"MultiInstanceRelation","text":"<p><code>InMemoryRelation</code> is a MultiInstanceRelation so a new instance will be created to appear multiple times in a physical query plan.</p> <pre><code>val q = spark.range(10).cache\n\n// Make sure that q Dataset is cached\nval cache = spark.sharedState.cacheManager\nscala&gt; cache.lookupCachedData(q.queryExecution.logical).isDefined\nres0: Boolean = true\n\nscala&gt; q.explain\n== Physical Plan ==\nInMemoryTableScan [id#122L]\n   +- InMemoryRelation [id#122L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n         +- *Range (0, 10, step=1, splits=8)\n\nval qCrossJoined = q.crossJoin(q)\nscala&gt; println(qCrossJoined.queryExecution.optimizedPlan.numberedTreeString)\n00 Join Cross\n01 :- InMemoryRelation [id#122L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n02 :     +- *Range (0, 10, step=1, splits=8)\n03 +- InMemoryRelation [id#170L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n04       +- *Range (0, 10, step=1, splits=8)\n\n// Use sameResult for comparison\n// since the plans use different output attributes\n// and have to be canonicalized internally\nimport org.apache.spark.sql.execution.columnar.InMemoryRelation\nval optimizedPlan = qCrossJoined.queryExecution.optimizedPlan\nscala&gt; optimizedPlan.children(0).sameResult(optimizedPlan.children(1))\nres1: Boolean = true\n</code></pre>"},{"location":"logical-operators/InMemoryRelation/#simple-text-representation","title":"Simple Text Representation <p>The simple text representation of an <code>InMemoryRelation</code> (aka <code>simpleString</code>) uses the output and the CachedRDDBuilder):</p> <pre><code>InMemoryRelation [output], [storageLevel]\n</code></pre> <pre><code>val q = spark.range(1).cache\nval logicalPlan = q.queryExecution.withCachedData\nscala&gt; println(logicalPlan.simpleString)\nInMemoryRelation [id#40L], StorageLevel(disk, memory, deserialized, 1 replicas)\n</code></pre>","text":""},{"location":"logical-operators/InMemoryRelation/#query-planning-and-inmemorytablescanexec-physical-operator","title":"Query Planning and InMemoryTableScanExec Physical Operator <p><code>InMemoryRelation</code> is resolved to InMemoryTableScanExec leaf physical operator when InMemoryScans execution planning strategy is executed.</p>","text":""},{"location":"logical-operators/InMemoryRelation/#creating-inmemoryrelation","title":"Creating InMemoryRelation <pre><code>apply(\n  serializer: CachedBatchSerializer,\n  storageLevel: StorageLevel,\n  child: SparkPlan,\n  tableName: Option[String],\n  optimizedPlan: LogicalPlan): InMemoryRelation // (1)!\napply(\n  cacheBuilder: CachedRDDBuilder,\n  qe: QueryExecution): InMemoryRelation\napply(\n  output: Seq[Attribute],\n  cacheBuilder: CachedRDDBuilder,\n  outputOrdering: Seq[SortOrder],\n  statsOfPlanToCache: Statistics): InMemoryRelation\napply(\n  storageLevel: StorageLevel,\n  qe: QueryExecution,\n  tableName: Option[String]): InMemoryRelation\n</code></pre> <ol> <li>Intended and used only in tests</li> </ol> <p><code>apply</code> creates an InMemoryRelation logical operator with the following:</p>    Property Value     output The output of the executedPlan physical query plan (possibly convertToColumnarIfPossible if the <code>CachedBatchSerializer</code> supportsColumnarInput)   CachedRDDBuilder A new CachedRDDBuilder   outputOrdering The outputOrdering of the optimized logical query plan   statsOfPlanToCache The Statistics of the optimized logical query plan     <p><code>apply</code> is used when:</p> <ul> <li><code>CacheManager</code> is requested to cache and re-cache a structured query, and useCachedData</li> <li><code>InMemoryRelation</code> is requested to withOutput and newInstance</li> </ul>","text":""},{"location":"logical-operators/InMemoryRelation/#looking-up-cachedbatchserializer","title":"Looking Up CachedBatchSerializer <pre><code>getSerializer(\n  sqlConf: SQLConf): CachedBatchSerializer\n</code></pre> <p><code>getSerializer</code> uses spark.sql.cache.serializer configuration property to create a CachedBatchSerializer.</p>","text":""},{"location":"logical-operators/InMemoryRelation/#converttocolumnarifpossible","title":"convertToColumnarIfPossible <pre><code>convertToColumnarIfPossible(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>convertToColumnarIfPossible</code>...FIXME</p>","text":""},{"location":"logical-operators/InMemoryRelation/#partitionstatistics","title":"PartitionStatistics <p><code>PartitionStatistics</code> for the output schema</p> <p>Used when InMemoryTableScanExec physical operator is created (and initializes stats internal property).</p>","text":""},{"location":"logical-operators/InsertIntoDataSourceCommand/","title":"InsertIntoDataSourceCommand Logical Command","text":"<p><code>InsertIntoDataSourceCommand</code> is a RunnableCommand that &lt;&gt; (per &lt;&gt; flag). <p><code>InsertIntoDataSourceCommand</code> is &lt;&gt; exclusively when DataSourceAnalysis logical resolution is executed (and resolves an &lt;&gt; unary logical operator with a &lt;&gt; on an InsertableRelation). <pre><code>sql(\"DROP TABLE IF EXISTS t2\")\nsql(\"CREATE TABLE t2(id long)\")\n\nval query = \"SELECT * FROM RANGE(1)\"\n// Using INSERT INTO SQL statement so we can access QueryExecution\n// DataFrameWriter.insertInto returns no value\nval q = sql(\"INSERT INTO TABLE t2 \" + query)\nval logicalPlan = q.queryExecution.logical\nscala&gt; println(logicalPlan.numberedTreeString)\n00 'InsertIntoTable 'UnresolvedRelation `t2`, false, false\n01 +- 'Project [*]\n02    +- 'UnresolvedTableValuedFunction RANGE, [1]\n\nval analyzedPlan = q.queryExecution.analyzed\nscala&gt; println(analyzedPlan.numberedTreeString)\n00 InsertIntoHiveTable `default`.`t2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, false, false, [id#6L]\n01 +- Project [id#6L]\n02    +- Range (0, 1, step=1, splits=None)\n</code></pre> <p>[[innerChildren]] <code>InsertIntoDataSourceCommand</code> returns the &lt;&gt; when requested for the inner nodes (that should be shown as an inner nested tree of this node). <pre><code>val query = \"SELECT * FROM RANGE(1)\"\nval sqlText = \"INSERT INTO TABLE t2 \" + query\nval plan = spark.sessionState.sqlParser.parsePlan(sqlText)\nscala&gt; println(plan.numberedTreeString)\n00 'InsertIntoTable 'UnresolvedRelation `t2`, false, false\n01 +- 'Project [*]\n02    +- 'UnresolvedTableValuedFunction RANGE, [1]\n</code></pre>"},{"location":"logical-operators/InsertIntoDataSourceCommand/#creating-instance","title":"Creating Instance","text":"<p><code>InsertIntoDataSourceCommand</code> takes the following to be created:</p> <ul> <li>[[logicalRelation]] &lt;&gt; leaf logical operator <li>[[query]] &lt;&gt; <li>[[overwrite]] <code>overwrite</code> flag</li> <p>=== [[run]] Executing Logical Command (Inserting or Overwriting Data in InsertableRelation) -- <code>run</code> Method</p>"},{"location":"logical-operators/InsertIntoDataSourceCommand/#source-scala","title":"[source, scala]","text":"<p>run(   session: SparkSession): Seq[Row]</p> <p><code>run</code> is part of the RunnableCommand abstraction.</p> <p><code>run</code> takes the InsertableRelation (that is the &lt;&gt; of the &lt;&gt;). <p><code>run</code> then &lt;&gt; for the &lt;&gt; and the input <code>SparkSession</code>. <p><code>run</code> requests the <code>DataFrame</code> for the &lt;&gt; that in turn is requested for the RDD (of the structured query). <code>run</code> requests the &lt;&gt; for the &lt;&gt;. <p>With the RDD and the output schema, <code>run</code> creates &lt;&gt; that is the <code>RDD[InternalRow]</code> with the schema applied. <p><code>run</code> requests the <code>InsertableRelation</code> to insert or overwrite data.</p> <p>In the end, since the data in the <code>InsertableRelation</code> has changed, <code>run</code> requests the <code>CacheManager</code> to recacheByPlan with the &lt;&gt;. <p>NOTE: <code>run</code> requests the <code>SparkSession</code> for &lt;&gt; that is in turn requested for the CacheManager."},{"location":"logical-operators/InsertIntoDir/","title":"InsertIntoDir Unary Logical Operator","text":"<p><code>InsertIntoDir</code> is a spark-sql-LogicalPlan.md#UnaryNode[unary logical operator] that represents sql/AstBuilder.md#withInsertInto[INSERT OVERWRITE DIRECTORY] SQL statement.</p> <p>NOTE: <code>InsertIntoDir</code> is similar to InsertIntoTable.md[InsertIntoTable] logical operator.</p> <p>[[resolved]] <code>InsertIntoDir</code> can never be spark-sql-LogicalPlan.md#resolved[resolved] (i.e. <code>InsertIntoTable</code> should not be part of a logical plan after analysis and is supposed to be &lt;&gt; at analysis phase). <p>[[logical-conversions]] .InsertIntoDir's Logical Resolutions (Conversions) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Logical Command | Description</p> <p>| hive/InsertIntoHiveDirCommand.md[InsertIntoHiveDirCommand] | [[InsertIntoHiveDirCommand]] When hive/HiveAnalysis.md[HiveAnalysis] logical resolution rule transforms <code>InsertIntoDir</code> with a Hive table</p> <p>| InsertIntoDataSourceDirCommand | [[InsertIntoDataSourceDirCommand]] When DataSourceAnalysis logical resolution rule transforms <code>InsertIntoDir</code> with a Spark table</p> <p>|===</p> <p>[[output]] <code>InsertIntoDir</code> has no output columns.</p>"},{"location":"logical-operators/InsertIntoDir/#creating-instance","title":"Creating Instance","text":"<p><code>InsertIntoDir</code> takes the following to be created:</p> <ul> <li>[[isLocal]] <code>isLocal</code> Flag</li> <li>[[storage]] CatalogStorageFormat</li> <li>[[provider]] Table provider</li> <li>[[child]] Child logical operator</li> <li>[[overwrite]] <code>overwrite</code> Flag (default: <code>true</code>)</li> </ul>"},{"location":"logical-operators/InsertIntoHadoopFsRelationCommand/","title":"InsertIntoHadoopFsRelationCommand Logical Command","text":"<p><code>InsertIntoHadoopFsRelationCommand</code> is a logical command that writes the result of executing a query to an output path in the given format.</p>"},{"location":"logical-operators/InsertIntoHadoopFsRelationCommand/#creating-instance","title":"Creating Instance","text":"<p><code>InsertIntoHadoopFsRelationCommand</code> takes the following to be created:</p> <ul> <li> Output Path (as a Hadoop Path) <li>Static Partitions</li> <li> <code>ifPartitionNotExists</code> Flag <li> Partition Columns (<code>Seq[Attribute]</code>) <li> BucketSpec if defined <li> FileFormat <li> Options (<code>Map[String, String]</code>) <li> Query <li> SaveMode <li> CatalogTable if available <li> FileIndex if defined <li> Names of the output columns <p><code>InsertIntoHadoopFsRelationCommand</code> is created when:</p> <ul> <li><code>OptimizedCreateHiveTableAsSelectCommand</code> logical command is executed</li> <li><code>DataSource</code> is requested to planForWritingFileFormat</li> <li>DataSourceAnalysis logical resolution rule is executed (for a <code>InsertIntoStatement</code> over a LogicalRelation with a HadoopFsRelation)</li> </ul>"},{"location":"logical-operators/InsertIntoHadoopFsRelationCommand/#static-partitions","title":"Static Partitions <pre><code>type TablePartitionSpec = Map[String, String]\nstaticPartitions: TablePartitionSpec\n</code></pre> <p><code>InsertIntoHadoopFsRelationCommand</code> is given a specification of a table partition (as a mapping of column names to column values) when created.</p> <p>Partitions can only be given when created for DataSourceAnalysis posthoc logical resolution rule when executed for a <code>InsertIntoStatement</code> over a LogicalRelation with a HadoopFsRelation</p> <p>There will be no partitions when created for the following:</p> <ul> <li><code>OptimizedCreateHiveTableAsSelectCommand</code> logical command</li> <li><code>DataSource</code> when requested to planForWritingFileFormat</li> </ul>","text":""},{"location":"logical-operators/InsertIntoHadoopFsRelationCommand/#dynamic-partition-inserts-and-dynamicpartitionoverwrite-flag","title":"Dynamic Partition Inserts and dynamicPartitionOverwrite Flag <pre><code>dynamicPartitionOverwrite: Boolean\n</code></pre> <p><code>InsertIntoHadoopFsRelationCommand</code> defines a <code>dynamicPartitionOverwrite</code> flag to indicate whether dynamic partition inserts is enabled or not.</p> <p><code>dynamicPartitionOverwrite</code> is based on the following (in the order of precedence):</p> <ul> <li>partitionOverwriteMode option (<code>STATIC</code> or <code>DYNAMIC</code>) in the parameters if available</li> <li>spark.sql.sources.partitionOverwriteMode</li> </ul> <p><code>dynamicPartitionOverwrite</code> is used when:</p> <ul> <li>DataSourceAnalysis logical resolution rule is executed (for dynamic partition overwrite)</li> <li><code>InsertIntoHadoopFsRelationCommand</code> is executed</li> </ul>","text":""},{"location":"logical-operators/InsertIntoHadoopFsRelationCommand/#executing-command","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession,\n  child: SparkPlan): Seq[Row]\n</code></pre> <p><code>run</code> is part of the DataWritingCommand abstraction.</p>  <p><code>run</code> creates a new Hadoop <code>Configuration</code> with the options and resolves the outputPath.</p> <p><code>run</code> uses the following to determine whether partitions are tracked by a catalog (<code>partitionsTrackedByCatalog</code>):</p> <ul> <li>spark.sql.hive.manageFilesourcePartitions configuration property</li> <li>catalogTable is defined with the partitionColumnNames and tracksPartitionsInCatalog flag enabled</li> </ul>  FIXME <p>When is the catalogTable defined?</p>   FIXME <p>When is tracksPartitionsInCatalog enabled?</p>  <p>With partitions tracked by a catalog, <code>run</code>...FIXME</p> <p><code>run</code> uses <code>FileCommitProtocol</code> utility to instantiate a <code>FileCommitProtocol</code> based on the spark.sql.sources.commitProtocolClass with the following:</p> <ul> <li>Random job ID</li> <li>outputPath</li> <li>dynamicPartitionOverwrite</li> </ul> <p>For insertion (<code>doInsertion</code>), <code>run</code>...FIXME</p> <p>Otherwise (for a non-insertion case), <code>run</code> does nothing but prints out the following INFO message to the logs and finishes.</p> <pre><code>Skipping insertion into a relation that already exists.\n</code></pre>  <p><code>run</code> makes sure that there are no duplicates in the outputColumnNames.</p>","text":""},{"location":"logical-operators/InsertIntoStatement/","title":"InsertIntoStatement","text":"<p><code>InsertIntoStatement</code> is a UnaryParsedStatement.</p>"},{"location":"logical-operators/InsertIntoStatement/#creating-instance","title":"Creating Instance","text":"<p><code>InsertIntoStatement</code> takes the following to be created:</p> <ul> <li> Table (LogicalPlan) <li> Partition Specification (<code>Map[String, Option[String]]</code>) <li> User-specified column names <li> Query <li> <code>overwrite</code> flag <li> <code>ifPartitionNotExists</code> flag <p><code>InsertIntoStatement</code> is created\u00a0when:</p> <ul> <li>Catalyst DSL is used to insertInto</li> <li><code>AstBuilder</code> is requested to withInsertInto</li> <li><code>DataFrameWriter</code> is requested to insertInto</li> </ul>"},{"location":"logical-operators/InsertIntoStatement/#logical-resolution","title":"Logical Resolution","text":"<p><code>InsertIntoStatement</code> is resolved to the following logical operators:</p> <ul> <li>InsertIntoDataSourceCommand (for <code>InsertIntoStatement</code>s over LogicalRelation over InsertableRelation) by DataSourceAnalysis</li> <li>InsertIntoHadoopFsRelationCommand (for <code>InsertIntoStatement</code>s over LogicalRelation over HadoopFsRelation) by DataSourceAnalysis</li> </ul>"},{"location":"logical-operators/InsertIntoStatement/#logical-analysis","title":"Logical Analysis","text":"<p><code>InsertIntoStatement</code>s with <code>UnresolvedCatalogRelation</code>s are resolved by the following logical analysis rules:</p> <ul> <li>FindDataSourceTable</li> <li><code>FallBackFileSourceV2</code></li> <li><code>PreprocessTableInsertion</code></li> <li>PreWriteCheck</li> </ul>"},{"location":"logical-operators/InsertIntoTable/","title":"InsertIntoTable Unary Logical Operator","text":"<p><code>InsertIntoTable</code> is an unary logical operator that represents the following high-level operators in a logical plan:</p> <ul> <li> <p>INSERT INTO and INSERT OVERWRITE TABLE SQL statements</p> </li> <li> <p>DataFrameWriter.insertInto high-level operator</p> </li> </ul> <p><code>InsertIntoTable</code> is &lt;&gt; with &lt;&gt; that correspond to the <code>partitionSpec</code> part of the following SQL statements: <ul> <li> <p><code>INSERT INTO TABLE</code> (with the &lt;&gt; and &lt;&gt; flags off) <li> <p><code>INSERT OVERWRITE TABLE</code> (with the &lt;&gt; and &lt;&gt; flags off) <p><code>InsertIntoTable</code> has no &lt;&gt; when &lt;&gt; as follows: <ul> <li> <p>&lt;&gt; operator from the Catalyst DSL <li> <p>DataFrameWriter.insertInto operator</p> </li> <p>[[resolved]] <code>InsertIntoTable</code> can never be spark-sql-LogicalPlan.md#resolved[resolved] (i.e. <code>InsertIntoTable</code> should not be part of a logical plan after analysis and is supposed to be &lt;&gt; at analysis phase). <p>[[logical-conversions]] .InsertIntoTable's Logical Resolutions (Conversions) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Logical Command | Description</p> <p>| hive/InsertIntoHiveTable.md[InsertIntoHiveTable] | [[InsertIntoHiveTable]] When hive/HiveAnalysis.md#apply[HiveAnalysis] resolution rule transforms <code>InsertIntoTable</code> with a hive/HiveTableRelation.md[HiveTableRelation]</p> <p>| &lt;&gt; | [[InsertIntoDataSourceCommand]] When DataSourceAnalysis posthoc logical resolution resolves an <code>InsertIntoTable</code> with a &lt;&gt; over an InsertableRelation (with no partitions defined) <p>| InsertIntoHadoopFsRelationCommand | [[InsertIntoHadoopFsRelationCommand]] When DataSourceAnalysis posthoc logical resolution transforms <code>InsertIntoTable</code> with a &lt;&gt; over a HadoopFsRelation <p>|===</p> <p>CAUTION: FIXME What's the difference between HiveAnalysis that converts <code>InsertIntoTable(r: HiveTableRelation...)</code> to <code>InsertIntoHiveTable</code> and <code>RelationConversions</code> that converts <code>InsertIntoTable(r: HiveTableRelation,...)</code> to <code>InsertIntoTable</code> (with <code>LogicalRelation</code>)?</p> <p>NOTE: Inserting into &lt;&gt; or &lt;&gt; is not allowed (and fails at analysis). <p><code>InsertIntoTable</code> (with <code>UnresolvedRelation</code> leaf logical operator) is &lt;&gt; when: <ul> <li> <p>[[INSERT_INTO_TABLE]][[INSERT_OVERWRITE_TABLE]] <code>INSERT INTO</code> or <code>INSERT OVERWRITE TABLE</code> SQL statements are executed (as a sql/AstBuilder.md#visitSingleInsertQuery[single insert] or a sql/AstBuilder.md#visitMultiInsertQuery[multi-insert] query)</p> </li> <li> <p><code>DataFrameWriter</code> is requested to insert a DataFrame into a table</p> </li> <li> <p><code>RelationConversions</code> logical evaluation rule is hive/RelationConversions.md#apply[executed] (and transforms <code>InsertIntoTable</code> operators)</p> </li> <li> <p>hive/CreateHiveTableAsSelectCommand.md[CreateHiveTableAsSelectCommand] logical command is executed</p> </li> </ul> <p>[[output]] <code>InsertIntoTable</code> has an empty output schema.</p> <p>=== [[catalyst-dsl]][[insertInto]] Catalyst DSL -- <code>insertInto</code> Operator</p>"},{"location":"logical-operators/InsertIntoTable/#source-scala","title":"[source, scala]","text":"<p>insertInto(   tableName: String,   overwrite: Boolean = false): LogicalPlan</p> <p>insertInto operator in Catalyst DSL creates an <code>InsertIntoTable</code> logical operator, e.g. for testing or Spark SQL internals exploration.</p>"},{"location":"logical-operators/InsertIntoTable/#sourceplaintext","title":"[source,plaintext]","text":"<p>import org.apache.spark.sql.catalyst.dsl.plans._ val plan = table(\"a\").insertInto(tableName = \"t1\", overwrite = true) scala&gt; println(plan.numberedTreeString) 00 'InsertIntoTable 'UnresolvedRelation <code>t1</code>, true, false 01 +- 'UnresolvedRelation <code>a</code></p> <p>import org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable val op = plan.p(0) assert(op.isInstanceOf[InsertIntoTable])</p>"},{"location":"logical-operators/InsertIntoTable/#creating-instance","title":"Creating Instance","text":"<p><code>InsertIntoTable</code> takes the following when created:</p> <ul> <li>[[table]] Logical plan for the table to insert into</li> <li>[[partition]] Partition keys (with optional partition values for dynamic partition insert)</li> <li>[[query]] Logical plan representing the data to be written</li> <li>[[overwrite]] <code>overwrite</code> flag that indicates whether to overwrite an existing table or partitions (<code>true</code>) or not (<code>false</code>)</li> <li>[[ifPartitionNotExists]] <code>ifPartitionNotExists</code> flag</li> </ul>"},{"location":"logical-operators/InsertIntoTable/#inserting-into-view-not-allowed","title":"Inserting Into View Not Allowed","text":"<p>Inserting into a view is not allowed, i.e. a query plan with an <code>InsertIntoTable</code> operator with a <code>UnresolvedRelation</code> leaf operator that is resolved to a View unary operator fails at analysis (when ResolveRelations logical resolution is executed).</p> <pre><code>Inserting into a view is not allowed. View: [name].\n</code></pre>"},{"location":"logical-operators/InsertIntoTable/#source-scala_1","title":"[source, scala]","text":"<p>// Create a view val viewName = \"demo_view\" sql(s\"DROP VIEW IF EXISTS $viewName\") assert(spark.catalog.tableExists(viewName) == false) sql(s\"CREATE VIEW $viewName COMMENT 'demo view' AS SELECT 1,2,3\") assert(spark.catalog.tableExists(viewName))</p> <p>// The following should fail with an AnalysisException scala&gt; spark.range(0).write.insertInto(viewName) org.apache.spark.sql.AnalysisException: Inserting into a view is not allowed. View: <code>default</code>.<code>demo_view</code>.;   at org.apache.spark.sql.catalyst.analysis.packageAnalysisErrorAt.failAnalysis(package.scala:42)   at org.apache.spark.sql.catalyst.analysis.AnalyzerResolveRelationsanonfunapplyapplyapplyapply8.applyOrElse(Analyzer.scala:644)   at org.apache.spark.sql.catalyst.analysis.AnalyzerResolveRelationsanonfunapply8.applyOrElse(Analyzer.scala:640)   at org.apache.spark.sql.catalyst.trees.TreeNodeanonfuntransformUp1.apply(TreeNode.scala:289)   at org.apache.spark.sql.catalyst.trees.TreeNodeanonfuntransformUp1.apply(TreeNode.scala:289)   at org.apache.spark.sql.catalyst.trees.CurrentOrigin.withOrigin(TreeNode.scala:70)   at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)   at org.apache.spark.sql.catalyst.analysis.AnalyzerResolveRelations.apply(Analyzer.scala:640)   at org.apache.spark.sql.catalyst.analysis.AnalyzerResolveRelations.apply(Analyzer.scala:586)   at org.apache.spark.sql.catalyst.rules.RuleExecutoranonfunexecuteexecuteexecuteexecute1anonfunapplyapplyapplyapply1.apply(RuleExecutor.scala:87)   at org.apache.spark.sql.catalyst.rules.RuleExecutoranonfunexecuteexecuteexecuteexecute1anonfunapplyapplyapplyapply1.apply(RuleExecutor.scala:84)   at scala.collection.LinearSeqOptimizedclass.foldLeft(LinearSeqOptimized.scala:124)   at scala.collection.immutable.List.foldLeft(List.scala:84)   at org.apache.spark.sql.catalyst.rules.RuleExecutoranonfunexecute1.apply(RuleExecutor.scala:84)   at org.apache.spark.sql.catalyst.rules.RuleExecutoranonfunexecute1.apply(RuleExecutor.scala:76)   at scala.collection.immutable.List.foreach(List.scala:381)   at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)   at org.apache.spark.sql.catalyst.analysis.Analyzer.orgapachesparksqlcatalystanalysisAnalyzerexecuteSameContext(Analyzer.scala:124)   at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:118)   at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:103)   at org.apache.spark.sql.execution.QueryExecution.analyzedlzycompute(QueryExecution.scala:57)   at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)   at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)   at org.apache.spark.sql.execution.QueryExecution.withCachedDatalzycompute(QueryExecution.scala:61)   at org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:60)   at org.apache.spark.sql.execution.QueryExecution.optimizedPlanlzycompute(QueryExecution.scala:66)   at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)   at org.apache.spark.sql.execution.QueryExecution.sparkPlanlzycompute(QueryExecution.scala:72)   at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)   at org.apache.spark.sql.execution.QueryExecution.executedPlanlzycompute(QueryExecution.scala:77)   at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)   at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)   at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)   at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:322)   at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:308)   ... 49 elided</p> <p>=== [[inserting-into-rdd-based-table-not-allowed]] Inserting Into RDD-Based Table Not Allowed</p> <p>Inserting into an RDD-based table is not allowed, i.e. a query plan with an <code>InsertIntoTable</code> operator with one of the following logical operators (as the &lt;&gt;) fails at analysis (when PreWriteCheck extended logical check is executed): <ul> <li> <p>Logical operator is not a &lt;&gt; <li> <p><code>Range</code> leaf operator</p> </li> <li> <p><code>OneRowRelation</code> leaf operator</p> </li> <li> <p>LocalRelation leaf operator</p> </li>"},{"location":"logical-operators/InsertIntoTable/#source-scala_2","title":"[source, scala]","text":"<p>// Create a temporary view val data = spark.range(1) data.createOrReplaceTempView(\"demo\")</p> <p>scala&gt; spark.range(0).write.insertInto(\"demo\") org.apache.spark.sql.AnalysisException: Inserting into an RDD-based table is not allowed.;; 'InsertIntoTable Range (0, 1, step=1, splits=Some(8)), false, false +- Range (0, 0, step=1, splits=Some(8))</p> <p>at org.apache.spark.sql.execution.datasources.PreWriteCheck.failAnalysis(rules.scala:442)   at org.apache.spark.sql.execution.datasources.PreWriteCheckanonfunapply14.apply(rules.scala:473)   at org.apache.spark.sql.execution.datasources.PreWriteCheckanonfunapply14.apply(rules.scala:445)   at org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:117)   at org.apache.spark.sql.execution.datasources.PreWriteCheck.apply(rules.scala:445)   at org.apache.spark.sql.execution.datasources.PreWriteCheck.apply(rules.scala:440)   at org.apache.spark.sql.catalyst.analysis.CheckAnalysisanonfuncheckAnalysis2.apply(CheckAnalysis.scala:349)   at org.apache.spark.sql.catalyst.analysis.CheckAnalysisanonfuncheckAnalysis2.apply(CheckAnalysis.scala:349)   at scala.collection.mutable.ResizableArrayclass.foreach(ResizableArray.scala:59)   at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)   at org.apache.spark.sql.catalyst.analysis.CheckAnalysisclass.checkAnalysis(CheckAnalysis.scala:349)   at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)   at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)   at org.apache.spark.sql.execution.QueryExecution.analyzedlzycompute(QueryExecution.scala:57)   at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)   at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)   at org.apache.spark.sql.execution.QueryExecution.withCachedDatalzycompute(QueryExecution.scala:61)   at org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:60)   at org.apache.spark.sql.execution.QueryExecution.optimizedPlanlzycompute(QueryExecution.scala:66)   at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)   at org.apache.spark.sql.execution.QueryExecution.sparkPlanlzycompute(QueryExecution.scala:72)   at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)   at org.apache.spark.sql.execution.QueryExecution.executedPlanlzycompute(QueryExecution.scala:77)   at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)   at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)   at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)   at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:322)   at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:308)   ... 49 elided</p>"},{"location":"logical-operators/InsertIntoTable/#demo","title":"Demo","text":"<pre><code>// make sure that the tables are available in a catalog\nsql(\"CREATE TABLE IF NOT EXISTS t1(id long)\")\nsql(\"CREATE TABLE IF NOT EXISTS t2(id long)\")\n\nval q = sql(\"INSERT INTO TABLE t2 SELECT * from t1 LIMIT 100\")\nval plan = q.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'InsertIntoTable 'UnresolvedRelation `t2`, false, false\n01 +- 'GlobalLimit 100\n02    +- 'LocalLimit 100\n03       +- 'Project [*]\n04          +- 'UnresolvedRelation `t1`\n\n// Dataset API's version of \"INSERT OVERWRITE TABLE\" in SQL\nspark.range(10).write.mode(\"overwrite\").insertInto(\"t2\")\n</code></pre>"},{"location":"logical-operators/InsertIntoTable/#demo-insert-into-partitioned_table","title":"Demo: INSERT INTO partitioned_table","text":"<pre><code>spark.range(10)\n  .withColumn(\"p1\", 'id % 2)\n  .write\n  .mode(\"overwrite\")\n  .partitionBy(\"p1\")\n  .saveAsTable(\"partitioned_table\")\n\nval insertIntoQ = sql(\"INSERT INTO TABLE partitioned_table PARTITION (p1 = 4) VALUES 41, 42\")\nscala&gt; println(insertIntoQ.queryExecution.logical.numberedTreeString)\n00 'InsertIntoTable 'UnresolvedRelation `partitioned_table`, Map(p1 -&gt; Some(4)), false, false\n01 +- 'UnresolvedInlineTable [col1], [List(41), List(42)]\n</code></pre>"},{"location":"logical-operators/InsertIntoTable/#demo-insert-overwrite-table-partitioned_table","title":"Demo: INSERT OVERWRITE TABLE partitioned_table","text":"<pre><code>spark.range(10)\n  .withColumn(\"p1\", 'id % 2)\n  .write\n  .mode(\"overwrite\")\n  .partitionBy(\"p1\")\n  .saveAsTable(\"partitioned_table\")\n\nval insertOverwriteQ = sql(\"INSERT OVERWRITE TABLE partitioned_table PARTITION (p1 = 4) VALUES 40\")\nscala&gt; println(insertOverwriteQ.queryExecution.logical.numberedTreeString)\n00 'InsertIntoTable 'UnresolvedRelation `partitioned_table`, Map(p1 -&gt; Some(4)), true, false\n01 +- 'UnresolvedInlineTable [col1], [List(40)]\n</code></pre>"},{"location":"logical-operators/Join/","title":"Join Logical Operator","text":"<p><code>Join</code> is a binary logical operator that represents the following high-level operators in a logical plan:</p> <ul> <li>JOIN SQL statement</li> <li>Dataset.crossJoin, Dataset.join and Dataset.joinWith operators</li> </ul>"},{"location":"logical-operators/Join/#creating-instance","title":"Creating Instance","text":"<p><code>Join</code> takes the following to be created:</p> <ul> <li> Left logical operator <li> Right logical operator <li> JoinType <li> Optional Join Expression <li> <code>JoinHint</code> <p><code>Join</code> is created\u00a0when:</p> <ul> <li><code>AstBuilder</code> is requested to withJoinRelations (and visitFromClause)</li> <li>Dataset.crossJoin, Dataset.join and Dataset.joinWith operators are used</li> </ul>"},{"location":"logical-operators/Join/#catalyst-dsl","title":"Catalyst DSL","text":"<p><code>DslLogicalPlan</code> defines join operator to create a <code>Join</code>.</p> <pre><code>import org.apache.spark.sql.catalyst.dsl.plans._\nval t1 = table(\"t1\")\nval t2 = table(\"t2\")\nval j = t1.join(t2)\n\nimport org.apache.spark.sql.catalyst.plans.logical.Join\nassert(j.isInstanceOf[Join])\n</code></pre>"},{"location":"logical-operators/LeafNode/","title":"LeafNodes","text":"<p><code>LeafNode</code>\u00a0is an extension of the LogicalPlan abstraction for leaf logical operators (with no child nodes).</p>"},{"location":"logical-operators/LeafNode/#implementations","title":"Implementations","text":"<ul> <li>CTERelationRef</li> <li>DataSourceV2Relation</li> <li>DataSourceV2ScanRelation</li> <li>ExternalRDD</li> <li>HiveTableRelation</li> <li>InMemoryRelation</li> <li>LocalRelation</li> <li>LogicalQueryStage</li> <li>LogicalRDD</li> <li>LogicalRelation</li> <li>UnresolvedRelation</li> <li>others</li> </ul>"},{"location":"logical-operators/LeafNode/#children","title":"Children <pre><code>children: Seq[LogicalPlan]\n</code></pre> <p><code>children</code> is an empty collection (to denote being a leaf in an operator tree).</p> <p><code>children</code>\u00a0is part of the TreeNode abstraction.</p>","text":""},{"location":"logical-operators/LeafNode/#statistics","title":"Statistics <pre><code>computeStats(): Statistics\n</code></pre> <p><code>computeStats</code> throws an <code>UnsupportedOperationException</code>.</p>  <p><code>computeStats</code> is used when:</p> <ul> <li><code>BasicStatsPlanVisitor</code> is requested for the default size statistics (for <code>LeafNode</code>s)</li> <li><code>SizeInBytesOnlyStatsPlanVisitor</code> is requested for the default size statistics</li> </ul>","text":""},{"location":"logical-operators/LeafRunnableCommand/","title":"LeafRunnableCommand \u2014 Leaf Logical Runnable Commands","text":"<p><code>LeafRunnableCommand</code> is an extension of the RunnableCommand abstraction for leaf logical runnable commands.</p> <p>Important</p> <p>It looks like <code>LeafRunnableCommand</code> was introduced in SPARK-34989 to improve the performance of<code>mapChildren</code> and <code>withNewChildren</code> methods.</p> <p>I don't understand why such a simplistic trait definition could help as seems very Scala-specific. Help appreciated! \ud83d\ude4f</p> <pre><code>trait LeafRunnableCommand extends RunnableCommand with LeafLike[LogicalPlan]\n</code></pre>"},{"location":"logical-operators/LeafRunnableCommand/#implementations","title":"Implementations","text":"<ul> <li>AlterTableAddColumnsCommand</li> <li>AnalyzeColumnCommand</li> <li>AnalyzePartitionCommand</li> <li>AnalyzeTableCommand</li> <li>CacheTableCommand</li> <li>ClearCacheCommand</li> <li>CreateDataSourceTableCommand</li> <li>CreateTempViewUsing</li> <li>CreateViewCommand</li> <li>DescribeColumnCommand</li> <li>ExplainCommand</li> <li>InsertIntoDataSourceCommand</li> <li>SaveIntoDataSourceCommand</li> <li>ShowCreateTableCommand</li> <li>ShowTablePropertiesCommand</li> <li>TruncateTableCommand</li> <li>many others</li> </ul>"},{"location":"logical-operators/LoadDataCommand/","title":"LoadDataCommand","text":"<p><code>LoadDataCommand</code> is...FIXME</p>"},{"location":"logical-operators/LocalRelation/","title":"LocalRelation Leaf Logical Operator","text":"<p><code>LocalRelation</code> is a leaf logical operator that represents a scan over local collections (and so allows for optimizations for functions like <code>collect</code> or <code>take</code> to be executed locally on the driver with no executors).</p>"},{"location":"logical-operators/LocalRelation/#creating-instance","title":"Creating Instance","text":"<p><code>LocalRelation</code> takes the following to be created:</p> <ul> <li> Output Schema Attributes <li> Data (InternalRows) <li>isStreaming flag</li> <p>While created, <code>LocalRelation</code> asserts that the output attributes are all resolved or throws an <code>IllegalArgumentException</code>:</p> <pre><code>Unresolved attributes found when constructing LocalRelation.\n</code></pre> <p><code>LocalRelation</code> can be created\u00a0using apply, fromExternalRows, and fromProduct factory methods.</p>"},{"location":"logical-operators/LocalRelation/#isstreaming-flag","title":"isStreaming Flag <pre><code>isStreaming: Boolean\n</code></pre> <p><code>isStreaming</code> is part of the LogicalPlan abstraction.</p> <p><code>isStreaming</code> can be given when <code>LocalRelation</code> is created.</p> <p><code>isStreaming</code> is <code>false</code> by default.</p>","text":""},{"location":"logical-operators/LocalRelation/#multiinstancerelation","title":"MultiInstanceRelation <p><code>LocalRelation</code> is a MultiInstanceRelation.</p>","text":""},{"location":"logical-operators/LocalRelation/#local-datasets","title":"Local Datasets <p><code>Dataset</code> is local when the analyzed logical plan is a <code>LocalRelation</code>.</p> <pre><code>val data = Seq(1, 3, 4, 7)\nval nums = data.toDF\n\nscala&gt; :type nums\norg.apache.spark.sql.DataFrame\n\nval plan = nums.queryExecution.analyzed\nscala&gt; println(plan.numberedTreeString)\n00 LocalRelation [value#1]\n\nimport org.apache.spark.sql.catalyst.plans.logical.LocalRelation\nval relation = plan.collect { case r: LocalRelation =&gt; r }.head\nassert(relation.isInstanceOf[LocalRelation])\n\nval sql = relation.toSQL(inlineTableName = \"demo\")\nassert(sql == \"VALUES (1), (3), (4), (7) AS demo(value)\")\n\nval stats = relation.computeStats\nscala&gt; println(stats)\nStatistics(sizeInBytes=48.0 B, hints=none)\n</code></pre>","text":""},{"location":"logical-operators/LocalRelation/#execution-planning","title":"Execution Planning <p><code>LocalRelation</code> is resolved to LocalTableScanExec leaf physical operator by BasicOperators execution planning strategy.</p> <pre><code>import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\nassert(relation.isInstanceOf[LocalRelation])\n\nscala&gt; :type spark\norg.apache.spark.sql.SparkSession\n\nimport spark.sessionState.planner.BasicOperators\nval localScan = BasicOperators(relation).head\n\nimport org.apache.spark.sql.execution.LocalTableScanExec\nassert(localScan.isInstanceOf[LocalTableScanExec])\n</code></pre>","text":""},{"location":"logical-operators/LocalRelation/#statistics","title":"Statistics <pre><code>computeStats(): Statistics\n</code></pre> <p><code>computeStats</code>\u00a0is part of the LeafNode abstraction.</p> <p><code>computeStats</code> is the size of the objects in a single row (per the output schema) and multiplies it by the number of rows (in the data).</p>","text":""},{"location":"logical-operators/LocalRelation/#sql-representation","title":"SQL Representation <pre><code>toSQL(\n  inlineTableName: String): String\n</code></pre> <p><code>toSQL</code> generates a SQL statement of the format:</p> <pre><code>VALUES [data] AS [inlineTableName]([names])\n</code></pre>  <p>Note</p> <p><code>toSQL</code> does not seem to be used.</p>","text":""},{"location":"logical-operators/LocalRelation/#creating-non-empty-localrelation","title":"Creating Non-Empty LocalRelation <pre><code>fromProduct(\n  output: Seq[Attribute],\n  data: Seq[Product]): LocalRelation\n</code></pre> <p><code>fromProduct</code> creates a <code>LocalRelation</code> with the given output attributes and the data converted to InternalRows (using a Catalyst converter from the schema of the given attributes).</p>","text":""},{"location":"logical-operators/LocalRelation/#demo","title":"Demo <pre><code>import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.catalyst.expressions.AttributeReference\nimport org.apache.spark.sql.types.IntegerType\n</code></pre> <pre><code>val relation = LocalRelation.fromExternalRows(\n  output = Seq(AttributeReference(\"id\", IntegerType)()),\n  data = Seq(Row(1)))\n</code></pre>","text":""},{"location":"logical-operators/LogicalPlan/","title":"LogicalPlan \u2014 Logical Relational Operators of Structured Query","text":"<p><code>LogicalPlan</code> is an extension of the QueryPlan abstraction for logical operators to build a logical query plan (as a tree of logical operators).</p> <p><code>LogicalPlan</code> is eventually resolved (transformed) to a physical operator.</p>"},{"location":"logical-operators/LogicalPlan/#implementations","title":"Implementations","text":""},{"location":"logical-operators/LogicalPlan/#binarynode","title":"BinaryNode <p>Logical operators with two child logical operators</p>","text":""},{"location":"logical-operators/LogicalPlan/#command","title":"Command <p>Command</p>","text":""},{"location":"logical-operators/LogicalPlan/#leafnode","title":"LeafNode <p>LeafNode is a logical operator with no child operators</p>","text":""},{"location":"logical-operators/LogicalPlan/#unarynode","title":"UnaryNode <p>Logical operators with a single child logical operator</p>","text":""},{"location":"logical-operators/LogicalPlan/#other-logical-operators","title":"Other Logical Operators <ul> <li>CreateTable</li> <li>IgnoreCachedData</li> <li>NamedRelation</li> <li>ParsedStatement</li> <li>SupportsSubquery</li> <li>V2CreateTablePlan</li> <li>View</li> <li>others</li> </ul>","text":""},{"location":"logical-operators/LogicalPlan/#statistics-cache","title":"Statistics Cache <p>Cached plan statistics (as <code>Statistics</code>) of the <code>LogicalPlan</code></p> <p>Computed and cached in stats</p> <p>Used in stats and verboseStringWithSuffix</p> <p>Reset in invalidateStatsCache</p>","text":""},{"location":"logical-operators/LogicalPlan/#estimated-statistics","title":"Estimated Statistics <pre><code>stats(\n  conf: CatalystConf): Statistics\n</code></pre> <p><code>stats</code> returns the &lt;&gt; or &lt;&gt; (and caches it as &lt;&gt;). <p><code>stats</code> is used when:</p> <ul> <li>A <code>LogicalPlan</code> &lt;Statistics&gt;&gt; <li><code>QueryExecution</code> is requested to build a complete text representation</li> <li><code>JoinSelection</code> checks whether a plan can be broadcast et al</li> <li>CostBasedJoinReorder.md[CostBasedJoinReorder] attempts to reorder inner joins</li> <li><code>LimitPushDown</code> is executed (for FullOuter join)</li> <li><code>AggregateEstimation</code> estimates <code>Statistics</code></li> <li><code>FilterEstimation</code> estimates child <code>Statistics</code></li> <li><code>InnerOuterEstimation</code> estimates <code>Statistics</code> of the left and right sides of a join</li> <li><code>LeftSemiAntiEstimation</code> estimates <code>Statistics</code></li> <li><code>ProjectEstimation</code> estimates <code>Statistics</code></li>","text":""},{"location":"logical-operators/LogicalPlan/#refreshing-child-logical-operators","title":"Refreshing Child Logical Operators <pre><code>refresh(): Unit\n</code></pre> <p><code>refresh</code> calls itself recursively for every child logical operator.</p>  <p>Note</p> <p><code>refresh</code> is overriden by LogicalRelation only (that refreshes the location of <code>HadoopFsRelation</code> relations only).</p>  <p><code>refresh</code> is used when:</p> <ul> <li> <p><code>SessionCatalog</code> is requested to refresh a table</p> </li> <li> <p><code>CatalogImpl</code> is requested to refresh a table</p> </li> </ul>","text":""},{"location":"logical-operators/LogicalPlan/#resolving-column-attributes-to-references-in-query-plan","title":"Resolving Column Attributes to References in Query Plan <pre><code>resolve(\n  nameParts: Seq[String],\n  resolver: Resolver): Option[NamedExpression]\nresolve(\n  schema: StructType,\n  resolver: Resolver): Seq[Attribute]\n</code></pre> <p><code>resolve</code> requests the outputAttributes to resolve and then the outputMetadataAttributes if the first resolve did not give a NamedExpression.</p>","text":""},{"location":"logical-operators/LogicalPlan/#accessing-logical-query-plan-of-structured-query","title":"Accessing Logical Query Plan of Structured Query <p>In order to get the logical plan of a structured query you should use the &lt;&gt;. <pre><code>scala&gt; :type q\norg.apache.spark.sql.Dataset[Long]\n\nval plan = q.queryExecution.logical\nscala&gt; :type plan\norg.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n</code></pre> <p><code>LogicalPlan</code> goes through execution stages (as a QueryExecution). In order to convert a <code>LogicalPlan</code> to a <code>QueryExecution</code> you should use <code>SessionState</code> and request it to \"execute\" the plan.</p> <pre><code>scala&gt; :type spark\norg.apache.spark.sql.SparkSession\n\n// You could use Catalyst DSL to create a logical query plan\nscala&gt; :type plan\norg.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n\nval qe = spark.sessionState.executePlan(plan)\nscala&gt; :type qe\norg.apache.spark.sql.execution.QueryExecution\n</code></pre>","text":""},{"location":"logical-operators/LogicalPlan/#maximum-number-of-records","title":"Maximum Number of Records <pre><code>maxRows: Option[Long]\n</code></pre> <p><code>maxRows</code> is undefined by default (<code>None</code>).</p> <p><code>maxRows</code> is used when <code>LogicalPlan</code> is requested for maxRowsPerPartition.</p>","text":""},{"location":"logical-operators/LogicalPlan/#maximum-number-of-records-per-partition","title":"Maximum Number of Records per Partition <pre><code>maxRowsPerPartition: Option[Long]\n</code></pre> <p><code>maxRowsPerPartition</code> is exactly the maximum number of records by default.</p> <p><code>maxRowsPerPartition</code> is used when LimitPushDown logical optimization is executed.</p>","text":""},{"location":"logical-operators/LogicalPlan/#executing-logical-plan","title":"Executing Logical Plan <p>A common idiom in Spark SQL to make sure that a logical plan can be analyzed is to request a <code>SparkSession</code> for the SessionState that is in turn requested to \"execute\" the logical plan (which simply creates a QueryExecution).</p> <pre><code>scala&gt; :type plan\norg.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n\nval qe = sparkSession.sessionState.executePlan(plan)\nqe.assertAnalyzed()\n// the following gives the analyzed logical plan\n// no exceptions are expected since analysis went fine\nval analyzedPlan = qe.analyzed\n</code></pre>","text":""},{"location":"logical-operators/LogicalPlan/#converting-logical-plan-to-dataset","title":"Converting Logical Plan to Dataset <p>Another common idiom in Spark SQL to convert a <code>LogicalPlan</code> into a <code>Dataset</code> is to use Dataset.ofRows internal method that \"executes\" the logical plan followed by creating a Dataset with the QueryExecution and RowEncoder.</p>","text":""},{"location":"logical-operators/LogicalPlan/#childrenresolved","title":"childrenResolved <pre><code>childrenResolved: Boolean\n</code></pre> <p>A logical operator is considered partially resolved when its child operators are resolved (aka children resolved).</p>","text":""},{"location":"logical-operators/LogicalPlan/#resolved","title":"resolved <pre><code>resolved: Boolean\n</code></pre> <p><code>resolved</code> is <code>true</code> for all expressions and the children resolved.</p>  Lazy Value <p><code>resolved</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p>","text":""},{"location":"logical-operators/LogicalPlan/#metadataOutput","title":"Metadata Output Attributes <pre><code>metadataOutput: Seq[Attribute]\n</code></pre> <p><code>metadataOutput</code> requests the children for the metadata output attributes (recursively).</p>  <p>Note</p> <p><code>metadataOutput</code> should be overridden if this operators does not propagate its children's output.</p>  <p>See:</p> <ul> <li>DataSourceV2Relation</li> <li>Join</li> <li>LogicalRelation</li> <li>Project</li> <li>SubqueryAlias</li> </ul>  <p><code>metadataOutput</code> is used when:</p> <ul> <li>AddMetadataColumns logical resolution rule is executed</li> <li><code>UnresolvedStar</code> expression is requested to expand</li> <li><code>LogicalPlan</code> is requested for the child metadata output attributes and outputMetadataAttributes</li> </ul>","text":""},{"location":"logical-operators/LogicalPlan/#childMetadataAttributes","title":"childMetadataAttributes <pre><code>childMetadataAttributes: AttributeSeq\n</code></pre> <p><code>childMetadataAttributes</code> is an AttributeSeq of the (non-empty) metadataOutputs of the children of this operator.</p>  Lazy Value <p><code>childMetadataAttributes</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>   <p><code>childMetadataAttributes</code> is used when:</p> <ul> <li><code>LogicalPlan</code> is requested to resolveChildren</li> </ul>","text":""},{"location":"logical-operators/LogicalPlan/#outputMetadataAttributes","title":"outputMetadataAttributes <pre><code>outputMetadataAttributes: AttributeSeq\n</code></pre> <p><code>outputMetadataAttributes</code> is an AttributeSeq of the metadataOutput of this operator.</p>  Lazy Value <p><code>outputMetadataAttributes</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>   <p><code>outputMetadataAttributes</code> is used when:</p> <ul> <li><code>LogicalPlan</code> is requested to resolve</li> </ul>","text":""},{"location":"logical-operators/LogicalQueryStage/","title":"LogicalQueryStage Leaf Logical Operator","text":"<p><code>LogicalQueryStage</code> is a leaf logical operator for Adaptive Query Execution.</p>"},{"location":"logical-operators/LogicalQueryStage/#creating-instance","title":"Creating Instance","text":"<p><code>LogicalQueryStage</code> takes the following to be created:</p> <ul> <li> Logical Plan <li> Physical Plan <p><code>LogicalQueryStage</code> is created when:</p> <ul> <li>AdaptiveSparkPlanExec physical operator is executed</li> </ul>"},{"location":"logical-operators/LogicalQueryStage/#query-optimization","title":"Query Optimization","text":"<p><code>LogicalQueryStage</code> is a \"target\" of the following logical optimizations:</p> <ul> <li>AQEPropagateEmptyRelation</li> <li>DynamicJoinSelection</li> </ul>"},{"location":"logical-operators/LogicalQueryStage/#query-planning","title":"Query Planning","text":"<p><code>LogicalQueryStage</code> is planned by LogicalQueryStageStrategy execution planning strategy.</p>"},{"location":"logical-operators/LogicalQueryStage/#computing-runtime-statistics","title":"Computing Runtime Statistics <pre><code>computeStats(): Statistics\n</code></pre> <p><code>computeStats</code> is part of the LeafNode abstraction.</p>  <p><code>computeStats</code> tries to find the first QueryStageExec leaf physical operators in the physical plan that is then requested for the statistics.</p> <p><code>computeStats</code> prints out the following DEBUG messages to the logs based on the availability of the statistics.</p> <pre><code>Physical stats available as [physicalStats] for plan: [physicalPlan]\n</code></pre> <pre><code>Physical stats not available for plan: [physicalPlan]\n</code></pre> <p>In the end, <code>computeStats</code> gives the statistics of the physical operator or requests the logical plan for them.</p>","text":""},{"location":"logical-operators/LogicalQueryStage/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.adaptive.LogicalQueryStage</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.LogicalQueryStage.name = org.apache.spark.sql.execution.adaptive.LogicalQueryStage\nlogger.LogicalQueryStage.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"logical-operators/LogicalRDD/","title":"LogicalRDD Leaf Logical Operator","text":"<p><code>LogicalRDD</code> is a leaf logical operator with &lt;&gt; support for a logical representation of a scan over &lt;&gt;. <p><code>LogicalRDD</code> is &lt;&gt; when: <ul> <li> <p><code>Dataset</code> is requested to checkpoint</p> </li> <li> <p><code>SparkSession</code> is requested to create a DataFrame from an RDD of internal binary rows</p> </li> </ul> <p>Note</p> <p><code>LogicalRDD</code> is resolved to <code>RDDScanExec</code> physical operator when BasicOperators execution planning strategy is executed.</p> <p>=== [[newInstance]] <code>newInstance</code> Method</p>"},{"location":"logical-operators/LogicalRDD/#source-scala","title":"[source, scala]","text":""},{"location":"logical-operators/LogicalRDD/#newinstance-logicalrddthistype","title":"newInstance(): LogicalRDD.this.type","text":"<p><code>newInstance</code> is part of MultiInstanceRelation abstraction.</p> <p><code>newInstance</code>...FIXME</p> <p>=== [[computeStats]] Computing Statistics -- <code>computeStats</code> Method</p>"},{"location":"logical-operators/LogicalRDD/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-operators/LogicalRDD/#computestats-statistics","title":"computeStats(): Statistics","text":"<p><code>computeStats</code>...FIXME</p> <p><code>computeStats</code> is part of the LeafNode abstraction.</p>"},{"location":"logical-operators/LogicalRDD/#creating-instance","title":"Creating Instance","text":"<p><code>LogicalRDD</code> takes the following to be created:</p> <ul> <li>[[output]] Output schema attributes</li> <li>[[rdd]] <code>RDD</code> of InternalRows</li> <li>[[outputPartitioning]] Output Partitioning</li> <li>[[outputOrdering]] Output ordering (<code>SortOrder</code>)</li> <li>[[isStreaming]] <code>isStreaming</code> flag</li> <li>[[session]] SparkSession</li> </ul>"},{"location":"logical-operators/LogicalRelation/","title":"LogicalRelation Leaf Logical Operator","text":"<p><code>LogicalRelation</code> is a leaf logical operator that represents a BaseRelation in a logical query plan.</p> <p><code>LogicalRelation</code> is a MultiInstanceRelation.</p>"},{"location":"logical-operators/LogicalRelation/#creating-instance","title":"Creating Instance","text":"<p><code>LogicalRelation</code> takes the following to be created:</p> <ul> <li> BaseRelation <li> Output Schema (<code>AttributeReference</code>s) <li> Optional CatalogTable <li> <code>isStreaming</code> flag <p><code>LogicalRelation</code> is created using apply factory.</p>"},{"location":"logical-operators/LogicalRelation/#apply-utility","title":"apply Utility <pre><code>apply(\n  relation: BaseRelation,\n  isStreaming: Boolean = false): LogicalRelation\napply(\n  relation: BaseRelation,\n  table: CatalogTable): LogicalRelation\n</code></pre> <p><code>apply</code> wraps the given BaseRelation into a <code>LogicalRelation</code> (so it could be used in a logical query plan).</p> <p><code>apply</code> creates a LogicalRelation for the given BaseRelation (with a CatalogTable and <code>isStreaming</code> flag).</p> <pre><code>import org.apache.spark.sql.sources.BaseRelation\nval baseRelation: BaseRelation = ???\n\nval data = spark.baseRelationToDataFrame(baseRelation)\n</code></pre> <p><code>apply</code> is used when:</p> <ul> <li><code>SparkSession</code> is requested for a DataFrame for a BaseRelation</li> <li>CreateTempViewUsing command is executed</li> <li><code>FallBackFileSourceV2</code> logical resolution rule is executed</li> <li>ResolveSQLOnFile and FindDataSourceTable logical evaluation rules are executed</li> <li><code>HiveMetastoreCatalog</code> is requested to convert a HiveTableRelation</li> <li><code>FileStreamSource</code> (Spark Structured Streaming) is requested to <code>getBatch</code></li> </ul>","text":""},{"location":"logical-operators/LogicalRelation/#refresh","title":"refresh <pre><code>refresh(): Unit\n</code></pre> <p><code>refresh</code> is part of LogicalPlan abstraction.</p> <p><code>refresh</code> requests the FileIndex (of the HadoopFsRelation) to refresh.</p>  <p>Note</p> <p><code>refresh</code> does the work for HadoopFsRelation relations only.</p>","text":""},{"location":"logical-operators/LogicalRelation/#simple-text-representation","title":"Simple Text Representation <pre><code>simpleString(\n  maxFields: Int): String\n</code></pre> <p><code>simpleString</code> is part of the QueryPlan abstraction.</p> <p><code>simpleString</code> is made up of the output schema (truncated to <code>maxFields</code>) and the relation:</p> <pre><code>Relation[[output]] [relation]\n</code></pre>","text":""},{"location":"logical-operators/LogicalRelation/#demo","title":"Demo <pre><code>val q = spark.read.text(\"README.md\")\nval logicalPlan = q.queryExecution.logical\n\nscala&gt; println(logicalPlan.simpleString)\nRelation[value#2] text\n</code></pre>","text":""},{"location":"logical-operators/LogicalRelation/#computestats","title":"computeStats <pre><code>computeStats(): Statistics\n</code></pre> <p><code>computeStats</code> is part of the LeafNode abstraction.</p>  <p><code>computeStats</code> takes the optional CatalogTable.</p> <p>If available, <code>computeStats</code> requests the <code>CatalogTable</code> for the CatalogStatistics that, if available, is requested to toPlanStats (with the <code>planStatsEnabled</code> flag enabled when either spark.sql.cbo.enabled or spark.sql.cbo.planStats.enabled is enabled).</p> <p>Otherwise, <code>computeStats</code> creates a Statistics with the <code>sizeInBytes</code> only to be the sizeInBytes of the BaseRelation.</p>","text":""},{"location":"logical-operators/LogicalRelation/#demo_1","title":"Demo <p>The following are two logically-equivalent batch queries described using different Spark APIs: Scala and SQL.</p> <pre><code>val format = \"csv\"\nval path = \"../datasets/people.csv\"\n</code></pre> <pre><code>val q = spark\n  .read\n  .option(\"header\", true)\n  .format(format)\n  .load(path)\n</code></pre> <pre><code>scala&gt; println(q.queryExecution.logical.numberedTreeString)\n00 Relation[id#16,name#17] csv\n</code></pre> <pre><code>val q = sql(s\"select * from `$format`.`$path`\")\n</code></pre> <pre><code>scala&gt; println(q.queryExecution.optimizedPlan.numberedTreeString)\n00 Relation[_c0#74,_c1#75] csv\n</code></pre>","text":""},{"location":"logical-operators/MapPartitions/","title":"MapPartitions Unary Logical Operator","text":"<p><code>MapPartitions</code> is a unary logical operator to represent Dataset.mapPartitions operator in a logical query plan.</p> <p><code>MapPartitions</code> is an <code>ObjectConsumer</code> and an <code>ObjectProducer</code>.</p>"},{"location":"logical-operators/MapPartitions/#creating-instance","title":"Creating Instance","text":"<p><code>MapPartitions</code> takes the following to be created:</p> <ul> <li> Map Partitions Function (<code>Iterator[Any] =&gt; Iterator[Any]</code>) <li> Output Object Attribute <li> Child Logical Plan <p><code>MapPartitions</code> is created (indirectly using apply utility) when:</p> <ul> <li>Dataset.mapPartitions operator is used</li> </ul>"},{"location":"logical-operators/MapPartitions/#query-planning","title":"Query Planning","text":"<p><code>MapPartitions</code> is planned as <code>MapPartitionsExec</code> physical operator when BasicOperators execution planning strategy is executed.</p>"},{"location":"logical-operators/MapPartitions/#creating-mappartitions","title":"Creating MapPartitions <pre><code>apply[T : Encoder, U : Encoder](\n  func: Iterator[T] =&gt; Iterator[U],\n  child: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> creates a MapPartitions unary logical operator with a DeserializeToObject child (for the type <code>T</code> of the input objects) and a SerializeFromObject parent (for the type <code>U</code> of the output objects).</p> <p><code>apply</code> is used when:</p> <ul> <li>Dataset.mapPartitions operator is used</li> </ul>","text":""},{"location":"logical-operators/MapPartitions/#demo","title":"Demo <pre><code>val ds = spark.range(5)\nval fn: Iterator[Long] =&gt; Iterator[String] = { ns =&gt;\n  ns.map { n =&gt;\n    if (n % 2 == 0) {\n      s\"even ($n)\"\n    } else {\n      s\"odd ($n)\"\n    }\n  }\n}\nds.mapPartitions(fn) // FIXME That does not seem to work!\n</code></pre>","text":""},{"location":"logical-operators/MergeIntoTable/","title":"MergeIntoTable Logical Command","text":"<p><code>MergeIntoTable</code> is a Command that represents MERGE INTO SQL statement.</p> <p><code>MergeIntoTable</code> is a SupportsSubquery (for the source).</p>"},{"location":"logical-operators/MergeIntoTable/#creating-instance","title":"Creating Instance","text":"<p><code>MergeIntoTable</code> takes the following to be created:</p> <ul> <li> Target Table (LogicalPlan) <li> Source Table or Subquery (LogicalPlan) <li> Merge Condition (Expression) <li> Matched <code>MergeAction</code>s <li> Not-Matched <code>MergeAction</code>s <p><code>MergeIntoTable</code> is created\u00a0when:</p> <ul> <li><code>AstBuilder</code> is requested to parse MERGE INTO SQL statement</li> </ul>"},{"location":"logical-operators/MergeIntoTable/#execution-planning","title":"Execution Planning","text":"<p><code>MergeIntoTable</code> command is not supported in Spark SQL and BasicOperators execution planning strategy throws an <code>UnsupportedOperationException</code> when finds any:</p> <pre><code>MERGE INTO TABLE is not supported temporarily.\n</code></pre> <p>Note</p> <p><code>MergeIntoTable</code> is to allow custom data sources to support <code>MERGE</code> SQL statement (and so does Delta Lake).</p>"},{"location":"logical-operators/MultiInstanceRelation/","title":"MultiInstanceRelation Logical Operators","text":"<p><code>MultiInstanceRelation</code> is an abstraction of logical operators for which a single instance might appear multiple times in a logical query plan.</p>"},{"location":"logical-operators/MultiInstanceRelation/#contract","title":"Contract","text":""},{"location":"logical-operators/MultiInstanceRelation/#creating-new-instance","title":"Creating New Instance <pre><code>newInstance(): LogicalPlan\n</code></pre> <p>Used when:</p> <ul> <li>ResolveRelations logical resolution rule is executed</li> <li><code>DeduplicateRelations</code> logical resolution rule is executed</li> </ul>","text":""},{"location":"logical-operators/MultiInstanceRelation/#implementations","title":"Implementations","text":"<ul> <li>CTERelationRef</li> <li>DataSourceV2Relation</li> <li>ExternalRDD</li> <li>HiveTableRelation</li> <li>InMemoryRelation</li> <li>LocalRelation</li> <li>LogicalRDD</li> <li>LogicalRelation</li> <li>others</li> </ul>"},{"location":"logical-operators/NamedRelation/","title":"NamedRelation Logical Operators","text":"<p><code>NamedRelation</code>\u00a0is an extension of the LogicalPlan abstraction for logical operators with a name and support for skipSchemaResolution.</p>"},{"location":"logical-operators/NamedRelation/#contract","title":"Contract","text":""},{"location":"logical-operators/NamedRelation/#name","title":"Name <pre><code>name: String\n</code></pre> <p>Name of this relation</p> <p>See:</p> <ul> <li>DataSourceV2Relation</li> <li>DataSourceV2ScanRelation</li> <li>UnresolvedRelation</li> </ul>","text":""},{"location":"logical-operators/NamedRelation/#skipschemaresolution","title":"skipSchemaResolution <pre><code>skipSchemaResolution: Boolean\n</code></pre> <p>Indicates that it is acceptable to skip schema resolution during write operations (and let the schema of input data be different from the schema of this relation during write)</p> <p>Default: <code>false</code></p> <p>See:</p> <ul> <li>DataSourceV2Relation</li> </ul> <p>Used when:</p> <ul> <li>ResolveReferences logical resolution rule is executed (for MergeIntoTable logical operators)</li> <li>ResolveDefaultColumns logical resolution rule is executed (to getSchemaForTargetTable)</li> <li><code>V2WriteCommand</code> logical command is requested to outputResolved</li> <li>UpdateTable logical command is executed (and skipSchemaResolution)</li> <li>MergeIntoTable logical command is executed (and skipSchemaResolution)</li> </ul>","text":""},{"location":"logical-operators/NamedRelation/#implementations","title":"Implementations","text":"<ul> <li>DataSourceV2Relation</li> <li>DataSourceV2ScanRelation</li> <li>UnresolvedRelation</li> </ul>"},{"location":"logical-operators/OverwriteByExpression/","title":"OverwriteByExpression Logical Command","text":"<p><code>OverwriteByExpression</code> is a V2WriteCommand.</p>"},{"location":"logical-operators/OverwriteByExpression/#creating-instance","title":"Creating Instance","text":"<p><code>OverwriteByExpression</code> takes the following to be created:</p> <ul> <li> NamedRelation <li> Delete Expression <li> Logical Query Plan <li> Write Options <li> <code>isByName</code> flag <p><code>OverwriteByExpression</code> is created (using byName and byPosition utilities)\u00a0when...FIXME</p>"},{"location":"logical-operators/OverwriteByExpression/#resolved","title":"resolved <pre><code>resolved: Boolean\n</code></pre> <p><code>resolved</code> is <code>true</code> when outputResolved and delete expression are.</p> <p><code>resolved</code>\u00a0is part of the LogicalPlan abstraction.</p>","text":""},{"location":"logical-operators/OverwriteByExpression/#creating-overwritebyexpression-by-name","title":"Creating OverwriteByExpression by Name <pre><code>byName(\n  table: NamedRelation,\n  df: LogicalPlan,\n  deleteExpr: Expression,\n  writeOptions: Map[String, String] = Map.empty): OverwriteByExpression\n</code></pre> <p><code>byName</code> creates a OverwriteByExpression with isByName enabled (<code>true</code>).</p> <p><code>byName</code>\u00a0is used when:</p> <ul> <li><code>DataFrameWriter</code> is requested to save</li> <li><code>DataFrameWriterV2</code> is requested to overwrite</li> </ul>","text":""},{"location":"logical-operators/OverwriteByExpression/#creating-overwritebyexpression-by-position","title":"Creating OverwriteByExpression by Position <pre><code>byPosition(\n  table: NamedRelation,\n  query: LogicalPlan,\n  deleteExpr: Expression,\n  writeOptions: Map[String, String] = Map.empty): OverwriteByExpression\n</code></pre> <p><code>byPosition</code> creates a OverwriteByExpression with isByName disabled (<code>false</code>).</p> <p><code>byPosition</code>\u00a0is used:</p> <ul> <li>ResolveInsertInto logical resolution rule is executed</li> <li><code>DataFrameWriter</code> is requested to insertInto</li> </ul>","text":""},{"location":"logical-operators/OverwritePartitionsDynamic/","title":"OverwritePartitionsDynamic Logical Command","text":"<p><code>OverwritePartitionsDynamic</code> is a V2WriteCommand for dynamically overwrite partitions in an existing table (that supports dynamic overwrite in batch mode).</p>"},{"location":"logical-operators/OverwritePartitionsDynamic/#creating-instance","title":"Creating Instance","text":"<p><code>OverwritePartitionsDynamic</code> takes the following to be created:</p> <ul> <li> NamedRelation <li> Query <li> Write Options <li> <code>isByName</code> flag <p><code>OverwritePartitionsDynamic</code> is created (indirectly) using byName and byPosition utilities.</p>"},{"location":"logical-operators/OverwritePartitionsDynamic/#creating-overwritepartitionsdynamic-by-name","title":"Creating OverwritePartitionsDynamic by Name <pre><code>byName(\n  table: NamedRelation,\n  df: LogicalPlan,\n  writeOptions: Map[String, String] = Map.empty): OverwritePartitionsDynamic\n</code></pre> <p><code>byName</code> creates a OverwritePartitionsDynamic with the isByName flag enabled (<code>true</code>).</p> <p><code>byName</code>\u00a0is used when:</p> <ul> <li><code>DataFrameWriterV2</code> is requested to overwritePartitions</li> </ul>","text":""},{"location":"logical-operators/OverwritePartitionsDynamic/#creating-overwritepartitionsdynamic-by-position","title":"Creating OverwritePartitionsDynamic by Position <pre><code>byPosition(\n  table: NamedRelation,\n  query: LogicalPlan,\n  writeOptions: Map[String, String] = Map.empty): OverwritePartitionsDynamic\n</code></pre> <p><code>byPosition</code> creates a OverwritePartitionsDynamic with the isByName flag disabled (<code>false</code>).</p> <p><code>byPosition</code>\u00a0is used when:</p> <ul> <li>ResolveInsertInto logical resolution rule is executed (for InsertIntoStatements over DataSourceV2Relation)</li> <li><code>DataFrameWriter</code> is requested to insertInto</li> </ul>","text":""},{"location":"logical-operators/OverwritePartitionsDynamic/#execution-planning","title":"Execution Planning <p><code>OverwritePartitionsDynamic</code> (over DataSourceV2Relation) is planned as <code>OverwritePartitionsDynamicExec</code> physical operator by DataSourceV2Strategy execution planning strategy.</p>","text":""},{"location":"logical-operators/OverwritePartitionsDynamic/#tablecapabilitycheck","title":"TableCapabilityCheck <p>TableCapabilityCheck extended analysis check asserts that <code>OverwritePartitionsDynamic</code> uses DataSourceV2Relation that supports dynamic overwrite in batch mode.</p>","text":""},{"location":"logical-operators/ParsedStatement/","title":"ParsedStatement Logical Operators","text":"<p><code>ParsedStatement</code> is an extension of the LogicalPlan abstraction for logical operators that hold exactly what was parsed from SQL statements.</p> <p><code>ParsedStatement</code> are never resolved and must be converted to concrete logical plans.</p>"},{"location":"logical-operators/ParsedStatement/#implementations","title":"Implementations","text":"<ul> <li><code>LeafParsedStatement</code></li> <li><code>UnaryParsedStatement</code><ul> <li>InsertIntoStatement</li> </ul> </li> </ul>"},{"location":"logical-operators/Pivot/","title":"Pivot Unary Logical Operator","text":"<p><code>Pivot</code> is an unary logical operator that represents pivot operator.</p>"},{"location":"logical-operators/Pivot/#creating-instance","title":"Creating Instance","text":"<p><code>Pivot</code> takes the following to be created:</p> <ul> <li> Grouping NamedExpressions <li> Pivot Column Expression <li> Pivot Value Expressions <li> Aggregate Expressions <li> Child LogicalPlan <p><code>Pivot</code> is created\u00a0when:</p> <ul> <li><code>AstBuilder</code> is requested to parse PIVOT clause</li> <li><code>RelationalGroupedDataset</code> is requested to toDF (with <code>PivotType</code>)</li> </ul>"},{"location":"logical-operators/Pivot/#analysis-phase","title":"Analysis Phase <p><code>Pivot</code> is resolved to a Aggregate logical operator by ResolvePivot logical evaluation rule.</p> <pre><code>scala&gt; :type spark\norg.apache.spark.sql.SparkSession\n\nimport spark.sessionState.analyzer.ResolveAliases\n\n// see q in the demo on this page\nval plan = q.queryExecution.logical\n\nscala&gt; println(plan.numberedTreeString)\n00 Pivot [city#8], year#9: int, [2015, 2016, 2017], [count(1) AS count#24L]\n01 +- Project [_1#3 AS id#7, _2#4 AS city#8, _3#5 AS year#9]\n02    +- LocalRelation [_1#3, _2#4, _3#5]\n\n// FIXME Find a plan to show the effect of ResolveAliases\nval planResolved = ResolveAliases(plan)\n</code></pre> <p><code>Pivot</code> operator \"disappears\" behind (i.e. is converted to) an Aggregate logical operator.</p> <pre><code>import spark.sessionState.analyzer.ResolvePivot\nval planAfterResolvePivot = ResolvePivot(plan)\nscala&gt; println(planAfterResolvePivot.numberedTreeString)\n00 Project [city#8, __pivot_count(1) AS `count` AS `count(1) AS ``count```#62[0] AS 2015#63L, __pivot_count(1) AS `count` AS `count(1) AS ``count```#62[1] AS 2016#64L, __pivot_count(1) AS `count` AS `count(1) AS ``count```#62[2] AS 2017#65L]\n01 +- Aggregate [city#8], [city#8, pivotfirst(year#9, count(1) AS `count`#54L, 2015, 2016, 2017, 0, 0) AS __pivot_count(1) AS `count` AS `count(1) AS ``count```#62]\n02    +- Aggregate [city#8, year#9], [city#8, year#9, count(1) AS count#24L AS count(1) AS `count`#54L]\n03       +- Project [_1#3 AS id#7, _2#4 AS city#8, _3#5 AS year#9]\n04          +- LocalRelation [_1#3, _2#4, _3#5]\n</code></pre>","text":""},{"location":"logical-operators/Pivot/#demo","title":"Demo <pre><code>val visits = Seq(\n  (0, \"Warsaw\", 2015),\n  (1, \"Warsaw\", 2016),\n  (2, \"Boston\", 2017)\n).toDF(\"id\", \"city\", \"year\")\n</code></pre> <pre><code>val q = visits\n  .groupBy(\"city\")\n  .pivot(\"year\", Seq(\"2015\", \"2016\", \"2017\"))\n  .count()\n</code></pre> <pre><code>scala&gt; println(q.queryExecution.logical.numberedTreeString)\n00 Pivot [city#8], year#9: int, [2015, 2016, 2017], [count(1) AS count#157L]\n01 +- Project [_1#3 AS id#7, _2#4 AS city#8, _3#5 AS year#9]\n02    +- LocalRelation [_1#3, _2#4, _3#5]\n</code></pre>","text":""},{"location":"logical-operators/Project/","title":"Project Unary Logical Operator","text":"<p>[[creating-instance]] <code>Project</code> is a &lt;&gt; that takes the following when created: <ul> <li>[[projectList]] Project &lt;&gt; <li>[[child]] Child &lt;&gt; <p><code>Project</code> is &lt;&gt; to represent the following: <ul> <li>Dataset operators, i.e. joinWith, select (incl. <code>selectUntyped</code>), <code>unionByName</code></li> <li><code>KeyValueGroupedDataset</code> operators, i.e. <code>keys</code>, <code>mapValues</code></li> <li><code>CreateViewCommand</code> logical command is &lt;&gt; (and &lt;&gt;) <li>SQL's SELECT queries with named expressions</li> <p><code>Project</code> can also appear in a logical plan after analysis or optimization phases.</p> <pre><code>// FIXME Add examples for the following operators\n// Dataset.unionByName\n// KeyValueGroupedDataset.mapValues\n// KeyValueGroupedDataset.keys\n// CreateViewCommand.aliasPlan\n\n// joinWith operator\ncase class Person(id: Long, name: String, cityId: Long)\ncase class City(id: Long, name: String)\nval family = Seq(\n  Person(0, \"Agata\", 0),\n  Person(1, \"Iweta\", 0),\n  Person(2, \"Patryk\", 2),\n  Person(3, \"Maksym\", 0)).toDS\nval cities = Seq(\n  City(0, \"Warsaw\"),\n  City(1, \"Washington\"),\n  City(2, \"Sopot\")).toDS\nval q = family.joinWith(cities, family(\"cityId\") === cities(\"id\"), \"inner\")\nscala&gt; println(q.queryExecution.logical.numberedTreeString)\n00 Join Inner, (_1#41.cityId = _2#42.id)\n01 :- Project [named_struct(id, id#32L, name, name#33, cityId, cityId#34L) AS _1#41]\n02 :  +- LocalRelation [id#32L, name#33, cityId#34L]\n03 +- Project [named_struct(id, id#38L, name, name#39) AS _2#42]\n04    +- LocalRelation [id#38L, name#39]\n\n// select operator\nval qs = spark.range(10).select($\"id\")\nscala&gt; println(qs.queryExecution.logical.numberedTreeString)\n00 'Project [unresolvedalias('id, None)]\n01 +- Range (0, 10, step=1, splits=Some(8))\n\n// select[U1](c1: TypedColumn[T, U1])\nscala&gt; :type q\norg.apache.spark.sql.Dataset[(Person, City)]\n\nval left = $\"_1\".as[Person]\nval ql = q.select(left)\nscala&gt; println(ql.queryExecution.logical.numberedTreeString)\n00 'SerializeFromObject [assertnotnull(assertnotnull(input[0, $line14.$read$$iw$$iw$Person, true])).id AS id#87L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(assertnotnull(input[0, $line14.$read$$iw$$iw$Person, true])).name, true, false) AS name#88, assertnotnull(assertnotnull(input[0, $line14.$read$$iw$$iw$Person, true])).cityId AS cityId#89L]\n01 +- 'MapElements &lt;function1&gt;, class scala.Tuple1, [StructField(_1,StructType(StructField(id,LongType,false), StructField(name,StringType,true), StructField(cityId,LongType,false)),true)], obj#86: $line14.$read$$iw$$iw$Person\n02    +- 'DeserializeToObject unresolveddeserializer(newInstance(class scala.Tuple1)), obj#85: scala.Tuple1\n03       +- Project [_1#44]\n04          +- Join Inner, (_1#44.cityId = _2#45.id)\n05             :- Project [named_struct(id, id#32L, name, name#33, cityId, cityId#34L) AS _1#44]\n06             :  +- LocalRelation [id#32L, name#33, cityId#34L]\n07             +- Project [named_struct(id, id#38L, name, name#39) AS _2#45]\n08                +- LocalRelation [id#38L, name#39]\n\n// SQL\nspark.range(10).createOrReplaceTempView(\"nums\")\nval qn = spark.sql(\"select * from nums\")\nscala&gt; println(qn.queryExecution.logical.numberedTreeString)\n00 'Project [*]\n01 +- 'UnresolvedRelation `nums`\n\n// Examples with Project that was added during analysis\n// Examples with Project that was added during optimization\n</code></pre> <p>NOTE: spark-sql-Expression-Nondeterministic.md[Nondeterministic] expressions are allowed in <code>Project</code> logical operator and enforced by CheckAnalysis.md#deterministic[CheckAnalysis].</p> <p>[[output]] The catalyst/QueryPlan.md#output[output schema] of a <code>Project</code> is...FIXME</p> <p>[[maxRows]] <code>maxRows</code>...FIXME</p> <p>[[resolved]] <code>resolved</code>...FIXME</p> <p>[[validConstraints]] <code>validConstraints</code>...FIXME</p>"},{"location":"logical-operators/Project/#tip","title":"[TIP]","text":"<p>Use <code>select</code> operator from Catalyst DSL to create a <code>Project</code> logical operator, e.g. for testing or Spark SQL internals exploration.</p>"},{"location":"logical-operators/Project/#source-scala","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.dsl.plans._  // \u2190 gives table and select import org.apache.spark.sql.catalyst.dsl.expressions.star val plan = table(\"a\").select(star()) scala&gt; println(plan.numberedTreeString) 00 'Project [*] 01 +- 'UnresolvedRelation <code>a</code></p> <p>====</p>"},{"location":"logical-operators/RebalancePartitions/","title":"RebalancePartitions Unary Logical Operator","text":"<p><code>RebalancePartitions</code> is a unary logical operator that represents a <code>REBALANCE</code> hint in a logical query plan.</p>"},{"location":"logical-operators/RebalancePartitions/#creating-instance","title":"Creating Instance","text":"<p><code>RebalancePartitions</code> takes the following to be created:</p> <ul> <li> Partition Expressions <li> Child logical operator <p><code>RebalancePartitions</code> is created\u00a0when:</p> <ul> <li><code>ResolveCoalesceHints</code> logical resolution rule is executed (to resolve a <code>REBALANCE</code> hint with Adaptive Query Execution enabled)</li> </ul>"},{"location":"logical-operators/RebalancePartitions/#partitioning","title":"Partitioning <pre><code>partitioning: Partitioning\n</code></pre> <p><code>partitioning</code> is one of the following:</p> <p>With no partition expressions, <code>partitioning</code> is <code>RoundRobinPartitioning</code> (with the numShufflePartitions). Otherwise, <code>partitioning</code> is a HashPartitioning (with the partition expressions and the numShufflePartitions).</p> <p><code>partitioning</code>\u00a0is used when:</p> <ul> <li><code>BasicOperators</code> execution planning strategy is executed (for a <code>RebalancePartitions</code> logical operator)</li> </ul>","text":""},{"location":"logical-operators/RebalancePartitions/#query-planning","title":"Query Planning <p><code>RebalancePartitions</code> logical operators are planned by BasicOperators execution planning strategy (to ShuffleExchangeExec physical operators).</p>","text":""},{"location":"logical-operators/Repartition/","title":"Repartition Logical Operator","text":"<p><code>Repartition</code> is a concrete RepartitionOperation.</p>"},{"location":"logical-operators/Repartition/#creating-instance","title":"Creating Instance","text":"<p><code>Repartition</code> takes the following to be created:</p> <ul> <li> Number of partitions <li> shuffle flag <li> Child LogicalPlan <p><code>Repartition</code> is created for the following:</p> <ul> <li>Dataset.coalesce and Dataset.repartition operators (with shuffle disabled and enabled, respectively)</li> <li><code>COALESCE</code> and <code>REPARTITION</code> hints (via ResolveCoalesceHints logical analysis rule, with shuffle disabled and enabled, respectively)</li> </ul>"},{"location":"logical-operators/Repartition/#query-planning","title":"Query Planning","text":"<p><code>Repartition</code> is planned to the following physical operators based on shuffle flag:</p> <ul> <li>ShuffleExchangeExec with <code>shuffle</code> enabled</li> <li>CoalesceExec with <code>shuffle</code> disabled</li> </ul>"},{"location":"logical-operators/Repartition/#catalyst-dsl","title":"Catalyst DSL","text":"<p>Catalyst DSL defines the following operators to create a <code>Repartition</code> logical operator:</p> <ul> <li>coalesce (with shuffle disabled)</li> <li>repartition (with shuffle enabled)</li> </ul>"},{"location":"logical-operators/Repartition/#partitioning","title":"Partitioning <pre><code>partitioning: Partitioning\n</code></pre> <p><code>partitioning</code> uses the numPartitions to determine the Partitioning:</p> <ul> <li>SinglePartition for <code>1</code></li> <li>RoundRobinPartitioning otherwise</li> </ul>  <p><code>partitioning</code> requires that the shuffle flag is enabled or throws an exception:</p> <pre><code>Partitioning can only be used in shuffle.\n</code></pre>  <p><code>partitioning</code> is part of the RepartitionOperation abstraction.</p>","text":""},{"location":"logical-operators/RepartitionByExpression/","title":"RepartitionByExpression Logical Operator","text":"<p><code>RepartitionByExpression</code> is a concrete RepartitionOperation.</p> <p><code>RepartitionByExpression</code> is also called distribute operator.</p>"},{"location":"logical-operators/RepartitionByExpression/#creating-instance","title":"Creating Instance","text":"<p><code>RepartitionByExpression</code> takes the following to be created:</p> <ul> <li> Partition Expressions <li> Child LogicalPlan <li> Number of partitions <p><code>RepartitionByExpression</code> is created when:</p> <ul> <li>Dataset.repartition and Dataset.repartitionByRange operators</li> <li><code>COALESCE</code>, <code>REPARTITION</code> and <code>REPARTITION_BY_RANGE</code> hints (via ResolveCoalesceHints logical analysis rule)</li> <li><code>DISTRIBUTE BY</code> and <code>CLUSTER BY</code> SQL clauses (via SparkSqlAstBuilder)</li> </ul>"},{"location":"logical-operators/RepartitionByExpression/#query-planning","title":"Query Planning","text":"<p><code>RepartitionByExpression</code> is planned to ShuffleExchangeExec physical operator.</p>"},{"location":"logical-operators/RepartitionByExpression/#catalyst-dsl","title":"Catalyst DSL <p>Catalyst DSL defines distribute operator to create <code>RepartitionByExpression</code> logical operators.</p>","text":""},{"location":"logical-operators/RepartitionByExpression/#partitioning","title":"Partitioning <p><code>RepartitionByExpression</code> determines a Partitioning when created.</p>","text":""},{"location":"logical-operators/RepartitionByExpression/#maximum-number-of-rows","title":"Maximum Number of Rows <pre><code>maxRows: Option[Long]\n</code></pre> <p><code>maxRows</code> simply requests the child logical operator for the maximum number of rows.</p> <p><code>maxRows</code> is part of the LogicalPlan abstraction.</p>","text":""},{"location":"logical-operators/RepartitionByExpression/#shuffle","title":"shuffle <pre><code>shuffle: Boolean\n</code></pre> <p><code>shuffle</code> is always <code>true</code>.</p> <p><code>shuffle</code> is part of the RepartitionOperation abstraction.</p>","text":""},{"location":"logical-operators/RepartitionOperation/","title":"RepartitionOperation Unary Logical Operators","text":"<p><code>RepartitionOperation</code> is an extension of the UnaryNode abstraction for repartition operations.</p>"},{"location":"logical-operators/RepartitionOperation/#contract","title":"Contract","text":""},{"location":"logical-operators/RepartitionOperation/#shuffle","title":"shuffle <pre><code>shuffle: Boolean\n</code></pre>","text":""},{"location":"logical-operators/RepartitionOperation/#number-of-partitions","title":"Number of Partitions <pre><code>numPartitions: Int\n</code></pre>","text":""},{"location":"logical-operators/RepartitionOperation/#partitioning","title":"Partitioning <pre><code>partitioning: Partitioning\n</code></pre> <p>Partitioning</p>","text":""},{"location":"logical-operators/RepartitionOperation/#implementations","title":"Implementations","text":"<ul> <li>Repartition</li> <li>RepartitionByExpression</li> </ul>"},{"location":"logical-operators/RepartitionOperation/#logical-optimizations","title":"Logical Optimizations","text":"<ul> <li> <p>CollapseRepartition logical optimization collapses adjacent repartition operations</p> </li> <li> <p>Repartition operations allow FoldablePropagation and PushDownPredicate logical optimizations to \"push through\"</p> </li> <li> <p>PropagateEmptyRelation logical optimization may result in an empty LocalRelation for repartition operations</p> </li> </ul>"},{"location":"logical-operators/RepartitionOperation/#output-attributes","title":"Output Attributes <pre><code>output: Seq[Attribute]\n</code></pre> <p><code>output</code> simply requests the child logical operator for the output attributes.</p> <p><code>output</code> is part of the QueryPlan abstraction.</p>","text":""},{"location":"logical-operators/RepartitionOperation/#demo-coalesce-operator","title":"Demo: Coalesce Operator <pre><code>val numsCoalesced = nums.coalesce(numPartitions = 4)\nassert(numsCoalesced.rdd.getNumPartitions == 4, \"Number of partitions should be 4\")\n\nscala&gt; numsCoalesced.explain(extended = true)\n== Parsed Logical Plan ==\nRepartition 4, false\n+- Range (0, 5, step=1, splits=Some(16))\n\n== Analyzed Logical Plan ==\nid: bigint\nRepartition 4, false\n+- Range (0, 5, step=1, splits=Some(16))\n\n== Optimized Logical Plan ==\nRepartition 4, false\n+- Range (0, 5, step=1, splits=Some(16))\n\n== Physical Plan ==\nCoalesce 4\n+- *(1) Range (0, 5, step=1, splits=16)\n</code></pre>","text":""},{"location":"logical-operators/ReplaceData/","title":"ReplaceData Logical Operator","text":"<p><code>ReplaceData</code> is a RowLevelWrite logical operator.</p> <p><code>ReplaceData</code> represents DataSourceV2Relation with SupportsRowLevelOperations in DeleteFromTable operators.</p>"},{"location":"logical-operators/ReplaceData/#creating-instance","title":"Creating Instance","text":"<p><code>ReplaceData</code> takes the following to be created:</p> <ul> <li> NamedRelation of the target table <li> Condition Expression <li> Query LogicalPlan <li> NamedRelation of the original table <li> Optional Write (default: undefined) <p><code>ReplaceData</code> is created when:</p> <ul> <li>RewriteDeleteFromTable analysis rule is executed (to buildReplaceDataPlan)</li> </ul>"},{"location":"logical-operators/ResolvedHint/","title":"ResolvedHint Unary Logical Operator","text":"<p><code>ResolvedHint</code> is a unary logical operator to represent resolved hint nodes in a logical query plan.</p>"},{"location":"logical-operators/ResolvedHint/#creating-instance","title":"Creating Instance","text":"<p><code>ResolvedHint</code> takes the following to be created:</p> <ul> <li> Child LogicalPlan <li> HintInfo <p><code>ResolvedHint</code> is created\u00a0when:</p> <ul> <li>ResolveJoinStrategyHints logical resolution rule is executed</li> <li>broadcast standard function is used (on a <code>Dataset</code>)</li> <li><code>CacheManager</code> is requested to useCachedData</li> </ul>"},{"location":"logical-operators/ResolvedHint/#query-execution-planning","title":"Query Execution Planning","text":"<p>BasicOperators execution planning strategy throws an <code>IllegalStateException</code> for <code>ResolvedHint</code>s when executed.</p>"},{"location":"logical-operators/ResolvedTable/","title":"ResolvedTable Leaf Logical Operator","text":"<p><code>ResolvedTable</code> is a leaf logical operator.</p>"},{"location":"logical-operators/ResolvedTable/#creating-instance","title":"Creating Instance","text":"<p><code>ResolvedTable</code> takes the following to be created:</p> <ul> <li> TableCatalog <li> <code>Identifier</code> <li> Table <p><code>ResolvedTable</code> is created\u00a0when:</p> <ul> <li>ResolveTables logical resolution rule is executed (for UnresolvedTable and UnresolvedTableOrView)</li> <li>ResolveRelations logical resolution rule is executed (lookupTableOrView)</li> <li>DataSourceV2Strategy execution planning strategy is executed (for <code>RenameTable</code>)</li> </ul>"},{"location":"logical-operators/RowLevelWrite/","title":"RowLevelWrite Logical Operators","text":"<p><code>RowLevelWrite</code> is...FIXME</p>"},{"location":"logical-operators/RunnableCommand/","title":"RunnableCommand Logical Operators","text":"<p><code>RunnableCommand</code>\u00a0is an extension of the Command abstraction for logical commands that can be executed for side effects.</p>"},{"location":"logical-operators/RunnableCommand/#contract","title":"Contract","text":""},{"location":"logical-operators/RunnableCommand/#executing-command","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p>Executes the command for side effects (possibly giving Row back with the result)</p> <p>Used when:</p> <ul> <li>ExecutedCommandExec leaf physical operator is executed (and caches the result)</li> </ul>","text":""},{"location":"logical-operators/RunnableCommand/#implementations","title":"Implementations","text":"<ul> <li><code>AlterViewAsCommand</code></li> <li>CreateViewCommand</li> <li>LeafRunnableCommand</li> </ul>"},{"location":"logical-operators/RunnableCommand/#query-planning","title":"Query Planning","text":"<p><code>RunnableCommand</code> logical operators are resolved to ExecutedCommandExec physical operators in BasicOperators execution planning strategy.</p>"},{"location":"logical-operators/RunnableCommand/#performance-metrics","title":"Performance Metrics <pre><code>metrics: Map[String, SQLMetric]\n</code></pre> <p>Performance metrics</p> <p><code>metrics</code> is empty by default.</p>  Lazy Value <p><code>metrics</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards.</p>  <p><code>metrics</code>\u00a0is used when:</p> <ul> <li>ExecutedCommandExec leaf physical operator is executed (and requested for performance metrics)</li> </ul>","text":""},{"location":"logical-operators/SaveIntoDataSourceCommand/","title":"SaveIntoDataSourceCommand Logical Command","text":"<p><code>SaveIntoDataSourceCommand</code> is a logical runnable command.</p> <p><code>SaveIntoDataSourceCommand</code> is &lt;&gt; exclusively when <code>DataSource</code> is requested to create a logical command for writing (to a CreatableRelationProvider data source). <p>[[innerChildren]] <code>SaveIntoDataSourceCommand</code> returns the &lt;&gt; when requested for the inner nodes (that should be shown as an inner nested tree of this node)."},{"location":"logical-operators/SaveIntoDataSourceCommand/#source-scala","title":"[source, scala]","text":"<p>// DEMO Example with inner nodes that should be shown as an inner nested tree of this node</p> <p>val lines = Seq(\"SaveIntoDataSourceCommand\").toDF(\"line\")</p> <p>// NOTE: There are two CreatableRelationProviders: jdbc and kafka // jdbc is simpler to use in spark-shell as it does not need --packages val url = \"jdbc:derby:memory:;databaseName=/tmp/test;create=true\" val requiredOpts = Map(\"url\" -&gt; url, \"dbtable\" -&gt; \"lines\") // Use overwrite SaveMode to make the demo reproducible import org.apache.spark.sql.SaveMode.Overwrite lines.write.options(requiredOpts).format(\"jdbc\").mode(Overwrite).save</p>"},{"location":"logical-operators/SaveIntoDataSourceCommand/#go-to-web-uis-sql-tab-and-see-the-last-executed-query","title":"// Go to web UI's SQL tab and see the last executed query","text":"<p>[[simpleString]] <code>SaveIntoDataSourceCommand</code> redacts the &lt;&gt; for the &lt;&gt;. <pre><code>SaveIntoDataSourceCommand [dataSource], [redacted], [mode]\n</code></pre> <p>=== [[run]] Executing Logical Command -- <code>run</code> Method</p>"},{"location":"logical-operators/SaveIntoDataSourceCommand/#source-scala_1","title":"[source, scala]","text":"<p>run(   sparkSession: SparkSession): Seq[Row]</p> <p>NOTE: <code>run</code> is part of &lt;&gt; to execute (run) a logical command. <p><code>run</code> simply requests the &lt;&gt; to save the rows of a structured query (a DataFrame). <p>In the end, <code>run</code> returns an empty <code>Seq[Row]</code> (just to follow the signature and please the Scala compiler).</p>"},{"location":"logical-operators/SaveIntoDataSourceCommand/#creating-instance","title":"Creating Instance","text":"<p><code>SaveIntoDataSourceCommand</code> takes the following when created:</p> <ul> <li>[[query]] Logical query plan</li> <li>[[dataSource]] CreatableRelationProvider data source</li> <li>[[options]] Options (as <code>Map[String, String]</code>)</li> <li>[[mode]] SaveMode</li> </ul>"},{"location":"logical-operators/SetCatalogAndNamespace/","title":"SetCatalogAndNamespace Logical Command","text":"<p><code>SetCatalogAndNamespace</code> is a <code>UnaryCommand</code> that represents USE SQL statement.</p>"},{"location":"logical-operators/SetCatalogAndNamespace/#creating-instance","title":"Creating Instance","text":"<p><code>SetCatalogAndNamespace</code> takes the following to be created:</p> <ul> <li> Child LogicalPlan <p><code>SetCatalogAndNamespace</code> is created when:</p> <ul> <li><code>AstBuilder</code> is requested to parse USE statement</li> </ul>"},{"location":"logical-operators/SetCatalogAndNamespace/#execution-planning","title":"Execution Planning","text":"<p><code>SetCatalogAndNamespace</code> is resolved to SetCatalogAndNamespaceExec physical command by DataSourceV2Strategy execution planning strategy.</p>"},{"location":"logical-operators/ShowCreateTable/","title":"ShowCreateTable Logical Command","text":"<p><code>ShowCreateTable</code> is a UnaryCommand that represents SHOW CREATE TABLE SQL statement in a logical query plan.</p> <p><code>ShowCreateTable</code> is planned as ShowCreateTableExec physical command at execution.</p>"},{"location":"logical-operators/ShowCreateTable/#creating-instance","title":"Creating Instance","text":"<p><code>ShowCreateTable</code> takes the following to be created:</p> <ul> <li> Child LogicalPlan <li> <code>asSerde</code> flag (default: <code>false</code>) <li> Output Attributes <p><code>ShowCreateTable</code> is created when:</p> <ul> <li><code>AstBuilder</code> is requested to parse SHOW CREATE TABLE SQL statement</li> </ul>"},{"location":"logical-operators/ShowCreateTable/#logical-analysis","title":"Logical Analysis","text":"<p>The child (that is initially a UnresolvedTableOrView) of <code>ShowCreateTable</code> is resolved using ResolveSessionCatalog logical analysis rule.</p> Child ShowCreateTableCommandBase Note <code>ResolvedV1TableOrViewIdentifier</code> <code>ShowCreateTableAsSerdeCommand</code> Only when asSerde is used (<code>true</code>) <code>ResolvedViewIdentifier</code> ShowCreateTableCommand <code>ResolvedV1TableIdentifier</code> ShowCreateTableCommand Only with spark.sql.legacy.useV1Command enabled ResolvedTable ShowCreateTableCommand Only for <code>spark_catalog</code> session catalog and <code>hive</code> tables"},{"location":"logical-operators/ShowCreateTable/#execution-planning","title":"Execution Planning","text":"<p><code>ShowCreateTable</code> (over a ResolvedTable) is planned as ShowCreateTableExec physical command using DataSourceV2Strategy execution planning strategy.</p>"},{"location":"logical-operators/ShowCreateTableCommand/","title":"ShowCreateTableCommand Logical Command","text":"<p><code>ShowCreateTableCommand</code> is a logical command that &lt;&gt; a <code>SHOW CREATE TABLE</code> SQL statement (with a data source / non-Hive or a Hive table). <p><code>ShowCreateTableCommand</code> is &lt;&gt; when <code>SparkSqlAstBuilder</code> is requested to parse &lt;&gt; SQL statement. <p>[[output]] <code>ShowCreateTableCommand</code> uses a single <code>createtab_stmt</code> column (of type StringType) for the output schema.</p> <pre><code>import org.apache.spark.sql.SaveMode\nspark.range(10e4.toLong)\n  .write\n  .bucketBy(4, \"id\")\n  .sortBy(\"id\")\n  .mode(SaveMode.Overwrite)\n  .saveAsTable(\"bucketed_4_10e4\")\nscala&gt; sql(\"SHOW CREATE TABLE bucketed_4_10e4\").show(truncate = false)\n+----------------------------------------------------------------------------------------------------------------------------------------------------+\n|createtab_stmt                                                                                                                                      |\n+----------------------------------------------------------------------------------------------------------------------------------------------------+\n|CREATE TABLE `bucketed_4_10e4` (`id` BIGINT)\nUSING parquet\nOPTIONS (\n  `serialization.format` '1'\n)\nCLUSTERED BY (id)\nSORTED BY (id)\nINTO 4 BUCKETS\n|\n+----------------------------------------------------------------------------------------------------------------------------------------------------+\n\nscala&gt; sql(\"SHOW CREATE TABLE bucketed_4_10e4\").as[String].collect.foreach(println)\nCREATE TABLE `bucketed_4_10e4` (`id` BIGINT)\nUSING parquet\nOPTIONS (\n  `serialization.format` '1'\n)\nCLUSTERED BY (id)\nSORTED BY (id)\nINTO 4 BUCKETS\n</code></pre> <p>[[table]] [[creating-instance]] <code>ShowCreateTableCommand</code> takes a single <code>TableIdentifier</code> when created.</p> <p>=== [[run]] Executing Logical Command -- <code>run</code> Method</p>"},{"location":"logical-operators/ShowCreateTableCommand/#source-scala","title":"[source, scala]","text":""},{"location":"logical-operators/ShowCreateTableCommand/#runsparksession-sparksession-seqrow","title":"run(sparkSession: SparkSession): Seq[Row]","text":"<p>NOTE: <code>run</code> is part of &lt;&gt; to execute (run) a logical command. <p><code>run</code> requests the <code>SparkSession</code> for the &lt;&gt; that is used to access the &lt;&gt;. <p><code>run</code> then requests the <code>SessionCatalog</code> to retrieve the table metadata from the external catalog (metastore).</p> <p><code>run</code> then &lt;&gt; for a data source / non-Hive table or &lt;&gt; for a Hive table (per the table metadata). <p>In the end, <code>run</code> returns the <code>CREATE TABLE</code> statement in a single <code>Row</code>.</p> <p>=== [[showHiveTableNonDataColumns]] <code>showHiveTableNonDataColumns</code> Internal Method</p>"},{"location":"logical-operators/ShowCreateTableCommand/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-operators/ShowCreateTableCommand/#showhivetablenondatacolumnsmetadata-catalogtable-builder-stringbuilder-unit","title":"showHiveTableNonDataColumns(metadata: CatalogTable, builder: StringBuilder): Unit","text":"<p><code>showHiveTableNonDataColumns</code>...FIXME</p> <p>NOTE: <code>showHiveTableNonDataColumns</code> is used exclusively when <code>ShowCreateTableCommand</code> logical command is requested to &lt;&gt;. <p>=== [[showCreateHiveTable]] <code>showCreateHiveTable</code> Internal Method</p>"},{"location":"logical-operators/ShowCreateTableCommand/#source-scala_2","title":"[source, scala]","text":""},{"location":"logical-operators/ShowCreateTableCommand/#showcreatehivetablemetadata-catalogtable-string","title":"showCreateHiveTable(metadata: CatalogTable): String","text":"<p><code>showCreateHiveTable</code>...FIXME</p> <p>NOTE: <code>showCreateHiveTable</code> is used exclusively when <code>ShowCreateTableCommand</code> logical command is executed (with a Hive &lt;&gt;). <p>=== [[showHiveTableHeader]] <code>showHiveTableHeader</code> Internal Method</p>"},{"location":"logical-operators/ShowCreateTableCommand/#source-scala_3","title":"[source, scala]","text":""},{"location":"logical-operators/ShowCreateTableCommand/#showhivetableheadermetadata-catalogtable-builder-stringbuilder-unit","title":"showHiveTableHeader(metadata: CatalogTable, builder: StringBuilder): Unit","text":"<p><code>showHiveTableHeader</code>...FIXME</p> <p>NOTE: <code>showHiveTableHeader</code> is used exclusively when <code>ShowCreateTableCommand</code> logical command is requested to &lt;&gt;."},{"location":"logical-operators/ShowTableProperties/","title":"ShowTableProperties Logical Command","text":"<p><code>ShowTableProperties</code> is a logical command that represents <code>SHOW TBLPROPERTIES</code> SQL statement.</p> <pre><code>sql(\"SHOW TBLPROPERTIES d1\").show(truncate = false)\n</code></pre> <pre><code>+----------------------+-------+\n|key                   |value  |\n+----------------------+-------+\n|Type                  |MANAGED|\n|delta.minReaderVersion|1      |\n|delta.minWriterVersion|2      |\n+----------------------+-------+\n</code></pre>"},{"location":"logical-operators/ShowTableProperties/#creating-instance","title":"Creating Instance","text":"<p><code>ShowTableProperties</code> takes the following to be created:</p> <ul> <li> Table LogicalPlan <li> (optional) Property Key <p><code>ShowTableProperties</code> is created when:</p> <ul> <li> <p><code>AstBuilder</code> is requested to parse SHOW TBLPROPERTIES SQL statement</p> </li> <li> <p>ResolveCatalogs logical analyzer rule is executed</p> </li> </ul>"},{"location":"logical-operators/ShowTableProperties/#output-attributes","title":"Output Attributes <pre><code>output: Seq[Attribute]\n</code></pre> <p><code>output</code> is part of the Command abstraction.</p> <p><code>output</code> is two <code>AttributeReference</code>s:</p> <ul> <li> <code>key</code> <li> <code>value</code>  <p>Both are of <code>StringType</code> and non-nullable.</p>","text":""},{"location":"logical-operators/ShowTableProperties/#execution-planning","title":"Execution Planning <p><code>ShowTableProperties</code> is resolved to ShowTablePropertiesCommand logical command using ResolveSessionCatalog logical resolution rule.</p> <p><code>ShowTableProperties</code> is planned to ShowTablePropertiesExec physical command using DataSourceV2Strategy execution planning strategy.</p>","text":""},{"location":"logical-operators/ShowTablePropertiesCommand/","title":"ShowTablePropertiesCommand Logical Command","text":"<p><code>ShowTablePropertiesCommand</code> is a RunnableCommand that represents ShowTableProperties logical operator with the following logical operators at execution:</p> <ul> <li>ResolvedTable for V1Table in SessionCatalog</li> <li><code>ResolvedView</code></li> </ul> <p>Note</p> <p>ShowTableProperties logical operator can also be planned to ShowTablePropertiesExec physical command for the other cases.</p>"},{"location":"logical-operators/ShowTablePropertiesCommand/#creating-instance","title":"Creating Instance","text":"<p><code>ShowTablePropertiesCommand</code> takes the following to be created:</p> <ul> <li> <code>TableIdentifier</code> <li> (optional) Property Key <p><code>ShowTablePropertiesCommand</code> is created\u00a0when:</p> <ul> <li>ResolveSessionCatalog logical resolution rule is executed (and resolves a ShowTableProperties logical operator with a v1 catalog table or a view)</li> </ul>"},{"location":"logical-operators/ShowTables/","title":"ShowTables Logical Command","text":"<p><code>ShowTables</code> is a logical command that represents SHOW TABLES SQL statement.</p> <pre><code>SHOW TABLES ((FROM | IN) multipartIdentifier)?\n  (LIKE? pattern=STRING)?\n</code></pre> <p>Note</p> <p><code>ShowTables</code> is resolved to ShowTablesExec physical command.</p>"},{"location":"logical-operators/ShowTables/#creating-instance","title":"Creating Instance","text":"<p><code>ShowTables</code> takes the following to be created:</p> <ul> <li> Logical Operator <li> Optional Pattern (of tables to show) <p><code>ShowTables</code> is created when <code>AstBuilder</code> is requested to visitShowTables.</p>"},{"location":"logical-operators/ShowTables/#output-attributes","title":"Output Attributes <pre><code>output: Seq[Attribute]\n</code></pre> <p><code>output</code> is the following attributes:</p> <ul> <li>namespace</li> <li>tableName</li> </ul> <p><code>output</code> is part of the Command abstraction.</p>","text":""},{"location":"logical-operators/Sort/","title":"Sort Unary Logical Operator","text":"<p><code>Sort</code> is a unary logical operator that represents the following operators in a logical plan:</p> <ul> <li> <p><code>ORDER BY</code>, <code>SORT BY</code>, <code>SORT BY ... DISTRIBUTE BY</code> and <code>CLUSTER BY</code> clauses (when <code>AstBuilder</code> is requested to parse a query)</p> </li> <li> <p>Dataset.sortWithinPartitions, Dataset.sort and Dataset.randomSplit operators</p> </li> </ul>"},{"location":"logical-operators/Sort/#creating-instance","title":"Creating Instance","text":"<p><code>Sort</code> takes the following to be created:</p> <ul> <li> SortOrder expressions <li> <code>global</code> flag (for global (<code>true</code>) or partition-only (<code>false</code>) sorting) <li> Child logical operator"},{"location":"logical-operators/Sort/#execution-planning","title":"Execution Planning","text":"<p><code>Sort</code> logical operator is resolved to SortExec unary physical operator by BasicOperators execution planning strategy.</p>"},{"location":"logical-operators/Sort/#catalyst-dsl","title":"Catalyst DSL","text":"<p>Catalyst DSL defines orderBy and sortBy operators to create <code>Sort</code> operators (with the global flag enabled or not, respectively).</p> <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\nimport org.apache.spark.sql.catalyst.dsl.plans._\nval t1 = table(\"t1\")\n</code></pre> <pre><code>val globalSortById = t1.orderBy('id.asc_nullsLast)\n</code></pre> <pre><code>// Note true for the global flag\nscala&gt; println(globalSortById.numberedTreeString)\n00 'Sort ['id ASC NULLS LAST], true\n01 +- 'UnresolvedRelation `t1`\n</code></pre> <pre><code>val partitionOnlySortById = t1.sortBy('id.asc_nullsLast)\n</code></pre> <pre><code>// Note false for the global flag\nscala&gt; println(partitionOnlySortById.numberedTreeString)\n00 'Sort ['id ASC NULLS LAST], false\n01 +- 'UnresolvedRelation `t1`\n</code></pre>"},{"location":"logical-operators/Sort/#demo","title":"Demo","text":"<pre><code>// Using the feature of ordinal literal\nval ids = Seq(1,3,2).toDF(\"id\").sort(lit(1))\nval logicalPlan = ids.queryExecution.logical\nscala&gt; println(logicalPlan.numberedTreeString)\n00 Sort [1 ASC NULLS FIRST], true\n01 +- AnalysisBarrier\n02       +- Project [value#22 AS id#24]\n03          +- LocalRelation [value#22]\n\nimport org.apache.spark.sql.catalyst.plans.logical.Sort\nval sortOp = logicalPlan.collect { case s: Sort =&gt; s }.head\nscala&gt; println(sortOp.numberedTreeString)\n00 Sort [1 ASC NULLS FIRST], true\n01 +- AnalysisBarrier\n02       +- Project [value#22 AS id#24]\n03          +- LocalRelation [value#22]\n</code></pre> <pre><code>val nums = Seq((0, \"zero\"), (1, \"one\")).toDF(\"id\", \"name\")\n// Creates a Sort logical operator:\n// - descending sort direction for id column (specified explicitly)\n// - name column is wrapped with ascending sort direction\nval numsOrdered = nums.sort('id.desc, 'name)\nval logicalPlan = numsOrdered.queryExecution.logical\nscala&gt; println(logicalPlan.numberedTreeString)\n00 'Sort ['id DESC NULLS LAST, 'name ASC NULLS FIRST], true\n01 +- Project [_1#11 AS id#14, _2#12 AS name#15]\n02    +- LocalRelation [_1#11, _2#12]\n</code></pre>"},{"location":"logical-operators/SubqueryAlias/","title":"SubqueryAlias Unary Logical Operator","text":"<p><code>SubqueryAlias</code> is a &lt;&gt; that represents an aliased subquery (i.e. the &lt;&gt; logical query plan with the &lt;&gt; in the &lt;&gt;). <p><code>SubqueryAlias</code> is &lt;&gt; when: <ul> <li> <p><code>AstBuilder</code> is requested to parse a &lt;&gt; or &lt;&gt; query, &lt;&gt; and &lt;&gt; in a SQL statement <li> <p>&lt;&gt; operator is used <li> <p><code>SessionCatalog</code> is requested to find a table or view in catalogs</p> </li> <li> <p><code>RewriteCorrelatedScalarSubquery</code> logical optimization is requested to &lt;&gt; (when &lt;&gt; to &lt;&gt;, Project or <code>Filter</code> logical operators with correlated scalar subqueries) <p>[[doCanonicalize]] <code>SubqueryAlias</code> simply requests the &lt;&gt; for the &lt;&gt;. <p>[[output]] When requested for &lt;&gt;, <code>SubqueryAlias</code> requests the &lt;&gt; logical operator for them and adds the &lt;&gt; as a &lt;&gt;. <p>NOTE: &lt;&gt; logical optimization eliminates (removes) <code>SubqueryAlias</code> operators from a logical query plan. <p>NOTE: &lt;&gt; logical optimization rewrites correlated scalar subqueries with <code>SubqueryAlias</code> operators. <p>=== [[catalyst-dsl]][[subquery]][[as]] Catalyst DSL -- <code>subquery</code> And <code>as</code> Operators</p>"},{"location":"logical-operators/SubqueryAlias/#source-scala","title":"[source, scala]","text":"<p>as(alias: String): LogicalPlan subquery(alias: Symbol): LogicalPlan</p> <p>subquery and as operators in Catalyst DSL create a &lt;&gt; logical operator, e.g. for testing or Spark SQL internals exploration."},{"location":"logical-operators/SubqueryAlias/#source-scala_1","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.dsl.plans._ val t1 = table(\"t1\")</p> <p>val plan = t1.subquery('a) scala&gt; println(plan.numberedTreeString) 00 'SubqueryAlias a 01 +- 'UnresolvedRelation <code>t1</code></p> <p>val plan = t1.as(\"a\") scala&gt; println(plan.numberedTreeString) 00 'SubqueryAlias a 01 +- 'UnresolvedRelation <code>t1</code></p> <p>=== [[creating-instance]] Creating SubqueryAlias Instance</p> <p><code>SubqueryAlias</code> takes the following when created:</p> <ul> <li>[[alias]] Alias</li> <li>[[child]] Child &lt;&gt;"},{"location":"logical-operators/SupportsSubquery/","title":"SupportsSubquery Logical Operators","text":"<p><code>SupportsSubquery</code> is a marker interface (and an extension of the LogicalPlan abstraction) for logical operators that support subqueries.</p> <p><code>SupportsSubquery</code> is resolved by ResolveSubquery logical resolution rule.</p>"},{"location":"logical-operators/SupportsSubquery/#implementations","title":"Implementations","text":"<ul> <li>DeleteFromTable</li> <li>MergeIntoTable</li> <li>UpdateTable</li> </ul>"},{"location":"logical-operators/TruncateTableCommand/","title":"TruncateTableCommand Logical Command","text":"<p><code>TruncateTableCommand</code> is a RunnableCommand.md[logical command] that represents spark-sql-SparkSqlAstBuilder.md#visitTruncateTable[TRUNCATE TABLE] SQL statement.</p> <p>=== [[creating-instance]] Creating TruncateTableCommand Instance</p> <p><code>TruncateTableCommand</code> takes the following to be created:</p> <ul> <li>[[tableName]] <code>TableIdentifier</code></li> <li>[[partitionSpec]] Optional <code>TablePartitionSpec</code></li> </ul> <p>=== [[run]] Executing Logical Command -- <code>run</code> Method</p>"},{"location":"logical-operators/TruncateTableCommand/#source-scala","title":"[source, scala]","text":""},{"location":"logical-operators/TruncateTableCommand/#runspark-sparksession-seqrow","title":"run(spark: SparkSession): Seq[Row]","text":"<p>NOTE: <code>run</code> is part of RunnableCommand.md#run[RunnableCommand Contract] to execute (run) a logical command.</p> <p><code>run</code>...FIXME</p> <p><code>run</code> throws an <code>AnalysisException</code> when executed on external tables:</p> <pre><code>Operation not allowed: TRUNCATE TABLE on external tables: [tableIdentWithDB]\n</code></pre> <p><code>run</code> throws an <code>AnalysisException</code> when executed on views:</p> <pre><code>Operation not allowed: TRUNCATE TABLE on views: [tableIdentWithDB]\n</code></pre> <p><code>run</code> throws an <code>AnalysisException</code> when executed with &lt;&gt; on non-partitioned tables: <pre><code>Operation not allowed: TRUNCATE TABLE ... PARTITION is not supported for tables that are not partitioned: [tableIdentWithDB]\n</code></pre> <p><code>run</code> throws an <code>AnalysisException</code> when executed with &lt;&gt; with filesource partition disabled or partition metadata not in a Hive metastore."},{"location":"logical-operators/UnresolvedHaving/","title":"UnresolvedHaving Unary Logical Operator","text":"<p><code>UnresolvedHaving</code> is a unary logical operator.</p>"},{"location":"logical-operators/UnresolvedHaving/#creating-instance","title":"Creating Instance","text":"<p><code>UnresolvedHaving</code> takes the following to be created:</p> <ul> <li> Having Expression <li> Child LogicalPlan <p><code>UnresolvedHaving</code> is created\u00a0when:</p> <ul> <li><code>AstBuilder</code> is requested to parse HAVING clause in a SQL query</li> </ul>"},{"location":"logical-operators/UnresolvedHaving/#never-resolved","title":"Never Resolved <pre><code>resolved: Boolean\n</code></pre> <p><code>resolved</code>\u00a0is part of the LogicalPlan abstraction.</p> <p><code>resolved</code> is <code>false</code>.</p>","text":""},{"location":"logical-operators/UnresolvedHaving/#logical-analysis","title":"Logical Analysis <p><code>UnresolvedHaving</code> cannot be resolved and is supposed to be handled at analysis using the following rules:</p> <ul> <li>ExtractWindowExpressions</li> <li>ResolveAggregateFunctions</li> <li>ResolveGroupingAnalytics</li> </ul>","text":""},{"location":"logical-operators/UnresolvedHaving/#catalyst-dsl","title":"Catalyst DSL <p><code>UnresolvedHaving</code> can be created using the having operator in Catalyst DSL.</p>","text":""},{"location":"logical-operators/UnresolvedHint/","title":"UnresolvedHint Unary Logical Operator","text":"<p><code>UnresolvedHint</code> is an unary logical operator that represents a hint (by name) for the child logical plan.</p>"},{"location":"logical-operators/UnresolvedHint/#creating-instance","title":"Creating Instance","text":"<p><code>UnresolvedHint</code> takes the following to be created:</p> <ul> <li> Hint Name <li> Hint Parameters (if any) <li> Child LogicalPlan <p><code>UnresolvedHint</code> is created\u00a0when:</p> <ul> <li>Dataset.hint operator is used</li> <li><code>AstBuilder</code> is requested to parse hints in a SQL query</li> </ul>"},{"location":"logical-operators/UnresolvedHint/#never-resolved","title":"Never Resolved <pre><code>resolved: Boolean\n</code></pre> <p><code>resolved</code>\u00a0is part of the LogicalPlan abstraction.</p> <p><code>resolved</code> is <code>false</code>.</p>","text":""},{"location":"logical-operators/UnresolvedHint/#logical-analysis","title":"Logical Analysis <p><code>UnresolvedHint</code>s cannot be resolved and are supposed to be converted to ResolvedHint unary logical operators at analysis or removed from a logical plan.</p> <p>The following logical rules are used to act on <code>UnresolvedHint</code> logical operators (the order of executing the rules matters):</p> <ul> <li>ResolveJoinStrategyHints</li> <li>ResolveCoalesceHints</li> <li>RemoveAllHints</li> </ul> <p>Analyzer throws an <code>IllegalStateException</code> for any <code>UnresolvedHint</code>s left (unresolved):</p> <pre><code>Internal error: logical hint operator should have been removed during analysis\n</code></pre>","text":""},{"location":"logical-operators/UnresolvedHint/#catalyst-dsl","title":"Catalyst DSL <p><code>UnresolvedHint</code> can be created using the hint operator in Catalyst DSL.</p> <pre><code>import org.apache.spark.sql.catalyst.plans.logical.LocalRelation\nval r1 = LocalRelation('a.int, 'b.timestamp, 'c.boolean)\n</code></pre> <pre><code>scala&gt; println(r1.numberedTreeString)\n00 LocalRelation &lt;empty&gt;, [a#0, b#1, c#2]\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.dsl.plans._\nval plan = r1.hint(name = \"myHint\", 100, true)\n</code></pre> <pre><code>scala&gt; println(plan.numberedTreeString)\n00 'UnresolvedHint myHint, [100, true]\n01 +- LocalRelation &lt;empty&gt;, [a#0, b#1, c#2]\n</code></pre>","text":""},{"location":"logical-operators/UnresolvedHint/#demo","title":"Demo <pre><code>// Dataset API\nval q = spark.range(1).hint(\"myHint\", 100, true)\nval plan = q.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'UnresolvedHint myHint, [100, true]\n01 +- Range (0, 1, step=1, splits=Some(8))\n\n// SQL\nval q = sql(\"SELECT /*+ myHint (100, true) */ 1\")\nval plan = q.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'UnresolvedHint myHint, [100, true]\n01 +- 'Project [unresolvedalias(1, None)]\n02    +- OneRowRelation\n</code></pre> <pre><code>// Let's hint the query twice\n// The order of hints matters as every hint operator executes Spark analyzer\n// That will resolve all but the last hint\nval q = spark.range(100).\n  hint(\"broadcast\").\n  hint(\"myHint\", 100, true)\nval plan = q.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'UnresolvedHint myHint, [100, true]\n01 +- ResolvedHint (broadcast)\n02    +- Range (0, 100, step=1, splits=Some(8))\n\n// Let's resolve unresolved hints\nimport org.apache.spark.sql.catalyst.rules.RuleExecutor\nimport org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\nimport org.apache.spark.sql.catalyst.analysis.ResolveHints\nimport org.apache.spark.sql.internal.SQLConf\nobject HintResolver extends RuleExecutor[LogicalPlan] {\n  lazy val batches =\n    Batch(\"Hints\", FixedPoint(maxIterations = 100),\n      new ResolveHints.ResolveJoinStrategyHints(SQLConf.get),\n      ResolveHints.RemoveAllHints) :: Nil\n}\nval resolvedPlan = HintResolver.execute(plan)\nscala&gt; println(resolvedPlan.numberedTreeString)\n00 ResolvedHint (broadcast)\n01 +- Range (0, 100, step=1, splits=Some(8))\n</code></pre>","text":""},{"location":"logical-operators/UnresolvedRelation/","title":"UnresolvedRelation Logical Operator","text":"<p><code>UnresolvedRelation</code> is a leaf logical operator with a name (as a NamedRelation) that represents a named relation that has yet to be looked up in a catalog at analysis.</p> <p><code>UnresolvedRelation</code> is never resolved and has to be replaced using ResolveRelations logical resolution rule.</p>"},{"location":"logical-operators/UnresolvedRelation/#creating-instance","title":"Creating Instance","text":"<p><code>UnresolvedRelation</code> takes the following to be created:</p> <ul> <li> Multi-part identifier <li> Options <li> <code>isStreaming</code> flag (default: <code>false</code>) <p><code>UnresolvedRelation</code> is created (possibly indirectly using apply factory) when:</p> <ul> <li><code>AstBuilder</code> is requested to parse TABLE statement and table name, and create an UnresolvedRelation</li> <li>DataFrameReader.table operator is used</li> <li>DataFrameWriterV2 operators are used:<ul> <li>DataFrameWriterV2.append</li> <li>DataFrameWriterV2.overwrite</li> <li>DataFrameWriterV2.overwritePartitions</li> </ul> </li> <li><code>DataStreamReader.table</code> (Spark Structured Streaming) operator is used</li> <li><code>SparkConnectPlanner</code> is requested to transformReadRel</li> <li>table (Catalyst DSL) operator is used</li> </ul>"},{"location":"logical-operators/UnresolvedRelation/#apply","title":"Creating UnresolvedRelation","text":"<pre><code>apply(\ntableIdentifier: TableIdentifier): UnresolvedRelation\napply(\ntableIdentifier: TableIdentifier,\nextraOptions: CaseInsensitiveStringMap,\nisStreaming: Boolean): UnresolvedRelation\n</code></pre> <p><code>apply</code> creates an UnresolvedRelation.</p> <p><code>apply</code> is used when:</p> <ul> <li>DataFrameWriter.insertInto operator is used</li> <li>SparkSession.table operator is used</li> </ul>"},{"location":"logical-operators/UnresolvedRelation/#name","title":"Name","text":"Signature <pre><code>name: String\n</code></pre> <p><code>name</code> is part of the NamedRelation abstraction.</p> <p><code>name</code> is quoted multi-part dot-separated table name.</p>"},{"location":"logical-operators/UnresolvedRelation/#tableName","title":"Quoted Multi-Part Dot-Separated Table Name","text":"<pre><code>tableName: String\n</code></pre> <p><code>tableName</code> is a quoted multi-part <code>.</code>-separated multipartIdentifier.</p>"},{"location":"logical-operators/UnresolvedRelation/#resolved","title":"resolved","text":"Signature <pre><code>resolved: Boolean\n</code></pre> <p><code>resolved</code> is part of the LogicalPlan abstraction.</p> <p><code>resolved</code> is disabled (<code>false</code>).</p>"},{"location":"logical-operators/UnresolvedRelation/#nodePatterns","title":"nodePatterns","text":"Signature <pre><code>nodePatterns: Seq[TreePattern]\n</code></pre> <p><code>nodePatterns</code> is part of the TreeNode abstraction.</p> <p><code>nodePatterns</code> is a single-element collection with UNRESOLVED_RELATION.</p>"},{"location":"logical-operators/UnresolvedTable/","title":"UnresolvedTable Leaf Logical Operator","text":"<p><code>UnresolvedTable</code> is a leaf logical operator.</p>"},{"location":"logical-operators/UnresolvedTable/#creating-instance","title":"Creating Instance","text":"<p><code>UnresolvedTable</code> takes the following to be created:</p> <ul> <li> Multi-Part Identifier <li> Command Name <p><code>UnresolvedTable</code> is created\u00a0when:</p> <ul> <li><code>AstBuilder</code> is requested to visitLoadData, visitTruncateTable, visitShowPartitions, visitAddTablePartition, visitDropTablePartitions, visitCommentTable</li> </ul>"},{"location":"logical-operators/UnresolvedTable/#resolved","title":"resolved <pre><code>resolved: Boolean\n</code></pre> <p><code>resolved</code>\u00a0is part of the LogicalPlan abstraction.</p> <p><code>resolved</code> is <code>false</code>.</p>","text":""},{"location":"logical-operators/UnresolvedTable/#logical-analysis","title":"Logical Analysis <p><code>UnresolvedTable</code> is resolved to a ResolvedTable by ResolveTables logical resolution rule.</p>","text":""},{"location":"logical-operators/UnresolvedTableOrView/","title":"UnresolvedTableOrView Leaf Logical Operator","text":"<p><code>UnresolvedTableOrView</code> is a leaf logical operator.</p>"},{"location":"logical-operators/UnresolvedTableOrView/#creating-instance","title":"Creating Instance","text":"<p><code>UnresolvedTableOrView</code> takes the following to be created:</p> <ul> <li> Multi-Part Identifier <li> Command Name <li> <code>allowTempView</code> flag (default: <code>true</code>) <p><code>UnresolvedTableOrView</code> is created\u00a0when:</p> <ul> <li><code>AstBuilder</code> is requested to visitDropTable, visitDescribeRelation, visitAnalyze, visitShowCreateTable, visitRefreshTable, visitShowColumns, visitShowTblProperties</li> </ul>"},{"location":"logical-operators/UnresolvedTableOrView/#resolved","title":"resolved <pre><code>resolved: Boolean\n</code></pre> <p><code>resolved</code>\u00a0is part of the LogicalPlan abstraction.</p> <p><code>resolved</code> is <code>false</code>.</p>","text":""},{"location":"logical-operators/UnresolvedTableOrView/#logical-analysis","title":"Logical Analysis <p><code>UnresolvedTableOrView</code> is resolved to the following logical operators:</p> <ul> <li><code>ResolvedView</code> (by ResolveTempViews and ResolveRelations logical resolution rules)</li> <li>ResolvedTable (by ResolveTables and ResolveRelations logical resolution rules)</li> </ul>","text":""},{"location":"logical-operators/UnresolvedTableValuedFunction/","title":"UnresolvedTableValuedFunction Logical Operator","text":"<p><code>UnresolvedTableValuedFunction</code> is a LeafNode that represents a table-valued function (e.g. <code>range</code>, <code>explode</code>).</p>"},{"location":"logical-operators/UnresolvedTableValuedFunction/#creating-instance","title":"Creating Instance","text":"<p><code>UnresolvedTableValuedFunction</code> takes the following to be created:</p> <ul> <li> Name <li> Function Arguments (Expressions) <p><code>UnresolvedTableValuedFunction</code> is created when:</p> <ul> <li><code>UnresolvedTableValuedFunction</code> is requested to apply</li> <li><code>AstBuilder</code> is requested to visitTableValuedFunction</li> </ul>"},{"location":"logical-operators/UnresolvedTableValuedFunction/#creating-unresolvedtablevaluedfunction","title":"Creating UnresolvedTableValuedFunction <pre><code>apply(\n  name: FunctionIdentifier,\n  functionArgs: Seq[Expression]): UnresolvedTableValuedFunction\napply(\n  name: String,\n  functionArgs: Seq[Expression]): UnresolvedTableValuedFunction\n</code></pre> <p><code>apply</code> creates a UnresolvedTableValuedFunction.</p>  Unused <p><code>apply</code> does not seem to be used beside the tests.</p>","text":""},{"location":"logical-operators/UnresolvedTableValuedFunction/#logical-analysis","title":"Logical Analysis <p><code>UnresolvedTableValuedFunction</code> is resolved in ResolveFunctions logical analysis rule.</p>","text":""},{"location":"logical-operators/UnresolvedWith/","title":"UnresolvedWith Unary Logical Operator","text":"<p><code>UnresolvedWith</code> is a unary logical operator that represents a WITH clause in logical query plans.</p>"},{"location":"logical-operators/UnresolvedWith/#creating-instance","title":"Creating Instance","text":"<p><code>UnresolvedWith</code> takes the following to be created:</p> <ul> <li> Child logical operator <li> Named CTE SubqueryAliases (CTE Relations) <p><code>UnresolvedWith</code> is created\u00a0when:</p> <ul> <li><code>AstBuilder</code> is requested to parse WITH clauses</li> </ul>"},{"location":"logical-operators/UnresolvedWith/#simple-node-description","title":"Simple Node Description <pre><code>simpleString(\n  maxFields: Int): String\n</code></pre> <p><code>simpleString</code> uses the names of the CTE Relations for the description:</p> <pre><code>CTE [cteAliases]\n</code></pre> <p><code>simpleString</code>\u00a0is part of the TreeNode abstraction.</p>","text":""},{"location":"logical-operators/UnresolvedWith/#logical-analysis","title":"Logical Analysis <p><code>UnresolvedWith</code>s are resolved to WithCTE logical operators by CTESubstitution logical analysis rule.</p>","text":""},{"location":"logical-operators/UpdateTable/","title":"UpdateTable Logical Operator","text":"<p><code>UpdateTable</code> is a Command that represents UPDATE SQL statement.</p> <p><code>UpdateTable</code> is a SupportsSubquery.</p>"},{"location":"logical-operators/UpdateTable/#creating-instance","title":"Creating Instance","text":"<p><code>UpdateTable</code> takes the following to be created:</p> <ul> <li> Table (LogicalPlan) <li> <code>Assignment</code>s <li> Condition Expression (optional) <p><code>UpdateTable</code> is created\u00a0when:</p> <ul> <li><code>AstBuilder</code> is requested to parse UPDATE SQL statement</li> </ul>"},{"location":"logical-operators/UpdateTable/#execution-planning","title":"Execution Planning","text":"<p><code>UpdateTable</code> command is not supported in Spark SQL and BasicOperators execution planning strategy throws an <code>UnsupportedOperationException</code> when finds any:</p> <pre><code>UPDATE TABLE is not supported temporarily.\n</code></pre> <p>Note</p> <p><code>UpdateTable</code> is to allow custom data sources to support <code>UPDATE</code> SQL statement (and so does Delta Lake).</p>"},{"location":"logical-operators/V2CreateTablePlan/","title":"V2CreateTablePlan Logical Operators","text":"<p><code>V2CreateTablePlan</code> is an extension of the LogicalPlan abstraction for logical operators that create or replace V2 table definitions.</p>"},{"location":"logical-operators/V2CreateTablePlan/#contract","title":"Contract","text":""},{"location":"logical-operators/V2CreateTablePlan/#partitioning-transforms","title":"Partitioning Transforms <pre><code>partitioning: Seq[Transform]\n</code></pre> <p>Partitioning Transforms</p>","text":""},{"location":"logical-operators/V2CreateTablePlan/#tablename","title":"tableName <pre><code>tableName: Identifier\n</code></pre> <p>Used when PreprocessTableCreation post-hoc logical resolution rule is executed</p>","text":""},{"location":"logical-operators/V2CreateTablePlan/#table-schema","title":"Table Schema <pre><code>tableSchema: StructType\n</code></pre>","text":""},{"location":"logical-operators/V2CreateTablePlan/#withpartitioning","title":"withPartitioning <pre><code>withPartitioning(\n  rewritten: Seq[Transform]): V2CreateTablePlan\n</code></pre>","text":""},{"location":"logical-operators/V2CreateTablePlan/#implementations","title":"Implementations","text":"<ul> <li>CreateTableAsSelect</li> <li><code>ReplaceTable</code></li> <li><code>ReplaceTableAsSelect</code></li> </ul>"},{"location":"logical-operators/V2WriteCommand/","title":"V2WriteCommand Logical Commands","text":"<p><code>V2WriteCommand</code>\u00a0is an extension of the UnaryCommand abstraction for unary logical commands that write data out to tables.</p>"},{"location":"logical-operators/V2WriteCommand/#contract","title":"Contract","text":""},{"location":"logical-operators/V2WriteCommand/#isbyname","title":"isByName <pre><code>isByName: Boolean\n</code></pre> <p>Always disabled (<code>false</code>):</p> <ul> <li>ReplaceData</li> <li>WriteDelta</li> </ul> <p>Used when:</p> <ul> <li>ResolveOutputRelation logical resolution rule is executed (for <code>TableOutputResolver</code> to <code>resolveOutputColumns</code>)</li> </ul>","text":""},{"location":"logical-operators/V2WriteCommand/#query","title":"Query <pre><code>query: LogicalPlan\n</code></pre> <p>LogicalPlan of the data to be written out</p>","text":""},{"location":"logical-operators/V2WriteCommand/#table","title":"Table <pre><code>table: NamedRelation\n</code></pre> <p>NamedRelation of the table to write data to</p>","text":""},{"location":"logical-operators/V2WriteCommand/#withnewquery","title":"withNewQuery <pre><code>withNewQuery(\n  newQuery: LogicalPlan): V2WriteCommand\n</code></pre> <p>Used when:</p> <ul> <li>ResolveOutputRelation logical analysis rule is executed</li> </ul>","text":""},{"location":"logical-operators/V2WriteCommand/#withnewtable","title":"withNewTable <pre><code>withNewTable(\n  newTable: NamedRelation): V2WriteCommand\n</code></pre> <p>Used when:</p> <ul> <li>ResolveRelations logical analysis rule is executed</li> <li>ResolveOutputRelation logical analysis rule is executed</li> </ul>","text":""},{"location":"logical-operators/V2WriteCommand/#implementations","title":"Implementations","text":"<ul> <li>AppendData</li> <li>OverwriteByExpression</li> <li>OverwritePartitionsDynamic</li> <li>RowLevelWrite</li> </ul>"},{"location":"logical-operators/View/","title":"View Unary Logical Operator","text":"<p>[[children]] <code>View</code> is a &lt;&gt; with a single &lt;&gt; logical operator. <p><code>View</code> is &lt;&gt; exclusively when <code>SessionCatalog</code> is requested to find a relation in the catalogs (e.g. when <code>DescribeTableCommand</code> logical command is &lt;&gt; and the table type is <code>VIEW</code>)."},{"location":"logical-operators/View/#source-scala","title":"[source, scala]","text":"<p>// Let's create a view first // Using SQL directly to manage views is so much nicer val name = \"demo_view\" sql(s\"CREATE OR REPLACE VIEW $name COMMENT 'demo view' AS VALUES 1,2\") assert(spark.catalog.tableExists(name))</p> <p>val q = sql(s\"DESC EXTENDED $name\")</p> <p>val allRowsIncluded = 100 scala&gt; q.show(numRows = allRowsIncluded) +--------------------+--------------------+-------+ |            col_name|           data_type|comment| +--------------------+--------------------+-------+ |                col1|                 int|   null| |                    |                    |       | |# Detailed Table ...|                    |       | |            Database|             default|       | |               Table|           demo_view|       | |               Owner|               jacek|       | |        Created Time|Thu Aug 30 08:55:...|       | |         Last Access|Thu Jan 01 01:00:...|       | |          Created By|         Spark 2.3.1|       | |                Type|                VIEW|       | |             Comment|           demo view|       | |           View Text|          VALUES 1,2|       | |View Default Data...|             default|       | |View Query Output...|              [col1]|       | |    Table Properties|[transient_lastDd...|       | |       Serde Library|org.apache.hadoop...|       | |         InputFormat|org.apache.hadoop...|       | |        OutputFormat|org.apache.hadoop...|       | |  Storage Properties|[serialization.fo...|       | +--------------------+--------------------+-------+</p> <p>[[newInstance]] <code>View</code> is a MultiInstanceRelation so a &lt;&gt; to appear multiple times in a physical query plan. When requested for a new instance, <code>View</code> &lt;&gt; of the &lt;&gt;. <p>[[resolved]] <code>View</code> is considered &lt;&gt; only when the &lt;&gt; is. <p>[[simpleString]] <code>View</code> has the following &lt;&gt;: <pre><code>View ([identifier], [output])\n</code></pre>"},{"location":"logical-operators/View/#source-scala_1","title":"[source, scala]","text":"<p>val name = \"demo_view\" sql(s\"CREATE OR REPLACE VIEW $name COMMENT 'demo view' AS VALUES 1,2\") assert(spark.catalog.tableExists(name))</p> <p>val q = spark.table(name) val qe = q.queryExecution</p> <p>val logicalPlan = qe.logical scala&gt; println(logicalPlan.simpleString) 'UnresolvedRelation <code>demo_view</code></p> <p>val analyzedPlan = qe.analyzed scala&gt; println(analyzedPlan.numberedTreeString) 00 SubqueryAlias demo_view 01 +- View (<code>default</code>.<code>demo_view</code>, [col1#33]) 02    +- Project [cast(col1#34 as int) AS col1#33] 03       +- LocalRelation [col1#34]</p> <p>// Skip SubqueryAlias scala&gt; println(analyzedPlan.children.head.simpleString) View (<code>default</code>.<code>demo_view</code>, [col1#33])</p> <p>NOTE: <code>View</code> is resolved by ResolveRelationslogical resolution.</p> <p>NOTE: AliasViewChild logical analysis rule makes sure that the &lt;&gt; of a <code>View</code> matches the output of the &lt;&gt; logical operator. <p>NOTE: &lt;&gt; logical optimization removes (eliminates) <code>View</code> operators from a logical query plan. <p>NOTE: &lt;&gt;."},{"location":"logical-operators/View/#creating-instance","title":"Creating Instance","text":"<p><code>View</code> takes the following when created:</p> <ul> <li>[[desc]] CatalogTable</li> <li>[[output]] Output schema attributes (as <code>Seq[Attribute]</code>)</li> <li>[[child]] Child logical operator</li> </ul>"},{"location":"logical-operators/Window/","title":"Window Unary Logical Operator","text":"<p><code>Window</code> is a spark-sql-LogicalPlan.md#UnaryNode[unary logical operator] that...FIXME</p> <p><code>Window</code> is &lt;&gt; when: <ul> <li> <p>ExtractWindowExpressions logical resolution rule is executed</p> </li> <li> <p>CleanupAliases logical analysis rule is executed</p> </li> </ul> <p>[[output]] When requested for &lt;&gt;, <code>Window</code> requests the &lt;&gt; logical operator for them and adds the &lt;&gt; of the &lt;&gt;. <p>NOTE: <code>Window</code> logical operator is a subject of pruning unnecessary window expressions in &lt;&gt; logical optimization and collapsing window operators in &lt;&gt; logical optimization. <p>Note</p> <p><code>Window</code> logical operator is resolved to a WindowExec in BasicOperators execution planning strategy.</p> <p>=== [[catalyst-dsl]] Catalyst DSL -- <code>window</code> Operator</p>"},{"location":"logical-operators/Window/#source-scala","title":"[source, scala]","text":"<p>window(   windowExpressions: Seq[NamedExpression],   partitionSpec: Seq[Expression],   orderSpec: Seq[SortOrder]): LogicalPlan</p> <p>window operator in Catalyst DSL creates a &lt;&gt; logical operator, e.g. for testing or Spark SQL internals exploration."},{"location":"logical-operators/Window/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-operators/Window/#fixme-demo","title":"// FIXME: DEMO","text":"<p>=== [[creating-instance]] Creating Window Instance</p> <p><code>Window</code> takes the following when created:</p> <ul> <li>[[windowExpression]] Window expressions/NamedExpression.md[named expressions]</li> <li>[[partitionSpec]] Window partition specification expressions/Expression.md[expressions]</li> <li>[[orderSpec]] Window order specification (as a collection of <code>SortOrder</code> expressions)</li> <li>[[child]] Child &lt;&gt; <p>=== [[windowOutputSet]] Creating AttributeSet with Window Expression Attributes -- <code>windowOutputSet</code> Method</p>"},{"location":"logical-operators/Window/#source-scala_2","title":"[source, scala]","text":""},{"location":"logical-operators/Window/#windowoutputset-attributeset","title":"windowOutputSet: AttributeSet","text":"<p><code>windowOutputSet</code> simply creates a <code>AttributeSet</code> with the &lt;&gt; of the &lt;&gt;."},{"location":"logical-operators/Window/#note","title":"[NOTE]","text":"<p><code>windowOutputSet</code> is used when:</p> <ul> <li><code>ColumnPruning</code> logical optimization is &lt;&gt; (on a &lt;&gt; operator with a <code>Window</code> as the &lt;&gt;)"},{"location":"logical-operators/Window/#collapsewindow-logical-optimization-is-on-a-window-operator-with-another-window-operator-as-the","title":"* <code>CollapseWindow</code> logical optimization is &lt;&gt; (on a <code>Window</code> operator with another <code>Window</code> operator as the &lt;&gt;)","text":""},{"location":"logical-operators/WithCTE/","title":"WithCTE Logical Operator","text":"<p><code>WithCTE</code> is a logical operator that represents UnresolvedWith unary logical operators after analysis.</p>"},{"location":"logical-operators/WithCTE/#creating-instance","title":"Creating Instance","text":"<p><code>WithCTE</code> takes the following to be created:</p> <ul> <li> Logical operator <li> CTERelationDefs <p><code>WithCTE</code> is created when:</p> <ul> <li>CTESubstitution logical analysis rule is executed</li> <li>InlineCTE logical optimization is executed</li> <li>UpdateCTERelationStats logical optimization is executed</li> </ul>"},{"location":"logical-operators/WithCTE/#node-patterns","title":"Node Patterns <pre><code>nodePatterns: Seq[TreePattern]\n</code></pre> <p><code>nodePatterns</code> is CTE.</p> <p><code>nodePatterns</code> is part of the TreeNode abstraction.</p>","text":""},{"location":"logical-operators/WithCTE/#query-planning","title":"Query Planning <p><code>WithCTE</code> logical operators are planned by WithCTEStrategy execution planning strategy.</p>","text":""},{"location":"logical-operators/WithWindowDefinition/","title":"WithWindowDefinition Unary Logical Operator","text":"<p>[[windowDefinitions]][[child]] <code>WithWindowDefinition</code> is a spark-sql-LogicalPlan.md#UnaryNode[unary logical plan] with a single <code>child</code> logical plan and a <code>windowDefinitions</code> lookup table of spark-sql-Expression-WindowSpecDefinition.md[WindowSpecDefinition] per name.</p> <p>[[creating-instance]] <code>WithWindowDefinition</code> is created exclusively when <code>AstBuilder</code> sql/AstBuilder.md#withWindows[parses window definitions].</p> <p>[[output]] The catalyst/QueryPlan.md#output[output schema] of <code>WithWindowDefinition</code> is exactly the output attributes of the &lt;&gt; logical operator. <p>[[example]] [source, scala]</p> <p>// Example with window specification alias and definition val sqlText = \"\"\"   SELECT count(*) OVER anotherWindowSpec   FROM range(5)   WINDOW     anotherWindowSpec AS myWindowSpec,     myWindowSpec AS (       PARTITION BY id       RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW     ) \"\"\"</p> <p>import spark.sessionState.{analyzer, sqlParser} val parsedPlan = sqlParser.parsePlan(sqlText)</p> <p>scala&gt; println(parsedPlan.numberedTreeString) 00 'WithWindowDefinition Map(anotherWindowSpec -&gt; windowspecdefinition('id, RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW), myWindowSpec -&gt; windowspecdefinition('id, RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) 01 +- 'Project [unresolvedalias(unresolvedwindowexpression('count(1), WindowSpecReference(anotherWindowSpec)), None)] 02    +- 'UnresolvedTableValuedFunction range, [5]</p> <p>val plan = analyzer.execute(parsedPlan) scala&gt; println(plan.numberedTreeString) 00 Project [count(1) OVER (PARTITION BY id RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#75L] 01 +- Project [id#73L, count(1) OVER (PARTITION BY id RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#75L, count(1) OVER (PARTITION BY id RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#75L] 02    +- Window [count(1) windowspecdefinition(id#73L, RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS count(1) OVER (PARTITION BY id RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#75L], [id#73L] 03       +- Project [id#73L] 04          +- Range (0, 5, step=1, splits=None)</p>"},{"location":"logical-operators/WriteDelta/","title":"WriteDelta Logical Operator","text":"<p><code>WriteDelta</code> is...FIXME</p>"},{"location":"logical-operators/WriteFiles/","title":"WriteFiles Unary Logical Operator","text":"<p><code>WriteFiles</code> is a unary logical operator.</p>"},{"location":"logical-operators/WriteFiles/#creating-instance","title":"Creating Instance","text":"<p><code>WriteFiles</code> takes the following to be created:</p> <ul> <li> Child LogicalPlan <li> FileFormat <li> Partition Columns (Attributes) <li> BucketSpec <li> Options <li> Static Partitions (<code>TablePartitionSpec</code>) <p><code>WriteFiles</code> is created when:</p> <ul> <li><code>V1Writes</code> logical optimization is executed</li> </ul>"},{"location":"logical-operators/WriteFiles/#query-execution","title":"Query Execution","text":"<p><code>WriteFiles</code> is planned as WriteFilesExec physical operator by BasicOperators execution planning strategy.</p>"},{"location":"logical-optimizations/AQEPropagateEmptyRelation/","title":"AQEPropagateEmptyRelation Adaptive Logical Optimization","text":"<p><code>AQEPropagateEmptyRelation</code> is a logical optimization in Adaptive Query Execution.</p> <p><code>AQEPropagateEmptyRelation</code> is a Catalyst rule for transforming logical plans (<code>Rule[LogicalPlan]</code>).</p>"},{"location":"logical-optimizations/AQEPropagateEmptyRelation/#creating-instance","title":"Creating Instance","text":"<p><code>AQEPropagateEmptyRelation</code> takes no arguments to be created.</p> <p><code>AQEPropagateEmptyRelation</code> is created when:</p> <ul> <li><code>AQEOptimizer</code> is requested for the default batches (of adaptive optimizations)</li> </ul>"},{"location":"logical-optimizations/AQEPropagateEmptyRelation/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/CleanupDynamicPruningFilters/","title":"CleanupDynamicPruningFilters Logical Optimization","text":"<p><code>CleanupDynamicPruningFilters</code> is a logical optimization for Dynamic Partition Pruning.</p> <p><code>CleanupDynamicPruningFilters</code> is a <code>Rule[LogicalPlan]</code> (a Catalyst Rule for logical operators).</p> <p><code>CleanupDynamicPruningFilters</code> is part of the Cleanup filters that cannot be pushed down batch of the SparkOptimizer.</p>"},{"location":"logical-optimizations/CleanupDynamicPruningFilters/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p>   <p>spark.sql.optimizer.dynamicPartitionPruning.enabled</p> <p><code>apply</code> is a noop (does nothing and returns the given LogicalPlan) when executed with spark.sql.optimizer.dynamicPartitionPruning.enabled configuration property disabled.</p>  <p><code>apply</code> finds logical operators with the following tree patterns:</p> <ul> <li>DYNAMIC_PRUNING_EXPRESSION</li> <li>DYNAMIC_PRUNING_SUBQUERY</li> </ul> <p><code>apply</code> transforms the given logical plan as follows:</p> <ul> <li> <p>For LogicalRelation logical operators over HadoopFsRelations, <code>apply</code> removeUnnecessaryDynamicPruningSubquery</p> </li> <li> <p>For HiveTableRelation logical operators, <code>apply</code> removeUnnecessaryDynamicPruningSubquery</p> </li> <li> <p>For DataSourceV2ScanRelation logical operators, <code>apply</code> removeUnnecessaryDynamicPruningSubquery</p> </li> <li> <p><code>DynamicPruning</code> predicate expressions in <code>Filter</code> logical operators are replaced with <code>true</code> literals (cleaned up)</p> </li> </ul>","text":""},{"location":"logical-optimizations/CollapseWindow/","title":"CollapseWindow Logical Optimization","text":"<p><code>CollapseWindow</code> is a base logical optimization that &lt;&gt;. <p><code>CollapseWindow</code> is part of the Operator Optimization fixed-point batch in the standard batches of the Logical Optimizer.</p> <p><code>CollapseWindow</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>."},{"location":"logical-optimizations/CollapseWindow/#source-scala","title":"[source, scala]","text":"<p>// FIXME: DEMO import org.apache.spark.sql.catalyst.optimizer.CollapseWindow</p> <p>val logicalPlan = ??? val afterCollapseWindow = CollapseWindow(logicalPlan)</p>"},{"location":"logical-optimizations/CollapseWindow/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/ColumnPruning/","title":"ColumnPruning Logical Optimization","text":"<p><code>ColumnPruning</code> is a base logical optimization that &lt;&gt;. <p><code>ColumnPruning</code> is part of the RewriteSubquery once-executed batch in the standard batches of the Logical Optimizer.</p> <p><code>ColumnPruning</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>. <p>=== [[example1]] Example 1</p>"},{"location":"logical-optimizations/ColumnPruning/#source-scala","title":"[source, scala]","text":"<p>val dataset = spark.range(10).withColumn(\"bucket\", 'id % 3)</p> <p>import org.apache.spark.sql.expressions.Window val rankCol = rank over Window.partitionBy('bucket).orderBy('id) as \"rank\"</p> <p>val ranked = dataset.withColumn(\"rank\", rankCol)</p> <p>scala&gt; ranked.explain(true) ... TRACE SparkOptimizer: === Applying Rule org.apache.spark.sql.catalyst.optimizer.ColumnPruning ===  Project [id#73L, bucket#76L, rank#192]                                                                                                                              Project [id#73L, bucket#76L, rank#192] !+- Project [id#73L, bucket#76L, rank#82, rank#82 AS rank#192]                                                                                                       +- Project [id#73L, bucket#76L, rank#82 AS rank#192]     +- Window [rank(id#73L) windowspecdefinition(bucket#76L, id#73L ASC, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank#82], [bucket#76L], [id#73L ASC]      +- Window [rank(id#73L) windowspecdefinition(bucket#76L, id#73L ASC, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank#82], [bucket#76L], [id#73L ASC] !      +- Project [id#73L, bucket#76L]                                                                                                                                     +- Project [id#73L, (id#73L % cast(3 as bigint)) AS bucket#76L] !         +- Project [id#73L, (id#73L % cast(3 as bigint)) AS bucket#76L]                                                                                                     +- Range (0, 10, step=1, splits=Some(8)) !            +- Range (0, 10, step=1, splits=Some(8)) ... TRACE SparkOptimizer: Fixed point reached for batch Operator Optimizations after 2 iterations. DEBUG SparkOptimizer: === Result of Batch Operator Optimizations === !Project [id#73L, bucket#76L, rank#192]                                                                                                                              Window [rank(id#73L) windowspecdefinition(bucket#76L, id#73L ASC, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank#82], [bucket#76L], [id#73L ASC] !+- Project [id#73L, bucket#76L, rank#82, rank#82 AS rank#192]                                                                                                       +- Project [id#73L, (id#73L % 3) AS bucket#76L] !   +- Window [rank(id#73L) windowspecdefinition(bucket#76L, id#73L ASC, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank#82], [bucket#76L], [id#73L ASC]      +- Range (0, 10, step=1, splits=Some(8)) !      +- Project [id#73L, bucket#76L] !         +- Project [id#73L, (id#73L % cast(3 as bigint)) AS bucket#76L] !            +- Range (0, 10, step=1, splits=Some(8)) ...</p> <p>=== [[example2]] Example 2</p>"},{"location":"logical-optimizations/ColumnPruning/#source-scala_1","title":"[source, scala]","text":"<p>// the business object case class Person(id: Long, name: String, city: String)</p> <p>// the dataset to query over val dataset = Seq(Person(0, \"Jacek\", \"Warsaw\")).toDS</p> <p>// the query // Note that we work with names only (out of 3 attributes in Person) val query = dataset.groupBy(upper('name) as 'name).count</p> <p>scala&gt; query.explain(extended = true) ... TRACE SparkOptimizer: === Applying Rule org.apache.spark.sql.catalyst.optimizer.ColumnPruning ===  Aggregate [upper(name#126)], [upper(name#126) AS name#160, count(1) AS count#166L]   Aggregate [upper(name#126)], [upper(name#126) AS name#160, count(1) AS count#166L] !+- LocalRelation [id#125L, name#126, city#127]                                       +- Project [name#126] !                                                                                        +- LocalRelation [id#125L, name#126, city#127] ... == Parsed Logical Plan == 'Aggregate [upper('name) AS name#160], [upper('name) AS name#160, count(1) AS count#166L] +- LocalRelation [id#125L, name#126, city#127]</p> <p>== Analyzed Logical Plan == name: string, count: bigint Aggregate [upper(name#126)], [upper(name#126) AS name#160, count(1) AS count#166L] +- LocalRelation [id#125L, name#126, city#127]</p> <p>== Optimized Logical Plan == Aggregate [upper(name#126)], [upper(name#126) AS name#160, count(1) AS count#166L] +- LocalRelation [name#126]</p> <p>== Physical Plan == *HashAggregate(keys=[upper(name#126)#171], functions=[count(1)], output=[name#160, count#166L]) +- Exchange hashpartitioning(upper(name#126)#171, 200)    +- *HashAggregate(keys=[upper(name#126) AS upper(name#126)#171], functions=[partial_count(1)], output=[upper(name#126)#171, count#173L])       +- LocalTableScan [name#126]</p>"},{"location":"logical-optimizations/ColumnPruning/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/CombineTypedFilters/","title":"CombineTypedFilters Logical Optimization","text":"<p><code>CombineTypedFilters</code> is a base logical optimization that &lt;&gt; that ultimately ends up as a single method call. <p><code>CombineTypedFilters</code> is part of the Object Expressions Optimization fixed-point batch in the standard batches of the Logical Optimizer.</p> <p><code>CombineTypedFilters</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>."},{"location":"logical-optimizations/CombineTypedFilters/#source-scala","title":"[source, scala]","text":"<p>scala&gt; :type spark org.apache.spark.sql.SparkSession</p> <p>// A query with two consecutive typed filters val q = spark.range(10).filter(_ % 2 == 0).filter(_ == 0) scala&gt; q.queryExecution.optimizedPlan ... TRACE SparkOptimizer: === Applying Rule org.apache.spark.sql.catalyst.optimizer.CombineTypedFilters ===  TypedFilter , class java.lang.Long, [StructField(value,LongType,true)], newInstance(class java.lang.Long)      TypedFilter , class java.lang.Long, [StructField(value,LongType,true)], newInstance(class java.lang.Long) !+- TypedFilter , class java.lang.Long, [StructField(value,LongType,true)], newInstance(class java.lang.Long)   +- Range (0, 10, step=1, splits=Some(8)) !   +- Range (0, 10, step=1, splits=Some(8)) <p>TRACE SparkOptimizer: Fixed point reached for batch Typed Filter Optimization after 2 iterations. DEBUG SparkOptimizer: === Result of Batch Typed Filter Optimization ===  TypedFilter , class java.lang.Long, [StructField(value,LongType,true)], newInstance(class java.lang.Long)      TypedFilter , class java.lang.Long, [StructField(value,LongType,true)], newInstance(class java.lang.Long) !+- TypedFilter , class java.lang.Long, [StructField(value,LongType,true)], newInstance(class java.lang.Long)   +- Range (0, 10, step=1, splits=Some(8)) !   +- Range (0, 10, step=1, splits=Some(8)) ..."},{"location":"logical-optimizations/CombineTypedFilters/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/CombineUnions/","title":"CombineUnions Logical Optimization","text":"<p><code>CombineUnions</code> is a base logical optimization that &lt;&gt;. <p><code>CombineUnions</code> is part of the Union once-executed batch in the standard batches of the Logical Optimizer.</p> <p><code>CombineUnions</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>."},{"location":"logical-optimizations/CombineUnions/#source-scala","title":"[source, scala]","text":""},{"location":"logical-optimizations/CombineUnions/#fixme-demo","title":"// FIXME Demo","text":""},{"location":"logical-optimizations/CombineUnions/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/ComputeCurrentTime/","title":"ComputeCurrentTime Logical Optimization","text":"<p><code>ComputeCurrentTime</code> is a base logical optimization that &lt;&gt;. <p><code>ComputeCurrentTime</code> is part of the Finish Analysis once-executed batch in the standard batches of the Logical Optimizer.</p> <p><code>ComputeCurrentTime</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>."},{"location":"logical-optimizations/ComputeCurrentTime/#source-scala","title":"[source, scala]","text":"<p>// Query with two current_date's import org.apache.spark.sql.functions.current_date val q = spark.range(1).select(current_date() as \"d1\", current_date() as \"d2\") val analyzedPlan = q.queryExecution.analyzed</p> <p>scala&gt; println(analyzedPlan.numberedTreeString) 00 Project [current_date(Some(Europe/Warsaw)) AS d1#12, current_date(Some(Europe/Warsaw)) AS d2#13] 01 +- Range (0, 1, step=1, splits=Some(8))</p> <p>import org.apache.spark.sql.catalyst.optimizer.ComputeCurrentTime</p> <p>val afterComputeCurrentTime = ComputeCurrentTime(analyzedPlan) scala&gt; println(afterComputeCurrentTime.numberedTreeString) 00 Project [17773 AS d1#12, 17773 AS d2#13] 01 +- Range (0, 1, step=1, splits=Some(8))</p> <p>// Another query with two current_timestamp's // Here the millis play a bigger role so it is easier to notice the results import org.apache.spark.sql.functions.current_timestamp val q = spark.range(1).select(current_timestamp() as \"ts1\", current_timestamp() as \"ts2\") val analyzedPlan = q.queryExecution.analyzed val afterComputeCurrentTime = ComputeCurrentTime(analyzedPlan) scala&gt; println(afterComputeCurrentTime.numberedTreeString) 00 Project [1535629687768000 AS ts1#18, 1535629687768000 AS ts2#19] 01 +- Range (0, 1, step=1, splits=Some(8))</p>"},{"location":"logical-optimizations/ComputeCurrentTime/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/ConstantFolding/","title":"ConstantFolding Logical Optimization","text":"<p><code>ConstantFolding</code> is a base logical optimization that &lt;&gt;. <p><code>ConstantFolding</code> is part of the Operator Optimization before Inferring Filters fixed-point batch in the standard batches of the Logical Optimizer.</p> <p><code>ConstantFolding</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>."},{"location":"logical-optimizations/ConstantFolding/#source-scala","title":"[source, scala]","text":"<p>scala&gt; spark.range(1).select(lit(3) &gt; 2).explain(true) ... TRACE SparkOptimizer: === Applying Rule org.apache.spark.sql.catalyst.optimizer.ConstantFolding === !Project [(3 &gt; 2) AS (3 &gt; 2)#3]            Project [true AS (3 &gt; 2)#3]  +- Range (0, 1, step=1, splits=Some(8))   +- Range (0, 1, step=1, splits=Some(8))</p> <p>scala&gt; spark.range(1).select('id + 'id &gt; 0).explain(true) ... TRACE SparkOptimizer: === Applying Rule org.apache.spark.sql.catalyst.optimizer.ConstantFolding === !Project [((id#7L + id#7L) &gt; cast(0 as bigint)) AS ((id + id) &gt; 0)#10]   Project [((id#7L + id#7L) &gt; 0) AS ((id + id) &gt; 0)#10]  +- Range (0, 1, step=1, splits=Some(8))                                 +- Range (0, 1, step=1, splits=Some(8))</p>"},{"location":"logical-optimizations/ConstantFolding/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/ConvertToLocalRelation/","title":"ConvertToLocalRelation Logical Optimization","text":"<p><code>ConvertToLocalRelation</code> is...FIXME</p>"},{"location":"logical-optimizations/CostBasedJoinReorder/","title":"CostBasedJoinReorder Logical Optimization -- Join Reordering in Cost-Based Optimization","text":"<p><code>CostBasedJoinReorder</code> is a base logical optimization that reorders joins in Cost-Based Optimization.</p> <p><code>ReorderJoin</code> is part of the Join Reorder once-executed batch in the standard batches of the Logical Optimizer.</p> <p><code>ReorderJoin</code> is simply a Catalyst rule for transforming LogicalPlans, i.e. <code>Rule[LogicalPlan]</code>.</p> <p><code>CostBasedJoinReorder</code> applies the join optimizations on a logical plan with 2 or more consecutive inner or cross joins (possibly separated by <code>Project</code> operators) when spark.sql.cbo.enabled and spark.sql.cbo.joinReorder.enabled configuration properties are both enabled.</p> <pre><code>// Use shortcuts to read the values of the properties\nscala&gt; spark.sessionState.conf.cboEnabled\nres0: Boolean = true\n\nscala&gt; spark.sessionState.conf.joinReorderEnabled\nres1: Boolean = true\n</code></pre> <p><code>CostBasedJoinReorder</code> uses row count statistic that is computed using ANALYZE TABLE COMPUTE STATISTICS SQL command with no <code>NOSCAN</code> option.</p> <pre><code>// Create tables and compute their row count statistics\n// There have to be at least 2 joins\n// Make the example reproducible\nval tableNames = Seq(\"t1\", \"t2\", \"tiny\")\nimport org.apache.spark.sql.catalyst.TableIdentifier\nval tableIds = tableNames.map(TableIdentifier.apply)\nval sessionCatalog = spark.sessionState.catalog\ntableIds.foreach { tableId =&gt;\n  sessionCatalog.dropTable(tableId, ignoreIfNotExists = true, purge = true)\n}\n\nval belowBroadcastJoinThreshold = spark.sessionState.conf.autoBroadcastJoinThreshold - 1\nspark.range(belowBroadcastJoinThreshold).write.saveAsTable(\"t1\")\n// t2 is twice as big as t1\nspark.range(2 * belowBroadcastJoinThreshold).write.saveAsTable(\"t2\")\nspark.range(5).write.saveAsTable(\"tiny\")\n\n// Compute row count statistics\ntableNames.foreach { t =&gt;\n  sql(s\"ANALYZE TABLE $t COMPUTE STATISTICS\")\n}\n\n// Load the tables\nval t1 = spark.table(\"t1\")\nval t2 = spark.table(\"t2\")\nval tiny = spark.table(\"tiny\")\n\n// Example: Inner join with join condition\nval q = t1.join(t2, Seq(\"id\")).join(tiny, Seq(\"id\"))\nval plan = q.queryExecution.analyzed\nscala&gt; println(plan.numberedTreeString)\n00 Project [id#51L]\n01 +- Join Inner, (id#51L = id#57L)\n02    :- Project [id#51L]\n03    :  +- Join Inner, (id#51L = id#54L)\n04    :     :- SubqueryAlias t1\n05    :     :  +- Relation[id#51L] parquet\n06    :     +- SubqueryAlias t2\n07    :        +- Relation[id#54L] parquet\n08    +- SubqueryAlias tiny\n09       +- Relation[id#57L] parquet\n\n// Eliminate SubqueryAlias logical operators as they no longer needed\n// And \"confuse\" CostBasedJoinReorder\n// CostBasedJoinReorder cares about how deep Joins are and reorders consecutive joins only\nimport org.apache.spark.sql.catalyst.analysis.EliminateSubqueryAliases\nval noAliasesPlan = EliminateSubqueryAliases(plan)\nscala&gt; println(noAliasesPlan.numberedTreeString)\n00 Project [id#51L]\n01 +- Join Inner, (id#51L = id#57L)\n02    :- Project [id#51L]\n03    :  +- Join Inner, (id#51L = id#54L)\n04    :     :- Relation[id#51L] parquet\n05    :     +- Relation[id#54L] parquet\n06    +- Relation[id#57L] parquet\n\n// Let's go pro and create a custom RuleExecutor (i.e. an Optimizer)\nimport org.apache.spark.sql.catalyst.rules.RuleExecutor\nimport org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\nimport org.apache.spark.sql.catalyst.analysis.EliminateSubqueryAliases\nimport org.apache.spark.sql.catalyst.optimizer.CostBasedJoinReorder\nobject Optimize extends RuleExecutor[LogicalPlan] {\n  val batches =\n    Batch(\"EliminateSubqueryAliases\", Once, EliminateSubqueryAliases) ::\n    Batch(\"Join Reorder\", Once, CostBasedJoinReorder) :: Nil\n}\n\nval joinsReordered = Optimize.execute(plan)\nscala&gt; println(joinsReordered.numberedTreeString)\n00 Project [id#51L]\n01 +- Join Inner, (id#51L = id#54L)\n02    :- Project [id#51L]\n03    :  +- Join Inner, (id#51L = id#57L)\n04    :     :- Relation[id#51L] parquet\n05    :     +- Relation[id#57L] parquet\n06    +- Relation[id#54L] parquet\n\n// Execute the plans\n// Compare the plans as diagrams in web UI @ http://localhost:4040/SQL\n// We'd have to use too many internals so let's turn CBO on and off\n// Moreover, please remember that the query \"phases\" are cached\n// That's why we copy and paste the entire query for execution\nimport org.apache.spark.sql.internal.SQLConf\nval cc = SQLConf.get\ncc.setConf(SQLConf.CBO_ENABLED, false)\nval q = t1.join(t2, Seq(\"id\")).join(tiny, Seq(\"id\"))\nq.collect.foreach(_ =&gt; ())\n\ncc.setConf(SQLConf.CBO_ENABLED, true)\nval q = t1.join(t2, Seq(\"id\")).join(tiny, Seq(\"id\"))\nq.collect.foreach(_ =&gt; ())\n</code></pre>"},{"location":"logical-optimizations/DecimalAggregates/","title":"DecimalAggregates Logical Optimization","text":"<p><code>DecimalAggregates</code> is a base logical optimization that &lt;&gt; <code>Sum</code> and <code>Average</code> aggregate functions on fixed-precision <code>DecimalType</code> values to use <code>UnscaledValue</code> (unscaled Long) values in spark-sql-Expression-WindowExpression.md[WindowExpression] and AggregateExpression expressions. <p><code>DecimalAggregates</code> is part of the Decimal Optimizations fixed-point batch in the standard batches of the Logical Optimizer.</p> <p><code>DecimalAggregates</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>."},{"location":"logical-optimizations/DecimalAggregates/#tip","title":"[TIP]","text":"<p>Import <code>DecimalAggregates</code> and apply the rule directly on your structured queries to learn how the rule works.</p>"},{"location":"logical-optimizations/DecimalAggregates/#source-scala","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.optimizer.DecimalAggregates val da = DecimalAggregates(spark.sessionState.conf)</p> <p>// Build analyzed logical plan // with sum aggregate function and Decimal field import org.apache.spark.sql.types.DecimalType val query = spark.range(5).select(sum($\"id\" cast DecimalType(1,0)) as \"sum\") scala&gt; val plan = query.queryExecution.analyzed plan: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = Aggregate [sum(cast(id#91L as decimal(1,0))) AS sum#95] +- Range (0, 5, step=1, splits=Some(8))</p> <p>// Apply DecimalAggregates rule // Note MakeDecimal and UnscaledValue operators scala&gt; da.apply(plan) res27: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = Aggregate [MakeDecimal(sum(UnscaledValue(cast(id#91L as decimal(1,0)))),11,0) AS sum#95] +- Range (0, 5, step=1, splits=Some(8))</p> <p>====</p> <p>=== [[example-sum-decimal]] Example: sum Aggregate Function on Decimal with Precision Smaller Than 9</p>"},{"location":"logical-optimizations/DecimalAggregates/#source-scala_1","title":"[source, scala]","text":"<p>// sum aggregate with Decimal field with precision &lt;= 8 val q = \"SELECT sum(cast(id AS DECIMAL(5,0))) FROM range(1)\"</p> <p>scala&gt; sql(q).explain(true) == Parsed Logical Plan == 'Project [unresolvedalias('sum(cast('id as decimal(5,0))), None)] +- 'UnresolvedTableValuedFunction range, [1]</p> <p>== Analyzed Logical Plan == sum(CAST(id AS DECIMAL(5,0))): decimal(15,0) Aggregate [sum(cast(id#104L as decimal(5,0))) AS sum(CAST(id AS DECIMAL(5,0)))#106] +- Range (0, 1, step=1, splits=None)</p> <p>== Optimized Logical Plan == Aggregate [MakeDecimal(sum(UnscaledValue(cast(id#104L as decimal(5,0)))),15,0) AS sum(CAST(id AS DECIMAL(5,0)))#106] +- Range (0, 1, step=1, splits=None)</p> <p>== Physical Plan == *HashAggregate(keys=[], functions=[sum(UnscaledValue(cast(id#104L as decimal(5,0))))], output=[sum(CAST(id AS DECIMAL(5,0)))#106]) +- Exchange SinglePartition    +- *HashAggregate(keys=[], functions=[partial_sum(UnscaledValue(cast(id#104L as decimal(5,0))))], output=[sum#108L])       +- *Range (0, 1, step=1, splits=None)</p> <p>=== [[example-avg-decimal]] Example: avg Aggregate Function on Decimal with Precision Smaller Than 12</p>"},{"location":"logical-optimizations/DecimalAggregates/#source-scala_2","title":"[source, scala]","text":"<p>// avg aggregate with Decimal field with precision &lt;= 11 val q = \"SELECT avg(cast(id AS DECIMAL(10,0))) FROM range(1)\"</p> <p>scala&gt; val q = \"SELECT avg(cast(id AS DECIMAL(10,0))) FROM range(1)\" q: String = SELECT avg(cast(id AS DECIMAL(10,0))) FROM range(1)</p> <p>scala&gt; sql(q).explain(true) == Parsed Logical Plan == 'Project [unresolvedalias('avg(cast('id as decimal(10,0))), None)] +- 'UnresolvedTableValuedFunction range, [1]</p> <p>== Analyzed Logical Plan == avg(CAST(id AS DECIMAL(10,0))): decimal(14,4) Aggregate [avg(cast(id#115L as decimal(10,0))) AS avg(CAST(id AS DECIMAL(10,0)))#117] +- Range (0, 1, step=1, splits=None)</p> <p>== Optimized Logical Plan == Aggregate [cast((avg(UnscaledValue(cast(id#115L as decimal(10,0)))) / 1.0) as decimal(14,4)) AS avg(CAST(id AS DECIMAL(10,0)))#117] +- Range (0, 1, step=1, splits=None)</p> <p>== Physical Plan == *HashAggregate(keys=[], functions=[avg(UnscaledValue(cast(id#115L as decimal(10,0))))], output=[avg(CAST(id AS DECIMAL(10,0)))#117]) +- Exchange SinglePartition    +- *HashAggregate(keys=[], functions=[partial_avg(UnscaledValue(cast(id#115L as decimal(10,0))))], output=[sum#120, count#121L])       +- *Range (0, 1, step=1, splits=None)</p>"},{"location":"logical-optimizations/DecimalAggregates/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/DynamicJoinSelection/","title":"DynamicJoinSelection Adaptive Logical Optimization","text":"<p><code>DynamicJoinSelection</code> is a logical optimization in Adaptive Query Execution to transform Join logical operators with JoinHints.</p> <p><code>DynamicJoinSelection</code> is a Catalyst rule for transforming logical plans (<code>Rule[LogicalPlan]</code>).</p>"},{"location":"logical-optimizations/DynamicJoinSelection/#creating-instance","title":"Creating Instance","text":"<p><code>DynamicJoinSelection</code> takes no arguments to be created.</p> <p><code>DynamicJoinSelection</code> is created when:</p> <ul> <li><code>AQEOptimizer</code> is requested for the default batches (of adaptive optimizations)</li> </ul>"},{"location":"logical-optimizations/DynamicJoinSelection/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> traverses the given LogicalPlan down (the tree) and rewrites Join logical operators as follows:</p> <ol> <li> <p>If there is no JoinStrategyHint defined for the left side, <code>apply</code> selects the JoinStrategy for the left operator.</p> </li> <li> <p>If there is no JoinStrategyHint defined for the right side, <code>apply</code> selects the JoinStrategy for the right operator.</p> </li> <li> <p><code>apply</code> associates the new JoinHint with the <code>Join</code> logical operator</p> </li> </ol> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/DynamicJoinSelection/#selectjoinstrategy","title":"selectJoinStrategy <pre><code>selectJoinStrategy(\n  plan: LogicalPlan): Option[JoinStrategyHint]\n</code></pre> <p><code>selectJoinStrategy</code> works only with LogicalQueryStages of ShuffleQueryStageExecs that are materialized and have mapStats defined (and returns <code>None</code> otherwise).</p> <p><code>selectJoinStrategy</code> selects a JoinStrategyHint based on shouldDemoteBroadcastHashJoin and preferShuffledHashJoin with the mapStats.</p>    demoteBroadcastHash preferShuffleHash JoinStrategyHint     <code>true</code> <code>true</code> SHUFFLE_HASH   <code>true</code> <code>false</code> NO_BROADCAST_HASH   <code>false</code> <code>true</code> PREFER_SHUFFLE_HASH   <code>false</code> <code>false</code> <code>None</code> (undefined)","text":""},{"location":"logical-optimizations/DynamicJoinSelection/#prefershuffledhashjoin","title":"preferShuffledHashJoin <pre><code>preferShuffledHashJoin(\n  mapStats: MapOutputStatistics): Boolean\n</code></pre> <p><code>preferShuffledHashJoin</code> takes a <code>MapOutputStatistics</code> (Apache Spark) and holds (<code>true</code>) when all of the following hold:</p> <ol> <li>spark.sql.adaptive.advisoryPartitionSizeInBytes is at most spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold</li> <li>Approximate number of output bytes (<code>bytesByPartitionId</code>) of every map output partition of the given <code>MapOutputStatistics</code> is at most spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold</li> </ol>","text":""},{"location":"logical-optimizations/DynamicJoinSelection/#shoulddemotebroadcasthashjoin","title":"shouldDemoteBroadcastHashJoin <pre><code>shouldDemoteBroadcastHashJoin(\n  mapStats: MapOutputStatistics): Boolean\n</code></pre> <p><code>shouldDemoteBroadcastHashJoin</code> takes a <code>MapOutputStatistics</code> (Apache Spark) and holds (<code>true</code>) when all of the following hold:</p> <ol> <li>There is at least 1 partition with data (based on the <code>bytesByPartitionId</code> collection of the given <code>MapOutputStatistics</code>)</li> <li>The ratio of the non-empty partitions to all partitions is below spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin configuration property</li> </ol>","text":""},{"location":"logical-optimizations/DynamicJoinSelection/#demo","title":"Demo <pre><code>// :paste -raw\npackage org.apache.spark.japila\n\nimport org.apache.spark.MapOutputStatistics\n\nimport org.apache.spark.sql.execution.SparkPlan\nimport org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec\n\nclass MyShuffleQueryStageExec(\n    override val id: Int,\n    override val plan: SparkPlan,\n    override val _canonicalized: SparkPlan) extends ShuffleQueryStageExec(id, plan, _canonicalized) {\n\n  override def isMaterialized: Boolean = true\n\n  override def mapStats: Option[MapOutputStatistics] = {\n    val shuffleId = 0\n    // must be smaller than conf.nonEmptyPartitionRatioForBroadcastJoin\n    val bytesByPartitionId = Array[Long](1, 0, 0, 0, 0, 0)\n    Some(new MapOutputStatistics(shuffleId, bytesByPartitionId))\n  }\n}\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.dsl.plans._\nval logicalPlan = table(\"t1\")\n\nimport org.apache.spark.sql.execution.exchange.ShuffleExchangeExec\nimport org.apache.spark.sql.catalyst.plans.physical.RoundRobinPartitioning\nimport org.apache.spark.sql.execution.exchange.ENSURE_REQUIREMENTS\nimport org.apache.spark.sql.execution.PlanLater\n\nimport org.apache.spark.sql.catalyst.dsl.plans._\nval child = PlanLater(table(\"t2\"))\n\nval shuffleExec = ShuffleExchangeExec(RoundRobinPartitioning(10), child, ENSURE_REQUIREMENTS)\n\nimport org.apache.spark.japila.MyShuffleQueryStageExec\nval stage = new MyShuffleQueryStageExec(id = 0, plan = shuffleExec, _canonicalized = shuffleExec)\n\nassert(stage.isMaterialized,\n  \"DynamicJoinSelection expects materialized ShuffleQueryStageExecs\")\nassert(stage.mapStats.isDefined,\n  \"DynamicJoinSelection expects ShuffleQueryStageExecs with MapOutputStatistics\")\n\nimport org.apache.spark.sql.catalyst.plans.logical.JoinHint\nimport org.apache.spark.sql.catalyst.plans.Inner\nimport org.apache.spark.sql.catalyst.plans.logical.Join\nimport org.apache.spark.sql.execution.adaptive.LogicalQueryStage\n\nval left = LogicalQueryStage(logicalPlan, physicalPlan = stage)\nval right = LogicalQueryStage(logicalPlan, physicalPlan = stage)\n\nval plan = Join(left, right, joinType = Inner, condition = None, hint = JoinHint.NONE)\n\nimport org.apache.spark.sql.execution.adaptive.DynamicJoinSelection\nval newPlan = DynamicJoinSelection(plan)\n</code></pre> <pre><code>scala&gt; println(newPlan.numberedTreeString)\n00 Join Inner, leftHint=(strategy=no_broadcast_hash), rightHint=(strategy=no_broadcast_hash)\n01 :- LogicalQueryStage 'UnresolvedRelation [t1], [], false, MyShuffleQueryStage 0\n02 +- LogicalQueryStage 'UnresolvedRelation [t1], [], false, MyShuffleQueryStage 0\n</code></pre> <pre><code>// cf. DynamicJoinSelection.shouldDemoteBroadcastHashJoin\nval mapStats = stage.mapStats.get\nval conf = spark.sessionState.conf\n</code></pre>","text":""},{"location":"logical-optimizations/EliminateResolvedHint/","title":"EliminateResolvedHint Logical Optimization","text":"<p><code>EliminateResolvedHint</code> is a default logical optimization.</p>"},{"location":"logical-optimizations/EliminateResolvedHint/#non-excludable-rule","title":"Non-Excludable Rule <p><code>EliminateResolvedHint</code> is a non-excludable rule.</p>","text":""},{"location":"logical-optimizations/EliminateResolvedHint/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>\u00a0is part of the Rule abstraction.</p> <p><code>apply</code> transforms Join logical operators with no hints defined in the given LogicalPlan:</p> <ol> <li> <p>Extracts hints from the left and right sides of the join (that gives new operators and JoinHints for either side)</p> </li> <li> <p>Creates a new JoinHint with the hints merged for the left and right sides</p> </li> <li> <p>Creates a new Join logical operator with the new left and right operators and the new <code>JoinHint</code></p> </li> </ol> <p>In the end, <code>apply</code> finds ResolvedHints and, if found, requests the HintErrorHandler to joinNotFoundForJoinHint and ignores the hint (returns the child of the <code>ResolvedHint</code>).</p>","text":""},{"location":"logical-optimizations/EliminateResolvedHint/#hinterrorhandler","title":"HintErrorHandler <pre><code>hintErrorHandler: HintErrorHandler\n</code></pre> <p><code>hintErrorHandler</code> is the default HintErrorHandler.</p>","text":""},{"location":"logical-optimizations/EliminateResolvedHint/#extracting-hints-from-logical-plan","title":"Extracting Hints from Logical Plan <pre><code>extractHintsFromPlan(\n  plan: LogicalPlan): (LogicalPlan, Seq[HintInfo])\n</code></pre> <p><code>extractHintsFromPlan</code> collects (extracts) HintInfos from the ResolvedHint unary logical operators in the given LogicalPlan and gives:</p> <ul> <li>HintInfos</li> <li>Transformed plan with ResolvedHint nodes removed</li> </ul> <p>While collecting, <code>extractHintsFromPlan</code> removes the ResolvedHint unary logical operators.</p>  <p>Note</p> <p>It is possible (yet still unclear) that some <code>ResolvedHint</code>s won't get extracted.</p>  <p><code>extractHintsFromPlan</code> is used when:</p> <ul> <li><code>EliminateResolvedHint</code> is requested to execute</li> <li><code>CacheManager</code> is requested to useCachedData</li> </ul>","text":""},{"location":"logical-optimizations/EliminateResolvedHint/#merging-hints","title":"Merging Hints <pre><code>mergeHints(\n  hints: Seq[HintInfo]): Option[HintInfo]\n</code></pre> <p><code>mergeHints</code>...FIXME</p>","text":""},{"location":"logical-optimizations/EliminateResolvedHint/#demo","title":"Demo","text":""},{"location":"logical-optimizations/EliminateResolvedHint/#logical-query-plan","title":"Logical Query Plan","text":"<p>Create a logical plan using Catalyst DSL.</p> <pre><code>import org.apache.spark.sql.catalyst.dsl.plans._\nimport org.apache.spark.sql.catalyst.plans.logical.{SHUFFLE_HASH, SHUFFLE_MERGE}\nimport org.apache.spark.sql.catalyst.plans.logical.LocalRelation\n</code></pre> <pre><code>val t1 = LocalRelation('id.long, 'name.string).hint(SHUFFLE_HASH.displayName)\nval t2 = LocalRelation('id.long, 'age.int).hint(SHUFFLE_MERGE.displayName)\nval logical = t1.join(t2)\n</code></pre> <pre><code>scala&gt; println(logical.numberedTreeString)\n00 'Join Inner\n01 :- 'UnresolvedHint shuffle_hash\n02 :  +- LocalRelation &lt;empty&gt;, [id#0L, name#1]\n03 +- 'UnresolvedHint merge\n04    +- LocalRelation &lt;empty&gt;, [id#2L, age#3]\n</code></pre>"},{"location":"logical-optimizations/EliminateResolvedHint/#analyze-plan","title":"Analyze Plan","text":"<pre><code>val analyzed = logical.analyze\n</code></pre> <pre><code>scala&gt; println(analyzed.numberedTreeString)\n00 Join Inner\n01 :- ResolvedHint (strategy=shuffle_hash)\n02 :  +- LocalRelation &lt;empty&gt;, [id#0L, name#1]\n03 +- ResolvedHint (strategy=merge)\n04    +- LocalRelation &lt;empty&gt;, [id#2L, age#3]\n</code></pre>"},{"location":"logical-optimizations/EliminateResolvedHint/#optimize-plan","title":"Optimize Plan","text":"<p>Optimize the plan (using <code>EliminateResolvedHint</code> only).</p> <pre><code>import org.apache.spark.sql.catalyst.optimizer.EliminateResolvedHint\nval optimizedPlan = EliminateResolvedHint(analyzed)\n</code></pre> <pre><code>scala&gt; println(optimizedPlan.numberedTreeString)\n00 Join Inner, leftHint=(strategy=shuffle_hash), rightHint=(strategy=merge)\n01 :- LocalRelation &lt;empty&gt;, [id#0L, name#1]\n02 +- LocalRelation &lt;empty&gt;, [id#2L, age#3]\n</code></pre>"},{"location":"logical-optimizations/EliminateSerialization/","title":"EliminateSerialization Logical Optimization","text":"<p><code>EliminateSerialization</code> is a base logical optimization that &lt;&gt; logical plans with DeserializeToObject.md[DeserializeToObject] (after <code>SerializeFromObject</code> or <code>TypedFilter</code>), <code>AppendColumns</code> (after <code>SerializeFromObject</code>), <code>TypedFilter</code> (after <code>SerializeFromObject</code>) logical operators. <p><code>EliminateSerialization</code> is part of the Operator Optimization before Inferring Filters fixed-point batch in the standard batches of the Logical Optimizer.</p> <p><code>EliminateSerialization</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>. <p>Examples include:</p> <ol> <li>&lt;map followed by <code>filter</code> Logical Plan&gt;&gt; <li>&lt;map followed by another <code>map</code> Logical Plan&gt;&gt; <li>&lt;groupByKey followed by <code>agg</code> Logical Plan&gt;&gt; <p>=== [[example-map-filter]] Example -- <code>map</code> followed by <code>filter</code> Logical Plan</p> <pre><code>scala&gt; spark.range(4).map(n =&gt; n * 2).filter(n =&gt; n &lt; 3).explain(extended = true)\n== Parsed Logical Plan ==\n'TypedFilter &lt;function1&gt;, long, [StructField(value,LongType,false)], unresolveddeserializer(upcast(getcolumnbyordinal(0, LongType), LongType, - root class: \"scala.Long\"))\n+- SerializeFromObject [input[0, bigint, true] AS value#185L]\n   +- MapElements &lt;function1&gt;, class java.lang.Long, [StructField(value,LongType,true)], obj#184: bigint\n      +- DeserializeToObject newInstance(class java.lang.Long), obj#183: java.lang.Long\n         +- Range (0, 4, step=1, splits=Some(8))\n\n== Analyzed Logical Plan ==\nvalue: bigint\nTypedFilter &lt;function1&gt;, long, [StructField(value,LongType,false)], cast(value#185L as bigint)\n+- SerializeFromObject [input[0, bigint, true] AS value#185L]\n   +- MapElements &lt;function1&gt;, class java.lang.Long, [StructField(value,LongType,true)], obj#184: bigint\n      +- DeserializeToObject newInstance(class java.lang.Long), obj#183: java.lang.Long\n         +- Range (0, 4, step=1, splits=Some(8))\n\n== Optimized Logical Plan ==\nSerializeFromObject [input[0, bigint, true] AS value#185L]\n+- Filter &lt;function1&gt;.apply\n   +- MapElements &lt;function1&gt;, class java.lang.Long, [StructField(value,LongType,true)], obj#184: bigint\n      +- DeserializeToObject newInstance(class java.lang.Long), obj#183: java.lang.Long\n         +- Range (0, 4, step=1, splits=Some(8))\n\n== Physical Plan ==\n*SerializeFromObject [input[0, bigint, true] AS value#185L]\n+- *Filter &lt;function1&gt;.apply\n   +- *MapElements &lt;function1&gt;, obj#184: bigint\n      +- *DeserializeToObject newInstance(class java.lang.Long), obj#183: java.lang.Long\n         +- *Range (0, 4, step=1, splits=Some(8))\n</code></pre> <p>=== [[example-map-map]] Example -- <code>map</code> followed by another <code>map</code> Logical Plan</p> <pre><code>// Notice unnecessary mapping between String and Int types\nval query = spark.range(3).map(_.toString).map(_.toInt)\n\nscala&gt; query.explain(extended = true)\n...\nTRACE SparkOptimizer:\n=== Applying Rule org.apache.spark.sql.catalyst.optimizer.EliminateSerialization ===\n SerializeFromObject [input[0, int, true] AS value#91]                                                                                                                     SerializeFromObject [input[0, int, true] AS value#91]\n +- MapElements &lt;function1&gt;, class java.lang.String, [StructField(value,StringType,true)], obj#90: int                                                                     +- MapElements &lt;function1&gt;, class java.lang.String, [StructField(value,StringType,true)], obj#90: int\n!   +- DeserializeToObject value#86.toString, obj#89: java.lang.String                                                                                                        +- Project [obj#85 AS obj#89]\n!      +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true) AS value#86]         +- MapElements &lt;function1&gt;, class java.lang.Long, [StructField(value,LongType,true)], obj#85: java.lang.String\n!         +- MapElements &lt;function1&gt;, class java.lang.Long, [StructField(value,LongType,true)], obj#85: java.lang.String                                                            +- DeserializeToObject newInstance(class java.lang.Long), obj#84: java.lang.Long\n!            +- DeserializeToObject newInstance(class java.lang.Long), obj#84: java.lang.Long                                                                                          +- Range (0, 3, step=1, splits=Some(8))\n!               +- Range (0, 3, step=1, splits=Some(8))\n...\n== Parsed Logical Plan ==\n'SerializeFromObject [input[0, int, true] AS value#91]\n+- 'MapElements &lt;function1&gt;, class java.lang.String, [StructField(value,StringType,true)], obj#90: int\n   +- 'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: \"java.lang.String\").toString), obj#89: java.lang.String\n      +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true) AS value#86]\n         +- MapElements &lt;function1&gt;, class java.lang.Long, [StructField(value,LongType,true)], obj#85: java.lang.String\n            +- DeserializeToObject newInstance(class java.lang.Long), obj#84: java.lang.Long\n               +- Range (0, 3, step=1, splits=Some(8))\n\n== Analyzed Logical Plan ==\nvalue: int\nSerializeFromObject [input[0, int, true] AS value#91]\n+- MapElements &lt;function1&gt;, class java.lang.String, [StructField(value,StringType,true)], obj#90: int\n   +- DeserializeToObject cast(value#86 as string).toString, obj#89: java.lang.String\n      +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true) AS value#86]\n         +- MapElements &lt;function1&gt;, class java.lang.Long, [StructField(value,LongType,true)], obj#85: java.lang.String\n            +- DeserializeToObject newInstance(class java.lang.Long), obj#84: java.lang.Long\n               +- Range (0, 3, step=1, splits=Some(8))\n\n== Optimized Logical Plan ==\nSerializeFromObject [input[0, int, true] AS value#91]\n+- MapElements &lt;function1&gt;, class java.lang.String, [StructField(value,StringType,true)], obj#90: int\n   +- MapElements &lt;function1&gt;, class java.lang.Long, [StructField(value,LongType,true)], obj#85: java.lang.String\n      +- DeserializeToObject newInstance(class java.lang.Long), obj#84: java.lang.Long\n         +- Range (0, 3, step=1, splits=Some(8))\n\n== Physical Plan ==\n*SerializeFromObject [input[0, int, true] AS value#91]\n+- *MapElements &lt;function1&gt;, obj#90: int\n   +- *MapElements &lt;function1&gt;, obj#85: java.lang.String\n      +- *DeserializeToObject newInstance(class java.lang.Long), obj#84: java.lang.Long\n         +- *Range (0, 3, step=1, splits=Some(8))\n</code></pre> <p>=== [[example-groupByKey-agg]] Example -- <code>groupByKey</code> followed by <code>agg</code> Logical Plan</p> <pre><code>scala&gt; spark.range(4).map(n =&gt; (n, n % 2)).groupByKey(_._2).agg(typed.sum(_._2)).explain(true)\n== Parsed Logical Plan ==\n'Aggregate [value#454L], [value#454L, unresolvedalias(typedsumdouble(org.apache.spark.sql.execution.aggregate.TypedSumDouble@4fcb0de4, Some(unresolveddeserializer(newInstance(class scala.Tuple2), _1#450L, _2#451L)), Some(class scala.Tuple2), Some(StructType(StructField(_1,LongType,true), StructField(_2,LongType,false))), input[0, double, true] AS value#457, unresolveddeserializer(upcast(getcolumnbyordinal(0, DoubleType), DoubleType, - root class: \"scala.Double\"), value#457), input[0, double, true] AS value#456, DoubleType, DoubleType, false), Some(&lt;function1&gt;))]\n+- AppendColumns &lt;function1&gt;, class scala.Tuple2, [StructField(_1,LongType,true), StructField(_2,LongType,false)], newInstance(class scala.Tuple2), [input[0, bigint, true] AS value#454L]\n   +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._1.longValue AS _1#450L, assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._2 AS _2#451L]\n      +- MapElements &lt;function1&gt;, class java.lang.Long, [StructField(value,LongType,true)], obj#449: scala.Tuple2\n         +- DeserializeToObject newInstance(class java.lang.Long), obj#448: java.lang.Long\n            +- Range (0, 4, step=1, splits=Some(8))\n\n== Analyzed Logical Plan ==\nvalue: bigint, TypedSumDouble(scala.Tuple2): double\nAggregate [value#454L], [value#454L, typedsumdouble(org.apache.spark.sql.execution.aggregate.TypedSumDouble@4fcb0de4, Some(newInstance(class scala.Tuple2)), Some(class scala.Tuple2), Some(StructType(StructField(_1,LongType,true), StructField(_2,LongType,false))), input[0, double, true] AS value#457, cast(value#457 as double), input[0, double, true] AS value#456, DoubleType, DoubleType, false) AS TypedSumDouble(scala.Tuple2)#462]\n+- AppendColumns &lt;function1&gt;, class scala.Tuple2, [StructField(_1,LongType,true), StructField(_2,LongType,false)], newInstance(class scala.Tuple2), [input[0, bigint, true] AS value#454L]\n   +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._1.longValue AS _1#450L, assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._2 AS _2#451L]\n      +- MapElements &lt;function1&gt;, class java.lang.Long, [StructField(value,LongType,true)], obj#449: scala.Tuple2\n         +- DeserializeToObject newInstance(class java.lang.Long), obj#448: java.lang.Long\n            +- Range (0, 4, step=1, splits=Some(8))\n\n== Optimized Logical Plan ==\nAggregate [value#454L], [value#454L, typedsumdouble(org.apache.spark.sql.execution.aggregate.TypedSumDouble@4fcb0de4, Some(newInstance(class scala.Tuple2)), Some(class scala.Tuple2), Some(StructType(StructField(_1,LongType,true), StructField(_2,LongType,false))), input[0, double, true] AS value#457, value#457, input[0, double, true] AS value#456, DoubleType, DoubleType, false) AS TypedSumDouble(scala.Tuple2)#462]\n+- AppendColumnsWithObject &lt;function1&gt;, [assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._1.longValue AS _1#450L, assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._2 AS _2#451L], [input[0, bigint, true] AS value#454L]\n   +- MapElements &lt;function1&gt;, class java.lang.Long, [StructField(value,LongType,true)], obj#449: scala.Tuple2\n      +- DeserializeToObject newInstance(class java.lang.Long), obj#448: java.lang.Long\n         +- Range (0, 4, step=1, splits=Some(8))\n\n== Physical Plan ==\n*HashAggregate(keys=[value#454L], functions=[typedsumdouble(org.apache.spark.sql.execution.aggregate.TypedSumDouble@4fcb0de4, Some(newInstance(class scala.Tuple2)), Some(class scala.Tuple2), Some(StructType(StructField(_1,LongType,true), StructField(_2,LongType,false))), input[0, double, true] AS value#457, value#457, input[0, double, true] AS value#456, DoubleType, DoubleType, false)], output=[value#454L, TypedSumDouble(scala.Tuple2)#462])\n+- Exchange hashpartitioning(value#454L, 200)\n   +- *HashAggregate(keys=[value#454L], functions=[partial_typedsumdouble(org.apache.spark.sql.execution.aggregate.TypedSumDouble@4fcb0de4, Some(newInstance(class scala.Tuple2)), Some(class scala.Tuple2), Some(StructType(StructField(_1,LongType,true), StructField(_2,LongType,false))), input[0, double, true] AS value#457, value#457, input[0, double, true] AS value#456, DoubleType, DoubleType, false)], output=[value#454L, value#463])\n      +- AppendColumnsWithObject &lt;function1&gt;, [assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._1.longValue AS _1#450L, assertnotnull(input[0, scala.Tuple2, true], top level non-flat input object)._2 AS _2#451L], [input[0, bigint, true] AS value#454L]\n         +- MapElements &lt;function1&gt;, obj#449: scala.Tuple2\n            +- DeserializeToObject newInstance(class java.lang.Long), obj#448: java.lang.Long\n               +- *Range (0, 4, step=1, splits=Some(8))\n</code></pre>"},{"location":"logical-optimizations/EliminateSerialization/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/EliminateSubqueryAliases/","title":"EliminateSubqueryAliases Logical Optimization","text":"<p><code>EliminateSubqueryAliases</code> is a base logical optimization that &lt;&gt;. <p><code>EliminateSubqueryAliases</code> is part of the Finish Analysis once-executed batch in the standard batches of the Logical Optimizer.</p> <p><code>EliminateSubqueryAliases</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>."},{"location":"logical-optimizations/EliminateSubqueryAliases/#source-scala","title":"[source, scala]","text":"<p>// Using Catalyst DSL import org.apache.spark.sql.catalyst.dsl.plans._ val t1 = table(\"t1\") val logicalPlan = t1.subquery('a)</p> <p>import org.apache.spark.sql.catalyst.analysis.EliminateSubqueryAliases val afterEliminateSubqueryAliases = EliminateSubqueryAliases(logicalPlan) scala&gt; println(afterEliminateSubqueryAliases.numberedTreeString) 00 'UnresolvedRelation <code>t1</code></p>"},{"location":"logical-optimizations/EliminateSubqueryAliases/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> simply removes (eliminates) &lt;&gt; unary logical operators from the input &lt;&gt;. <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/EliminateView/","title":"EliminateView Logical Optimization","text":"<p><code>EliminateView</code> is a base logical optimization that &lt;&gt;. <p><code>EliminateView</code> is part of the Finish Analysis once-executed batch in the standard batches of the Logical Optimizer.</p> <p><code>EliminateView</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>."},{"location":"logical-optimizations/EliminateView/#source-scala","title":"[source, scala]","text":"<p>val name = \"demo_view\" sql(s\"CREATE OR REPLACE VIEW $name COMMENT 'demo view' AS VALUES 1,2\") assert(spark.catalog.tableExists(name))</p> <p>val q = spark.table(name)</p> <p>val analyzedPlan = q.queryExecution.analyzed scala&gt; println(analyzedPlan.numberedTreeString) 00 SubqueryAlias demo_view 01 +- View (<code>default</code>.<code>demo_view</code>, [col1#37]) 02    +- Project [cast(col1#38 as int) AS col1#37] 03       +- LocalRelation [col1#38]</p> <p>import org.apache.spark.sql.catalyst.analysis.EliminateView val afterEliminateView = EliminateView(analyzedPlan) // Notice no View operator scala&gt; println(afterEliminateView.numberedTreeString) 00 SubqueryAlias demo_view 01 +- Project [cast(col1#38 as int) AS col1#37] 02    +- LocalRelation [col1#38]</p>"},{"location":"logical-optimizations/EliminateView/#tip-you-may-also-want-to-use-eliminatesubqueryaliases-to-eliminate-subqueryaliases","title":"// TIP: You may also want to use EliminateSubqueryAliases to eliminate SubqueryAliases","text":""},{"location":"logical-optimizations/EliminateView/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> simply removes (eliminates) &lt;&gt; unary logical operators from the input &lt;&gt; and replaces them with their &lt;&gt; logical operator. <p><code>apply</code> throws an <code>AssertionError</code> when the &lt;&gt; of the <code>View</code> operator does not match the &lt;&gt; of the &lt;&gt; logical operator. <pre><code>assertion failed: The output of the child [output] is different from the view output [output]\n</code></pre> <p>NOTE: The assertion should not really happen since AliasViewChild logical analysis rule is executed earlier and takes care of not allowing for such difference in the output schema (by throwing an <code>AnalysisException</code> earlier).</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/ExtractPythonUDFFromAggregate/","title":"ExtractPythonUDFFromAggregate Logical Optimization","text":"<p><code>ExtractPythonUDFFromAggregate</code> is...FIXME</p>"},{"location":"logical-optimizations/ExtractPythonUDFFromAggregate/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/GetCurrentDatabase/","title":"GetCurrentDatabase Logical Optimization","text":"<p><code>GetCurrentDatabase</code> is a base logical optimization that &lt;&gt; for <code>current_database</code> SQL function. <p><code>GetCurrentDatabase</code> is part of the Finish Analysis once-executed batch in the standard batches of the Logical Optimizer.</p> <p><code>GetCurrentDatabase</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>. <pre><code>val q = sql(\"SELECT current_database() AS db\")\nval analyzedPlan = q.queryExecution.analyzed\n\nscala&gt; println(analyzedPlan.numberedTreeString)\n00 Project [current_database() AS db#22]\n01 +- OneRowRelation\n\nimport org.apache.spark.sql.catalyst.optimizer.GetCurrentDatabase\n\nval afterGetCurrentDatabase = GetCurrentDatabase(spark.sessionState.catalog)(analyzedPlan)\nscala&gt; println(afterGetCurrentDatabase.numberedTreeString)\n00 Project [default AS db#22]\n01 +- OneRowRelation\n</code></pre> <p>Note</p> <p><code>GetCurrentDatabase</code> corresponds to SQL's <code>current_database()</code> function.</p> <p>You can access the current database in Scala using</p> <pre><code>scala&gt; val database = spark.catalog.currentDatabase\ndatabase: String = default\n</code></pre>"},{"location":"logical-optimizations/GetCurrentDatabase/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/GroupBasedRowLevelOperationScanPlanning/","title":"GroupBasedRowLevelOperationScanPlanning Logical Optimization","text":"<p><code>GroupBasedRowLevelOperationScanPlanning</code> is a non-excludable logical optimization (<code>Rule[LogicalPlan]</code>) that is part of earlyScanPushDownRules of SparkOptimizer.</p>"},{"location":"logical-optimizations/GroupBasedRowLevelOperationScanPlanning/#executing-rule","title":"Executing Rule  Signature <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p>  <p><code>apply</code> transforms ReplaceData with DataSourceV2Relations in the given LogicalPlan.</p> <p><code>apply</code>...FIXME</p> <p><code>apply</code> prints out the following INFO message to the logs:</p> <pre><code>Pushing operators to [relationName]\nPushed filters: [pushedFilters]\nFilters that were not pushed: [remainingFilters]\nOutput: [output]\n</code></pre>","text":""},{"location":"logical-optimizations/GroupBasedRowLevelOperationScanPlanning/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.v2.GroupBasedRowLevelOperationScanPlanning</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.GroupBasedRowLevelOperationScanPlanning.name = org.apache.spark.sql.execution.datasources.v2.GroupBasedRowLevelOperationScanPlanning\nlogger.GroupBasedRowLevelOperationScanPlanning.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"logical-optimizations/InferFiltersFromConstraints/","title":"InferFiltersFromConstraints Logical Optimization Rule","text":"<p><code>InferFiltersFromConstraints</code> is a catalyst/Rule.md[Catalyst rule] for transforming spark-sql-LogicalPlan.md[logical plans] (i.e. <code>Rule[LogicalPlan]</code>).</p> <p>[[apply]] When catalyst/Rule.md#apply[executed], <code>InferFiltersFromConstraints</code> simply &lt;&gt; when spark.sql.constraintPropagation.enabled configuration property is enabled. <p><code>InferFiltersFromConstraints</code> is a part of the Infer Filters once-executed rule batch of the base Logical Optimizer.</p> <p>=== [[inferFilters]] <code>inferFilters</code> Internal Method</p>"},{"location":"logical-optimizations/InferFiltersFromConstraints/#source-scala","title":"[source, scala]","text":"<p>inferFilters(   plan: LogicalPlan): LogicalPlan</p> <p><code>inferFilters</code> supports &lt;&gt; and &lt;&gt; logical operators. <p>[[inferFilters-Filter]] For <code>Filter</code> logical operators, <code>inferFilters</code>...FIXME</p> <p>[[inferFilters-Join]] For Join.md[Join] logical operators, <code>inferFilters</code> branches off per the join type:</p> <ul> <li> <p>For InnerLike and LeftSemi...FIXME</p> </li> <li> <p>For RightOuter...FIXME</p> </li> <li> <p>For LeftOuter and LeftAnti, <code>inferFilters</code> &lt;&gt; and then replaces the right join operator with &lt;&gt; <p>NOTE: <code>inferFilters</code> is used when <code>InferFiltersFromConstraints</code> is &lt;&gt;. <p>=== [[getAllConstraints]] <code>getAllConstraints</code> Internal Method</p>"},{"location":"logical-optimizations/InferFiltersFromConstraints/#source-scala_1","title":"[source, scala]","text":"<p>getAllConstraints(   left: LogicalPlan,   right: LogicalPlan,   conditionOpt: Option[Expression]): Set[Expression]</p> <p><code>getAllConstraints</code>...FIXME</p> <p>NOTE: <code>getAllConstraints</code> is used when...FIXME</p> <p>=== [[inferNewFilter]] Adding Filter -- <code>inferNewFilter</code> Internal Method</p>"},{"location":"logical-optimizations/InferFiltersFromConstraints/#source-scala_2","title":"[source, scala]","text":"<p>inferNewFilter(   plan: LogicalPlan,   constraints: Set[Expression]): LogicalPlan</p> <p><code>inferNewFilter</code>...FIXME</p> <p>NOTE: <code>inferNewFilter</code> is used when...FIXME</p>"},{"location":"logical-optimizations/InjectRuntimeFilter/","title":"InjectRuntimeFilter Logical Optimization","text":"<p><code>InjectRuntimeFilter</code> is a logical optimization (i.e., a Rule of LogicalPlan).</p> <p><code>InjectRuntimeFilter</code> is part of InjectRuntimeFilter fixed-point batch of rules.</p>"},{"location":"logical-optimizations/InjectRuntimeFilter/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p>  <p><code>apply</code> tryInjectRuntimeFilter unless one of the following holds (and the rule is a noop):</p> <ul> <li>The given query plan is a correlated <code>Subquery</code></li> <li>spark.sql.optimizer.runtimeFilter.semiJoinReduction.enabled and spark.sql.optimizer.runtime.bloomFilter.enabled are both disabled</li> </ul>","text":""},{"location":"logical-optimizations/InjectRuntimeFilter/#tryinjectruntimefilter","title":"tryInjectRuntimeFilter <pre><code>tryInjectRuntimeFilter(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>tryInjectRuntimeFilter</code> finds equi-joins in the given LogicalPlan.</p> <p>When some requirements are met, <code>tryInjectRuntimeFilter</code> injectFilter on the left side first and on the right side if on the left was not successful.</p> <p><code>tryInjectRuntimeFilter</code> uses spark.sql.optimizer.runtimeFilter.number.threshold configuration property.</p>","text":""},{"location":"logical-optimizations/InjectRuntimeFilter/#injecting-filter-operator","title":"Injecting Filter Operator <pre><code>injectFilter(\n  filterApplicationSideExp: Expression,\n  filterApplicationSidePlan: LogicalPlan,\n  filterCreationSideExp: Expression,\n  filterCreationSidePlan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>injectFilter</code>...FIXME</p>","text":""},{"location":"logical-optimizations/InjectRuntimeFilter/#injecting-bloomfilter","title":"Injecting BloomFilter <pre><code>injectBloomFilter(\n  filterApplicationSideExp: Expression,\n  filterApplicationSidePlan: LogicalPlan,\n  filterCreationSideExp: Expression,\n  filterCreationSidePlan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>injectBloomFilter</code>...FIXME</p>  <p>Note</p> <p><code>injectBloomFilter</code> is used when <code>InjectRuntimeFilter</code> is requested to inject a Filter with spark.sql.optimizer.runtime.bloomFilter.enabled configuration properties enabled.</p>","text":""},{"location":"logical-optimizations/InjectRuntimeFilter/#injectinsubqueryfilter","title":"injectInSubqueryFilter <pre><code>injectInSubqueryFilter(\n  filterApplicationSideExp: Expression,\n  filterApplicationSidePlan: LogicalPlan,\n  filterCreationSideExp: Expression,\n  filterCreationSidePlan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>injectInSubqueryFilter</code>...FIXME</p>  <p>Note</p> <p><code>injectInSubqueryFilter</code> is used when <code>InjectRuntimeFilter</code> is requested to injectFilter with spark.sql.optimizer.runtime.bloomFilter.enabled configuration properties disabled (unlike spark.sql.optimizer.runtimeFilter.semiJoinReduction.enabled).</p>","text":""},{"location":"logical-optimizations/InlineCTE/","title":"InlineCTE Logical Optimization","text":"<p><code>InlineCTE</code> is a base logical optimization that...FIXME</p> <p><code>InlineCTE</code> is part of the Finish Analysis once-executed batch in the standard batches of the Logical Optimizer.</p> <p><code>InlineCTE</code> is a Catalyst Rule for transforming LogicalPlans (<code>Rule[LogicalPlan]</code>).</p>"},{"location":"logical-optimizations/InlineCTE/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> does nothing and simply returns the given LogicalPlan when applied to a <code>Subquery</code> or a non-CTE query plan. Otherwise, <code>apply</code> buildCTEMap followed by inlineCTE (with <code>forceInline</code> off).</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/InlineCTE/#buildctemap","title":"buildCTEMap <pre><code>buildCTEMap(\n  plan: LogicalPlan,\n  cteMap: mutable.HashMap[Long, (CTERelationDef, Int)]): Unit\n</code></pre> <p>For a WithCTE logical operator <code>buildCTEMap</code>...FIXME</p> <p>For a CTERelationRef logical operator <code>buildCTEMap</code>...FIXME</p>","text":""},{"location":"logical-optimizations/InlineCTE/#inlinecte","title":"inlineCTE <pre><code>inlineCTE(\n  plan: LogicalPlan,\n  cteMap: mutable.HashMap[Long, (CTERelationDef, Int)],\n  forceInline: Boolean): LogicalPlan\n</code></pre> <p><code>inlineCTE</code>...FIXME</p>","text":""},{"location":"logical-optimizations/LimitPushDown/","title":"LimitPushDown Logical Optimization","text":"<p><code>LimitPushDown</code> is a logical optimization to transform the following logical operators:</p> <ul> <li><code>LocalLimit</code> with <code>Union</code></li> <li><code>LocalLimit</code> with Join</li> </ul> <p><code>LimitPushDown</code> is a Catalyst rule for transforming logical plans (<code>Rule[LogicalPlan]</code>).</p> <p><code>LimitPushDown</code> is part of Operator Optimization before Inferring Filters and Operator Optimization after Inferring Filters batch of rules of Logical Optimizer.</p>"},{"location":"logical-optimizations/LimitPushDown/#creating-instance","title":"Creating Instance","text":"<p><code>LimitPushDown</code> takes no arguments to be created.</p> <p><code>LimitPushDown</code> is created when Logical Optimizer is requested for the default batches of rules.</p>"},{"location":"logical-optimizations/LimitPushDown/#executing-rule","title":"Executing Rule <pre><code>apply(\n   plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/LimitPushDown/#demo","title":"Demo <pre><code>// test datasets\nscala&gt; val ds1 = spark.range(4)\nds1: org.apache.spark.sql.Dataset[Long] = [value: bigint]\n\nscala&gt; val ds2 = spark.range(2)\nds2: org.apache.spark.sql.Dataset[Long] = [value: bigint]\n\n// Case 1. Rather than `LocalLimit` of `Union` do `Union` of `LocalLimit`\nscala&gt; ds1.union(ds2).limit(2).explain(true)\n== Parsed Logical Plan ==\nGlobalLimit 2\n+- LocalLimit 2\n   +- Union\n      :- Range (0, 4, step=1, splits=Some(8))\n      +- Range (0, 2, step=1, splits=Some(8))\n\n== Analyzed Logical Plan ==\nid: bigint\nGlobalLimit 2\n+- LocalLimit 2\n   +- Union\n      :- Range (0, 4, step=1, splits=Some(8))\n      +- Range (0, 2, step=1, splits=Some(8))\n\n== Optimized Logical Plan ==\nGlobalLimit 2\n+- LocalLimit 2\n   +- Union\n      :- LocalLimit 2\n      :  +- Range (0, 4, step=1, splits=Some(8))\n      +- LocalLimit 2\n         +- Range (0, 2, step=1, splits=Some(8))\n\n== Physical Plan ==\nCollectLimit 2\n+- Union\n   :- *LocalLimit 2\n   :  +- *Range (0, 4, step=1, splits=Some(8))\n   +- *LocalLimit 2\n      +- *Range (0, 2, step=1, splits=Some(8))\n</code></pre>","text":""},{"location":"logical-optimizations/NullPropagation/","title":"NullPropagation Logical Optimization -- Nullability (NULL Value) Propagation","text":"<p><code>NullPropagation</code> is a base logical optimization that &lt;&gt;. <p><code>NullPropagation</code> is part of the Operator Optimization before Inferring Filters fixed-point batch in the standard batches of the Logical Optimizer.</p> <p><code>NullPropagation</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>. <p>=== [[example-count-with-nullable-expressions-only]] Example: Count Aggregate Operator with Nullable Expressions Only</p> <p><code>NullPropagation</code> optimization rewrites <code>Count</code> aggregate expressions that include expressions that are all nullable to <code>Cast(Literal(0L))</code>.</p> <pre><code>val table = (0 to 9).toDF(\"num\").as[Int]\n\n// NullPropagation applied\nscala&gt; table.select(countDistinct($\"num\" === null)).explain(true)\n== Parsed Logical Plan ==\n'Project [count(distinct ('num = null)) AS count(DISTINCT (num = NULL))#45]\n+- Project [value#1 AS num#3]\n   +- LocalRelation [value#1]\n\n== Analyzed Logical Plan ==\ncount(DISTINCT (num = NULL)): bigint\nAggregate [count(distinct (num#3 = cast(null as int))) AS count(DISTINCT (num = NULL))#45L]\n+- Project [value#1 AS num#3]\n   +- LocalRelation [value#1]\n\n== Optimized Logical Plan ==\nAggregate [0 AS count(DISTINCT (num = NULL))#45L] // &lt;-- HERE\n+- LocalRelation\n\n== Physical Plan ==\n*HashAggregate(keys=[], functions=[], output=[count(DISTINCT (num = NULL))#45L])\n+- Exchange SinglePartition\n   +- *HashAggregate(keys=[], functions=[], output=[])\n      +- LocalTableScan\n</code></pre> <p>=== [[example-count-without-nullable-distinct-expressions]] Example: Count Aggregate Operator with Non-Nullable Non-Distinct Expressions</p> <p><code>NullPropagation</code> optimization rewrites any non-<code>nullable</code> non-distinct <code>Count</code> aggregate expressions to <code>Literal(1)</code>.</p> <pre><code>val table = (0 to 9).toDF(\"num\").as[Int]\n\n// NullPropagation applied\n// current_timestamp() is a non-nullable expression (see the note below)\nval query = table.select(count(current_timestamp()) as \"count\")\n\nscala&gt; println(query.queryExecution.optimizedPlan)\nAggregate [count(1) AS count#64L]\n+- LocalRelation\n\n// NullPropagation skipped\nval tokens = Seq((0, null), (1, \"hello\")).toDF(\"id\", \"word\")\nval query = tokens.select(count(\"word\") as \"count\")\n\nscala&gt; println(query.queryExecution.optimizedPlan)\nAggregate [count(word#55) AS count#71L]\n+- LocalRelation [word#55]\n</code></pre>"},{"location":"logical-optimizations/NullPropagation/#note","title":"[NOTE]","text":"<p><code>Count</code> aggregate expression represents <code>count</code> function internally.</p>"},{"location":"logical-optimizations/NullPropagation/#import-orgapachesparksqlcatalystexpressionsaggregatecount-import-orgapachesparksqlfunctionscount-scala-countexprchildren0asinstanceofcount-res0-orgapachesparksqlcatalystexpressionsaggregatecount-count1","title":"<pre><code>import org.apache.spark.sql.catalyst.expressions.aggregate.Count\nimport org.apache.spark.sql.functions.count\n\nscala&gt; count(\"*\").expr.children(0).asInstanceOf[Count]\nres0: org.apache.spark.sql.catalyst.expressions.aggregate.Count = count(1)\n</code></pre>","text":""},{"location":"logical-optimizations/NullPropagation/#note_1","title":"[NOTE]","text":"<p><code>current_timestamp()</code> function is non-<code>nullable</code> expression.</p>"},{"location":"logical-optimizations/NullPropagation/#source-scala","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.expressions.CurrentTimestamp import org.apache.spark.sql.functions.current_timestamp</p> <p>scala&gt; current_timestamp().expr.asInstanceOf[CurrentTimestamp].nullable res38: Boolean = false</p> <p>====</p> <p>=== [[example]] Example</p>"},{"location":"logical-optimizations/NullPropagation/#source-scala_1","title":"[source, scala]","text":"<p>val table = (0 to 9).toDF(\"num\").as[Int] val query = table.where('num === null)</p> <p>scala&gt; query.explain(extended = true) == Parsed Logical Plan == 'Filter ('num = null) +- Project [value#1 AS num#3]    +- LocalRelation [value#1]</p> <p>== Analyzed Logical Plan == num: int Filter (num#3 = cast(null as int)) +- Project [value#1 AS num#3]    +- LocalRelation [value#1]</p> <p>== Optimized Logical Plan == LocalRelation , [num#3] <p>== Physical Plan == LocalTableScan , [num#3]"},{"location":"logical-optimizations/NullPropagation/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/OptimizeIn/","title":"OptimizeIn Logical Optimization","text":"<p><code>OptimizeIn</code> is a base logical optimization that &lt;&gt; as follows: <ol> <li> <p>Replaces an <code>In</code> expression that has an empty list and the value expression not nullable to <code>false</code></p> </li> <li> <p>Eliminates duplicates of Literal expressions in an In predicate expression that is inSetConvertible</p> </li> <li> <p>Replaces an <code>In</code> predicate expression that is inSetConvertible with InSet expressions when the number of literal expressions in the list expression is greater than spark.sql.optimizer.inSetConversionThreshold internal configuration property</p> </li> </ol> <p><code>OptimizeIn</code> is part of the Operator Optimization before Inferring Filters fixed-point batch in the standard batches of the Logical Optimizer.</p> <p><code>OptimizeIn</code> is simply a Catalyst rule for transforming logical plans, i.e. <code>Rule[LogicalPlan]</code>.</p> <pre><code>// Use Catalyst DSL to define a logical plan\n\n// HACK: Disable symbolToColumn implicit conversion\n// It is imported automatically in spark-shell (and makes demos impossible)\n// implicit def symbolToColumn(s: Symbol): org.apache.spark.sql.ColumnName\ntrait ThatWasABadIdea\nimplicit def symbolToColumn(ack: ThatWasABadIdea) = ack\n\nimport org.apache.spark.sql.catalyst.dsl.expressions._\nimport org.apache.spark.sql.catalyst.dsl.plans._\n\nimport org.apache.spark.sql.catalyst.plans.logical.LocalRelation\nval rel = LocalRelation('a.int, 'b.int, 'c.int)\n\nimport org.apache.spark.sql.catalyst.expressions.{In, Literal}\nval plan = rel\n  .where(In('a, Seq[Literal](1, 2, 3)))\n  .analyze\nscala&gt; println(plan.numberedTreeString)\n00 Filter a#6 IN (1,2,3)\n01 +- LocalRelation &lt;empty&gt;, [a#6, b#7, c#8]\n\n// In --&gt; InSet\nspark.conf.set(\"spark.sql.optimizer.inSetConversionThreshold\", 0)\n\nimport org.apache.spark.sql.catalyst.optimizer.OptimizeIn\nval optimizedPlan = OptimizeIn(plan)\nscala&gt; println(optimizedPlan.numberedTreeString)\n00 Filter a#6 INSET (1,2,3)\n01 +- LocalRelation &lt;empty&gt;, [a#6, b#7, c#8]\n</code></pre>"},{"location":"logical-optimizations/OptimizeIn/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/OptimizeMetadataOnlyQuery/","title":"OptimizeMetadataOnlyQuery Logical Optimization","text":"<p><code>OptimizeMetadataOnlyQuery</code> is...FIXME</p>"},{"location":"logical-optimizations/OptimizeMetadataOnlyQuery/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/OptimizeSubqueries/","title":"OptimizeSubqueries Logical Optimization","text":"<p><code>OptimizeSubqueries</code> is a base logical optimization that &lt;&gt;. <p><code>OptimizeSubqueries</code> is part of the Subquery once-executed batch in the standard batches of the Logical Optimizer.</p> <p><code>OptimizeSubqueries</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>."},{"location":"logical-optimizations/OptimizeSubqueries/#source-scala","title":"[source, scala]","text":""},{"location":"logical-optimizations/OptimizeSubqueries/#fixme-demo","title":"// FIXME Demo","text":"<p>=== [[apply]] Executing Rule -- <code>apply</code> Method</p>"},{"location":"logical-optimizations/OptimizeSubqueries/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-optimizations/OptimizeSubqueries/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p>NOTE: <code>apply</code> is part of the &lt;&gt; to execute (apply) a rule on a &lt;&gt; (e.g. &lt;&gt;). <p><code>apply</code>...FIXME</p>"},{"location":"logical-optimizations/PartitionPruning/","title":"PartitionPruning Logical Optimization","text":"<p><code>PartitionPruning</code> is a logical optimization for Dynamic Partition Pruning.</p> <p><code>PartitionPruning</code> is a <code>Rule[LogicalPlan]</code> (a Catalyst Rule for logical operators).</p> <p><code>PartitionPruning</code> is part of the PartitionPruning batch of the SparkOptimizer.</p>"},{"location":"logical-optimizations/PartitionPruning/#executing-rule","title":"Executing Rule  Signature <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p>  <p><code>apply</code> is a noop (does nothing and returns the given LogicalPlan) when executed with one of the following:</p> <ul> <li><code>Subquery</code> operators that are <code>correlated</code></li> <li>spark.sql.optimizer.dynamicPartitionPruning.enabled configuration property is disabled</li> </ul> <p>Otherwise, when enabled, <code>apply</code> prunes the given LogicalPlan.</p>","text":""},{"location":"logical-optimizations/PartitionPruning/#pruning","title":"Pruning <pre><code>prune(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>prune</code> transforms up all logical operators in the given logical query plan.</p> <p><code>prune</code> skips Join logical operators (leaves unmodified) when either left or right child operators are <code>Filter</code>s with DynamicPruningSubquery condition.</p> <p><code>prune</code> transforms Join operators with EqualTo join conditions.</p>  FIXME More Work Needed <p><code>prune</code> needs more love and would benefit from more insight on how it works.</p>","text":""},{"location":"logical-optimizations/PartitionPruning/#getfilterabletablescan","title":"getFilterableTableScan <pre><code>getFilterableTableScan(\n  a: Expression,\n  plan: LogicalPlan): Option[LogicalPlan]\n</code></pre> <p><code>getFilterableTableScan</code> findExpressionAndTrackLineageDown (that finds a LeafNode with the output schema that includes all the Attribute references of the given Expression).</p>  <p>Leaf Nodes</p> <p><code>getFilterableTableScan</code> is only interested in the following leaf logical operators:</p> <ul> <li>DataSourceV2ScanRelation over <code>SupportsRuntimeFiltering</code> scans</li> <li>HiveTableRelation</li> <li>LogicalRelation over HadoopFsRelation</li> </ul>  <p><code>getFilterableTableScan</code>...FIXME</p>","text":""},{"location":"logical-optimizations/PartitionPruning/#logicalrelation-over-partitioned-hadoopfsrelation","title":"LogicalRelation over (Partitioned) HadoopFsRelation <p>For LogicalRelation with (the relation that is) a partitioned HadoopFsRelation, <code>getFilterableTableScan</code> checks if the references (of the given Expression) are all among the partition columns.</p> <p>If so, <code>getFilterableTableScan</code> returns the <code>LogicalRelation</code> with the partitioned <code>HadoopFsRelation</code>.</p>","text":""},{"location":"logical-optimizations/PartitionPruning/#haspartitionpruningfilter","title":"hasPartitionPruningFilter <pre><code>hasPartitionPruningFilter(\n  plan: LogicalPlan): Boolean\n</code></pre>  <p>Note</p> <p><code>hasPartitionPruningFilter</code> is hasSelectivePredicate with a streaming check to make sure it disregards streaming queries.</p>  <p><code>hasPartitionPruningFilter</code> is <code>true</code> when all of the following hold true:</p> <ol> <li>The given LogicalPlan is not streaming</li> <li>hasSelectivePredicate</li> </ol>","text":""},{"location":"logical-optimizations/PartitionPruning/#hasselectivepredicate","title":"hasSelectivePredicate <pre><code>hasSelectivePredicate(\n  plan: LogicalPlan): Boolean\n</code></pre> <p><code>hasSelectivePredicate</code> is <code>true</code> when there is a <code>Filter</code> logical operator with a likely-selective filter condition.</p>","text":""},{"location":"logical-optimizations/PartitionPruning/#inserting-predicate-with-dynamicpruningsubquery-expression","title":"Inserting Predicate with DynamicPruningSubquery Expression <pre><code>insertPredicate(\n  pruningKey: Expression,\n  pruningPlan: LogicalPlan,\n  filteringKey: Expression,\n  filteringPlan: LogicalPlan,\n  joinKeys: Seq[Expression],\n  partScan: LogicalPlan): LogicalPlan\n</code></pre> <p>With spark.sql.exchange.reuse enabled and pruningHasBenefit, <code>insertPredicate</code> creates (inserts into the given pruning plan) a <code>Filter</code> logical operator with a DynamicPruningSubquery expression (with onlyInBroadcast flag based on spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly and pruningHasBenefit).</p> <p>Otherwise, <code>insertPredicate</code> returns the given <code>pruningPlan</code> logical query plan unchanged.</p>  <p>Configuration Properties</p> <p><code>insertPredicate</code> is configured using the following:</p> <ul> <li>spark.sql.exchange.reuse</li> <li>spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly</li> </ul>","text":""},{"location":"logical-optimizations/PartitionPruning/#pruninghasbenefit","title":"pruningHasBenefit <pre><code>pruningHasBenefit(\n  partExpr: Expression,\n  partPlan: LogicalPlan,\n  otherExpr: Expression,\n  otherPlan: LogicalPlan): Boolean\n</code></pre>  <p>Column Statistics</p> <p><code>pruningHasBenefit</code> uses Column Statistics (for the number of distinct values), if available and spark.sql.optimizer.dynamicPartitionPruning.useStats is enabled.</p>  <p><code>pruningHasBenefit</code> computes a filtering ratio based on the columns (references) in the given <code>partExpr</code> and <code>otherExpr</code> expressions.</p>  <p>One Column Reference Only</p> <p><code>pruningHasBenefit</code> supports one column reference only in the given <code>partExpr</code> and <code>otherExpr</code> expressions.</p>  <p>With spark.sql.optimizer.dynamicPartitionPruning.useStats enabled, <code>pruningHasBenefit</code> uses the Distinct Count statistic (CBO stats) for each attribute (in the join condition).</p> <p>The filtering ratio is the ratio of Distinct Count of <code>rightAttr</code> to Distinct Count of <code>leftAttr</code> (remaining of 1) unless:</p> <ol> <li>Distinct Count are not available or <code>leftAttr</code>'s <code>Distinct Count</code> is <code>0</code> or negative</li> <li>Distinct Count of <code>leftAttr</code> is the same or lower than of <code>otherDistinctCount</code></li> <li>There are more than one attribute in <code>partExpr</code> or <code>otherExpr</code> expressions</li> </ol> <p>For such cases, the filtering ratio is spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio.</p> <p><code>pruningHasBenefit</code> calculates <code>estimatePruningSideSize</code> as the filtering ratio of sizeInBytes statistic of the given <code>partPlan</code>.</p> <p><code>pruningHasBenefit</code> is enabled (<code>true</code>) when <code>estimatePruningSideSize</code> is greater than calculatePlanOverhead of the given <code>otherPlan</code> logical plan.</p>","text":""},{"location":"logical-optimizations/PropagateEmptyRelation/","title":"PropagateEmptyRelation Logical Optimization","text":"<p><code>PropagateEmptyRelation</code> is a base logical optimization that &lt;&gt;, e.g. &lt;&gt; or &lt;&gt;. <p><code>PropagateEmptyRelation</code> is part of the LocalRelation fixed-point batch in the standard batches of the Logical Optimizer.</p> <p><code>PropagateEmptyRelation</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>. <p>=== [[explode]] Explode</p>"},{"location":"logical-optimizations/PropagateEmptyRelation/#source-scala","title":"[source, scala]","text":"<p>scala&gt; val emp = spark.emptyDataset[Seq[String]] emp: org.apache.spark.sql.Dataset[Seq[String]] = [value: array] <p>scala&gt; emp.select(explode($\"value\")).show +---+ |col| +---+ +---+</p> <p>scala&gt; emp.select(explode($\"value\")).explain(true) == Parsed Logical Plan == 'Project [explode('value) AS List()] +- LocalRelation , [value#77] <p>== Analyzed Logical Plan == col: string Project [col#89] +- Generate explode(value#77), false, false, [col#89]    +- LocalRelation , [value#77] <p>== Optimized Logical Plan == LocalRelation , [col#89] <p>== Physical Plan == LocalTableScan , [col#89] <p>=== [[join]] Join</p>"},{"location":"logical-optimizations/PropagateEmptyRelation/#source-scala_1","title":"[source, scala]","text":"<p>scala&gt; spark.emptyDataset[Int].join(spark.range(1)).explain(extended = true) ... TRACE SparkOptimizer: === Applying Rule org.apache.spark.sql.catalyst.optimizer.PropagateEmptyRelation === !Join Inner                                LocalRelation , [value#40, id#42L] !:- LocalRelation , [value#40] !+- Range (0, 1, step=1, splits=Some(8)) <p>TRACE SparkOptimizer: Fixed point reached for batch LocalRelation after 2 iterations. DEBUG SparkOptimizer: === Result of Batch LocalRelation === !Join Inner                                LocalRelation , [value#40, id#42L] !:- LocalRelation , [value#40] !+- Range (0, 1, step=1, splits=Some(8)) ... == Parsed Logical Plan == Join Inner :- LocalRelation , [value#40] +- Range (0, 1, step=1, splits=Some(8)) <p>== Analyzed Logical Plan == value: int, id: bigint Join Inner :- LocalRelation , [value#40] +- Range (0, 1, step=1, splits=Some(8)) <p>== Optimized Logical Plan == LocalRelation , [value#40, id#42L] <p>== Physical Plan == LocalTableScan , [value#40, id#42L]"},{"location":"logical-optimizations/PropagateEmptyRelation/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/PruneFileSourcePartitions/","title":"PruneFileSourcePartitions Logical Optimization","text":"<p><code>PruneFileSourcePartitions</code> is the only logical optimization rule in the Prune File Source Table Partitions batch of the SparkOptimizer.</p> <p><code>PruneFileSourcePartitions</code> &lt;&gt; into a Project.md[Project] operator with a <code>Filter</code> logical operator over a \"pruned\" <code>LogicalRelation</code> with the HadoopFsRelation of a Hive partitioned table (with a PrunedInMemoryFileIndex.md[PrunedInMemoryFileIndex]). <p>=== [[apply]] Executing Rule -- <code>apply</code> Method</p>"},{"location":"logical-optimizations/PruneFileSourcePartitions/#source-scala","title":"[source, scala]","text":"<p>apply(   plan: LogicalPlan): LogicalPlan</p> <p><code>apply</code> PhysicalOperation.md#unapply[destructures the input logical plan] into a tuple of projection and filter expressions together with a leaf logical operator.</p> <p><code>apply</code> transforms LogicalRelation.md[LogicalRelations] with a HadoopFsRelation and a CatalogFileIndex (i.e. for Hive tables) when there are filter expressions defined and the Hive table is partitioned.</p> <p><code>apply</code> resolves partition column references (by requesting the logical operator to spark-sql-LogicalPlan.md#resolve[resolve partition column attributes to concrete references in the query plan]) and excludes spark-sql-Expression-SubqueryExpression.md#hasSubquery[subquery expressions].</p> <p>If there are no predicates (filter expressions) left for partition pruning, <code>apply</code> simply does nothing more and returns the input logical query untouched.</p> <p>With predicates left for partition pruning, <code>apply</code> requests the CatalogFileIndex for the partitions by the predicate expressions (that gives a PrunedInMemoryFileIndex for a partitioned table).</p> <p><code>apply</code> replaces the FileIndex in the HadoopFsRelation with the <code>PrunedInMemoryFileIndex</code> and the CatalogStatistics.md#sizeInBytes[total size] statistic to the PartitioningAwareFileIndex.md#sizeInBytes[PrunedInMemoryFileIndex's].</p> <p>In the end, <code>apply</code> creates a <code>Filter</code> logical operator (with the \"pruned\" <code>LogicalRelation</code> as a child operator and all the filter predicate expressions combined together with <code>And</code> expression) and makes it a child operator of a Project.md[Project] operator.</p> <p><code>apply</code> is part of the Rule abstraction.</p>"},{"location":"logical-optimizations/PruneFilters/","title":"PruneFilters","text":"<p><code>PruneFilters</code> is...FIXME</p>"},{"location":"logical-optimizations/PruneHiveTablePartitions/","title":"PruneHiveTablePartitions Logical Optimization","text":"<p><code>PruneHiveTablePartitions</code> is a logical optimization for partitioned Hive tables.</p>"},{"location":"logical-optimizations/PruneHiveTablePartitions/#creating-instance","title":"Creating Instance","text":"<p><code>PruneHiveTablePartitions</code> takes the following to be created:</p> <ul> <li> SparkSession <p><code>PruneHiveTablePartitions</code> is created when:</p> <ul> <li><code>HiveSessionStateBuilder</code> is requested to customEarlyScanPushDownRules</li> </ul>"},{"location":"logical-optimizations/PruneHiveTablePartitions/#updatetablemeta","title":"updateTableMeta <pre><code>updateTableMeta(\n  relation: HiveTableRelation,\n  prunedPartitions: Seq[CatalogTablePartition],\n  partitionKeyFilters: ExpressionSet): CatalogTable\n</code></pre> <p><code>updateTableMeta</code>...FIXME</p>  <p><code>updateTableMeta</code> is used when:</p> <ul> <li><code>PruneHiveTablePartitions</code> is executed (for a partitioned HiveTableRelation logical operator under a filter)</li> </ul>","text":""},{"location":"logical-optimizations/PullupCorrelatedPredicates/","title":"PullupCorrelatedPredicates Logical Optimization","text":"<p><code>PullupCorrelatedPredicates</code> is a base logical optimization that &lt;&gt; with the following operators: <p>. <code>Filter</code> operators with an Aggregate.md[Aggregate] child operator</p> <p>. <code>UnaryNode</code> operators</p> <p><code>PullupCorrelatedPredicates</code> is part of the Pullup Correlated Expressions once-executed batch in the standard batches of the Logical Optimizer.</p> <p><code>PullupCorrelatedPredicates</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>."},{"location":"logical-optimizations/PullupCorrelatedPredicates/#source-scala","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.optimizer.PullupCorrelatedPredicates</p> <p>// FIXME // Demo: Filter + Aggregate // Demo: Filter + UnaryNode</p> <p>val plan = ??? val optimizedPlan = PullupCorrelatedPredicates(plan)</p> <p><code>PullupCorrelatedPredicates</code> uses PredicateHelper.</p> <p>=== [[pullOutCorrelatedPredicates]] <code>pullOutCorrelatedPredicates</code> Internal Method</p>"},{"location":"logical-optimizations/PullupCorrelatedPredicates/#source-scala_1","title":"[source, scala]","text":"<p>pullOutCorrelatedPredicates(   sub: LogicalPlan,   outer: Seq[LogicalPlan]): (LogicalPlan, Seq[Expression])</p> <p><code>pullOutCorrelatedPredicates</code>...FIXME</p> <p>NOTE: <code>pullOutCorrelatedPredicates</code> is used exclusively when <code>PullupCorrelatedPredicates</code> is requested to &lt;&gt;. <p>=== [[rewriteSubQueries]] <code>rewriteSubQueries</code> Internal Method</p>"},{"location":"logical-optimizations/PullupCorrelatedPredicates/#source-scala_2","title":"[source, scala]","text":""},{"location":"logical-optimizations/PullupCorrelatedPredicates/#rewritesubqueriesplan-logicalplan-outerplans-seqlogicalplan-logicalplan","title":"rewriteSubQueries(plan: LogicalPlan, outerPlans: Seq[LogicalPlan]): LogicalPlan","text":"<p><code>rewriteSubQueries</code>...FIXME</p> <p>NOTE: <code>rewriteSubQueries</code> is used exclusively when <code>PullupCorrelatedPredicates</code> is &lt;&gt; (i.e. applied to a spark-sql-LogicalPlan.md[logical plan]). <p>=== [[apply]] Executing Rule -- <code>apply</code> Method</p>"},{"location":"logical-optimizations/PullupCorrelatedPredicates/#source-scala_3","title":"[source, scala]","text":""},{"location":"logical-optimizations/PullupCorrelatedPredicates/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p>NOTE: <code>apply</code> is part of the &lt;&gt; to execute (apply) a rule on a TreeNode (e.g. &lt;&gt;). <p><code>apply</code> transforms the input spark-sql-LogicalPlan.md[logical plan] as follows:</p> <p>. For <code>Filter</code> operators with an Aggregate.md[Aggregate] child operator, <code>apply</code> &lt;&gt; with the <code>Filter</code> and the <code>Aggregate</code> and its Aggregate.md#child[child] as the outer plans <p>. For spark-sql-LogicalPlan.md#UnaryNode[UnaryNode] operators, <code>apply</code> &lt;&gt; with the operator and its children as the outer plans"},{"location":"logical-optimizations/PushDownLeftSemiAntiJoin/","title":"PushDownLeftSemiAntiJoin Logical Optimization","text":"<p><code>PushDownLeftSemiAntiJoin</code> is...FIXME</p>"},{"location":"logical-optimizations/PushDownOperatorsToDataSource/","title":"PushDownOperatorsToDataSource Logical Optimization","text":"<p><code>PushDownOperatorsToDataSource</code> is a logical optimization that &lt;&gt; (i.e. &lt;&gt;) (before planning so that data source can report statistics more accurately). <p>Technically, <code>PushDownOperatorsToDataSource</code> is a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>. <p><code>PushDownOperatorsToDataSource</code> is part of the Push down operators to data source scan once-executed rule batch of the SparkOptimizer.</p> <p>=== [[apply]] Executing Rule -- <code>apply</code> Method</p>"},{"location":"logical-optimizations/PushDownOperatorsToDataSource/#source-scala","title":"[source, scala]","text":""},{"location":"logical-optimizations/PushDownOperatorsToDataSource/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p> <p>=== [[pushDownRequiredColumns]] <code>pushDownRequiredColumns</code> Internal Method</p>"},{"location":"logical-optimizations/PushDownOperatorsToDataSource/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-optimizations/PushDownOperatorsToDataSource/#pushdownrequiredcolumnsplan-logicalplan-requiredbyparent-attributeset-logicalplan","title":"pushDownRequiredColumns(plan: LogicalPlan, requiredByParent: AttributeSet): LogicalPlan","text":"<p><code>pushDownRequiredColumns</code> branches off per the input &lt;&gt; (that is supposed to have at least one child node): <p>. For &lt;&gt; unary logical operator, <code>pushDownRequiredColumns</code> takes the &lt;&gt; of the &lt;&gt; as the required columns (attributes) and executes itself recursively on the &lt;&gt; + Note that the input <code>requiredByParent</code> attributes are not considered in the required columns. <p>. For <code>Filter</code> unary logical operator, <code>pushDownRequiredColumns</code> adds the &lt;&gt; of the filter condition to the input <code>requiredByParent</code> attributes and executes itself recursively on the child logical operator <p>. For &lt;&gt; unary logical operator, <code>pushDownRequiredColumns</code>...FIXME <p>. For other logical operators, <code>pushDownRequiredColumns</code> simply executes itself (using TreeNode.mapChildren) recursively on the child nodes (logical operators)</p> <p><code>pushDownRequiredColumns</code> is used when <code>PushDownOperatorsToDataSource</code> logical optimization is requested to &lt;&gt;. <p>=== [[FilterAndProject]][[unapply]] Destructuring Logical Operator -- <code>FilterAndProject.unapply</code> Method</p>"},{"location":"logical-optimizations/PushDownOperatorsToDataSource/#source-scala_2","title":"[source, scala]","text":""},{"location":"logical-optimizations/PushDownOperatorsToDataSource/#unapplyplan-logicalplan-optionseqnamedexpression-expression-datasourcev2relation","title":"unapply(plan: LogicalPlan): Option[(Seq[NamedExpression], Expression, DataSourceV2Relation)]","text":"<p><code>unapply</code> is part of <code>FilterAndProject</code> extractor object to destructure the input &lt;&gt; into a tuple with...FIXME <p><code>unapply</code> works with (matches) the following logical operators:</p> <p>. For a <code>Filter</code> with a &lt;&gt; leaf logical operator, <code>unapply</code>...FIXME <p>. For a <code>Filter</code> with a &lt;&gt; over a &lt;&gt; leaf logical operator, <code>unapply</code>...FIXME <p>. For others, <code>unapply</code> returns <code>None</code> (i.e. does nothing / does not match)</p> <p>NOTE: <code>unapply</code> is used exclusively when <code>PushDownOperatorsToDataSource</code> logical optimization is requested to &lt;&gt;."},{"location":"logical-optimizations/PushDownPredicate/","title":"PushDownPredicate Logical Optimization","text":"<p>Danger</p> <p><code>PushDownPredicate</code> no longer exists in Spark SQL and is being migrated to PushDownPredicates logical optimization.</p>"},{"location":"logical-optimizations/PushDownPredicate/#review-me","title":"Review Me","text":"<p>When you execute Dataset.md#where[where] or Dataset.md#filter[filter] operators right after loading a dataset, Spark SQL will try to push the where/filter predicate down to the data source using a corresponding SQL query with <code>WHERE</code> clause (or whatever the proper language for the data source is).</p> <p>This optimization is called filter pushdown or predicate pushdown and aims at pushing down the filtering to the \"bare metal\", i.e. a data source engine. That is to increase the performance of queries since the filtering is performed at the very low level rather than dealing with the entire dataset after it has been loaded to Spark's memory and perhaps causing memory issues.</p> <p><code>PushDownPredicate</code> is also applied to structured queries with filters after projections or filtering on window partitions.</p>"},{"location":"logical-optimizations/PushDownPredicate/#pushing-filter-operator-down-using-projection","title":"Pushing Filter Operator Down Using Projection <pre><code>val dataset = spark.range(2)\n\nscala&gt; dataset.select('id as \"_id\").filter('_id === 0).explain(extended = true)\n...\nTRACE SparkOptimizer:\n=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushDownPredicate ===\n!Filter (_id#14L = cast(0 as bigint))         Project [id#11L AS _id#14L]\n!+- Project [id#11L AS _id#14L]               +- Filter (id#11L = cast(0 as bigint))\n    +- Range (0, 2, step=1, splits=Some(8))      +- Range (0, 2, step=1, splits=Some(8))\n...\n== Parsed Logical Plan ==\n'Filter ('_id = 0)\n+- Project [id#11L AS _id#14L]\n   +- Range (0, 2, step=1, splits=Some(8))\n\n== Analyzed Logical Plan ==\n_id: bigint\nFilter (_id#14L = cast(0 as bigint))\n+- Project [id#11L AS _id#14L]\n   +- Range (0, 2, step=1, splits=Some(8))\n\n== Optimized Logical Plan ==\nProject [id#11L AS _id#14L]\n+- Filter (id#11L = 0)\n   +- Range (0, 2, step=1, splits=Some(8))\n\n== Physical Plan ==\n*Project [id#11L AS _id#14L]\n+- *Filter (id#11L = 0)\n   +- *Range (0, 2, step=1, splits=Some(8))\n</code></pre>","text":""},{"location":"logical-optimizations/PushDownPredicate/#optimizing-window-aggregate-operators","title":"Optimizing Window Aggregate Operators <pre><code>val dataset = spark.range(5).withColumn(\"group\", 'id % 3)\nscala&gt; dataset.show\n+---+-----+\n| id|group|\n+---+-----+\n|  0|    0|\n|  1|    1|\n|  2|    2|\n|  3|    0|\n|  4|    1|\n+---+-----+\n\nimport org.apache.spark.sql.expressions.Window\nval groupW = Window.partitionBy('group).orderBy('id)\n\n// Filter out group 2 after window\n// No need to compute rank for group 2\n// Push the filter down\nval ranked = dataset.withColumn(\"rank\", rank over groupW).filter('group !== 2)\n\nscala&gt; ranked.queryExecution.optimizedPlan\n...\nTRACE SparkOptimizer:\n=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushDownPredicate ===\n!Filter NOT (group#35L = cast(2 as bigint))                                                                                                                            Project [id#32L, group#35L, rank#203]\n!+- Project [id#32L, group#35L, rank#203]                                                                                                                              +- Project [id#32L, group#35L, rank#203, rank#203]\n!   +- Project [id#32L, group#35L, rank#203, rank#203]                                                                                                                    +- Window [rank(id#32L) windowspecdefinition(group#35L, id#32L ASC, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank#203], [group#35L], [id#32L ASC]\n!      +- Window [rank(id#32L) windowspecdefinition(group#35L, id#32L ASC, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank#203], [group#35L], [id#32L ASC]         +- Project [id#32L, group#35L]\n!         +- Project [id#32L, group#35L]                                                                                                                                        +- Project [id#32L, (id#32L % cast(3 as bigint)) AS group#35L]\n!            +- Project [id#32L, (id#32L % cast(3 as bigint)) AS group#35L]                                                                                                        +- Filter NOT ((id#32L % cast(3 as bigint)) = cast(2 as bigint))\n                +- Range (0, 5, step=1, splits=Some(8))                                                                                                                               +- Range (0, 5, step=1, splits=Some(8))\n...\nres1: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =\nWindow [rank(id#32L) windowspecdefinition(group#35L, id#32L ASC, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank#203], [group#35L], [id#32L ASC]\n+- Project [id#32L, (id#32L % 3) AS group#35L]\n   +- Filter NOT ((id#32L % 3) = 2)\n      +- Range (0, 5, step=1, splits=Some(8))\n</code></pre>","text":""},{"location":"logical-optimizations/PushDownPredicate/#jdbc-data-source","title":"JDBC Data Source <p>Given the following code:</p> <pre><code>// Start with the PostgreSQL driver on CLASSPATH\n\ncase class Project(id: Long, name: String, website: String)\n\n// No optimizations for typed queries\n// LOG:  execute &lt;unnamed&gt;: SELECT \"id\",\"name\",\"website\" FROM projects\nval df = spark.read\n  .format(\"jdbc\")\n  .option(\"url\", \"jdbc:postgresql:sparkdb\")\n  .option(\"dbtable\", \"projects\")\n  .load()\n  .as[Project]\n  .filter(_.name.contains(\"Spark\"))\n\n// Only the following would end up with the pushdown\nval df = spark.read\n  .format(\"jdbc\")\n  .option(\"url\", \"jdbc:postgresql:sparkdb\")\n  .option(\"dbtable\", \"projects\")\n  .load()\n  .where(\"\"\"name like \"%Spark%\"\"\"\")\n</code></pre> <p><code>PushDownPredicate</code> translates the above query to the following SQL query:</p> <pre><code>LOG:  execute &lt;unnamed&gt;: SELECT \"id\",\"name\",\"website\" FROM projects WHERE (name LIKE '%Spark%')\n</code></pre>  <p>Tip</p> <p>Enable <code>all</code> logs in PostgreSQL to see the above SELECT and other query statements.</p> <pre><code>log_statement = 'all'\n</code></pre> <p>Add <code>log_statement = 'all'</code> to <code>/usr/local/var/postgres/postgresql.conf</code> on Mac OS X with PostgreSQL installed using <code>brew</code>.</p>","text":""},{"location":"logical-optimizations/PushDownPredicate/#parquet-data-source","title":"Parquet Data Source <pre><code>val spark: SparkSession = ...\nimport spark.implicits._\n\n// paste it to REPL individually to make the following line work\ncase class City(id: Long, name: String)\n\nimport org.apache.spark.sql.SaveMode.Overwrite\nSeq(\n  City(0, \"Warsaw\"),\n  City(1, \"Toronto\"),\n  City(2, \"London\"),\n  City(3, \"Redmond\"),\n  City(4, \"Boston\")).toDF.write.mode(Overwrite).parquet(\"cities.parquet\")\n\nval cities = spark.read.parquet(\"cities.parquet\").as[City]\n\n// Using DataFrame's Column-based query\nscala&gt; cities.where('name === \"Warsaw\").queryExecution.executedPlan\nres21: org.apache.spark.sql.execution.SparkPlan =\n*Project [id#128L, name#129]\n+- *Filter (isnotnull(name#129) &amp;&amp; (name#129 = Warsaw))\n   +- *FileScan parquet [id#128L,name#129] Batched: true, Format: ParquetFormat, InputPaths: file:/Users/jacek/dev/oss/spark/cities.parquet, PartitionFilters: [], PushedFilters: [IsNotNull(name), EqualTo(name,Warsaw)], ReadSchema: struct&lt;id:bigint,name:string&gt;\n\n// Using SQL query\nscala&gt; cities.where(\"\"\"name = \"Warsaw\"\"\"\").queryExecution.executedPlan\nres23: org.apache.spark.sql.execution.SparkPlan =\n*Project [id#128L, name#129]\n+- *Filter (isnotnull(name#129) &amp;&amp; (name#129 = Warsaw))\n   +- *FileScan parquet [id#128L,name#129] Batched: true, Format: ParquetFormat, InputPaths: file:/Users/jacek/dev/oss/spark/cities.parquet, PartitionFilters: [], PushedFilters: [IsNotNull(name), EqualTo(name,Warsaw)], ReadSchema: struct&lt;id:bigint,name:string&gt;\n\n// Using Dataset's strongly type-safe filter\n// Why does the following not push the filter down?\nscala&gt; cities.filter(_.name == \"Warsaw\").queryExecution.executedPlan\nres24: org.apache.spark.sql.execution.SparkPlan =\n*Filter &lt;function1&gt;.apply\n+- *FileScan parquet [id#128L,name#129] Batched: true, Format: ParquetFormat, InputPaths: file:/Users/jacek/dev/oss/spark/cities.parquet, PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint,name:string&gt;\n</code></pre>","text":""},{"location":"logical-optimizations/PushDownPredicates/","title":"PushDownPredicates Logical Optimization","text":"<p><code>PushDownPredicates</code> is a logical optimization (of the Logical Optimizer and the SparkOptimizer).</p> <p><code>PushDownPredicates</code> is a Catalyst rule for transforming logical plans (<code>Rule[LogicalPlan]</code>).</p>"},{"location":"logical-optimizations/PushDownPredicates/#creating-instance","title":"Creating Instance","text":"<p><code>PushDownPredicates</code> takes no arguments to be created (and is a Scala <code>object</code>).</p>"},{"location":"logical-optimizations/PushDownPredicates/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> requests the given LogicalPlan to transformWithPruning operators with FILTER or JOIN tree patterns.</p> <p><code>apply</code>...FIXME (migrate PushDownPredicate logical optimization)</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/PushPredicateThroughJoin/","title":"PushPredicateThroughJoin Logical Optimization","text":"<p><code>PushPredicateThroughJoin</code> is a catalyst/Rule.md[Catalyst rule] for transforming spark-sql-LogicalPlan.md[logical plans] (i.e. <code>Rule[LogicalPlan]</code>).</p> <p>When &lt;&gt;, <code>PushPredicateThroughJoin</code>...FIXME <p><code>PushPredicateThroughJoin</code> is a part of the Operator Optimization before Inferring Filters and Operator Optimization after Inferring Filters fixed-point rule batches of the base Logical Optimizer.</p> <p>[[demo]] .Demo: PushPredicateThroughJoin <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\nimport org.apache.spark.sql.catalyst.dsl.plans._\n\n// Using hacks to disable two Catalyst DSL implicits\nimplicit def symbolToColumn(ack: ThatWasABadIdea) = ack\nimplicit class StringToColumn(val sc: StringContext) {}\n\nimport org.apache.spark.sql.catalyst.plans.logical.LocalRelation\nval t1 = LocalRelation('a.int, 'b.int)\nval t2 = LocalRelation('C.int, 'D.int).where('C &gt; 10)\n\nval plan = t1.join(t2)...FIXME\n\nimport org.apache.spark.sql.catalyst.optimizer.PushPredicateThroughJoin\nval optimizedPlan = PushPredicateThroughJoin(plan)\nscala&gt; println(optimizedPlan.numberedTreeString)\n...FIXME\n</code></pre></p> <p>=== [[apply]] Executing Rule -- <code>apply</code> Method</p>"},{"location":"logical-optimizations/PushPredicateThroughJoin/#source-scala","title":"[source, scala]","text":"<p>apply(   plan: LogicalPlan): LogicalPlan</p> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p> <p>=== [[split]] <code>split</code> Internal Method</p>"},{"location":"logical-optimizations/PushPredicateThroughJoin/#source-scala_1","title":"[source, scala]","text":"<p>split(   condition: Seq[Expression],   left: LogicalPlan,   right: LogicalPlan): (Seq[Expression], Seq[Expression], Seq[Expression])</p> <p><code>split</code> splits (partitions) the given condition expressions into expressions/Expression.md#deterministic[deterministic] or not.</p> <p><code>split</code> further splits (partitions) the deterministic expressions (pushDownCandidates) into expressions that reference the catalyst/QueryPlan.md#outputSet[output expressions] of the left logical operator (leftEvaluateCondition) or not (rest).</p> <p><code>split</code> further splits (partitions) the expressions that do not reference left output expressions into expressions that reference the catalyst/QueryPlan.md#outputSet[output expressions] of the right logical operator (rightEvaluateCondition) or not (commonCondition).</p> <p>In the end, <code>split</code> returns the leftEvaluateCondition, rightEvaluateCondition, and commonCondition with the non-deterministic condition expressions.</p> <p>NOTE: <code>split</code> is used when <code>PushPredicateThroughJoin</code> is &lt;&gt;."},{"location":"logical-optimizations/ReorderJoin/","title":"ReorderJoin Logical Optimization -- Reordering Inner and Cross Joins","text":"<p><code>ReorderJoin</code> is a logical optimization for &lt;&gt;. <p><code>ReorderJoin</code> &lt;&gt; the join optimizations on a logical plan with 2 or more inner and cross joins with at least one join condition. <p>CAUTION: FIXME A diagram of a logical plan tree before and after the rule.</p> <p>Technically, <code>ReorderJoin</code> is a catalyst/Rule.md[Catalyst rule] for transforming spark-sql-LogicalPlan.md[logical plans], i.e. <code>Rule[LogicalPlan]</code>.</p>"},{"location":"logical-optimizations/ReorderJoin/#source-scala","title":"[source, scala]","text":"<p>// Build analyzed logical plan with at least 3 joins and zero or more filters val belowBroadcastJoinThreshold = spark.sessionState.conf.autoBroadcastJoinThreshold - 1 val belowBroadcast = spark.range(belowBroadcastJoinThreshold) val large = spark.range(2 * belowBroadcastJoinThreshold) val tiny = Seq(1,2,3,4,5).toDF(\"id\")</p> <p>val q = belowBroadcast.   crossJoin(large). // \u2190 CROSS JOIN of two fairly big datasets   join(tiny).   where(belowBroadcast(\"id\") === tiny(\"id\")) val plan = q.queryExecution.analyzed scala&gt; println(plan.numberedTreeString) 00 Filter (id#0L = cast(id#9 as bigint)) 01 +- Join Inner 02    :- Join Cross 03    :  :- Range (0, 10485759, step=1, splits=Some(8)) 04    :  +- Range (0, 20971518, step=1, splits=Some(8)) 05    +- Project [value#7 AS id#9] 06       +- LocalRelation [value#7]</p> <p>// Apply ReorderJoin rule // ReorderJoin alone is (usually?) not enough // Let's go pro and create a custom RuleExecutor (i.e. a Optimizer) import org.apache.spark.sql.catalyst.rules.RuleExecutor import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan import org.apache.spark.sql.catalyst.analysis.EliminateSubqueryAliases object Optimize extends RuleExecutor[LogicalPlan] {   import org.apache.spark.sql.catalyst.optimizer._   val batches =     Batch(\"EliminateSubqueryAliases\", Once, EliminateSubqueryAliases) ::     Batch(\"Operator Optimization\", FixedPoint(maxIterations = 100),       ConvertToLocalRelation,       PushDownPredicate,       PushPredicateThroughJoin) :: Nil } val preOptimizedPlan = Optimize.execute(plan) // Note Join Cross as a child of Join Inner scala&gt; println(preOptimizedPlan.numberedTreeString) 00 Join Inner, (id#0L = cast(id#9 as bigint)) 01 :- Join Cross 02 :  :- Range (0, 10485759, step=1, splits=Some(8)) 03 :  +- Range (0, 20971518, step=1, splits=Some(8)) 04 +- LocalRelation [id#9]</p> <p>// Time...for...ReorderJoin! import org.apache.spark.sql.catalyst.optimizer.ReorderJoin val optimizedPlan = ReorderJoin(preOptimizedPlan) scala&gt; println(optimizedPlan.numberedTreeString) 00 Join Cross 01 :- Join Inner, (id#0L = cast(id#9 as bigint)) 02 :  :- Range (0, 10485759, step=1, splits=Some(8)) 03 :  +- LocalRelation [id#9] 04 +- Range (0, 20971518, step=1, splits=Some(8))</p> <p>// ReorderJoin works differently when the following holds: // * starSchemaDetection is enabled // * cboEnabled is disabled import org.apache.spark.sql.internal.SQLConf.STARSCHEMA_DETECTION spark.sessionState.conf.setConf(STARSCHEMA_DETECTION, true)</p> <p>spark.sessionState.conf.starSchemaDetection spark.sessionState.conf.cboEnabled</p> <p><code>ReorderJoin</code> is part of the Operator Optimizations fixed-point batch in the standard batches of the Logical Optimizer.</p> <p>=== [[apply]] Applying ReorderJoin Rule To Logical Plan (Executing ReorderJoin) -- <code>apply</code> Method</p>"},{"location":"logical-optimizations/ReorderJoin/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-optimizations/ReorderJoin/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p>NOTE: <code>apply</code> is part of catalyst/Rule.md#apply[Rule Contract] to apply a rule to a spark-sql-LogicalPlan.md[logical plan].</p> <p><code>apply</code> traverses the input spark-sql-LogicalPlan.md[logical plan] down and finds the following logical operators for &lt;&gt;: <ul> <li> <p>Filter.md[Filter] with a inner or cross Join.md[Join] child operator</p> </li> <li> <p>Join.md[Join] (of any type)</p> </li> </ul> <p>NOTE: <code>apply</code> uses <code>ExtractFiltersAndInnerJoins</code> Scala extractor object (using &lt;&gt; method) to \"destructure\" a logical plan to its logical operators. <p>=== [[createOrderedJoin]] Creating Join Logical Operator (Possibly as Child of Filter Operator) -- <code>createOrderedJoin</code> Method</p>"},{"location":"logical-optimizations/ReorderJoin/#source-scala_2","title":"[source, scala]","text":""},{"location":"logical-optimizations/ReorderJoin/#createorderedjoininput-seqlogicalplan-innerlike-conditions-seqexpression-logicalplan","title":"createOrderedJoin(input: Seq[(LogicalPlan, InnerLike)], conditions: Seq[Expression]): LogicalPlan","text":"<p><code>createOrderedJoin</code> takes a collection of pairs of a spark-sql-LogicalPlan.md[logical plan] and the join type with join condition expressions/Expression.md[expressions] and...FIXME</p> <p>NOTE: <code>createOrderedJoin</code> makes sure that the <code>input</code> has at least two pairs in the <code>input</code>.</p> <p>NOTE: <code>createOrderedJoin</code> is used recursively when <code>ReorderJoin</code> is &lt;&gt; to a logical plan. <p>==== [[createOrderedJoin-two-joins]] \"Two Logical Plans\" Case</p> <p>For two joins exactly (i.e. the <code>input</code> has two logical plans and their join types), <code>createOrderedJoin</code> partitions (aka splits) the input condition expressions to the ones that can be evaluated within a join and not.</p> <p><code>createOrderedJoin</code> determines the join type of the result join. It chooses inner if the left and right join types are both inner and cross otherwise.</p> <p><code>createOrderedJoin</code> creates a Join.md#creating-instance[Join] logical operator with the input join conditions combined together using <code>And</code> expression and the join type (inner or cross).</p> <p>If there are condition expressions that could not be evaluated within a join, <code>createOrderedJoin</code> creates a <code>Filter</code> logical operator with the join conditions combined together using <code>And</code> expression and the result join operator as the Filter.md#child[child] operator.</p> <pre><code>import org.apache.spark.sql.catalyst.expressions.Expression\nimport org.apache.spark.sql.catalyst.expressions.Literal\nval a: Expression = Literal(\"a\")\nval b: Expression = Literal(\"b\")\n// Use Catalyst DSL to compose expressions\nimport org.apache.spark.sql.catalyst.dsl.expressions._\nval cond1 = a === b\n\n// RowNumber is Unevaluable so it cannot be evaluated within a join\nimport org.apache.spark.sql.catalyst.expressions.RowNumber\nval rn = RowNumber()\nimport org.apache.spark.sql.catalyst.expressions.Unevaluable\nassert(rn.isInstanceOf[Unevaluable])\nval cond2 = rn === Literal(2)\n\nval cond3 = Literal.TrueLiteral\n\n// Use Catalyst DSL to create logical plans\nimport org.apache.spark.sql.catalyst.dsl.plans._\nval t1 = table(\"t1\")\nval t2 = table(\"t2\")\n\n// Use input with exactly 2 pairs\nimport org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\nimport org.apache.spark.sql.catalyst.plans.{Cross, Inner, InnerLike}\nval input: Seq[(LogicalPlan, InnerLike)] = (t1, Inner) :: (t2, Cross) :: Nil\nval conditions: Seq[Expression] = cond1 :: cond2 :: cond3 :: Nil\n\nimport org.apache.spark.sql.catalyst.optimizer.ReorderJoin\nval plan = ReorderJoin.createOrderedJoin(input, conditions)\nscala&gt; println(plan.numberedTreeString)\n00 'Filter (row_number() = 2)\n01 +- 'Join Cross, ((a = b) &amp;&amp; true)\n02    :- 'UnresolvedRelation `t1`\n03    +- 'UnresolvedRelation `t2`\n</code></pre> <p>==== [[createOrderedJoin-three-or-more-joins]] \"Three Or More Logical Plans\" Case</p> <p>For three or more spark-sql-LogicalPlan.md[logical plans] in the <code>input</code>, <code>createOrderedJoin</code> takes the first plan and tries to find another that has at least one matching join condition, i.e. a logical plan with the following:</p> <p>. catalyst/QueryPlan.md#outputSet[Output attributes] together with the first plan's output attributes are the superset of the expressions/Expression.md#references[references] of a join condition expression (i.e. both plans are required to resolve join references)</p> <p>. References of the join condition cannot be evaluated using the first plan's or the current plan's catalyst/QueryPlan.md#outputSet[output attributes] (i.e. neither the first plan nor the current plan themselves are enough to resolve join references)</p> <p>.createOrderedJoin with Three Joins (Before) image::images/ReorderJoin-createOrderedJoin-four-plans-before.png[align=\"center\"]</p> <p>.createOrderedJoin with Three Joins (After) image::images/ReorderJoin-createOrderedJoin-four-plans-after.png[align=\"center\"]</p>"},{"location":"logical-optimizations/ReorderJoin/#source-scala_3","title":"[source, scala]","text":"<p>// HACK: Disable symbolToColumn implicit conversion // It is imported automatically in spark-shell (and makes demos impossible) // implicit def symbolToColumn(s: Symbol): org.apache.spark.sql.ColumnName trait ThatWasABadIdea implicit def symbolToColumn(ack: ThatWasABadIdea) = ack</p> <p>import org.apache.spark.sql.catalyst.plans.logical.LocalRelation import org.apache.spark.sql.catalyst.dsl.expressions._ import org.apache.spark.sql.catalyst.dsl.plans._ // Note analyze at the end to analyze the queries val p1 = LocalRelation('id.long, 'a.long, 'b.string).as(\"t1\").where(\"id\".attr =!= 0).select('id).analyze val p2 = LocalRelation('id.long, 'b.long).as(\"t2\").analyze val p3 = LocalRelation('id.long, 'a.string).where(\"id\".attr &gt; 0).select('id, 'id * 2 as \"a\").as(\"t3\").analyze</p> <p>// The following input and conditions are equivalent to the following query val _p1 = Seq((0,1,\"one\")).toDF(\"id\", \"a\", \"b\").as(\"t1\").where(col(\"id\") =!= 0).select(\"id\") val _p2 = Seq((0,1)).toDF(\"id\", \"b\").as(\"t2\") val _p3 = Seq((0,\"one\")).toDF(\"id\", \"a\").where(col(\"id\") &gt; 0).select(col(\"id\"), col(\"id\") * 2 as \"a\").as(\"t3\") val _plan = _p1.   as(\"p1\").   crossJoin(_p1).   join(_p2).   join(_p3).   where((col(\"p1.id\") === col(\"t3.id\")) &amp;&amp; (col(\"t2.b\") === col(\"t3.a\"))).   queryExecution.   analyzed import org.apache.spark.sql.catalyst.planning.ExtractFiltersAndInnerJoins val Some((plans, conds)) = ExtractFiltersAndInnerJoins.unapply(_plan)</p> <p>import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan import org.apache.spark.sql.catalyst.plans.{Cross, Inner, InnerLike} val input: Seq[(LogicalPlan, InnerLike)] = Seq(   (p1, Cross),   (p1, Cross),   (p2, Inner),   (p3, Inner))</p> <p>// (left ++ right).outputSet &gt; expr.references // ! expr.references &gt; left.outputSet // ! expr.references &gt; right.outputSet val p1_id = p1.outputSet.head val p3_id = p3.outputSet.head val p2_b = p2.outputSet.tail.head val p3_a = p3.outputSet.tail.head val c1 = p1_id === p3_id val c2 = p2_b === p3_a</p> <p>// A condition has no references or the references are not a subset of left or right plans // A couple of assertions that createOrderedJoin does internally assert(c1.references.nonEmpty) assert(!c1.references.subsetOf(p1.outputSet)) assert(!c1.references.subsetOf(p3.outputSet)) val refs = p1.analyze.outputSet ++ p3.outputSet assert(c1.references.subsetOf(refs))</p> <p>import org.apache.spark.sql.catalyst.expressions.Expression val conditions: Seq[Expression] = Seq(c1, c2)</p> <p>assert(input.size &gt; 2) assert(conditions.nonEmpty)</p> <p>import org.apache.spark.sql.catalyst.optimizer.ReorderJoin val plan = ReorderJoin.createOrderedJoin(input, conditions) scala&gt; println(plan.numberedTreeString) 00 'Join Cross 01 :- Join Inner, (b#553L = a#556L) 02 :  :- Join Inner, (id#549L = id#554L) 03 :  :  :- Project [id#549L] 04 :  :  :  +- Filter NOT (id#549L = cast(0 as bigint)) 05 :  :  :     +- LocalRelation , [id#549L, a#550L, b#551] 06 :  :  +- Project [id#554L, (id#554L * cast(2 as bigint)) AS a#556L] 07 :  :     +- Filter (id#554L &gt; cast(0 as bigint)) 08 :  :        +- LocalRelation , [id#554L, a#555] 09 :  +- LocalRelation , [id#552L, b#553L] 10 +- Project [id#549L] 11    +- Filter NOT (id#549L = cast(0 as bigint)) 12       +- LocalRelation , [id#549L, a#550L, b#551] <p><code>createOrderedJoin</code> takes the plan that has at least one matching join condition if found or the next plan from the <code>input</code> plans.</p> <p><code>createOrderedJoin</code> partitions (aka splits) the input condition expressions to expressions that meet the following requirements (aka join conditions) or not (aka others):</p> <p>. expressions/Expression.md#references[Expression references] being a subset of the catalyst/QueryPlan.md#outputSet[output attributes] of the left and the right operators</p> <p>. Can be evaluated within a join</p> <p><code>createOrderedJoin</code> creates a Join.md#creating-instance[Join] logical operator with:</p> <p>. Left logical operator as the first operator from the <code>input</code></p> <p>. Right logical operator as the right as chosen above</p> <p>. Join type as the right's join type as chosen above</p> <p>. Join conditions combined together using <code>And</code> expression</p> <p><code>createOrderedJoin</code> calls itself recursively with the following:</p> <p>. <code>input</code> logical joins as a new pair of the new <code>Join</code> and <code>Inner</code> join type with the remaining logical plans (all but the right)</p> <p>. <code>conditions</code> expressions as the others conditions (all but the join conditions used for the new join)</p> <p>.createOrderedJoin with Three Joins image::images/ReorderJoin-createOrderedJoin-four-plans.png[align=\"center\"]</p> <pre><code>import org.apache.spark.sql.catalyst.expressions.Expression\nimport org.apache.spark.sql.catalyst.expressions.AttributeReference\nimport org.apache.spark.sql.types.LongType\nval t1_id: Expression = AttributeReference(name = \"id\", LongType)(qualifier = Some(\"t1\"))\nval t2_id: Expression = AttributeReference(name = \"id\", LongType)(qualifier = Some(\"t2\"))\nval t4_id: Expression = AttributeReference(name = \"id\", LongType)(qualifier = Some(\"t4\"))\n// Use Catalyst DSL to compose expressions\nimport org.apache.spark.sql.catalyst.dsl.expressions._\nval cond1 = t1_id === t2_id\n\n// RowNumber is Unevaluable so it cannot be evaluated within a join\nimport org.apache.spark.sql.catalyst.expressions.RowNumber\nval rn = RowNumber()\nimport org.apache.spark.sql.catalyst.expressions.Unevaluable\nassert(rn.isInstanceOf[Unevaluable])\nimport org.apache.spark.sql.catalyst.expressions.Literal\nval cond2 = rn === Literal(2)\n\n// That would hardly appear in the condition list\n// Just for the demo\nval cond3 = Literal.TrueLiteral\n\nval cond4 = t4_id === t1_id\n\n// Use Catalyst DSL to create logical plans\nimport org.apache.spark.sql.catalyst.dsl.plans._\nval t1 = table(\"t1\")\nval t2 = table(\"t2\")\nval t3 = table(\"t3\")\nval t4 = table(\"t4\")\n\n// Use input with 3 or more pairs\nimport org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\nimport org.apache.spark.sql.catalyst.plans.{Cross, Inner, InnerLike}\nval input: Seq[(LogicalPlan, InnerLike)] = Seq(\n  (t1, Inner),\n  (t2, Inner),\n  (t3, Cross),\n  (t4, Inner))\nval conditions: Seq[Expression] = cond1 :: cond2 :: cond3 :: cond4 :: Nil\n\nimport org.apache.spark.sql.catalyst.optimizer.ReorderJoin\nval plan = ReorderJoin.createOrderedJoin(input, conditions)\nscala&gt; println(plan.numberedTreeString)\n00 'Filter (row_number() = 2)\n01 +- 'Join Inner, ((id#11L = id#12L) &amp;&amp; (id#13L = id#11L))\n02    :- 'Join Cross\n03    :  :- 'Join Inner, true\n04    :  :  :- 'UnresolvedRelation `t1`\n05    :  :  +- 'UnresolvedRelation `t2`\n06    :  +- 'UnresolvedRelation `t3`\n07    +- 'UnresolvedRelation `t4`\n</code></pre> <p>=== [[ExtractFiltersAndInnerJoins]][[ExtractFiltersAndInnerJoins-unapply]][[unapply]] Extracting Filter and Join Operators from Logical Plan -- <code>unapply</code> Method (of ExtractFiltersAndInnerJoins)</p>"},{"location":"logical-optimizations/ReorderJoin/#source-scala_4","title":"[source, scala]","text":""},{"location":"logical-optimizations/ReorderJoin/#unapplyplan-logicalplan-optionseqlogicalplan-innerlike-seqexpression","title":"unapply(plan: LogicalPlan): Option[(Seq[(LogicalPlan, InnerLike)], Seq[Expression])]","text":"<p><code>unapply</code> extracts Filter.md[Filter] (with an inner or cross join) or Join.md[Join] logical operators (per the input spark-sql-LogicalPlan.md[logical plan]) to...FIXME</p> <p>NOTE: <code>unapply</code> is a feature of the Scala programming language to define https://docs.scala-lang.org/tour/extractor-objects.html[extractor objects] that take an object and try to give the arguments back. This is most often used in pattern matching and partial functions.</p> <p>. For a Filter.md[Filter] logical operator with a cross or inner Join.md[Join] child operator, <code>unapply</code> &lt;&gt; on the <code>Filter</code>. <p>. For a Join.md[Join] logical operator, <code>unapply</code> &lt;&gt; on the <code>Join</code>."},{"location":"logical-optimizations/ReorderJoin/#source-scala_5","title":"[source, scala]","text":"<p>val d1 = Seq((0, \"a\"), (1, \"b\")).toDF(\"id\", \"c\") val d2 = Seq((0, \"c\"), (2, \"b\")).toDF(\"id\", \"c\") val q = d1.join(d2, \"id\").where($\"id\" &gt; 0) val plan = q.queryExecution.analyzed</p> <p>scala&gt; println(plan.numberedTreeString) 00 Filter (id#34 &gt; 0) 01 +- Project [id#34, c#35, c#44] 02    +- Join Inner, (id#34 = id#43) 03       :- Project [_1#31 AS id#34, _2#32 AS c#35] 04       :  +- LocalRelation [_1#31, _2#32] 05       +- Project [_1#40 AS id#43, _2#41 AS c#44] 06          +- LocalRelation [_1#40, _2#41]</p> <p>// Let's use Catalyst DSL instead so the plan is cleaner (e.g. no Project in-between) // We could have used logical rules to clean up the plan // Leaving the cleaning up as a home exercise for you :) import org.apache.spark.sql.catalyst.dsl.plans._ val t1 = table(\"t1\") val t2 = table(\"t2\") import org.apache.spark.sql.catalyst.expressions.Expression import org.apache.spark.sql.catalyst.expressions.Literal val id: Expression = Literal(\"id\") import org.apache.spark.sql.catalyst.dsl.expressions._ import org.apache.spark.sql.catalyst.plans.Cross val plan = t1.join(t1, joinType = Cross).join(t2).where(id &gt; 0) scala&gt; println(plan.numberedTreeString) 00 'Filter (id &gt; 0) 01 +- 'Join Inner 02    :- 'Join Cross 03    :  :- 'UnresolvedRelation <code>t1</code> 04    :  +- 'UnresolvedRelation <code>t1</code> 05    +- 'UnresolvedRelation <code>t2</code></p> <p>import org.apache.spark.sql.catalyst.planning.ExtractFiltersAndInnerJoins // Option[(Seq[(LogicalPlan, InnerLike)], Seq[Expression])] val Some((plans, conditions)) = ExtractFiltersAndInnerJoins.unapply(plan)</p> <p>assert(plans.size &gt; 2) assert(conditions.nonEmpty)</p>"},{"location":"logical-optimizations/ReorderJoin/#caution-fixme","title":"CAUTION: FIXME","text":"<p>NOTE: <code>unapply</code> is used exclusively when <code>ReorderJoin</code> is &lt;&gt;, i.e. applied to a logical plan. <p>=== [[ExtractFiltersAndInnerJoins-flattenJoin]][[flattenJoin]] Flattening Consecutive Joins -- <code>flattenJoin</code> Method (of ExtractFiltersAndInnerJoins)</p>"},{"location":"logical-optimizations/ReorderJoin/#source-scala_6","title":"[source, scala]","text":"<p>flattenJoin(plan: LogicalPlan, parentJoinType: InnerLike = Inner):   (Seq[(LogicalPlan, InnerLike)], Seq[Expression])</p> <p><code>flattenJoin</code> branches off per the input logical <code>plan</code>:</p> <ul> <li>For an inner or cross Join.md[Join] logical operator, <code>flattenJoin</code> calls itself recursively with the left-side of the join and the type of the join, and gives:</li> </ul> <p>a. The logical plans from recursive <code>flattenJoin</code> with the right-side of the join and the right join's type b. The join conditions from <code>flattenJoin</code> with the conditions of the join</p> <ul> <li>For a Filter.md[Filter] with an inner or cross Join.md[Join] child operator, <code>flattenJoin</code> calls itself recursively on the join (that simply removes the <code>Filter</code> \"layer\" and assumes an inner join) and gives:</li> </ul> <p>a. The logical plans from recursive <code>flattenJoin</code> b. The join conditions from <code>flattenJoin</code> with <code>Filter</code> conditions</p> <ul> <li>For all other logical operators, <code>flattenJoin</code> gives the input <code>plan</code>, the current join type (an inner or cross join) and the empty join condition.</li> </ul> <p>In either case, <code>flattenJoin</code> splits conjunctive predicates, i.e. removes <code>And</code> expressions and gives their child expressions.</p>"},{"location":"logical-optimizations/ReorderJoin/#source-scala_7","title":"[source, scala]","text":"<p>// Use Catalyst DSL to create a logical plan // Example 1: One cross join import org.apache.spark.sql.catalyst.dsl.plans._ val t1 = table(\"t1\") import org.apache.spark.sql.catalyst.dsl.expressions._ val id = \"id\".expr import org.apache.spark.sql.catalyst.plans.Cross val plan = t1.join(t1, joinType = Cross) scala&gt; println(plan.numberedTreeString) 00 'Join Cross 01 :- 'UnresolvedRelation <code>t1</code> 02 +- 'UnresolvedRelation <code>t1</code></p> <p>import org.apache.spark.sql.catalyst.planning.ExtractFiltersAndInnerJoins val (plans, conditions) = ExtractFiltersAndInnerJoins.flattenJoin(plan) assert(plans.size == 2) assert(conditions.size == 0)</p> <p>// Example 2: One inner join with a filter val t2 = table(\"t2\") val plan = t1.join(t2).where(\"t1\".expr === \"t2\".expr) scala&gt; println(plan.numberedTreeString) 00 'Filter (t1 = t2) 01 +- 'Join Inner 02    :- 'UnresolvedRelation <code>t1</code> 03    +- 'UnresolvedRelation <code>t2</code></p> <p>val (plans, conditions) = ExtractFiltersAndInnerJoins.flattenJoin(plan) assert(plans.size == 2) assert(conditions.size == 1)</p> <p>// Example 3: One inner and one cross join with a compound filter val plan = t1.   join(t1, joinType = Cross).   join(t2).   where(\"t2.id\".expr === \"t1.id\".expr &amp;&amp; \"t1.id\".expr &gt; 10) scala&gt; println(plan.numberedTreeString) 00 'Filter ((t2.id = t1.id) &amp;&amp; (t1.id &gt; 10)) 01 +- 'Join Inner 02    :- 'Join Cross 03    :  :- 'UnresolvedRelation <code>t1</code> 04    :  +- 'UnresolvedRelation <code>t1</code> 05    +- 'UnresolvedRelation <code>t2</code></p> <p>val (plans, conditions) = ExtractFiltersAndInnerJoins.flattenJoin(plan) assert(plans.size == 3) assert(conditions.size == 2)</p> <p>// Example 4 val t3 = table(\"t3\") val plan = t1.   join(t1, joinType = Cross).   join(t2).   where(\"t2.id\".expr === \"t1.id\".expr &amp;&amp; \"t1.id\".expr &gt; 10).   join(t3.select(star())).  // \u2190 just for more fun   where(\"t3.id\".expr === \"t1.id\".expr) scala&gt; println(plan.numberedTreeString) 00 'Filter (t3.id = t1.id) 01 +- 'Join Inner 02    :- 'Filter ((t2.id = t1.id) &amp;&amp; (t1.id &gt; 10)) 03    :  +- 'Join Inner 04    :     :- 'Join Cross 05    :     :  :- 'UnresolvedRelation <code>t1</code> 06    :     :  +- 'UnresolvedRelation <code>t1</code> 07    :     +- 'UnresolvedRelation <code>t2</code> 08    +- 'Project [*] 09       +- 'UnresolvedRelation <code>t3</code></p> <p>val (plans, conditions) = ExtractFiltersAndInnerJoins.flattenJoin(plan) assert(plans.size == 4) assert(conditions.size == 3)</p> <p>// Example 5: Join under project is no longer consecutive val plan = t1.   join(t1, joinType = Cross).   select(star()). // \u2190 separates the cross join from the other joins   join(t2).   where(\"t2.id\".expr === \"t1.id\".expr &amp;&amp; \"t1.id\".expr &gt; 10).   join(t3.select(star())).   where(\"t3.id\".expr === \"t1.id\".expr) scala&gt; println(plan.numberedTreeString) 00 'Filter (t3.id = t1.id) 01 +- 'Join Inner 02    :- 'Filter ((t2.id = t1.id) &amp;&amp; (t1.id &gt; 10)) 03    :  +- 'Join Inner 04    :     :- 'Project [] 05    :     :  +- 'Join Cross 06    :     :     :- 'UnresolvedRelation <code>t1</code> 07    :     :     +- 'UnresolvedRelation <code>t1</code> 08    :     +- 'UnresolvedRelation <code>t2</code> 09    +- 'Project [] 10       +- 'UnresolvedRelation <code>t3</code></p> <p>val (plans, conditions) = ExtractFiltersAndInnerJoins.flattenJoin(plan) assert(plans.size == 3) // \u2190 one join less due to Project assert(conditions.size == 3)</p> <p>// Example 6: Join on right-hand side is not considered val plan = t1.   join(     t1.join(t2).where(\"t2.id\".expr === \"t1.id\".expr &amp;&amp; \"t1.id\".expr &gt; 10), // \u2190 join on RHS     joinType = Cross).   join(t2).   where(\"t2.id\".expr === \"t1.id\".expr &amp;&amp; \"t1.id\".expr &gt; 10) scala&gt; println(plan.numberedTreeString) 00 'Filter ((t2.id = t1.id) &amp;&amp; (t1.id &gt; 10)) 01 +- 'Join Inner 02    :- 'Join Cross 03    :  :- 'UnresolvedRelation <code>t1</code> 04    :  +- 'Filter ((t2.id = t1.id) &amp;&amp; (t1.id &gt; 10)) 05    :     +- 'Join Inner 06    :        :- 'UnresolvedRelation <code>t1</code> 07    :        +- 'UnresolvedRelation <code>t2</code> 08    +- 'UnresolvedRelation <code>t2</code></p> <p>val (plans, conditions) = ExtractFiltersAndInnerJoins.flattenJoin(plan) assert(plans.size == 3) // \u2190 one join less due to being on right side assert(conditions.size == 2)</p> <p>NOTE: <code>flattenJoin</code> is used recursively when <code>ReorderJoin</code> is &lt;&gt; a logical plan (when &lt;&gt;)."},{"location":"logical-optimizations/ReplaceExceptWithAntiJoin/","title":"ReplaceExceptWithAntiJoin Logical Optimization Rule -- Rewriting Except (DISTINCT) Operators","text":"<p><code>ReplaceExceptWithAntiJoin</code> is a catalyst/Rule.md[Catalyst rule] for transforming spark-sql-LogicalPlan.md[logical plans] (i.e. <code>Rule[LogicalPlan]</code>).</p> <p>[[apply]] When catalyst/Rule.md#apply[executed], <code>ReplaceExceptWithAntiJoin</code> transforms an Except.md[Except (distinct)] logical operator to a <code>Distinct</code> unary logical operator with a left-anti Join.md[Join] operator. The output columns of the left and right child logical operators of the <code>Except</code> operator are used to build a logical <code>AND</code> join condition of EqualNullSafe expressions.</p> <p><code>ReplaceExceptWithAntiJoin</code> requires that the number of columns of the left- and right-side of the <code>Except</code> operator are the same or throws an <code>AssertionError</code>.</p> <p><code>ReplaceExceptWithAntiJoin</code> is a part of the Replace Operators fixed-point rule batch of the base Logical Optimizer.</p> <p>[[demo]] .Demo: ReplaceExceptWithAntiJoin <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\nimport org.apache.spark.sql.catalyst.dsl.plans._\n\n// Using hacks to disable two Catalyst DSL implicits\nimplicit def symbolToColumn(ack: ThatWasABadIdea) = ack\nimplicit class StringToColumn(val sc: StringContext) {}\n\nimport org.apache.spark.sql.catalyst.plans.logical.LocalRelation\nval t1 = LocalRelation('a.int, 'b.int)\nval t2 = LocalRelation('C.int, 'D.int)\n\nval plan = t1.except(t2, isAll = false)\n\nimport org.apache.spark.sql.catalyst.optimizer.ReplaceExceptWithAntiJoin\nval optimizedPlan = ReplaceExceptWithAntiJoin(plan)\nscala&gt; println(optimizedPlan.numberedTreeString)\n00 Distinct\n01 +- Join LeftAnti, ((a#14 &lt;=&gt; C#18) &amp;&amp; (b#15 &lt;=&gt; D#19))\n02    :- LocalRelation &lt;empty&gt;, [a#14, b#15]\n03    +- LocalRelation &lt;empty&gt;, [C#18, D#19]\n</code></pre></p>"},{"location":"logical-optimizations/ReplaceExceptWithFilter/","title":"ReplaceExceptWithFilter Logical Optimization Rule -- Rewriting Except (DISTINCT) Operators","text":"<p><code>ReplaceExceptWithFilter</code> is a catalyst/Rule.md[Catalyst rule] for transforming spark-sql-LogicalPlan.md[logical plans] (i.e. <code>Rule[LogicalPlan]</code>).</p> <p>[[apply]] When catalyst/Rule.md#apply[executed], <code>ReplaceExceptWithFilter</code> transforms an Except.md[Except (distinct)] logical operator to...FIXME</p> <p><code>ReplaceExceptWithFilter</code> is a part of the Replace Operators fixed-point rule batch of the base Logical Optimizer.</p> <p><code>ReplaceExceptWithFilter</code> can be turned off and on based on spark.sql.optimizer.replaceExceptWithFilter configuration property.</p> <p>[[demo]] .Demo: ReplaceExceptWithFilter <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\nimport org.apache.spark.sql.catalyst.dsl.plans._\n\n// Using hacks to disable two Catalyst DSL implicits\nimplicit def symbolToColumn(ack: ThatWasABadIdea) = ack\nimplicit class StringToColumn(val sc: StringContext) {}\n\nimport org.apache.spark.sql.catalyst.plans.logical.LocalRelation\nval t1 = LocalRelation('a.int, 'b.int)\nval t2 = t1.where('a &gt; 5)\n\nval plan = t1.except(t2, isAll = false)\n\nimport org.apache.spark.sql.catalyst.optimizer.ReplaceExceptWithFilter\nval optimizedPlan = ReplaceExceptWithFilter(plan)\nscala&gt; println(optimizedPlan.numberedTreeString)\n00 'Distinct\n01 +- 'Filter NOT coalesce(('a &gt; 5), false)\n02    +- LocalRelation &lt;empty&gt;, [a#12, b#13]\n</code></pre></p> <p>=== [[isEligible]] <code>isEligible</code> Internal Predicate</p>"},{"location":"logical-optimizations/ReplaceExceptWithFilter/#source-scala","title":"[source, scala]","text":"<p>isEligible(   left: LogicalPlan,   right: LogicalPlan): Boolean</p> <p><code>isEligible</code> is positive (<code>true</code>) when the right logical operator is a Project.md[Project] with a Filter.md[Filter] child operator or simply a Filter.md[Filter] operator itself and &lt;&gt;. <p>Otherwise, <code>isEligible</code> is negative (<code>false</code>).</p> <p>NOTE: <code>isEligible</code> is used when <code>ReplaceExceptWithFilter</code> is &lt;&gt;. <p>=== [[verifyConditions]] <code>verifyConditions</code> Internal Predicate</p>"},{"location":"logical-optimizations/ReplaceExceptWithFilter/#source-scala_1","title":"[source, scala]","text":"<p>verifyConditions(   left: LogicalPlan,   right: LogicalPlan): Boolean</p> <p><code>verifyConditions</code> is positive (<code>true</code>) when all of the following hold:</p> <ul> <li>FIXME</li> </ul> <p>Otherwise, <code>verifyConditions</code> is negative (<code>false</code>).</p> <p>NOTE: <code>verifyConditions</code> is used when <code>ReplaceExceptWithFilter</code> is &lt;&gt; (when requested to &lt;&gt;)."},{"location":"logical-optimizations/ReplaceExpressions/","title":"ReplaceExpressions Logical Optimization","text":"<p><code>ReplaceExpressions</code> is a base logical optimization that &lt;&gt;. <p><code>ReplaceExpressions</code> is part of the Finish Analysis once-executed batch in the standard batches of the Logical Optimizer.</p> <p><code>ReplaceExpressions</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>."},{"location":"logical-optimizations/ReplaceExpressions/#source-scala","title":"[source, scala]","text":"<p>val query = sql(\"select ifnull(NULL, array('2')) from values 1\") val analyzedPlan = query.queryExecution.analyzed scala&gt; println(analyzedPlan.numberedTreeString) 00 Project [ifnull(null, array(2)) AS ifnull(NULL, array('2'))#3] 01 +- LocalRelation [col1#2]</p> <p>import org.apache.spark.sql.catalyst.optimizer.ReplaceExpressions val optimizedPlan = ReplaceExpressions(analyzedPlan) scala&gt; println(optimizedPlan.numberedTreeString) 00 Project [coalesce(cast(null as array), cast(array(2) as array)) AS ifnull(NULL, array('2'))#3] 01 +- LocalRelation [col1#2] <p>=== [[apply]] Executing Rule -- <code>apply</code> Method</p>"},{"location":"logical-optimizations/ReplaceExpressions/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-optimizations/ReplaceExpressions/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p><code>apply</code> &lt;&gt; (in the input &lt;&gt;) and replaces a spark-sql-Expression-RuntimeReplaceable.md[RuntimeReplaceable] expression into its single child. <p><code>apply</code> is part of the Rule abstraction.</p>"},{"location":"logical-optimizations/RewriteCorrelatedScalarSubquery/","title":"RewriteCorrelatedScalarSubquery Logical Optimization","text":"<p><code>RewriteCorrelatedScalarSubquery</code> is a base logical optimization that &lt;&gt; with the following operators: <p>. FIXME</p> <p><code>RewriteCorrelatedScalarSubquery</code> is part of the Operator Optimization before Inferring Filters fixed-point batch in the standard batches of the Logical Optimizer.</p> <p><code>RewriteCorrelatedScalarSubquery</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>."},{"location":"logical-optimizations/RewriteCorrelatedScalarSubquery/#source-scala","title":"[source, scala]","text":"<p>import org.apache.spark.sql.catalyst.optimizer.RewriteCorrelatedScalarSubquery</p> <p>// FIXME // Demo: Filter + Aggregate // Demo: Filter + UnaryNode</p> <p>val plan = ??? val optimizedPlan = RewriteCorrelatedScalarSubquery(plan)</p> <p>=== [[evalExpr]] <code>evalExpr</code> Internal Method</p>"},{"location":"logical-optimizations/RewriteCorrelatedScalarSubquery/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-optimizations/RewriteCorrelatedScalarSubquery/#evalexprexpr-expression-bindings-mapexprid-optionany-optionany","title":"evalExpr(expr: Expression, bindings: Map[ExprId, Option[Any]]) : Option[Any]","text":"<p><code>evalExpr</code>...FIXME</p> <p>NOTE: <code>evalExpr</code> is used exclusively when <code>RewriteCorrelatedScalarSubquery</code> is...FIXME</p> <p>=== [[evalAggOnZeroTups]] <code>evalAggOnZeroTups</code> Internal Method</p>"},{"location":"logical-optimizations/RewriteCorrelatedScalarSubquery/#source-scala_2","title":"[source, scala]","text":""},{"location":"logical-optimizations/RewriteCorrelatedScalarSubquery/#evalaggonzerotupsexpr-expression-optionany","title":"evalAggOnZeroTups(expr: Expression) : Option[Any]","text":"<p><code>evalAggOnZeroTups</code>...FIXME</p> <p>NOTE: <code>evalAggOnZeroTups</code> is used exclusively when <code>RewriteCorrelatedScalarSubquery</code> is...FIXME</p> <p>=== [[evalSubqueryOnZeroTups]] <code>evalSubqueryOnZeroTups</code> Internal Method</p>"},{"location":"logical-optimizations/RewriteCorrelatedScalarSubquery/#source-scala_3","title":"[source, scala]","text":""},{"location":"logical-optimizations/RewriteCorrelatedScalarSubquery/#evalsubqueryonzerotupsplan-logicalplan-optionany","title":"evalSubqueryOnZeroTups(plan: LogicalPlan) : Option[Any]","text":"<p><code>evalSubqueryOnZeroTups</code>...FIXME</p> <p>NOTE: <code>evalSubqueryOnZeroTups</code> is used exclusively when <code>RewriteCorrelatedScalarSubquery</code> is requsted to &lt;&gt;. <p>=== [[constructLeftJoins]] <code>constructLeftJoins</code> Internal Method</p>"},{"location":"logical-optimizations/RewriteCorrelatedScalarSubquery/#source-scala_4","title":"[source, scala]","text":"<p>constructLeftJoins(   child: LogicalPlan,   subqueries: ArrayBuffer[ScalarSubquery]): LogicalPlan</p> <p><code>constructLeftJoins</code>...FIXME</p> <p>NOTE: <code>constructLeftJoins</code> is used exclusively when <code>RewriteCorrelatedScalarSubquery</code> logical optimization is &lt;&gt; (i.e. applied to &lt;&gt;, &lt;&gt; or <code>Filter</code> logical operators with correlated scalar subqueries) <p>=== [[extractCorrelatedScalarSubqueries]] Extracting ScalarSubquery Expressions with Children -- <code>extractCorrelatedScalarSubqueries</code> Internal Method</p>"},{"location":"logical-optimizations/RewriteCorrelatedScalarSubquery/#source-scala_5","title":"[source, scala]","text":"<p>extractCorrelatedScalarSubqueriesE &lt;: Expression: E</p> <p><code>extractCorrelatedScalarSubqueries</code> finds all ScalarSubquery expressions with at least one child in the input <code>expression</code> and adds them to the input <code>subqueries</code> collection.</p> <p><code>extractCorrelatedScalarSubqueries</code> traverses the input <code>expression</code> down (the expression tree) and, every time a <code>ScalarSubquery</code> with at least one child is found, returns the head of the output attributes of the subquery plan.</p> <p>In the end, <code>extractCorrelatedScalarSubqueries</code> returns the rewritten expression.</p> <p>NOTE: <code>extractCorrelatedScalarSubqueries</code> uses https://docs.scala-lang.org/overviews/collections/concrete-mutable-collection-classes.html[scala.collection.mutable.ArrayBuffer] and mutates an instance inside (i.e. adds <code>ScalarSubquery</code> expressions) that makes for two output values, i.e. the rewritten expression and the <code>ScalarSubquery</code> expressions.</p> <p>NOTE: <code>extractCorrelatedScalarSubqueries</code> is used exclusively when <code>RewriteCorrelatedScalarSubquery</code> is &lt;&gt; (i.e. applied to a spark-sql-LogicalPlan.md[logical plan])."},{"location":"logical-optimizations/RewriteExceptAll/","title":"RewriteExceptAll Logical Optimization Rule -- Rewriting Except (ALL) Operators","text":"<p><code>RewriteExceptAll</code> is a catalyst/Rule.md[Catalyst rule] for transforming spark-sql-LogicalPlan.md[logical plans] (i.e. <code>Rule[LogicalPlan]</code>).</p> <p>[[apply]] When catalyst/Rule.md#apply[executed], <code>RewriteExceptAll</code> transforms an Except.md[Except (ALL)] logical operator to...FIXME</p> <p><code>RewriteExceptAll</code> requires that the number of columns of the left- and right-side of the <code>Except</code> operator are the same or throws an <code>AssertionError</code>.</p> <p><code>RewriteExceptAll</code> is a part of the Replace Operators fixed-point rule batch of the base Logical Optimizer.</p> <p>[[demo]] .Demo: RewriteExceptAll <pre><code>import org.apache.spark.sql.catalyst.dsl.expressions._\nimport org.apache.spark.sql.catalyst.dsl.plans._\n\n// Using hacks to disable two Catalyst DSL implicits\nimplicit def symbolToColumn(ack: ThatWasABadIdea) = ack\nimplicit class StringToColumn(val sc: StringContext) {}\n\nimport org.apache.spark.sql.catalyst.plans.logical.LocalRelation\nval t1 = LocalRelation('a.int, 'b.int)\nval t2 = LocalRelation('C.int, 'D.int).where('C &gt; 10)\n\nval plan = t1.except(t2, isAll = true)\n\nimport org.apache.spark.sql.catalyst.optimizer.RewriteExceptAll\nval optimizedPlan = RewriteExceptAll(plan)\nscala&gt; println(optimizedPlan.numberedTreeString)\n00 'Project [a#20, b#21]\n01 +- 'Generate replicaterows(sum#27L, a#20, b#21), false, [a#20, b#21]\n02    +- 'Filter (sum#27L &gt; 0)\n03       +- 'Aggregate [a#20, b#21], [a#20, b#21, sum(vcol#24L) AS sum#27L]\n04          +- 'Union\n05             :- Project [1 AS vcol#24L, a#20, b#21]\n06             :  +- LocalRelation &lt;empty&gt;, [a#20, b#21]\n07             +- 'Project [-1 AS vcol#25L, C#22, D#23]\n08                +- 'Filter ('C &gt; 10)\n09                   +- LocalRelation &lt;empty&gt;, [C#22, D#23]\n</code></pre></p>"},{"location":"logical-optimizations/RewritePredicateSubquery/","title":"RewritePredicateSubquery Logical Optimization","text":"<p><code>RewritePredicateSubquery</code> is a base logical optimization that &lt;&gt; as follows: <ul> <li> <p><code>Filter</code> operators with <code>Exists</code> and <code>In</code> with <code>ListQuery</code> expressions give left-semi joins</p> </li> <li> <p><code>Filter</code> operators with <code>Not</code> with <code>Exists</code> and <code>In</code> with <code>ListQuery</code> expressions give left-anti joins</p> </li> </ul> <p>NOTE: Prefer <code>EXISTS</code> (over <code>Not</code> with <code>In</code> with <code>ListQuery</code> subquery expression) if performance matters since https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala?utf8=%E2%9C%93#L110[they say] \"that will almost certainly be planned as a Broadcast Nested Loop join\".</p> <p><code>RewritePredicateSubquery</code> is part of the RewriteSubquery once-executed batch in the standard batches of the Logical Optimizer.</p> <p><code>RewritePredicateSubquery</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>."},{"location":"logical-optimizations/RewritePredicateSubquery/#source-scala","title":"[source, scala]","text":"<p>// FIXME Examples of RewritePredicateSubquery // 1. Filters with Exists and In (with ListQuery) expressions // 2. NOTs</p> <p>// Based on RewriteSubquerySuite // FIXME Contribute back to RewriteSubquerySuite import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan import org.apache.spark.sql.catalyst.rules.RuleExecutor object Optimize extends RuleExecutor[LogicalPlan] {   import org.apache.spark.sql.catalyst.optimizer._   val batches = Seq(     Batch(\"Column Pruning\", FixedPoint(100), ColumnPruning),     Batch(\"Rewrite Subquery\", Once,       RewritePredicateSubquery,       ColumnPruning,       CollapseProject,       RemoveRedundantProject)) }</p> <p>val q = ... val optimized = Optimize.execute(q.analyze)</p> <p><code>RewritePredicateSubquery</code> is part of the RewriteSubquery once-executed batch in the standard batches of the Logical Optimizer.</p> <p>=== [[rewriteExistentialExpr]] <code>rewriteExistentialExpr</code> Internal Method</p>"},{"location":"logical-optimizations/RewritePredicateSubquery/#source-scala_1","title":"[source, scala]","text":"<p>rewriteExistentialExpr(   exprs: Seq[Expression],   plan: LogicalPlan): (Option[Expression], LogicalPlan)</p> <p><code>rewriteExistentialExpr</code>...FIXME</p> <p>NOTE: <code>rewriteExistentialExpr</code> is used when...FIXME</p> <p>=== [[dedupJoin]] <code>dedupJoin</code> Internal Method</p>"},{"location":"logical-optimizations/RewritePredicateSubquery/#source-scala_2","title":"[source, scala]","text":""},{"location":"logical-optimizations/RewritePredicateSubquery/#dedupjoinjoinplan-logicalplan-logicalplan","title":"dedupJoin(joinPlan: LogicalPlan): LogicalPlan","text":"<p><code>dedupJoin</code>...FIXME</p> <p>NOTE: <code>dedupJoin</code> is used when...FIXME</p> <p>=== [[getValueExpression]] <code>getValueExpression</code> Internal Method</p>"},{"location":"logical-optimizations/RewritePredicateSubquery/#source-scala_3","title":"[source, scala]","text":""},{"location":"logical-optimizations/RewritePredicateSubquery/#getvalueexpressione-expression-seqexpression","title":"getValueExpression(e: Expression): Seq[Expression]","text":"<p><code>getValueExpression</code>...FIXME</p> <p>NOTE: <code>getValueExpression</code> is used when...FIXME</p> <p>=== [[apply]] Executing Rule -- <code>apply</code> Method</p>"},{"location":"logical-optimizations/RewritePredicateSubquery/#source-scala_4","title":"[source, scala]","text":""},{"location":"logical-optimizations/RewritePredicateSubquery/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p><code>apply</code> transforms Filter.md[Filter] unary operators in the input spark-sql-LogicalPlan.md[logical plan].</p> <p><code>apply</code> splits conjunctive predicates in the Filter.md#condition[condition expression] (i.e. expressions separated by <code>And</code> expression) and then partitions them into two collections of expressions spark-sql-Expression-SubqueryExpression.md#hasInOrExistsSubquery[with and without In or Exists subquery expressions].</p> <p><code>apply</code> creates a Filter.md#creating-instance[Filter] operator for condition (sub)expressions without subqueries (combined with <code>And</code> expression) if available or takes the Filter.md#child[child] operator (of the input <code>Filter</code> unary operator).</p> <p>In the end, <code>apply</code> creates a new logical plan with Join.md[Join] operators for spark-sql-Expression-Exists.md[Exists] and spark-sql-Expression-In.md[In] expressions (and their negations) as follows:</p> <ul> <li> <p>For spark-sql-Expression-Exists.md[Exists] predicate expressions, <code>apply</code> &lt;&gt; and creates a Join.md#creating-instance[Join] operator with LeftSemi join type. In the end, <code>apply</code> &lt;&gt; <li> <p>For <code>Not</code> expressions with a spark-sql-Expression-Exists.md[Exists] predicate expression, <code>apply</code> &lt;&gt; and creates a Join.md#creating-instance[Join] operator with LeftAnti join type. In the end, <code>apply</code> &lt;&gt; <li> <p>For spark-sql-Expression-In.md[In] predicate expressions with a spark-sql-Expression-ListQuery.md[ListQuery] subquery expression, <code>apply</code> &lt;&gt; followed by &lt;&gt; and creates a Join.md#creating-instance[Join] operator with LeftSemi join type. In the end, <code>apply</code> &lt;&gt; <li> <p>For <code>Not</code> expressions with a spark-sql-Expression-In.md[In] predicate expression with a spark-sql-Expression-ListQuery.md[ListQuery] subquery expression, <code>apply</code> &lt;&gt;, &lt;&gt; followed by splitting conjunctive predicates and creates a Join.md#creating-instance[Join] operator with LeftAnti join type. In the end, <code>apply</code> &lt;&gt; <li> <p>For other predicate expressions, <code>apply</code> &lt;&gt; and creates a Project.md#creating-instance[Project] unary operator with a Filter.md#creating-instance[Filter] operator <p><code>apply</code> is part of the Rule abstraction.</p>"},{"location":"logical-optimizations/SchemaPruning/","title":"SchemaPruning Logical Optimization","text":"<p><code>SchemaPruning</code> is...FIXME</p>"},{"location":"logical-optimizations/SimplifyCasts/","title":"SimplifyCasts Logical Optimization","text":"<p><code>SimplifyCasts</code> is a base logical optimization that &lt;&gt; in the following cases: <p>. The input is already the type to cast to. . The input is of ArrayType or <code>MapType</code> type and contains no <code>null</code> elements.</p> <p><code>SimplifyCasts</code> is part of the Operator Optimization before Inferring Filters fixed-point batch in the standard batches of the Logical Optimizer.</p> <p><code>SimplifyCasts</code> is simply a &lt;&gt; for transforming &lt;&gt;, i.e. <code>Rule[LogicalPlan]</code>."},{"location":"logical-optimizations/SimplifyCasts/#source-scala","title":"[source, scala]","text":"<p>// Case 1. The input is already the type to cast to scala&gt; val ds = spark.range(1) ds: org.apache.spark.sql.Dataset[Long] = [id: bigint]</p> <p>scala&gt; ds.printSchema root  |-- id: long (nullable = false)</p> <p>scala&gt; ds.selectExpr(\"CAST (id AS long)\").explain(true) ... TRACE SparkOptimizer: === Applying Rule org.apache.spark.sql.catalyst.optimizer.SimplifyCasts === !Project [cast(id#0L as bigint) AS id#7L]   Project [id#0L AS id#7L]  +- Range (0, 1, step=1, splits=Some(8))    +- Range (0, 1, step=1, splits=Some(8))</p> <p>TRACE SparkOptimizer: === Applying Rule org.apache.spark.sql.catalyst.optimizer.RemoveAliasOnlyProject === !Project [id#0L AS id#7L]                  Range (0, 1, step=1, splits=Some(8)) !+- Range (0, 1, step=1, splits=Some(8))</p> <p>TRACE SparkOptimizer: Fixed point reached for batch Operator Optimizations after 2 iterations. DEBUG SparkOptimizer: === Result of Batch Operator Optimizations === !Project [cast(id#0L as bigint) AS id#7L]   Range (0, 1, step=1, splits=Some(8)) !+- Range (0, 1, step=1, splits=Some(8)) ... == Parsed Logical Plan == 'Project [unresolvedalias(cast('id as bigint), None)] +- Range (0, 1, step=1, splits=Some(8))</p> <p>== Analyzed Logical Plan == id: bigint Project [cast(id#0L as bigint) AS id#7L] +- Range (0, 1, step=1, splits=Some(8))</p> <p>== Optimized Logical Plan == Range (0, 1, step=1, splits=Some(8))</p> <p>== Physical Plan == *Range (0, 1, step=1, splits=Some(8))</p> <p>// Case 2A. The input is of <code>ArrayType</code> type and contains no <code>null</code> elements. scala&gt; val intArray = Seq(Array(1)).toDS intArray: org.apache.spark.sql.Dataset[Array[Int]] = [value: array] <p>scala&gt; intArray.printSchema root  |-- value: array (nullable = true)  |    |-- element: integer (containsNull = false)</p> <p>scala&gt; intArray.map(arr =&gt; arr.sum).explain(true) ... TRACE SparkOptimizer: === Applying Rule org.apache.spark.sql.catalyst.optimizer.SimplifyCasts ===  SerializeFromObject [input[0, int, true] AS value#36]                                                       SerializeFromObject [input[0, int, true] AS value#36]  +- MapElements , class [I, [StructField(value,ArrayType(IntegerType,false),true)], obj#35: int   +- MapElements , class [I, [StructField(value,ArrayType(IntegerType,false),true)], obj#35: int !   +- DeserializeToObject cast(value#15 as array).toIntArray, obj#34: [I                                  +- DeserializeToObject value#15.toIntArray, obj#34: [I        +- LocalRelation [value#15]                                                                                 +- LocalRelation [value#15] <p>TRACE SparkOptimizer: Fixed point reached for batch Operator Optimizations after 2 iterations. DEBUG SparkOptimizer: === Result of Batch Operator Optimizations ===  SerializeFromObject [input[0, int, true] AS value#36]                                                       SerializeFromObject [input[0, int, true] AS value#36]  +- MapElements , class [I, [StructField(value,ArrayType(IntegerType,false),true)], obj#35: int   +- MapElements , class [I, [StructField(value,ArrayType(IntegerType,false),true)], obj#35: int !   +- DeserializeToObject cast(value#15 as array).toIntArray, obj#34: [I                                  +- DeserializeToObject value#15.toIntArray, obj#34: [I        +- LocalRelation [value#15]                                                                                 +- LocalRelation [value#15] ... == Parsed Logical Plan == 'SerializeFromObject [input[0, int, true] AS value#36] +- 'MapElements , class [I, [StructField(value,ArrayType(IntegerType,false),true)], obj#35: int    +- 'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, ArrayType(IntegerType,false)), ArrayType(IntegerType,false), - root class: \"scala.Array\").toIntArray), obj#34: [I       +- LocalRelation [value#15] <p>== Analyzed Logical Plan == value: int SerializeFromObject [input[0, int, true] AS value#36] +- MapElements , class [I, [StructField(value,ArrayType(IntegerType,false),true)], obj#35: int    +- DeserializeToObject cast(value#15 as array).toIntArray, obj#34: [I       +- LocalRelation [value#15] <p>== Optimized Logical Plan == SerializeFromObject [input[0, int, true] AS value#36] +- MapElements , class [I, [StructField(value,ArrayType(IntegerType,false),true)], obj#35: int    +- DeserializeToObject value#15.toIntArray, obj#34: [I       +- LocalRelation [value#15] <p>== Physical Plan == *SerializeFromObject [input[0, int, true] AS value#36] +- *MapElements , obj#35: int    +- *DeserializeToObject value#15.toIntArray, obj#34: [I       +- LocalTableScan [value#15] <p>// Case 2B. The input is of <code>MapType</code> type and contains no <code>null</code> elements. scala&gt; val mapDF = Seq((\"one\", 1), (\"two\", 2)).toDF(\"k\", \"v\").withColumn(\"m\", map(col(\"k\"), col(\"v\"))) mapDF: org.apache.spark.sql.DataFrame = [k: string, v: int ... 1 more field]</p> <p>scala&gt; mapDF.printSchema root  |-- k: string (nullable = true)  |-- v: integer (nullable = false)  |-- m: map (nullable = false)  |    |-- key: string  |    |-- value: integer (valueContainsNull = false)</p> <p>scala&gt; mapDF.selectExpr(\"\"\"CAST (m AS map)\"\"\").explain(true) ... TRACE SparkOptimizer: === Applying Rule org.apache.spark.sql.catalyst.optimizer.SimplifyCasts === !Project [cast(map(_1#250, _2#251) as map) AS m#272]   Project [map(_1#250, _2#251) AS m#272]  +- LocalRelation [_1#250, _2#251]                                 +- LocalRelation [_1#250, _2#251] ... == Parsed Logical Plan == 'Project [unresolvedalias(cast('m as map), None)] +- Project [k#253, v#254, map(k#253, v#254) AS m#258]    +- Project [_1#250 AS k#253, _2#251 AS v#254]       +- LocalRelation [_1#250, _2#251] <p>== Analyzed Logical Plan == m: map Project [cast(m#258 as map) AS m#272] +- Project [k#253, v#254, map(k#253, v#254) AS m#258]    +- Project [_1#250 AS k#253, _2#251 AS v#254]       +- LocalRelation [_1#250, _2#251] <p>== Optimized Logical Plan == LocalRelation [m#272]</p> <p>== Physical Plan == LocalTableScan [m#272]</p> <p>=== [[apply]] Executing Rule -- <code>apply</code> Method</p>"},{"location":"logical-optimizations/SimplifyCasts/#source-scala_1","title":"[source, scala]","text":""},{"location":"logical-optimizations/SimplifyCasts/#applyplan-logicalplan-logicalplan","title":"apply(plan: LogicalPlan): LogicalPlan","text":"<p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>"},{"location":"logical-optimizations/UpdateAttributeNullability/","title":"UpdateAttributeNullability Logical Optimization","text":"<p><code>UpdateAttributeNullability</code> is...FIXME</p>"},{"location":"logical-optimizations/UpdateCTERelationStats/","title":"UpdateCTERelationStats Logical Optimization","text":"<p><code>UpdateCTERelationStats</code> is a logical optimization that updateCTEStats for CTE logical operators.</p> <p><code>UpdateCTERelationStats</code> is part of the Update CTE Relation Stats once-executed batch in the standard batches of the Logical Optimizer.</p> <p><code>UpdateCTERelationStats</code> is simply a Catalyst rule for transforming logical plans (<code>Rule[LogicalPlan]</code>).</p>"},{"location":"logical-optimizations/UpdateCTERelationStats/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> does nothing and simply returns the given LogicalPlan when applied to a <code>Subquery</code> or a non-CTE query plan. Otherwise, <code>apply</code> updateCTEStats.</p> <p><code>apply</code>\u00a0is part of the Rule abstraction.</p>","text":""},{"location":"logical-optimizations/UpdateCTERelationStats/#updatectestats","title":"updateCTEStats <pre><code>updateCTEStats(\n  plan: LogicalPlan,\n  statsMap: mutable.HashMap[Long, Statistics]): LogicalPlan\n</code></pre> <p><code>updateCTEStats</code> branches off based on the type of the logical operator:</p> <ol> <li>WithCTE</li> <li>CTERelationRef</li> <li>Others with CTE tree pattern</li> </ol> <p>For all other types, <code>updateCTEStats</code> returns the given <code>LogicalPlan</code>.</p> <p><code>updateCTEStats</code> is a recursive function.</p>","text":""},{"location":"logical-optimizations/V2ScanRelationPushDown/","title":"V2ScanRelationPushDown Logical Optimization","text":"<p><code>V2ScanRelationPushDown</code> is a logical optimization (<code>Rule[LogicalPlan]</code>).</p> <p><code>V2ScanRelationPushDown</code> is a non-excludable optimization and is part of earlyScanPushDownRules.</p>"},{"location":"logical-optimizations/V2ScanRelationPushDown/#executing-rule","title":"Executing Rule <pre><code>apply(plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p>  <p><code>apply</code> runs (applies) the following optimizations on the given LogicalPlan:</p> <ol> <li>Creating ScanBuilders</li> <li>pushDownSample</li> <li>pushDownFilters</li> <li>pushDownAggregates</li> <li>pushDownLimits</li> <li>pruneColumns</li> </ol>","text":""},{"location":"logical-optimizations/V2ScanRelationPushDown/#creating-scanbuilder-for-datasourcev2relation","title":"Creating ScanBuilder (for DataSourceV2Relation) <pre><code>createScanBuilder(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>createScanBuilder</code> transforms DataSourceV2Relations in the given LogicalPlan.</p> <p>For every <code>DataSourceV2Relation</code>, <code>createScanBuilder</code> creates a <code>ScanBuilderHolder</code> with the following:</p> <ul> <li>output schema of the <code>DataSourceV2Relation</code></li> <li>The <code>DataSourceV2Relation</code></li> <li>A ScanBuilder (with the options of the <code>DataSourceV2Relation</code>) of the SupportsRead of the Table of the <code>DataSourceV2Relation</code></li> </ul>","text":""},{"location":"logical-optimizations/V2ScanRelationPushDown/#pushdownsample","title":"pushDownSample <pre><code>pushDownSample(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>pushDownSample</code> transforms <code>Sample</code> operators in the given LogicalPlan.</p>","text":""},{"location":"logical-optimizations/V2ScanRelationPushDown/#pushdownfilters","title":"pushDownFilters <pre><code>pushDownFilters(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>pushDownFilters</code> transforms <code>Filter</code> operators over <code>ScanBuilderHolder</code> in the given LogicalPlan.</p> <p><code>pushDownFilters</code> prints out the following INFO message to the logs:</p> <pre><code>Pushing operators to [name]\nPushed Filters: [pushedFilters]\nPost-Scan Filters: [postScanFilters]\n</code></pre>","text":""},{"location":"logical-optimizations/V2ScanRelationPushDown/#pushdownaggregates","title":"pushDownAggregates <pre><code>pushDownAggregates(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>pushDownAggregates</code> transforms Aggregate operators in the given LogicalPlan.</p> <p><code>pushDownAggregates</code> prints out the following INFO message to the logs:</p> <pre><code>Pushing operators to [name]\nPushed Aggregate Functions: [aggregateExpressions]\nPushed Group by: [groupByExpressions]\nOutput: [output]\n</code></pre>","text":""},{"location":"logical-optimizations/V2ScanRelationPushDown/#pushdownlimits","title":"pushDownLimits <pre><code>pushDownLimits(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>pushDownLimits</code> transforms GlobalLimit operators in the given LogicalPlan.</p>","text":""},{"location":"logical-optimizations/V2ScanRelationPushDown/#prunecolumns","title":"pruneColumns <pre><code>pruneColumns(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>pruneColumns</code> transforms Project and <code>Filter</code> operators over <code>ScanBuilderHolder</code> in the given LogicalPlan and creates DataSourceV2ScanRelation.</p> <p><code>pruneColumns</code> prints out the following INFO message to the logs:</p> <pre><code>Output: [output]\n</code></pre>","text":""},{"location":"logical-optimizations/V2ScanRelationPushDown/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"logical-optimizations/V2Writes/","title":"V2Writes Logical Optimization","text":"<p><code>V2Writes</code> is...FIXME</p>"},{"location":"metadata-columns/","title":"Metadata Columns","text":"<p>Spark 3.1.1 (SPARK-31255) introduced support for MetadataColumns for additional metadata of a row.</p> <p><code>MetadataColumn</code>s can be defined for Tables with SupportsMetadataColumns.</p> <p>Use DESCRIBE TABLE EXTENDED SQL command to display the metadata columns of a table.</p>"},{"location":"metadata-columns/#datasourcev2relation","title":"DataSourceV2Relation <p><code>MetadataColumn</code>s are disregarded (filtered out) from the metadataOutput in DataSourceV2Relation leaf logical operator when in name-conflict with output columns.</p>","text":""},{"location":"new-and-noteworthy/catalog-plugin-api-and-multi-catalog-support/","title":"Catalog Plugin API and Multi-Catalog Support","text":"<p>New in 3.0.0</p> <p>Main abstractions:</p> <ul> <li>CatalogManager</li> <li>CatalogPlugin</li> <li>USE NAMESPACE SQL statement</li> <li>SHOW CURRENT NAMESPACE SQL statement</li> </ul>"},{"location":"new-and-noteworthy/catalog-plugin-api-and-multi-catalog-support/#demo","title":"Demo","text":"<ul> <li>Developing CatalogPlugin</li> </ul>"},{"location":"new-and-noteworthy/catalog-plugin-api-and-multi-catalog-support/#example","title":"Example","text":"<pre><code>SHOW NAMESPACES;\n\nSHOW CURRENT NAMESPACE;\n\nCREATE NAMESPACE IF NOT EXISTS my_ns;\n\nUSE NAMESPACE my_ns;\n\nSHOW CURRENT NAMESPACE;\n</code></pre>"},{"location":"new-and-noteworthy/catalog-plugin-api-and-multi-catalog-support/#references","title":"References","text":""},{"location":"new-and-noteworthy/catalog-plugin-api-and-multi-catalog-support/#articles","title":"Articles","text":"<ul> <li>SPIP: Identifiers for multi-catalog support</li> <li>SPIP: Catalog API for table metadata</li> </ul>"},{"location":"new-and-noteworthy/catalog-plugin-api-and-multi-catalog-support/#videos","title":"Videos","text":"<ul> <li>Improving Apache Spark\u2019s Reliability with DataSourceV2 by Ryan Blue, Netflix</li> </ul>"},{"location":"new-and-noteworthy/explain-command-improved/","title":"Explaining Query Plans Improved","text":"<p>New in 3.0.0</p> <p>Spark 3 comes with new output modes for explaining query plans (using EXPLAIN SQL statement or Dataset.explain operator).</p> <p>EXPLAIN SQL Examples</p> <p>Visit explain.sql for SQL examples of <code>EXPLAIN</code> SQL statement.</p> SPARK-27395 <p>JIRA issue: [SPARK-27395] New format of EXPLAIN command</p>"},{"location":"new-and-noteworthy/explain-command-improved/#example-1","title":"Example 1","text":"<pre><code>EXPLAIN\nSELECT key, max(val)\nFROM\nSELECT col1 key, col2 val\nFROM VALUES (0, 0), (0, 1), (1, 2))\nWHERE key &gt; 0\nGROUP BY key\nHAVING max(val) &gt; 0\n</code></pre> <pre><code>== Physical Plan ==\n*(2) Project [key#10, max(val)#20]\n+- *(2) Filter (isnotnull(max(val#11)#23) AND (max(val#11)#23 &gt; 0))\n   +- *(2) HashAggregate(keys=[key#10], functions=[max(val#11)])\n      +- Exchange hashpartitioning(key#10, 200), true, [id=#32]\n         +- *(1) HashAggregate(keys=[key#10], functions=[partial_max(val#11)])\n            +- *(1) LocalTableScan [key#10, val#11]\n</code></pre>"},{"location":"new-and-noteworthy/explain-command-improved/#example-2","title":"Example 2","text":"<pre><code>EXPLAIN FORMATTED\nSELECT (SELECT avg(a) FROM s1) + (SELECT avg(a) FROM s1)\nFROM s1\nLIMIT 1;\n</code></pre>"},{"location":"new-and-noteworthy/intervals/","title":"ANSI Intervals","text":"<p>Spark SQL supports interval type defined by the ANSI SQL standard using <code>AnsiIntervalType</code>:</p> <ul> <li><code>DayTimeIntervalType</code> for day-time intervals</li> <li><code>YearMonthIntervalType</code> for year-month intervals</li> </ul> <p>Intervals can be positive and negative.</p>"},{"location":"new-and-noteworthy/intervals/#parquet","title":"Parquet","text":"<p>ANSI intervals are supported by parquet data source as follows:</p> <ul> <li><code>DayTimeIntervalType</code> is the same as <code>LongType</code> (<code>INT64</code>)</li> <li><code>YearMonthIntervalType</code> is the same as <code>IntegerType</code> (<code>INT32</code>)</li> </ul>"},{"location":"new-and-noteworthy/intervals/#demo","title":"Demo","text":"<pre><code>select date'today' - date'2021-01-01' as diff\n</code></pre>"},{"location":"new-and-noteworthy/observable-metrics/","title":"Observable Metrics","text":"<p>New in 3.0.0</p> <p>Observable Metrics feature adds a new Dataset.observe operator (that simply creates a CollectMetrics unary logical operator).</p> <p>Observable Metrics were introduced to Apache Spark 3.0.0 as SPARK-29345.</p>"},{"location":"new-and-noteworthy/observable-metrics/#references","title":"References","text":""},{"location":"new-and-noteworthy/observable-metrics/#articles","title":"Articles","text":"<ul> <li>Introducing Apache Spark 3.0</li> </ul>"},{"location":"new-and-noteworthy/observable-metrics/#videos","title":"Videos","text":"<ul> <li>Deep Dive into the New Features of Apache Spark 3.0</li> </ul>"},{"location":"new-and-noteworthy/statistics/","title":"Statistics","text":"<p>Statistics are supported for the following only:</p> <ol> <li>Hive Metastore tables for which <code>ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan</code> has been executed</li> <li>File-based data source tables for which the statistics are computed directly on the files of data</li> </ol>"},{"location":"new-and-noteworthy/statistics/#broadcast-join","title":"Broadcast Join","text":"<p>Broadcast Join can be automatically selected by the Spark Planner based on the Statistics and the spark.sql.autoBroadcastJoinThreshold configuration property.</p>"},{"location":"noop/","title":"Noop Connector","text":"<p>Noop Connector is available as noop format.</p>"},{"location":"noop/NoopBatchWrite/","title":"NoopBatchWrite","text":"<p><code>NoopBatchWrite</code> is...FIXME</p>"},{"location":"noop/NoopDataSource/","title":"NoopDataSource","text":"<p><code>NoopDataSource</code> is a SimpleTableProvider of writable NoopTables.</p>"},{"location":"noop/NoopDataSource/#short-name","title":"Short Name <p><code>NoopDataSource</code> is registered under noop alias.</p>","text":""},{"location":"noop/NoopDataSource/#creating-table","title":"Creating Table <pre><code>getTable(\n  options: CaseInsensitiveStringMap): Table\n</code></pre> <p><code>getTable</code> simply creates a NoopTable.</p> <p><code>getTable</code>\u00a0is part of the SimpleTableProvider abstraction.</p>","text":""},{"location":"noop/NoopStreamingWrite/","title":"NoopStreamingWrite","text":"<p><code>NoopStreamingWrite</code> is...FIXME</p>"},{"location":"noop/NoopTable/","title":"NoopTable","text":"<p><code>NoopTable</code> is a Table that supports write in noop data source.</p>"},{"location":"noop/NoopTable/#name","title":"Name <pre><code>name(): String\n</code></pre> <p><code>name</code> is noop-table.</p> <p><code>name</code>\u00a0is part of the SupportsWrite abstraction.</p>","text":""},{"location":"noop/NoopTable/#capabilities","title":"Capabilities <pre><code>capabilities(): ju.Set[TableCapability]\n</code></pre> <p><code>NoopTable</code> supports the following capabilities:</p> <ul> <li>BATCH_WRITE</li> <li>STREAMING_WRITE</li> <li>TRUNCATE</li> <li>ACCEPT_ANY_SCHEMA</li> </ul> <p><code>capabilities</code>\u00a0is part of the Table abstraction.</p>","text":""},{"location":"noop/NoopTable/#creating-writebuilder","title":"Creating WriteBuilder <pre><code>newWriteBuilder(\n  info: LogicalWriteInfo): WriteBuilder\n</code></pre> <p><code>newWriteBuilder</code> creates a NoopWriteBuilder.</p> <p><code>newWriteBuilder</code>\u00a0is part of the SupportsWrite abstraction.</p>","text":""},{"location":"noop/NoopWriteBuilder/","title":"NoopWriteBuilder","text":"<p><code>NoopWriteBuilder</code> is a WriteBuilder with support for truncate and update.</p>"},{"location":"noop/NoopWriteBuilder/#truncating","title":"Truncating <pre><code>truncate(): WriteBuilder\n</code></pre> <p><code>truncate</code> simply returns this <code>NoopWriteBuilder</code>.</p> <p><code>truncate</code>\u00a0is part of the SupportsTruncate abstraction.</p>","text":""},{"location":"noop/NoopWriteBuilder/#streaming-update","title":"Streaming Update <pre><code>update(): WriteBuilder\n</code></pre> <p><code>update</code> simply returns this <code>NoopWriteBuilder</code>.</p> <p><code>update</code>\u00a0is part of the SupportsStreamingUpdate abstraction.</p>","text":""},{"location":"noop/NoopWriteBuilder/#buildforbatch","title":"buildForBatch <pre><code>buildForBatch(): BatchWrite\n</code></pre> <p><code>buildForBatch</code> gives a NoopBatchWrite.</p> <p><code>buildForBatch</code>\u00a0is part of the WriteBuilder abstraction.</p>","text":""},{"location":"noop/NoopWriteBuilder/#buildforstreaming","title":"buildForStreaming <pre><code>buildForStreaming(): StreamingWrite\n</code></pre> <p><code>buildForStreaming</code> gives a NoopStreamingWrite.</p> <p><code>buildForStreaming</code>\u00a0is part of the WriteBuilder abstraction.</p>","text":""},{"location":"parquet/","title":"Parquet Connector","text":"<p>Apache Parquet is a columnar storage format for the Apache Hadoop ecosystem with support for efficient storage and encoding of data.</p> <p>Parquet Connector uses ParquetDataSourceV2 for <code>parquet</code> datasets and tables with ParquetScan for table scanning (reading) and ParquetWrite for data writing.</p> ParquetFileFormat is Fallback FileFormat <p>The older ParquetFileFormat is used as a fallbackFileFormat for backward-compatibility and Hive (to name a few use cases).</p> <p>Parquet is the default connector format based on the spark.sql.sources.default configuration property.</p> <p>Parquet connector uses <code>spark.sql.parquet</code> prefix for parquet-specific configuration properties.</p>"},{"location":"parquet/#options","title":"Options","text":"<p>ParquetOptions</p>"},{"location":"parquet/#configuration-properties","title":"Configuration Properties","text":""},{"location":"parquet/#reading","title":"Reading","text":"<ul> <li>spark.sql.files.maxPartitionBytes</li> <li>spark.sql.files.minPartitionNum</li> <li>spark.sql.files.openCostInBytes</li> </ul>"},{"location":"parquet/#schema-discovery-inference","title":"Schema Discovery (Inference)","text":"<p>Parquet Connector uses distributed and multi-threaded (concurrent) process for schema discovery.</p> <p>Schema discovery can be configured using the following:</p> <ul> <li>mergeSchema option</li> <li>spark.sql.parquet.respectSummaryFiles</li> </ul>"},{"location":"parquet/#vectorized-parquet-decoding","title":"Vectorized Parquet Decoding","text":"<p>Parquet Connector uses VectorizedParquetRecordReader for Vectorized Parquet Decoding (and ParquetReadSupport otherwise).</p>"},{"location":"parquet/#parquet-cli","title":"Parquet CLI","text":"<p>parquet-cli is Apache Parquet's command-line tools and utilities</p> <pre><code>brew install parquet-cli\n</code></pre>"},{"location":"parquet/#print-parquet-metadata","title":"Print Parquet Metadata","text":"<pre><code>$ parquet help meta\n\nUsage: parquet [general options] meta &lt;parquet path&gt; [command options]\n\n  Description:\n\n    Print a Parquet file's metadata\n</code></pre> <pre><code>spark.range(0, 5, 1, numPartitions = 1)\n.write\n.mode(\"overwrite\")\n.parquet(\"demo.parquet\")\n</code></pre> <pre><code>$ parquet meta demo.parquet/part-00000-9cb6054e-9986-4f04-8ae7-730aac93e7db-c000.snappy.parquet\n\nFile path:  demo.parquet/part-00000-9cb6054e-9986-4f04-8ae7-730aac93e7db-c000.snappy.parquet\nCreated by: parquet-mr version 1.12.3 (build f8dced182c4c1fbdec6ccb3185537b5a01e6ed6b)\nProperties:\n                   org.apache.spark.version: 3.4.0\n  org.apache.spark.sql.parquet.row.metadata: {\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]}\nSchema:\nmessage spark_schema {\n  required int64 id;\n}\n\n\nRow group 0:  count: 5  10.60 B records  start: 4  total(compressed): 53 B total(uncompressed):63 B\n--------------------------------------------------------------------------------\n    type      encodings count     avg size   nulls   min / max\nid  INT64     S   _     5         10.60 B    0       \"0\" / \"4\"\n</code></pre>"},{"location":"parquet/#demo","title":"Demo","text":"<pre><code>val p = spark.read.parquet(\"/tmp/nums.parquet\")\n</code></pre> <pre><code>scala&gt; p.explain\n== Physical Plan ==\n*(1) ColumnarToRow\n+- FileScan parquet [id#3L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/nums.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt;\n</code></pre> <pre><code>val executedPlan = p.queryExecution.executedPlan\n</code></pre> <pre><code>scala&gt; executedPlan.foreachUp { op =&gt; println(op.getClass) }\nclass org.apache.spark.sql.execution.FileSourceScanExec\nclass org.apache.spark.sql.execution.InputAdapter\nclass org.apache.spark.sql.execution.ColumnarToRowExec\nclass org.apache.spark.sql.execution.WholeStageCodegenExec\n</code></pre> <pre><code>import org.apache.spark.sql.execution.FileSourceScanExec\nval scan = executedPlan.collectFirst { case scan: FileSourceScanExec =&gt; scan }.get\n\nimport org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\nval parquetFF = scan.relation.fileFormat.asInstanceOf[ParquetFileFormat]\n</code></pre>"},{"location":"parquet/ParquetDataSourceV2/","title":"ParquetDataSourceV2","text":"<p><code>ParquetDataSourceV2</code> is the FileDataSourceV2 (and hence indirectly a DataSourceRegister) of Parquet Data Source.</p> <p><code>ParquetDataSourceV2</code> uses ParquetTable for scanning and writing.</p> <p><code>ParquetDataSourceV2</code> is registered in <code>META-INF/services/org.apache.spark.sql.sources.DataSourceRegister</code>.</p>"},{"location":"parquet/ParquetDataSourceV2/#creating-instance","title":"Creating Instance","text":"<p><code>ParquetDataSourceV2</code> takes no arguments to be created.</p> <p><code>ParquetDataSourceV2</code> is created when:</p> <ul> <li><code>DataSource</code> utility is used to look up a DataSource for <code>parquet</code> alias</li> </ul>"},{"location":"parquet/ParquetDataSourceV2/#creating-table","title":"Creating Table  Signature <pre><code>getTable(\n  options: CaseInsensitiveStringMap): Table\ngetTable(\n  options: CaseInsensitiveStringMap,\n  schema: StructType): Table\n</code></pre> <p><code>getTable</code> is part of the FileDataSourceV2 abstraction.</p>  <p><code>getTable</code> creates a ParquetTable with the following:</p>    Property Value     name Table name from the paths (and based on the given <code>options</code>)   paths Paths (in the given <code>options</code>)   userSpecifiedSchema The given <code>schema</code>, if given   fallbackFileFormat ParquetFileFormat","text":""},{"location":"parquet/ParquetDataSourceV2/#shortname","title":"shortName  Signature <pre><code>shortName(): String\n</code></pre> <p><code>shortName</code> is part of the DataSourceRegister abstraction.</p>  <p><code>shortName</code> is the following text:</p> <pre><code>parquet\n</code></pre>","text":""},{"location":"parquet/ParquetDataSourceV2/#fallbackfileformat","title":"fallbackFileFormat  Signature <pre><code>fallbackFileFormat: Class[_ &lt;: FileFormat]\n</code></pre> <p><code>fallbackFileFormat</code> is part of the FileDataSourceV2 abstraction.</p>  <p><code>fallbackFileFormat</code> is ParquetFileFormat.</p>","text":""},{"location":"parquet/ParquetFileFormat/","title":"ParquetFileFormat","text":"<p>Obsolete</p> <p><code>ParquetFileFormat</code> is a mere fallbackFileFormat of ParquetDataSourceV2.</p> <p><code>ParquetFileFormat</code> is the FileFormat of Parquet Data Source.</p> <p><code>ParquetFileFormat</code> is splitable.</p> <p><code>ParquetFileFormat</code> is <code>Serializable</code>.</p>"},{"location":"parquet/ParquetFileFormat/#short-name","title":"Short Name <pre><code>shortName(): String\n</code></pre> <p><code>ParquetFileFormat</code> is a DataSourceRegister with the short name:</p> <pre><code>parquet\n</code></pre>","text":""},{"location":"parquet/ParquetFileFormat/#issplitable","title":"isSplitable  Signature <pre><code>isSplitable(\n  sparkSession: SparkSession,\n  options: Map[String, String],\n  path: Path): Boolean\n</code></pre> <p><code>isSplitable</code>\u00a0is part of the FileFormat abstraction.</p>  <p><code>ParquetFileFormat</code> is splitable (<code>true</code>).</p>","text":""},{"location":"parquet/ParquetFileFormat/#building-data-reader-with-partition-values","title":"Building Data Reader With Partition Values  Signature <pre><code>buildReaderWithPartitionValues(\n  sparkSession: SparkSession,\n  dataSchema: StructType,\n  partitionSchema: StructType,\n  requiredSchema: StructType,\n  filters: Seq[Filter],\n  options: Map[String, String],\n  hadoopConf: Configuration): (PartitionedFile) =&gt; Iterator[InternalRow]\n</code></pre> <p><code>buildReaderWithPartitionValues</code>\u00a0is part of the FileFormat abstraction.</p>   <p>Fixme</p> <p>Review Me</p>  <p><code>buildReaderWithPartitionValues</code> sets the following configuration options in the input <code>hadoopConf</code>.</p>    Name Value     parquet.read.support.class ParquetReadSupport   org.apache.spark.sql.parquet.row.requested_schema JSON representation of <code>requiredSchema</code>   org.apache.spark.sql.parquet.row.attributes JSON representation of <code>requiredSchema</code>   spark.sql.session.timeZone spark.sql.session.timeZone   spark.sql.parquet.binaryAsString spark.sql.parquet.binaryAsString   spark.sql.parquet.int96AsTimestamp spark.sql.parquet.int96AsTimestamp    <p><code>buildReaderWithPartitionValues</code> requests <code>ParquetWriteSupport</code> to <code>setSchema</code>.</p> <p><code>buildReaderWithPartitionValues</code> tries to push filters down to create a Parquet <code>FilterPredicate</code> (aka <code>pushed</code>).</p> <p>With spark.sql.parquet.filterPushdown configuration property enabled, <code>buildReaderWithPartitionValues</code> takes the input Spark data source <code>filters</code> and converts them to Parquet filter predicates if possible. Otherwise, the Parquet filter predicate is not specified.</p>  <p>Note</p> <p><code>buildReaderWithPartitionValues</code> creates filter predicates for the following types: BooleanType, IntegerType, (LongType, FloatType, DoubleType, StringType, BinaryType.</p>  <p><code>buildReaderWithPartitionValues</code> broadcasts the input <code>hadoopConf</code> Hadoop <code>Configuration</code>.</p> <p>In the end, <code>buildReaderWithPartitionValues</code> gives a function that takes a PartitionedFile and does the following:</p> <ol> <li> <p>Creates a Hadoop <code>FileSplit</code> for the input <code>PartitionedFile</code></p> </li> <li> <p>Creates a Parquet <code>ParquetInputSplit</code> for the Hadoop <code>FileSplit</code> created</p> </li> <li> <p>Gets the broadcast Hadoop <code>Configuration</code></p> </li> <li> <p>Creates a flag that says whether to apply timezone conversions to int96 timestamps or not (aka <code>convertTz</code>)</p> </li> <li> <p>Creates a Hadoop <code>TaskAttemptContextImpl</code> (with the broadcast Hadoop <code>Configuration</code> and a Hadoop <code>TaskAttemptID</code> for a map task)</p> </li> <li> <p>Sets the Parquet <code>FilterPredicate</code> (only when spark.sql.parquet.filterPushdown configuration property is enabled and it is by default)</p> </li> </ol> <p>The function then branches off on whether Parquet vectorized reader is enabled or not.</p> <p>With Parquet vectorized reader enabled, the function does the following:</p> <ul> <li> <p>Creates a VectorizedParquetRecordReader and a RecordReaderIterator</p> </li> <li> <p>Requests <code>VectorizedParquetRecordReader</code> to initialize (with the Parquet <code>ParquetInputSplit</code> and the Hadoop <code>TaskAttemptContextImpl</code>)</p> </li> <li> <p>Prints out the following DEBUG message to the logs:</p> <pre><code>Appending [partitionSchema] [partitionValues]\n</code></pre> </li> <li> <p>Requests <code>VectorizedParquetRecordReader</code> to initBatch</p> </li> <li> <p>(only with supportBatch enabled) Requests <code>VectorizedParquetRecordReader</code> to enableReturningBatches</p> </li> <li> <p>In the end, the function gives the RecordReaderIterator (over the <code>VectorizedParquetRecordReader</code>) as the <code>Iterator[InternalRow]</code></p> </li> </ul> <p>With Parquet vectorized reader disabled, the function does the following:</p> <ul> <li>FIXME (since Parquet vectorized reader is enabled by default it's of less interest)</li> </ul>","text":""},{"location":"parquet/ParquetFileFormat/#supportbatch","title":"supportBatch  Signature <pre><code>supportBatch(\n  sparkSession: SparkSession,\n  schema: StructType): Boolean\n</code></pre> <p><code>supportBatch</code>\u00a0is part of the FileFormat abstraction.</p>   <p>Fixme</p> <p>Review Me</p>  <p><code>supportBatch</code> supports vectorized parquet decoding in whole-stage code generation when the following all hold:</p> <ol> <li> <p>spark.sql.parquet.enableVectorizedReader configuration property is enabled</p> </li> <li> <p>spark.sql.codegen.wholeStage internal configuration property is enabled</p> </li> <li> <p>The number of fields in the schema is at most spark.sql.codegen.maxFields internal configuration property</p> </li> <li> <p>All the fields in the output schema are of AtomicType</p> </li> </ol>","text":""},{"location":"parquet/ParquetFileFormat/#vector-types","title":"Vector Types  Signature <pre><code>vectorTypes(\n  requiredSchema: StructType,\n  partitionSchema: StructType,\n  sqlConf: SQLConf): Option[Seq[String]]\n</code></pre> <p><code>vectorTypes</code>\u00a0is part of the FileFormat abstraction.</p>   <p>Fixme</p> <p>Review Me</p>  <p><code>vectorTypes</code> creates a collection of the names of OffHeapColumnVector or OnHeapColumnVector when spark.sql.columnVector.offheap.enabled property is enabled or disabled, respectively.</p> <p>The size of the collection are all the fields of the given <code>requiredSchema</code> and <code>partitionSchema</code> schemas.</p>","text":""},{"location":"parquet/ParquetFileFormat/#mergeschemasinparallel","title":"mergeSchemasInParallel <pre><code>mergeSchemasInParallel(\n  parameters: Map[String, String],\n  filesToTouch: Seq[FileStatus],\n  sparkSession: SparkSession): Option[StructType]\n</code></pre> <p><code>mergeSchemasInParallel</code> mergeSchemasInParallel with the given <code>filesToTouch</code> and a multi-threaded parquet footer reader.</p>  <p>FIXME</p> <p>Describe the multi-threaded parquet footer reader.</p>   <p>Note</p> <p>With the multi-threaded parquet footer reader, the whole <code>mergeSchemasInParallel</code> is distributed (using <code>RDD</code> while mergeSchemasInParallel) and multithreaded (per RDD partition).</p>   <p><code>mergeSchemasInParallel</code> is used when:</p> <ul> <li><code>ParquetUtils</code> is requested to infer schema</li> </ul>  <p>Refactoring Needed?</p> <p><code>mergeSchemasInParallel</code> should be moved to <code>ParquetUtils</code> if that's the only place it's called from, huh?!</p>","text":""},{"location":"parquet/ParquetFileFormat/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.ParquetFileFormat.name = org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\nlogger.ParquetFileFormat.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"parquet/ParquetFilters/","title":"ParquetFilters","text":"<p><code>ParquetFilters</code> is...FIXME</p>"},{"location":"parquet/ParquetOptions/","title":"ParquetOptions","text":"<p><code>ParquetOptions</code> is a <code>FileSourceOptions</code>.</p>"},{"location":"parquet/ParquetOptions/#creating-instance","title":"Creating Instance","text":"<p><code>ParquetOptions</code> takes the following to be created:</p> <ul> <li> Parameters <li> SQLConf <p><code>ParquetOptions</code> is created when:</p> <ul> <li><code>HiveOptions</code> is requested to <code>getHiveWriteCompression</code></li> <li><code>ParquetFileFormat</code> is requested to prepareWrite and buildReaderWithPartitionValues</li> <li><code>ParquetUtils</code> is requested to inferSchema</li> <li><code>ParquetScan</code> is requested to createReaderFactory</li> <li><code>ParquetWrite</code> is requested to prepareWrite</li> </ul>"},{"location":"parquet/ParquetOptions/#options","title":"Options","text":""},{"location":"parquet/ParquetOptions/#mergeschema","title":"mergeSchema <p>Controls merging schemas from all Parquet part-files</p> <p>Default: spark.sql.parquet.mergeSchema</p> <p>Used when:</p> <ul> <li><code>ParquetUtils</code> is requested to infer schema</li> </ul>","text":""},{"location":"parquet/ParquetPartitionReaderFactory/","title":"ParquetPartitionReaderFactory","text":"<p><code>ParquetPartitionReaderFactory</code> is a FilePartitionReaderFactory (of ParquetScan) for batch queries in Parquet Connector.</p>"},{"location":"parquet/ParquetPartitionReaderFactory/#creating-instance","title":"Creating Instance","text":"<p><code>ParquetPartitionReaderFactory</code> takes the following to be created:</p> <ul> <li> SQLConf <li> Broadcast variable with a Hadoop Configuration <li> Data schema <li> Read data schema <li> Partition schema <li> Filters <li> Aggregation <li> ParquetOptions <p><code>ParquetPartitionReaderFactory</code> is created when:</p> <ul> <li><code>ParquetScan</code> is requested for a PartitionReaderFactory</li> </ul>"},{"location":"parquet/ParquetPartitionReaderFactory/#enableVectorizedReader","title":"enableVectorizedReader","text":"<p><code>ParquetPartitionReaderFactory</code> defines <code>enableVectorizedReader</code> internal flag to indicate whether isBatchReadSupported for the resultSchema or not.</p> <p><code>enableVectorizedReader</code> internal flag is used for the following:</p> <ul> <li>Indicate whether <code>ParquetPartitionReaderFactory</code> supportsColumnar</li> <li>Creating a vectorized parquet RecordReader when requested for a PartitionReader</li> </ul> <p><code>ParquetPartitionReaderFactory</code> uses <code>enableVectorizedReader</code> flag to determine a Hadoop RecordReader to use when requested for a PartitionReader.</p>"},{"location":"parquet/ParquetPartitionReaderFactory/#enableOffHeapColumnVector","title":"columnVector.offheap.enabled <p><code>ParquetPartitionReaderFactory</code> uses spark.sql.columnVector.offheap.enabled configuration property when requested for the following:</p> <ul> <li>Create a Vectorized Reader (and create a VectorizedParquetRecordReader)</li> <li>Build a Columnar Reader (and <code>convertAggregatesRowToBatch</code>)</li> </ul>","text":""},{"location":"parquet/ParquetPartitionReaderFactory/#supportColumnarReads","title":"supportColumnarReads  Signature <pre><code>supportColumnarReads(\n  partition: InputPartition): Boolean\n</code></pre> <p><code>supportColumnarReads</code> is part of the PartitionReaderFactory abstraction.</p>  <p><code>ParquetPartitionReaderFactory</code> supports columnar reads when the following all hold:</p> <ol> <li>spark.sql.parquet.enableVectorizedReader is enabled</li> <li>spark.sql.codegen.wholeStage is enabled</li> <li>The number of the resultSchema fields is at most spark.sql.codegen.maxFields</li> </ol>","text":""},{"location":"parquet/ParquetPartitionReaderFactory/#buildColumnarReader","title":"Building Columnar PartitionReader  Signature <pre><code>buildColumnarReader(\n  file: PartitionedFile): PartitionReader[ColumnarBatch]\n</code></pre> <p><code>buildColumnarReader</code> is part of the FilePartitionReaderFactory abstraction.</p>  <p><code>buildColumnarReader</code> createVectorizedReader (for the given PartitionedFile) and requests it to enableReturningBatches.</p> <p>In the end, <code>buildColumnarReader</code> returns a PartitionReader that returns ColumnarBatches (when requested for records).</p>","text":""},{"location":"parquet/ParquetPartitionReaderFactory/#buildReader","title":"Building PartitionReader  Signature <pre><code>buildReader(\n  file: PartitionedFile): PartitionReader[InternalRow]\n</code></pre> <p><code>buildReader</code> is part of the FilePartitionReaderFactory abstraction.</p>  <p><code>buildReader</code> determines a Hadoop RecordReader to use based on the enableVectorizedReader flag. When enabled, <code>buildReader</code> createVectorizedReader and createRowBaseReader otherwise.</p> <p>In the end, <code>buildReader</code> creates a <code>PartitionReaderWithPartitionValues</code> (that is a PartitionReader with partition values appended).</p>","text":""},{"location":"parquet/ParquetPartitionReaderFactory/#createRowBaseReader","title":"Creating Row-Based RecordReader","text":"<pre><code>createRowBaseReader(\nfile: PartitionedFile): RecordReader[Void, InternalRow]\n</code></pre> <p><code>createRowBaseReader</code> buildReaderBase for the given PartitionedFile and with createRowBaseParquetReader factory.</p>"},{"location":"parquet/ParquetPartitionReaderFactory/#createRowBaseParquetReader","title":"createRowBaseParquetReader","text":"<pre><code>createRowBaseParquetReader(\npartitionValues: InternalRow,\npushed: Option[FilterPredicate],\nconvertTz: Option[ZoneId],\ndatetimeRebaseSpec: RebaseSpec,\nint96RebaseSpec: RebaseSpec): RecordReader[Void, InternalRow]\n</code></pre> <p><code>createRowBaseParquetReader</code> prints out the following DEBUG message to the logs:</p> <pre><code>Falling back to parquet-mr\n</code></pre> <p><code>createRowBaseParquetReader</code> creates a ParquetReadSupport (with enableVectorizedReader flag disabled).</p> <p><code>createRowBaseParquetReader</code> creates a RecordReaderIterator with a new <code>ParquetRecordReader</code>.</p> <p>In the end, <code>createRowBaseParquetReader</code> returns the <code>ParquetRecordReader</code>.</p>"},{"location":"parquet/ParquetPartitionReaderFactory/#createVectorizedReader","title":"Creating Vectorized Parquet RecordReader <pre><code>createVectorizedReader(\n  file: PartitionedFile): VectorizedParquetRecordReader\n</code></pre> <p><code>createVectorizedReader</code> buildReaderBase (for the given PartitionedFile and createParquetVectorizedReader).</p> <p>In the end, <code>createVectorizedReader</code> requests the VectorizedParquetRecordReader to initBatch (with the partitionSchema and the partitionValues of the given PartitionedFile) and returns it.</p>  <p><code>createVectorizedReader</code> is used when <code>ParquetPartitionReaderFactory</code> is requested for the following:</p> <ul> <li>Build a partition reader (for a file) (with enableVectorizedReader enabled)</li> <li>Build a columnar partition reader (for a file)</li> </ul>","text":""},{"location":"parquet/ParquetPartitionReaderFactory/#createParquetVectorizedReader","title":"createParquetVectorizedReader","text":"<pre><code>createParquetVectorizedReader(\npartitionValues: InternalRow,\npushed: Option[FilterPredicate],\nconvertTz: Option[ZoneId],\ndatetimeRebaseSpec: RebaseSpec,\nint96RebaseSpec: RebaseSpec): VectorizedParquetRecordReader\n</code></pre> <p><code>createParquetVectorizedReader</code> creates a VectorizedParquetRecordReader (with capacity).</p> <p><code>createParquetVectorizedReader</code> creates a RecordReaderIterator (for the <code>VectorizedParquetRecordReader</code>).</p> <p><code>createParquetVectorizedReader</code> prints out the following DEBUG message to the logs (with the partitionSchema and the given <code>partitionValues</code>):</p> <pre><code>Appending [partitionSchema] [partitionValues]\n</code></pre> <p>In the end, <code>createParquetVectorizedReader</code> returns the <code>VectorizedParquetRecordReader</code>.</p> Unused RecordReaderIterator? <p>It appears that the <code>RecordReaderIterator</code> is created but not used. Feeling confused.</p>"},{"location":"parquet/ParquetPartitionReaderFactory/#buildReaderBase","title":"buildReaderBase <pre><code>buildReaderBase[T](\n  file: PartitionedFile,\n  buildReaderFunc: (\n    FileSplit,\n    InternalRow,\n    TaskAttemptContextImpl,\n    Option[FilterPredicate],\n    Option[ZoneId],\n    RebaseSpec,\n    RebaseSpec) =&gt; RecordReader[Void, T]): RecordReader[Void, T]\n</code></pre> <p><code>buildReaderBase</code>...FIXME</p>  <p><code>buildReaderBase</code> is used when:</p> <ul> <li><code>ParquetPartitionReaderFactory</code> is requested to createRowBaseReader and createVectorizedReader</li> </ul>","text":""},{"location":"parquet/ParquetPartitionReaderFactory/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.v2.parquet.ParquetPartitionReaderFactory</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.ParquetPartitionReaderFactory.name = org.apache.spark.sql.execution.datasources.v2.parquet.ParquetPartitionReaderFactory\nlogger.ParquetPartitionReaderFactory.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"parquet/ParquetReadSupport/","title":"ParquetReadSupport","text":"<p><code>ParquetReadSupport</code> is a <code>ReadSupport</code> (Apache Parquet) of UnsafeRows for non-Vectorized Parquet Decoding.</p> <p><code>ParquetReadSupport</code> is the value of <code>parquet.read.support.class</code> Hadoop configuration property for the following:</p> <ul> <li>ParquetFileFormat</li> <li>ParquetScan</li> </ul>"},{"location":"parquet/ParquetReadSupport/#creating-instance","title":"Creating Instance","text":"<p><code>ParquetReadSupport</code> takes the following to be created:</p> <ul> <li> <code>ZoneId</code> (optional) <li> <code>enableVectorizedReader</code> <li> DateTime RebaseSpec <li> int96 RebaseSpec <p><code>ParquetReadSupport</code> is created when:</p> <ul> <li><code>ParquetFileFormat</code> is requested to buildReaderWithPartitionValues (with enableVectorizedReader disabled)</li> <li><code>ParquetPartitionReaderFactory</code> is requested to createRowBaseParquetReader</li> </ul>"},{"location":"parquet/ParquetReadSupport/#logging","title":"Logging","text":"<p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.ParquetReadSupport.name = org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport\nlogger.ParquetReadSupport.level = all\n</code></pre> <p>Refer to Logging.</p>"},{"location":"parquet/ParquetScan/","title":"ParquetScan","text":"<p><code>ParquetScan</code> is the FileScan of Parquet Connector that uses ParquetPartitionReaderFactory with ParquetReadSupport.</p>"},{"location":"parquet/ParquetScan/#creating-instance","title":"Creating Instance","text":"<p><code>ParquetScan</code> takes the following to be created:</p> <ul> <li> SparkSession <li> Hadoop Configuration <li> PartitioningAwareFileIndex <li> Data schema <li> Read data schema <li> Read partition schema <li> Pushed Filters <li> Case-insensitive options <li>Pushed Aggregation</li> <li> Partition filter expressions (optional) <li> Data filter expressions (optional) <p><code>ParquetScan</code> is created when:</p> <ul> <li><code>ParquetScanBuilder</code> is requested to build a Scan</li> </ul>"},{"location":"parquet/ParquetScan/#pushed-aggregation","title":"Pushed Aggregation <pre><code>pushedAggregate: Option[Aggregation] = None\n</code></pre> <p><code>ParquetScan</code> can be given an Aggregation expression (<code>pushedAggregate</code>) when created. The <code>Aggregation</code> is optional and undefined by default (<code>None</code>).</p> <p>The <code>pushedAggregate</code> is pushedAggregations when <code>ParquetScanBuilder</code> is requested to build a ParquetScan.</p> <p>When defined, <code>ParquetScan</code> is no longer isSplitable (since with aggregate pushed down, only the file footer will be read once, so file should not be split across multiple tasks).</p> <p>The <code>Aggregation</code> is used in the following:</p> <ul> <li>getMetaData (as pushedAggregationsStr and pushedGroupByStr)</li> <li>readSchema</li> <li>createReaderFactory (to create a ParquetPartitionReaderFactory)</li> </ul>","text":""},{"location":"parquet/ParquetScan/#creating-partitionreaderfactory","title":"Creating PartitionReaderFactory  Signature <pre><code>createReaderFactory(): PartitionReaderFactory\n</code></pre> <p><code>createReaderFactory</code> is part of the Batch abstraction.</p>  <p><code>createReaderFactory</code> creates a ParquetPartitionReaderFactory (with the Hadoop Configuration broadcast).</p> <p><code>createReaderFactory</code> adds the following properties to the Hadoop Configuration before broadcasting it (to executors).</p>    Name Value     <code>ParquetInputFormat.READ_SUPPORT_CLASS</code> ParquetReadSupport   others","text":""},{"location":"parquet/ParquetScan/#issplitable","title":"isSplitable  Signature <pre><code>isSplitable(\n  path: Path): Boolean\n</code></pre> <p><code>isSplitable</code> is part of the FileScan abstraction.</p>  <p><code>isSplitable</code> is enabled (<code>true</code>) when all the following hold:</p> <ol> <li>pushedAggregate is not specified</li> <li><code>RowIndexUtil.isNeededForSchema</code> is <code>false</code> for the readSchema</li> </ol>","text":""},{"location":"parquet/ParquetScan/#readschema","title":"readSchema  Signature <pre><code>readSchema(): StructType\n</code></pre> <p><code>readSchema</code> is part of the Scan abstraction.</p>  <p><code>readSchema</code> is readDataSchema with aggregate pushed down. Otherwise, <code>readSchema</code> is the default readSchema.</p>","text":""},{"location":"parquet/ParquetScan/#custom-metadata","title":"Custom Metadata  Signature <pre><code>getMetaData(): Map[String, String]\n</code></pre> <p><code>getMetaData</code> is part of the SupportsMetadata abstraction.</p>  <p><code>getMetaData</code> adds the following metadata to the default file-based metadata:</p>    Metadata Value     <code>PushedFilters</code> pushedFilters   <code>PushedAggregation</code> pushedAggregationsStr   <code>PushedGroupBy</code> pushedGroupByStr","text":""},{"location":"parquet/ParquetScanBuilder/","title":"ParquetScanBuilder","text":"<p><code>ParquetScanBuilder</code> is a FileScanBuilder (of ParquetTable) that SupportsPushDownFilters.</p> <p><code>ParquetScanBuilder</code> builds ParquetScans.</p> <p><code>ParquetScanBuilder</code> supportsNestedSchemaPruning.</p>"},{"location":"parquet/ParquetScanBuilder/#creating-instance","title":"Creating Instance","text":"<p><code>ParquetScanBuilder</code> takes the following to be created:</p> <ul> <li> SparkSession <li> PartitioningAwareFileIndex <li> Schema <li> Data Schema <li> Case-Insensitive Options <p><code>ParquetScanBuilder</code> is created when:</p> <ul> <li><code>ParquetTable</code> is requested to newScanBuilder</li> </ul>"},{"location":"parquet/ParquetScanBuilder/#building-scan","title":"Building Scan  Signature <pre><code>build(): Scan\n</code></pre> <p><code>build</code> is part of the ScanBuilder abstraction.</p>  <p><code>build</code> creates a ParquetScan with the following:</p>    ParquetScan Value     fileIndex the given fileIndex   dataSchema the given dataSchema   readDataSchema finalSchema   readPartitionSchema readPartitionSchema   pushedFilters pushedDataFilters   options the given options   pushedAggregate pushedAggregations   partitionFilters partitionFilters   dataFilters dataFilters","text":""},{"location":"parquet/ParquetScanBuilder/#pushedaggregations","title":"pushedAggregations <pre><code>pushedAggregations: Option[Aggregation]\n</code></pre> <p><code>ParquetScanBuilder</code> defines <code>pushedAggregations</code> registry for an Aggregation.</p> <p>The <code>pushedAggregations</code> is undefined when <code>ParquetScanBuilder</code> is created and can only be assigned when pushAggregation.</p> <p><code>pushedAggregations</code> controls the finalSchema. When undefined, the finalSchema is readDataSchema when building a ParquetScan.</p> <p><code>pushedAggregations</code> is used to create a ParquetScan.</p>","text":""},{"location":"parquet/ParquetScanBuilder/#pushaggregation","title":"pushAggregation  Signature <pre><code>pushAggregation(\n  aggregation: Aggregation): Boolean\n</code></pre> <p><code>pushAggregation</code> is part of the SupportsPushDownAggregates abstraction.</p>  <p><code>pushAggregation</code> does nothing and returns <code>false</code> for spark.sql.parquet.aggregatePushdown disabled.</p> <p><code>pushAggregation</code> determines the data schema for aggregate to be pushed down.</p> <p>With the schema determined, <code>pushAggregation</code> registers it as finalSchema and the given Aggregation as pushedAggregations. <code>pushAggregation</code> returns <code>true</code>.</p> <p>Otherwise, <code>pushAggregation</code> returns <code>false</code>.</p>","text":""},{"location":"parquet/ParquetScanBuilder/#pushdatafilters","title":"pushDataFilters  Signature <pre><code>pushDataFilters(\n  dataFilters: Array[Filter]): Array[Filter]\n</code></pre> <p><code>pushDataFilters</code> is part of the FileScanBuilder abstraction.</p>   <p>spark.sql.parquet.filterPushdown</p> <p><code>pushDataFilters</code> does nothing and returns no Catalyst Filters with spark.sql.parquet.filterPushdown disabled.</p>  <p><code>pushDataFilters</code> creates a ParquetFilters with the readDataSchema (converted into the corresponding parquet schema) and the following configuration properties:</p> <ul> <li>spark.sql.parquet.filterPushdown.date</li> <li>spark.sql.parquet.filterPushdown.decimal</li> <li>spark.sql.parquet.filterPushdown.string.startsWith</li> <li>spark.sql.parquet.filterPushdown.timestamp</li> <li>spark.sql.parquet.pushdown.inFilterThreshold</li> <li>spark.sql.caseSensitive</li> </ul> <p>In the end, <code>pushedParquetFilters</code> requests the <code>ParquetFilters</code> for the convertibleFilters for the given <code>dataFilters</code>.</p>","text":""},{"location":"parquet/ParquetScanBuilder/#supportsnestedschemapruning","title":"supportsNestedSchemaPruning  Signature <pre><code>supportsNestedSchemaPruning: Boolean\n</code></pre> <p><code>supportsNestedSchemaPruning</code> is part of the FileScanBuilder abstraction.</p>  <p><code>supportsNestedSchemaPruning</code> is enabled (<code>true</code>).</p>","text":""},{"location":"parquet/ParquetTable/","title":"ParquetTable","text":"<p><code>ParquetTable</code> is a FileTable of ParquetDataSourceV2 in Parquet Data Source.</p> <p><code>ParquetTable</code> uses ParquetScanBuilder for scanning and ParquetWrite for writing.</p>"},{"location":"parquet/ParquetTable/#creating-instance","title":"Creating Instance","text":"<p><code>ParquetTable</code> takes the following to be created:</p> <ul> <li> Name <li> SparkSession <li> Case-insensitive options <li> Paths <li> User-specified schema <li> Fallback FileFormat <p><code>ParquetTable</code> is created when:</p> <ul> <li><code>ParquetDataSourceV2</code> is requested for a Table</li> </ul>"},{"location":"parquet/ParquetTable/#format-name","title":"Format Name  Signature <pre><code>formatName: String\n</code></pre> <p><code>formatName</code> is part of the FileTable abstraction.</p>  <p><code>formatName</code> is the following text:</p> <pre><code>Parquet\n</code></pre>","text":""},{"location":"parquet/ParquetTable/#schema-inference","title":"Schema Inference  Signature <pre><code>inferSchema(\n  files: Seq[FileStatus]): Option[StructType]\n</code></pre> <p><code>inferSchema</code> is part of the FileTable abstraction.</p>  <p><code>inferSchema</code> infers the schema (with the options and the input Hadoop <code>FileStatus</code>es).</p>","text":""},{"location":"parquet/ParquetTable/#creating-scanbuilder","title":"Creating ScanBuilder  Signature <pre><code>newScanBuilder(\n  options: CaseInsensitiveStringMap): ParquetScanBuilder\n</code></pre> <p><code>newScanBuilder</code> is part of the SupportsRead abstraction.</p>  <p><code>newScanBuilder</code> creates a ParquetScanBuilder with the following:</p> <ul> <li>fileIndex</li> <li>schema</li> <li>dataSchema</li> <li>options</li> </ul>","text":""},{"location":"parquet/ParquetTable/#creating-writebuilder","title":"Creating WriteBuilder  Signature <pre><code>newWriteBuilder(\n  info: LogicalWriteInfo): WriteBuilder\n</code></pre> <p><code>newWriteBuilder</code> is part of the SupportsWrite abstraction.</p>  <p><code>newWriteBuilder</code> creates a WriteBuilder that creates a ParquetWrite (when requested to build a Write).</p>","text":""},{"location":"parquet/ParquetTable/#supportsdatatype","title":"supportsDataType  Signature <pre><code>supportsDataType(\n  dataType: DataType): Boolean\n</code></pre> <p><code>supportsDataType</code> is part of the FileTable abstraction.</p>  <p><code>supportsDataType</code> supports all AtomicTypes and the following complex DataTypes with <code>AtomicType</code>s:</p> <ul> <li>ArrayType</li> <li><code>MapType</code></li> <li>StructType</li> <li>UserDefinedType</li> </ul>","text":""},{"location":"parquet/ParquetUtils/","title":"ParquetUtils","text":""},{"location":"parquet/ParquetUtils/#infering-schema-schema-discovery","title":"Infering Schema (Schema Discovery) <pre><code>inferSchema(\n  sparkSession: SparkSession,\n  parameters: Map[String, String],\n  files: Seq[FileStatus]): Option[StructType]\n</code></pre> <p><code>inferSchema</code> determines which file(s) to touch in order to determine the schema:</p> <ul> <li> <p>With mergeSchema enabled, <code>inferSchema</code> merges part, metadata and common metadata files.</p> <p>Data part-files are skipped with spark.sql.parquet.respectSummaryFiles enabled.</p> </li> <li> <p>With mergeSchema disabled, <code>inferSchema</code> prefers summary files (with <code>_common_metadata</code>s preferable over <code>_metadata</code>s as they contain no extra row groups information and hence are smaller for large Parquet files with lots of row groups). </p> <p><code>inferSchema</code> falls back to a random part-file.</p> <p><code>inferSchema</code> takes the very first parquet file (ordered by path) from the following (until a file is found):</p> <ol> <li><code>_common_metadata</code> files</li> <li><code>_metadata</code> files</li> <li>data part-files</li> </ol> </li> </ul>  <p><code>inferSchema</code> creates a ParquetOptions (with the input <code>parameters</code> and the <code>SparkSession</code>'s SQLConf) to read the value of mergeSchema option.</p> <p><code>inferSchema</code> reads the value of spark.sql.parquet.respectSummaryFiles configuration property.</p> <p><code>inferSchema</code> organizes parquet files by type for the given <code>FileStatus</code>es (Apache Hadoop).</p> <p><code>inferSchema</code> mergeSchemasInParallel for the files to touch.</p>  <p><code>inferSchema</code> is used when:</p> <ul> <li><code>ParquetFileFormat</code> is requested to infer schema</li> <li><code>ParquetTable</code> is requested to infer schema</li> </ul>","text":""},{"location":"parquet/ParquetUtils/#organizing-parquet-files-by-type","title":"Organizing Parquet Files by Type <pre><code>splitFiles(\n  allFiles: Seq[FileStatus]): FileTypes\n</code></pre> <p><code>splitFiles</code> sorts the given <code>FileStatus</code>es (Apache Hadoop) by path.</p> <p><code>splitFiles</code> creates a <code>FileTypes</code> with the following:</p> <ul> <li>Data files (i.e., files that are not summary files so neither <code>_common_metadata</code> nor <code>_metadata</code>)</li> <li>Metadata files (<code>_metadata</code>)</li> <li>Common metadata files (<code>_common_metadata</code>)</li> </ul>","text":""},{"location":"parquet/ParquetUtils/#isbatchreadsupportedforschema","title":"isBatchReadSupportedForSchema <pre><code>isBatchReadSupportedForSchema(\n  sqlConf: SQLConf,\n  schema: StructType): Boolean\n</code></pre> <p><code>isBatchReadSupportedForSchema</code>...FIXME</p>  <p><code>isBatchReadSupportedForSchema</code> is used when:</p> <ul> <li><code>ParquetFileFormat</code> is requested to supportBatch and buildReaderWithPartitionValues</li> <li><code>ParquetPartitionReaderFactory</code> is created</li> </ul>","text":""},{"location":"parquet/ParquetUtils/#isbatchreadsupported","title":"isBatchReadSupported <pre><code>isBatchReadSupported(\n  sqlConf: SQLConf,\n  dt: DataType): Boolean\n</code></pre> <p><code>isBatchReadSupported</code>...FIXME</p>","text":""},{"location":"parquet/ParquetUtils/#prepareWrite","title":"prepareWrite <pre><code>prepareWrite(\n  sqlConf: SQLConf,\n  job: Job,\n  dataSchema: StructType,\n  parquetOptions: ParquetOptions): OutputWriterFactory\n</code></pre> <p><code>prepareWrite</code>...FIXME</p>  <p><code>prepareWrite</code> is used when:</p> <ul> <li><code>ParquetFileFormat</code> is requested to prepareWrite</li> <li><code>ParquetWrite</code> is requested to prepareWrite</li> </ul>","text":""},{"location":"parquet/ParquetUtils/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.parquet.ParquetUtils</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.ParquetUtils.name = org.apache.spark.sql.execution.datasources.parquet.ParquetUtils\nlogger.ParquetUtils.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"parquet/ParquetWrite/","title":"ParquetWrite","text":"<p><code>ParquetWrite</code> is a FileWrite of ParquetTable in Parquet Connector.</p>"},{"location":"parquet/ParquetWrite/#creating-instance","title":"Creating Instance","text":"<p><code>ParquetWrite</code> takes the following to be created:</p> <ul> <li> Paths <li> Format Name <li> <code>supportsDataType</code> function (<code>DataType =&gt; Boolean</code>) <li> <code>LogicalWriteInfo</code> <p><code>ParquetWrite</code> is created when:</p> <ul> <li><code>ParquetTable</code> is requested for a WriteBuilder</li> </ul>"},{"location":"parquet/ParquetWrite/#prepareWrite","title":"Preparing Write Job","text":"FileWrite <pre><code>prepareWrite(\nsqlConf: SQLConf,\njob: Job,\noptions: Map[String, String],\ndataSchema: StructType): OutputWriterFactory\n</code></pre> <p><code>prepareWrite</code> is part of the FileWrite abstraction.</p> <p><code>prepareWrite</code> creates a ParquetOptions (for the given <code>options</code> and <code>SQLConf</code>).</p> <p>In the end, <code>prepareWrite</code> prepareWrite.</p>"},{"location":"parquet/ParquetWriteSupport/","title":"ParquetWriteSupport","text":"<p><code>ParquetWriteSupport</code> is...FIXME</p>"},{"location":"parquet/SparkToParquetSchemaConverter/","title":"SparkToParquetSchemaConverter","text":"<p><code>SparkToParquetSchemaConverter</code> is...FIXME</p>"},{"location":"parquet/SpecificParquetRecordReaderBase/","title":"SpecificParquetRecordReaderBase \u2014 Hadoop RecordReader","text":"<p><code>SpecificParquetRecordReaderBase</code> is the base Hadoop <code>RecordReader</code> for parquet format readers that directly materialize to <code>T</code>.</p> <p>NOTE: https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/RecordReader.html[RecordReader] reads <code>&lt;key, value&gt;</code> pairs from an Hadoop <code>InputSplit</code>.</p> <p>[[internal-registries]] .SpecificParquetRecordReaderBase's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| [[sparkSchema]] <code>sparkSchema</code> | Spark schema</p> <p>Initialized when <code>SpecificParquetRecordReaderBase</code> is requested to &lt;&gt; (from the value of org.apache.spark.sql.parquet.row.requested_schema configuration as set when <code>ParquetFileFormat</code> is requested to build a data reader with partition column values appended) |=== <p>=== [[initialize]] <code>initialize</code> Method</p>"},{"location":"parquet/SpecificParquetRecordReaderBase/#source-scala","title":"[source, scala]","text":""},{"location":"parquet/SpecificParquetRecordReaderBase/#void-initializeinputsplit-inputsplit-taskattemptcontext-taskattemptcontext","title":"void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)","text":"<p>NOTE: <code>initialize</code> is part of ++https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/RecordReader.html#initialize(org.apache.hadoop.mapreduce.InputSplit,%20org.apache.hadoop.mapreduce.TaskAttemptContext)++[RecordReader Contract] to initialize a <code>RecordReader</code>.</p> <p><code>initialize</code>...FIXME</p>"},{"location":"parquet/VectorizedColumnReader/","title":"VectorizedColumnReader","text":"<p><code>VectorizedColumnReader</code> is a vectorized column reader that VectorizedParquetRecordReader uses for Vectorized Parquet Decoding.</p>"},{"location":"parquet/VectorizedParquetRecordReader/","title":"VectorizedParquetRecordReader","text":"<p><code>VectorizedParquetRecordReader</code> is a SpecificParquetRecordReaderBase for Parquet Data Source for Vectorized Decoding.</p>"},{"location":"parquet/VectorizedParquetRecordReader/#creating-instance","title":"Creating Instance","text":"<p><code>VectorizedParquetRecordReader</code> takes the following to be created:</p> <ul> <li> <code>ZoneId</code> (for timezone conversion) <li> Datetime Rebase Mode <li> Datetime Rebase Timezone <li> int96 Rebase Mode <li> int96 Rebase Timezone <li>useOffHeap</li> <li>Capacity</li> <p><code>VectorizedParquetRecordReader</code> is created when:</p> <ul> <li><code>ParquetFileFormat</code> is requested to buildReaderWithPartitionValues (with enableVectorizedReader flag enabled)</li> <li><code>ParquetPartitionReaderFactory</code> is requested to createParquetVectorizedReader</li> </ul>"},{"location":"parquet/VectorizedParquetRecordReader/#capacity","title":"Capacity <p><code>VectorizedParquetRecordReader</code> is given capacity when created.</p> <p>The capacity is configured using spark.sql.parquet.columnarReaderBatchSize configuration property.</p>","text":""},{"location":"parquet/VectorizedParquetRecordReader/#memory-mode","title":"Memory Mode <p><code>VectorizedParquetRecordReader</code> uses the given useOffHeap to initialize the internal <code>MEMORY_MODE</code> registry when created:</p>    useOffHeap MEMORY_MODE WritableColumnVector     <code>true</code> <code>OFF_HEAP</code> OffHeapColumnVector   <code>false</code> <code>ON_HEAP</code> OnHeapColumnVector    <p>The <code>MEMORY_MODE</code> is used to initBatch (to choose the correct implementation of WritableColumnVector).</p>  <p>useOffHeap</p> <p><code>useOffHeap</code> is the value of spark.sql.columnVector.offheap.enabled configuration property when the following are executed with spark.sql.parquet.enableVectorizedReader enabled and supported schema for columnar read:</p> <ul> <li><code>ParquetFileFormat</code> is requested to buildReaderWithPartitionValues</li> <li><code>ParquetPartitionReaderFactory</code> is requested to createVectorizedReader</li> </ul>","text":""},{"location":"parquet/VectorizedParquetRecordReader/#writablecolumnvectors","title":"WritableColumnVectors <p><code>VectorizedParquetRecordReader</code> defines an array of allocated WritableColumnVectors.</p> <p><code>columnVectors</code> is allocated when initBatch.</p>  <p><code>columnVectors</code> is used when:</p> <ul> <li>initBatch</li> <li>nextBatch</li> </ul>","text":""},{"location":"parquet/VectorizedParquetRecordReader/#enablereturningbatches","title":"enableReturningBatches <pre><code>void enableReturningBatches()\n</code></pre> <p><code>enableReturningBatches</code> simply turns the returnColumnarBatch flag on.</p>  <p><code>enableReturningBatches</code> is used when:</p> <ul> <li><code>ParquetFileFormat</code> is requested to buildReaderWithPartitionValues</li> <li><code>ParquetPartitionReaderFactory</code> is requested to buildColumnarReader</li> </ul>","text":""},{"location":"parquet/VectorizedParquetRecordReader/#initializing-columnar-batch","title":"Initializing Columnar Batch <pre><code>void initBatch() // (1)!\nvoid initBatch(\n  StructType partitionColumns,\n  InternalRow partitionValues) // (2)!\nvoid initBatch(\n  MemoryMode memMode,\n  StructType partitionColumns,\n  InternalRow partitionValues) // (3)!\n</code></pre> <ol> <li>Uses the MEMORY_MODE and no partitionColumns nor partitionValues</li> <li>Uses the MEMORY_MODE</li> <li>A private helper method</li> </ol>  <p>MemoryMode</p> <p>The given <code>MemoryMode</code> is the value of MEMORY_MODE.</p>  <p><code>initBatch</code> creates a batch schema that is sparkSchema and the input <code>partitionColumns</code> schema (if available).</p> <p><code>initBatch</code> requests OffHeapColumnVector or OnHeapColumnVector to allocate column vectors per the input <code>memMode</code> (i.e., OFF_HEAP or ON_HEAP memory modes, respectively). <code>initBatch</code> records the allocated column vectors as the internal WritableColumnVectors.</p>  <p>spark.sql.columnVector.offheap.enabled</p> <p>OnHeapColumnVector is used based on spark.sql.columnVector.offheap.enabled configuration property.</p>  <p><code>initBatch</code> creates a ColumnarBatch (with the allocated WritableColumnVectors).</p> <p><code>initBatch</code> does some additional maintenance to the WritableColumnVectors.</p>  <p><code>initBatch</code> is used when:</p> <ul> <li><code>ParquetFileFormat</code> is requested to build a data reader (with partition column values appended)</li> <li><code>ParquetPartitionReaderFactory</code> is requested to createVectorizedReader</li> <li><code>VectorizedParquetRecordReader</code> is requested for a result ColumnarBatch</li> </ul>","text":""},{"location":"parquet/VectorizedParquetRecordReader/#allocating-columnvectors","title":"Allocating ColumnVectors <pre><code>ColumnVector[] allocateColumns(\n  int capacity,\n  StructType schema,\n  boolean useOffHeap,\n  int constantColumnLength)\n</code></pre> <p><code>allocateColumns</code> creates ColumnVectors (one per every field in the given StructType).</p> <p>For the given <code>useOffHeap</code> enabled, <code>allocateColumns</code> creates OffHeapColumnVectors. Otherwise, <code>allocateColumns</code> creates OnHeapColumnVectors.</p>  <p>useOffHeap</p> <p><code>useOffHeap</code> flag is enabled when <code>memMode</code> of initBatch is <code>MemoryMode.OFF_HEAP</code>.</p>","text":""},{"location":"parquet/VectorizedParquetRecordReader/#nextkeyvalue","title":"nextKeyValue  Signature <pre><code>boolean nextKeyValue()\n</code></pre> <p><code>nextKeyValue</code> is part of the <code>RecordReader</code> (Apache Hadoop) abstraction.</p>  <p><code>nextKeyValue</code> resultBatch.</p> <p>With returnColumnarBatch enabled, <code>nextKeyValue</code> returns nextBatch.</p> <p>Otherwise, <code>nextKeyValue</code>...FIXME</p>  <p><code>nextKeyValue</code> is used when:</p> <ul> <li><code>ParquetPartitionReaderFactory</code> is requested to build a PartitionReader and buildColumnarReader</li> </ul>","text":""},{"location":"parquet/VectorizedParquetRecordReader/#resultbatch","title":"resultBatch <pre><code>ColumnarBatch resultBatch()\n</code></pre> <p><code>resultBatch</code> returns the columnarBatch if available. Otherwise, <code>resultBatch</code> initBatch first.</p>","text":""},{"location":"partition-file-metadata-caching/","title":"Partition File Metadata Caching","text":"<p>Partition File Metadata Caching uses FileStatusCache to speed up partition file listing.</p>"},{"location":"partition-file-metadata-caching/#configuration-properties","title":"Configuration Properties","text":"<ul> <li>spark.sql.hive.filesourcePartitionFileCacheSize</li> <li>spark.sql.hive.manageFilesourcePartitions</li> <li>spark.sql.metadataCacheTTLSeconds</li> </ul>"},{"location":"physical-operators/","title":"Physical Operators","text":"<p>Physical Operators (Physical Relational Operators) are building blocks of physical query plans.</p> <p>Physical Query Plan is a tree of nodes of physical operators that in turn can have (trees of) Catalyst expressions. In other words, there are at least two trees at every level (operator).</p> <p>The main abstraction is SparkPlan that is a recursive data structure with zero, one, two or more child logical operators:</p> <ul> <li><code>LeafExecNode</code></li> <li>UnaryExecNode</li> <li><code>BinaryExecNode</code></li> </ul>"},{"location":"physical-operators/AQEShuffleReadExec/","title":"AQEShuffleReadExec Physical Operator","text":"<p><code>AQEShuffleReadExec</code> is a unary physical operator in Adaptive Query Execution.</p>"},{"location":"physical-operators/AQEShuffleReadExec/#creating-instance","title":"Creating Instance","text":"<p><code>AQEShuffleReadExec</code> takes the following to be created:</p> <ul> <li>Child physical operator</li> <li> <code>ShufflePartitionSpec</code>s (requires at least one partition) <p><code>AQEShuffleReadExec</code> is created\u00a0when the following adaptive physical optimizations are executed:</p> <ul> <li>CoalesceShufflePartitions</li> <li>OptimizeShuffleWithLocalRead</li> <li>OptimizeSkewedJoin</li> <li>OptimizeSkewInRebalancePartitions</li> </ul>"},{"location":"physical-operators/AQEShuffleReadExec/#performance-metrics","title":"Performance Metrics <p><code>metrics</code>\u00a0is part of the SparkPlan abstraction.</p>  <p><code>metrics</code> is defined only when the shuffleStage is defined.</p>  Lazy Value <p><code>metrics</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>","text":""},{"location":"physical-operators/AQEShuffleReadExec/#number-of-coalesced-partitions","title":"number of coalesced partitions <p>Only when hasCoalescedPartition</p>","text":""},{"location":"physical-operators/AQEShuffleReadExec/#number-of-partitions","title":"number of partitions","text":""},{"location":"physical-operators/AQEShuffleReadExec/#number-of-skewed-partition-splits","title":"number of skewed partition splits <p>Only when hasSkewedPartition</p>","text":""},{"location":"physical-operators/AQEShuffleReadExec/#number-of-skewed-partitions","title":"number of skewed partitions <p>Only when hasSkewedPartition</p>","text":""},{"location":"physical-operators/AQEShuffleReadExec/#partition-data-size","title":"partition data size <p>Only when non-isLocalRead</p>","text":""},{"location":"physical-operators/AQEShuffleReadExec/#child-shufflequerystageexec","title":"Child ShuffleQueryStageExec <pre><code>shuffleStage: Option[ShuffleQueryStageExec]\n</code></pre> <p><code>AQEShuffleReadExec</code> is given a child physical operator when created.</p> <p>When requested for a ShuffleQueryStageExec, <code>AQEShuffleReadExec</code> returns the child physical operator (if that is its type or returns <code>None</code>).</p> <p><code>shuffleStage</code> is used when:</p> <ul> <li><code>AQEShuffleReadExec</code> is requested for the partitionDataSizes, the performance metrics and the shuffleRDD</li> </ul>","text":""},{"location":"physical-operators/AQEShuffleReadExec/#shuffle-rdd","title":"Shuffle RDD <pre><code>shuffleRDD: RDD[_]\n</code></pre> <p><code>shuffleRDD</code> updates the performance metrics and requests the shuffleStage for the ShuffleExchangeLike that in turn is requested for the shuffle RDD (with the ShufflePartitionSpecs).</p>  Lazy Value <p><code>shuffleRDD</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>shuffleRDD</code>\u00a0is used when:</p> <ul> <li><code>AQEShuffleReadExec</code> operator is requested to doExecute and doExecuteColumnar</li> </ul>","text":""},{"location":"physical-operators/AQEShuffleReadExec/#updating-performance-metrics","title":"Updating Performance Metrics <pre><code>sendDriverMetrics(): Unit\n</code></pre> <p><code>sendDriverMetrics</code> posts a <code>SparkListenerDriverAccumUpdates</code> (with the query execution id and performance metrics).</p>","text":""},{"location":"physical-operators/AQEShuffleReadExec/#partition-data-sizes","title":"Partition Data Sizes <pre><code>partitionDataSizes: Option[Seq[Long]]\n</code></pre> <p><code>partitionDataSizes</code>...FIXME</p>  Lazy Value <p><code>partitionDataSizes</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>","text":""},{"location":"physical-operators/AQEShuffleReadExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code>\u00a0is part of the SparkPlan abstraction.</p>  <p><code>doExecute</code> returns the Shuffle RDD.</p>","text":""},{"location":"physical-operators/AQEShuffleReadExec/#columnar-execution","title":"Columnar Execution <pre><code>doExecuteColumnar(): RDD[ColumnarBatch]\n</code></pre> <p><code>doExecuteColumnar</code>\u00a0is part of the SparkPlan abstraction.</p>  <p><code>doExecuteColumnar</code> returns the Shuffle RDD.</p>","text":""},{"location":"physical-operators/AQEShuffleReadExec/#node-arguments","title":"Node Arguments <pre><code>stringArgs: Iterator[Any]\n</code></pre> <p><code>stringArgs</code>\u00a0is part of the TreeNode abstraction.</p>  <p><code>stringArgs</code> is one of the following:</p> <ul> <li><code>local</code> when isLocalRead</li> <li><code>coalesced and skewed</code> when hasCoalescedPartition and hasSkewedPartition</li> <li><code>coalesced</code> when hasCoalescedPartition</li> <li><code>skewed</code> when hasSkewedPartition</li> </ul>","text":""},{"location":"physical-operators/AQEShuffleReadExec/#islocalread","title":"isLocalRead <pre><code>isLocalRead: Boolean\n</code></pre> <p><code>isLocalRead</code> indicates whether either <code>PartialMapperPartitionSpec</code> or <code>CoalescedMapperPartitionSpec</code> are among the partition specs or not.</p>  <p><code>isLocalRead</code> is used when:</p> <ul> <li><code>AQEShuffleReadExec</code> is requested for the node arguments, the partition data sizes and the performance metrics</li> </ul>","text":""},{"location":"physical-operators/AQEShuffleReadExec/#iscoalescedread","title":"isCoalescedRead <pre><code>isCoalescedRead: Boolean\n</code></pre> <p><code>isCoalescedRead</code> indicates coalesced shuffle read and is whether the partition specs are all <code>CoalescedPartitionSpec</code>s pair-wise (with the <code>endReducerIndex</code> and <code>startReducerIndex</code> being adjacent) or not.</p>  <p><code>isCoalescedRead</code> is used when:</p> <ul> <li><code>AQEShuffleReadExec</code> is requested for the outputPartitioning</li> </ul>","text":""},{"location":"physical-operators/AQEShuffleReadExec/#hascoalescedpartition","title":"hasCoalescedPartition <pre><code>hasCoalescedPartition: Boolean\n</code></pre> <p><code>hasCoalescedPartition</code> is <code>true</code> when there is a CoalescedSpec among the ShufflePartitionSpecs.</p>  <p><code>hasCoalescedPartition</code> is used when:</p> <ul> <li><code>AQEShuffleReadExec</code> is requested for the stringArgs, sendDriverMetrics, and metrics</li> </ul>","text":""},{"location":"physical-operators/AQEShuffleReadExec/#iscoalescedspec","title":"isCoalescedSpec <pre><code>isCoalescedSpec(\n  spec: ShufflePartitionSpec)\n</code></pre> <p><code>isCoalescedSpec</code> is <code>true</code> when the given <code>ShufflePartitionSpec</code> is one of the following:</p> <ul> <li><code>CoalescedPartitionSpec</code> (with both <code>startReducerIndex</code> and <code>endReducerIndex</code> as <code>0</code>s)</li> <li><code>CoalescedPartitionSpec</code> with <code>endReducerIndex</code> larger than <code>startReducerIndex</code></li> </ul> <p>Otherwise, <code>isCoalescedSpec</code> is <code>false</code>.</p>  <p><code>isCoalescedSpec</code> is used when:</p> <ul> <li><code>AQEShuffleReadExec</code> is requested to hasCoalescedPartition and sendDriverMetrics</li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/","title":"AdaptiveSparkPlanExec Leaf Physical Operator","text":"<p><code>AdaptiveSparkPlanExec</code> is a leaf physical operator for Adaptive Query Execution.</p>"},{"location":"physical-operators/AdaptiveSparkPlanExec/#creating-instance","title":"Creating Instance","text":"<p><code>AdaptiveSparkPlanExec</code> takes the following to be created:</p> <ul> <li>Input Physical Plan</li> <li>AdaptiveExecutionContext</li> <li>Preprocessing Physical Optimizations</li> <li> <code>isSubquery</code> flag <li> <code>supportsColumnar</code> flag (default: <code>false</code>) <p><code>AdaptiveSparkPlanExec</code> is created when:</p> <ul> <li>InsertAdaptiveSparkPlan physical optimization is executed</li> </ul>"},{"location":"physical-operators/AdaptiveSparkPlanExec/#input-physical-plan","title":"Input Physical Plan <p><code>AdaptiveSparkPlanExec</code> is given a SparkPlan when created.</p> <p>The <code>SparkPlan</code> is determined when PlanAdaptiveDynamicPruningFilters adaptive physical optimization is executed and can be one of the following:</p> <ul> <li>BroadcastExchangeExec physical operator (with spark.sql.exchange.reuse configuration property enabled)</li> <li>Planned Aggregate logical operator (otherwise)</li> </ul> <p>The <code>SparkPlan</code> is used for the following:</p> <ul> <li>requiredDistribution, initialPlan, output, doCanonicalize, getFinalPhysicalPlan, hashCode and equals</li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#adaptiveexecutioncontext","title":"AdaptiveExecutionContext <p><code>AdaptiveSparkPlanExec</code> is given an AdaptiveExecutionContext when created.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#adaptive-logical-optimizer","title":"Adaptive Logical Optimizer <pre><code>optimizer: AQEOptimizer\n</code></pre> <p><code>AdaptiveSparkPlanExec</code> creates an AQEOptimizer (when created) that is used when requested to re-optimize a logical query plan.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#aqe-cost-evaluator","title":"AQE Cost Evaluator <pre><code>costEvaluator: CostEvaluator\n</code></pre> <p><code>AdaptiveSparkPlanExec</code> creates a CostEvaluator (when created) based on spark.sql.adaptive.customCostEvaluatorClass configuration property.</p> <p>Unless configured, <code>AdaptiveSparkPlanExec</code> uses SimpleCostEvaluator (with spark.sql.adaptive.forceOptimizeSkewedJoin configuration property).</p> <p><code>AdaptiveSparkPlanExec</code> uses the <code>CostEvaluator</code> to evaluate cost (of a candidate for a new <code>SparkPlan</code>) when requested for the adaptively-optimized physical query plan.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#preprocessing-physical-optimizations","title":"Preprocessing Physical Optimizations <pre><code>preprocessingRules: Seq[Rule[SparkPlan]]\n</code></pre> <p><code>AdaptiveSparkPlanExec</code> is given a collection of Rules to pre-process SparkPlans (before the QueryStage Physical Preparation Rules) when executing physical optimizations to reOptimize a logical query plan.</p> <p>The rules is just the single physical optimization:</p> <ul> <li>PlanAdaptiveSubqueries</li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#querystage-physical-preparation-rules","title":"QueryStage Physical Preparation Rules <pre><code>queryStagePreparationRules: Seq[Rule[SparkPlan]]\n</code></pre> <p><code>AdaptiveSparkPlanExec</code> creates a collection of physical preparation rules (<code>Rule[SparkPlan]</code>s) when created (in the order):</p> <ol> <li>RemoveRedundantProjects</li> <li>EnsureRequirements (based on the requiredDistribution)</li> <li>AdjustShuffleExchangePosition</li> <li>ValidateSparkPlan</li> <li>ReplaceHashWithSortAgg</li> <li>RemoveRedundantSorts</li> <li>DisableUnnecessaryBucketedScan</li> <li>OptimizeSkewedJoin (with the EnsureRequirements)</li> <li>queryStagePrepRules</li> </ol> <p><code>queryStagePreparationRules</code> is used for the initial plan and reOptimize.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#distribution-requirement","title":"Distribution Requirement <pre><code>requiredDistribution: Option[Distribution]\n</code></pre> <p><code>AdaptiveSparkPlanExec</code> creates <code>requiredDistribution</code> value when created:</p> <ul> <li><code>UnspecifiedDistribution</code> for a subquery (as a subquery output does not need a specific output partitioning)</li> <li>AQEUtils.getRequiredDistribution for the inputPlan otherwise</li> </ul> <p><code>requiredDistribution</code> is used for the following:</p> <ul> <li>queryStagePreparationRules (to create EnsureRequirements physical optimization)</li> <li>optimizeQueryStage</li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#adaptive-query-stage-physical-optimizations","title":"Adaptive Query Stage Physical Optimizations <pre><code>queryStageOptimizerRules: Seq[Rule[SparkPlan]]\n</code></pre> <p><code>AdaptiveSparkPlanExec</code> creates a collection of physical optimization rules (<code>Rule[SparkPlan]</code>s) when created (in the order):</p> <ol> <li>PlanAdaptiveDynamicPruningFilters</li> <li>ReuseAdaptiveSubquery</li> <li>OptimizeSkewInRebalancePartitions</li> <li>CoalesceShufflePartitions</li> <li>OptimizeShuffleWithLocalRead</li> </ol> <p><code>queryStageOptimizerRules</code> is used to optimizeQueryStage.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p>  <p><code>doExecute</code> takes the final physical plan to execute it (that generates an <code>RDD[InternalRow]</code> that will be the return value).</p> <p><code>doExecute</code> triggers finalPlanUpdate (unless done already).</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#doexecutecolumnar","title":"doExecuteColumnar <pre><code>doExecuteColumnar(): RDD[ColumnarBatch]\n</code></pre> <p><code>doExecuteColumnar</code> is part of the SparkPlan abstraction.</p>  <p><code>doExecuteColumnar</code> withFinalPlanUpdate to executeColumnar.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#doexecutebroadcast","title":"doExecuteBroadcast <pre><code>doExecuteBroadcast[T](): broadcast.Broadcast[T]\n</code></pre> <p><code>doExecuteBroadcast</code> is part of the SparkPlan abstraction.</p>  <p><code>doExecuteBroadcast</code> withFinalPlanUpdate to doExecuteBroadcast.</p> <p><code>doExecuteBroadcast</code> asserts that the final physical plan is a BroadcastQueryStageExec.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#specialized-execution-paths","title":"Specialized Execution Paths","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#collect","title":"collect <pre><code>executeCollect(): Array[InternalRow]\n</code></pre> <p><code>executeCollect</code> is part of the SparkPlan abstraction.</p>  <p><code>executeCollect</code>...FIXME</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#tail","title":"tail <pre><code>executeTail(\n  n: Int): Array[InternalRow]\n</code></pre> <p><code>executeTail</code> is part of the SparkPlan abstraction.</p>  <p><code>executeTail</code>...FIXME</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#take","title":"take <pre><code>executeTake(\n  n: Int): Array[InternalRow]\n</code></pre> <p><code>executeTake</code> is part of the SparkPlan abstraction.</p>  <p><code>executeTake</code>...FIXME</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#adaptively-optimized-physical-query-plan","title":"Adaptively-Optimized Physical Query Plan <pre><code>getFinalPhysicalPlan(): SparkPlan\n</code></pre>  <p>Note</p> <p><code>getFinalPhysicalPlan</code> uses the isFinalPlan internal flag (and an optimized physical query plan) to short-circuit (skip) the whole expensive computation.</p>  <p><code>getFinalPhysicalPlan</code> is used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested to withFinalPlanUpdate</li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#step-1-createquerystages","title":"Step 1. createQueryStages <p><code>getFinalPhysicalPlan</code> createQueryStages with the currentPhysicalPlan.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#step-2-until-allchildstagesmaterialized","title":"Step 2. Until allChildStagesMaterialized <p><code>getFinalPhysicalPlan</code> executes the following until <code>allChildStagesMaterialized</code>.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#step-21-new-querystageexecs","title":"Step 2.1 New QueryStageExecs <p><code>getFinalPhysicalPlan</code> does the following when there are new stages to be processed:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#step-22-stagematerializationevents","title":"Step 2.2 StageMaterializationEvents <p><code>getFinalPhysicalPlan</code> executes the following until <code>allChildStagesMaterialized</code>:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#step-23-errors","title":"Step 2.3 Errors <p>In case of errors, <code>getFinalPhysicalPlan</code> cleanUpAndThrowException.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#step-24-replacewithquerystagesinlogicalplan","title":"Step 2.4 replaceWithQueryStagesInLogicalPlan <p><code>getFinalPhysicalPlan</code> replaceWithQueryStagesInLogicalPlan with the <code>currentLogicalPlan</code> and the <code>stagesToReplace</code>.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#step-25-reoptimize","title":"Step 2.5 reOptimize <p><code>getFinalPhysicalPlan</code> reOptimize the new logical plan.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#step-26-evaluating-cost","title":"Step 2.6 Evaluating Cost <p><code>getFinalPhysicalPlan</code> requests the SimpleCostEvaluator to evaluateCost of the currentPhysicalPlan and the new <code>newPhysicalPlan</code>.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#step-27-adopting-new-physical-plan","title":"Step 2.7 Adopting New Physical Plan <p><code>getFinalPhysicalPlan</code> adopts the new plan if the cost is less than the currentPhysicalPlan or the costs are equal but the physical plans are different (likely better).</p> <p><code>getFinalPhysicalPlan</code> prints out the following message to the logs (using the logOnLevel):</p> <pre><code>Plan changed from [currentPhysicalPlan] to [newPhysicalPlan]\n</code></pre> <p><code>getFinalPhysicalPlan</code> cleanUpTempTags with the <code>newPhysicalPlan</code>.</p> <p><code>getFinalPhysicalPlan</code> saves the <code>newPhysicalPlan</code> as the currentPhysicalPlan (alongside the <code>currentLogicalPlan</code> with the <code>newLogicalPlan</code>).</p> <p><code>getFinalPhysicalPlan</code> resets the <code>stagesToReplace</code>.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#step-28-createquerystages","title":"Step 2.8 createQueryStages <p><code>getFinalPhysicalPlan</code> createQueryStages for the currentPhysicalPlan (that may have changed).</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#step-3-applyphysicalrules","title":"Step 3. applyPhysicalRules <p><code>getFinalPhysicalPlan</code> applyPhysicalRules on the final plan (with the finalStageOptimizerRules, the planChangeLogger and AQE Final Query Stage Optimization name).</p> <p><code>getFinalPhysicalPlan</code> turns the isFinalPlan internal flag on.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#finalstageoptimizerrules","title":"finalStageOptimizerRules <pre><code>finalStageOptimizerRules: Seq[Rule[SparkPlan]]\n</code></pre> <p><code>finalStageOptimizerRules</code>...FIXME</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#createquerystages","title":"createQueryStages <pre><code>createQueryStages(\n  plan: SparkPlan): CreateStageResult\n</code></pre> <p><code>createQueryStages</code> checks if the given SparkPlan is one of the following:</p> <ol> <li>Exchange unary physical operator</li> <li>QueryStageExec leaf physical operator</li> <li>Others</li> </ol> <p>The most interesting case is when the given <code>SparkPlan</code> is an Exchange unary physical operator.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#reusequerystage","title":"reuseQueryStage <pre><code>reuseQueryStage(\n  existing: QueryStageExec,\n  exchange: Exchange): QueryStageExec\n</code></pre> <p><code>reuseQueryStage</code> requests the given QueryStageExec to newReuseInstance (with the currentStageId).</p> <p><code>reuseQueryStage</code> increments the currentStageId.</p> <p><code>reuseQueryStage</code> setLogicalLinkForNewQueryStage.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#creating-querystageexec-for-exchange","title":"Creating QueryStageExec for Exchange <pre><code>newQueryStage(\n  e: Exchange): QueryStageExec\n</code></pre> <p><code>newQueryStage</code> creates a new QueryStageExec physical operator based on the type of the given Exchange physical operator.</p>    Exchange QueryStageExec     ShuffleExchangeLike ShuffleQueryStageExec   BroadcastExchangeLike BroadcastQueryStageExec     <p><code>newQueryStage</code> creates an optimized physical query plan for the child physical plan of the given Exchange. <code>newQueryStage</code> uses the adaptive optimizations, the PlanChangeLogger and AQE Query Stage Optimization batch name.</p> <p><code>newQueryStage</code> creates a new QueryStageExec physical operator for the given <code>Exchange</code> operator (using the currentStageId for the ID).</p> <p>After applyPhysicalRules for the child operator, <code>newQueryStage</code> creates an optimized physical query plan for the Exchange itself (with the new optimized physical query plan for the child). <code>newQueryStage</code> uses the post-stage-creation optimizations, the PlanChangeLogger and AQE Post Stage Creation batch name.</p> <p><code>newQueryStage</code> increments the currentStageId counter.</p> <p><code>newQueryStage</code> associates the new query stage operator with the <code>Exchange</code> physical operator.</p> <p>In the end, <code>newQueryStage</code> returns the <code>QueryStageExec</code> physical operator.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#adaptively-optimized-physical-query-plan_1","title":"Adaptively-Optimized Physical Query Plan <pre><code>currentPhysicalPlan: SparkPlan\n</code></pre> <p><code>AdaptiveSparkPlanExec</code> defines <code>currentPhysicalPlan</code> variable for an adaptively-optimized SparkPlan.</p> <p><code>currentPhysicalPlan</code> is the initialPlan when <code>AdaptiveSparkPlanExec</code> is created.</p> <p><code>currentPhysicalPlan</code> can only change in getFinalPhysicalPlan and only until the isFinalPlan internal flag is enabled.</p> <p>While getFinalPhysicalPlan, <code>AdaptiveSparkPlanExec</code> uses <code>currentPhysicalPlan</code> as an input argument for createQueryStages that gives a candidate for a new <code>currentPhysicalPlan</code>. <code>AdaptiveSparkPlanExec</code> replaces <code>currentPhysicalPlan</code> when the costEvaluator determines the following:</p> <ul> <li>A smaller Cost</li> <li>A different SparkPlan (than the <code>currentPhysicalPlan</code>)</li> </ul> <p>If a <code>SparkPlan</code> switch happens, <code>AdaptiveSparkPlanExec</code> prints out the following message to the logs:</p> <pre><code>Plan changed from [currentPhysicalPlan] to [newPhysicalPlan]\n</code></pre> <p><code>AdaptiveSparkPlanExec</code> applyPhysicalRules to optimize the new <code>SparkPlan</code>.</p> <p><code>currentPhysicalPlan</code> is available using executedPlan.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#executedplan","title":"executedPlan <pre><code>executedPlan: SparkPlan\n</code></pre> <p><code>executedPlan</code> returns the current physical query plan.</p>  <p><code>executedPlan</code> is used when:</p> <ul> <li><code>ExplainUtils</code> is requested to generateOperatorIDs, collectOperatorsWithID, generateWholeStageCodegenIds, getSubqueries, removeTags</li> <li><code>SparkPlanInfo</code> is requested to <code>fromSparkPlan</code> (for <code>SparkListenerSQLExecutionStart</code> and <code>SparkListenerSQLAdaptiveExecutionUpdate</code> events)</li> <li><code>AdaptiveSparkPlanExec</code> is requested to reset metrics</li> <li><code>AdaptiveSparkPlanHelper</code> is requested to <code>allChildren</code> and <code>stripAQEPlan</code></li> <li><code>PlanAdaptiveDynamicPruningFilters</code> is executed</li> <li><code>debug</code> package object is requested to <code>codegenStringSeq</code></li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#resetting-metrics","title":"Resetting Metrics <pre><code>resetMetrics(): Unit\n</code></pre> <p><code>resetMetrics</code> is part of the SparkPlan abstraction.</p>  <p><code>resetMetrics</code> requests all the metrics to reset.</p> <p>In the end, <code>resetMetrics</code> requests the executed query plan to resetMetrics.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#optimizequerystage","title":"optimizeQueryStage <pre><code>optimizeQueryStage(\n  plan: SparkPlan,\n  isFinalStage: Boolean): SparkPlan\n</code></pre>  <p>isFinalStage</p> <p>The given <code>isFinalStage</code> can be as follows:</p> <ul> <li><code>true</code> when requested for an adaptively-optimized physical query plan</li> <li><code>false</code> when requested to create a new QueryStageExec for an Exchange</li> </ul>  <p><code>optimizeQueryStage</code> executes (applies) the queryStageOptimizerRules to the given SparkPlan. While applying optimizations (executing rules), <code>optimizeQueryStage</code> requests the PlanChangeLogger to log plan changes (by a rule) with the name of the rule that has just been executed.</p>  <p>AQEShuffleReadRule</p> <p><code>optimizeQueryStage</code> is sensitive to AQEShuffleReadRule physical optimization and does a validation so it does not break distribution requirement of the query plan.</p>  <p><code>optimizeQueryStage</code> requests the PlanChangeLogger to log plan changes by the entire rule batch with the following batch name:</p> <pre><code>AQE Query Stage Optimization\n</code></pre>  <p><code>optimizeQueryStage</code> is used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> is requested for an adaptively-optimized physical query plan and to create a new QueryStageExec for an Exchange</li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#post-stage-creation-adaptive-optimizations","title":"Post-Stage-Creation Adaptive Optimizations <pre><code>postStageCreationRules: Seq[Rule[SparkPlan]]\n</code></pre> <p><code>postStageCreationRules</code> is the following adaptive optimizations (physical optimization rules):</p> <ul> <li>ApplyColumnarRulesAndInsertTransitions</li> <li>CollapseCodegenStages</li> </ul> <p><code>postStageCreationRules</code> is used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> is requested to finalStageOptimizerRules and newQueryStage</li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#generating-text-representation","title":"Generating Text Representation <pre><code>generateTreeString(\n  depth: Int,\n  lastChildren: Seq[Boolean],\n  append: String =&gt; Unit,\n  verbose: Boolean,\n  prefix: String = \"\",\n  addSuffix: Boolean = false,\n  maxFields: Int,\n  printNodeId: Boolean): Unit\n</code></pre> <p><code>generateTreeString</code> is part of the TreeNode abstraction.</p>  <p><code>generateTreeString</code>...FIXME</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#cleanupandthrowexception","title":"cleanUpAndThrowException <pre><code>cleanUpAndThrowException(\n  errors: Seq[Throwable],\n  earlyFailedStage: Option[Int]): Unit\n</code></pre> <p><code>cleanUpAndThrowException</code>...FIXME</p> <p><code>cleanUpAndThrowException</code> is used when <code>AdaptiveSparkPlanExec</code> physical operator is requested to getFinalPhysicalPlan (and materialization of new stages fails).</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#re-optimizing-logical-query-plan","title":"Re-Optimizing Logical Query Plan <pre><code>reOptimize(\n  logicalPlan: LogicalPlan): (SparkPlan, LogicalPlan)\n</code></pre> <p><code>reOptimize</code> returns a newly-optimized physical query plan with a newly-optimized logical query plan for the given logical query plan.</p>  <p><code>reOptimize</code> requests the given LogicalPlan to invalidate statistics cache.</p> <p><code>reOptimize</code> requests the Adaptive Logical Optimizer to execute (and generate an optimized logical query plan).</p> <p><code>reOptimize</code> requests the Spark Query Planner (bound to the AdaptiveExecutionContext) to plan the optimized logical query plan (and generate a physical query plan).</p> <p><code>reOptimize</code> executes physical optimizations using preprocessing and preparation rules (and generates an optimized physical query plan).</p>  <p><code>reOptimize</code> is used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested for an adaptively-optimized physical query plan</li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#querystagecreator-thread-pool","title":"QueryStageCreator Thread Pool <pre><code>executionContext: ExecutionContext\n</code></pre> <p><code>executionContext</code> is an <code>ExecutionContext</code> that is used when:</p> <ul> <li> <p><code>AdaptiveSparkPlanExec</code> operator is requested for a getFinalPhysicalPlan (to materialize QueryStageExec operators asynchronously)</p> </li> <li> <p>BroadcastQueryStageExec operator is requested for materializeWithTimeout</p> </li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#finalplanupdate-lazy-value","title":"finalPlanUpdate Lazy Value <pre><code>finalPlanUpdate: Unit\n</code></pre>  lazy value <p><code>finalPlanUpdate</code> is a Scala lazy value which is computed once when accessed and cached afterwards.</p>  <p><code>finalPlanUpdate</code>...FIXME</p> <p>In the end, <code>finalPlanUpdate</code> prints out the following message to the logs:</p> <pre><code>Final plan: [currentPhysicalPlan]\n</code></pre> <p><code>finalPlanUpdate</code> is used when <code>AdaptiveSparkPlanExec</code> physical operator is requested to executeCollect, executeTake, executeTail and doExecute.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#isfinalplan-available-already","title":"isFinalPlan (Available Already) <pre><code>isFinalPlan: Boolean\n</code></pre> <p><code>isFinalPlan</code> is an internal flag that is used to skip (short-circuit) the expensive process of producing an adaptively-optimized physical query plan (and immediately return the one that has already been prepared).</p> <p><code>isFinalPlan</code> is disabled (<code>false</code>) when <code>AdaptiveSparkPlanExec</code> is created. It is enabled right after an adaptively-optimized physical query plan has once been prepared.</p> <p><code>isFinalPlan</code> is also used for reporting when <code>AdaptiveSparkPlanExec</code> is requested for the following:</p> <ul> <li>String arguments</li> <li>Generate a text representation</li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#string-arguments","title":"String Arguments <pre><code>stringArgs: Iterator[Any]\n</code></pre> <p><code>stringArgs</code> is part of the TreeNode abstraction.</p>  <p><code>stringArgs</code> is the following (with the isFinalPlan flag):</p> <pre><code>isFinalPlan=[isFinalPlan]\n</code></pre>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#initial-plan","title":"Initial Plan <pre><code>initialPlan: SparkPlan\n</code></pre> <p><code>AdaptiveSparkPlanExec</code> initializes <code>initialPlan</code> value when created.</p> <p><code>initialPlan</code> is a SparkPlan after applying the queryStagePreparationRules to the inputPlan (with the planChangeLogger and AQE Preparations batch name).</p> <p><code>initialPlan</code> is the currentPhysicalPlan when <code>AdaptiveSparkPlanExec</code> is created.</p> <p><code>isFinalPlan</code> is used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> is requested for a text representation</li> <li><code>ExplainUtils</code> utility is used to process a query plan</li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#replacewithquerystagesinlogicalplan","title":"replaceWithQueryStagesInLogicalPlan <pre><code>replaceWithQueryStagesInLogicalPlan(\n  plan: LogicalPlan,\n  stagesToReplace: Seq[QueryStageExec]): LogicalPlan\n</code></pre> <p><code>replaceWithQueryStagesInLogicalPlan</code>...FIXME</p> <p><code>replaceWithQueryStagesInLogicalPlan</code> is used when <code>AdaptiveSparkPlanExec</code> physical operator is requested for a final physical plan.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#executing-physical-optimizations","title":"Executing Physical Optimizations <pre><code>applyPhysicalRules(\n  plan: SparkPlan,\n  rules: Seq[Rule[SparkPlan]],\n  loggerAndBatchName: Option[(PlanChangeLogger[SparkPlan], String)] = None): SparkPlan\n</code></pre> <p>By default (with no <code>loggerAndBatchName</code> given) <code>applyPhysicalRules</code> applies (executes) the given rules to the given physical query plan.</p> <p>With <code>loggerAndBatchName</code> specified, <code>applyPhysicalRules</code> executes the rules and, for every rule, requests the PlanChangeLogger to logRule. In the end, <code>applyPhysicalRules</code> requests the <code>PlanChangeLogger</code> to logBatch.</p>  <p><code>applyPhysicalRules</code> is used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is created (and initializes the initialPlan), is requested to getFinalPhysicalPlan, newQueryStage, reOptimize</li> <li>InsertAdaptiveSparkPlan physical optimization is executed</li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#withfinalplanupdate","title":"withFinalPlanUpdate <pre><code>withFinalPlanUpdate[T](\n  fun: SparkPlan =&gt; T): T\n</code></pre> <p><code>withFinalPlanUpdate</code> executes the given <code>fun</code> with the adaptively-optimized physical query plan and returns the result (of type <code>T</code>). <code>withFinalPlanUpdate</code> finalPlanUpdate.</p>  <p><code>withFinalPlanUpdate</code> is a helper method for <code>AdaptiveSparkPlanExec</code> when requested for the following:</p> <ul> <li>executeCollect</li> <li>executeTake</li> <li>executeTail</li> <li>doExecute</li> <li>doExecuteColumnar</li> <li>doExecuteBroadcast</li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#planchangelogger","title":"PlanChangeLogger <p><code>AdaptiveSparkPlanExec</code> uses a PlanChangeLogger for the following:</p> <ul> <li>initialPlan (<code>batchName</code>: AQE Preparations)</li> <li>getFinalPhysicalPlan (<code>batchName</code>: AQE Final Query Stage Optimization)</li> <li>newQueryStage (<code>batchName</code>: AQE Query Stage Optimization and AQE Post Stage Creation)</li> <li>reOptimize (<code>batchName</code>: AQE Replanning)</li> </ul>","text":""},{"location":"physical-operators/AdaptiveSparkPlanExec/#logonlevel","title":"logOnLevel <pre><code>logOnLevel: (=&gt; String) =&gt; Unit\n</code></pre> <p><code>logOnLevel</code> uses the internal spark.sql.adaptive.logLevel configuration property for the logging level and prints out the given message to the logs (at the log level).</p> <p><code>logOnLevel</code> is used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested to getFinalPhysicalPlan and finalPlanUpdate</li> </ul>","text":""},{"location":"physical-operators/AggregateCodegenSupport/","title":"AggregateCodegenSupport Physical Operators","text":"<p><code>AggregateCodegenSupport</code> is an extension of the BaseAggregateExec abstraction for aggregate physical operators that support Whole-Stage Java Code Generation (with produce and consume code execution paths).</p> <p><code>AggregateCodegenSupport</code> is a <code>BlockingOperatorWithCodegen</code>.</p>"},{"location":"physical-operators/AggregateCodegenSupport/#contract","title":"Contract","text":""},{"location":"physical-operators/AggregateCodegenSupport/#doconsumewithkeys","title":"doConsumeWithKeys <pre><code>doConsumeWithKeys(\n  ctx: CodegenContext,\n  input: Seq[ExprCode]): String\n</code></pre> <p>See:</p> <ul> <li>HashAggregateExec</li> <li>SortAggregateExec</li> </ul> <p>Used when:</p> <ul> <li><code>AggregateCodegenSupport</code> is requested to doConsume</li> </ul>","text":""},{"location":"physical-operators/AggregateCodegenSupport/#doproducewithkeys","title":"doProduceWithKeys <pre><code>doProduceWithKeys(\n  ctx: CodegenContext): String\n</code></pre> <p>See:</p> <ul> <li>HashAggregateExec</li> <li>SortAggregateExec</li> </ul> <p>Used when:</p> <ul> <li><code>AggregateCodegenSupport</code> is requested to doProduce</li> </ul>","text":""},{"location":"physical-operators/AggregateCodegenSupport/#needhashtable","title":"needHashTable <pre><code>needHashTable: Boolean\n</code></pre> <p>Whether this aggregate operator needs to build a hash table</p>    Aggregate Physical Operator needHashTable     HashAggregateExec    SortAggregateExec \u274c    <p>Used when:</p> <ul> <li><code>AggregateCodegenSupport</code> is requested to doProduceWithoutKeys</li> </ul>","text":""},{"location":"physical-operators/AggregateCodegenSupport/#implementations","title":"Implementations","text":"<ul> <li>HashAggregateExec</li> <li>SortAggregateExec</li> </ul>"},{"location":"physical-operators/AggregateCodegenSupport/#supportcodegen-flag","title":"supportCodegen Flag <pre><code>supportCodegen: Boolean\n</code></pre> <p><code>supportCodegen</code> is part of the CodegenSupport abstraction.</p>  <p><code>supportCodegen</code> is enabled (<code>true</code>) when all the following hold:</p> <ul> <li>All <code>AttributeReference</code>s in the aggregateBufferAttributes use mutable data types</li> <li>There are no ImperativeAggregates (among the AggregateFunctions of the AggregateExpressions)</li> </ul>","text":""},{"location":"physical-operators/AggregateCodegenSupport/#generating-java-source-code-for-produce-path","title":"Generating Java Source Code for Produce Path <pre><code>doProduce(\n  ctx: CodegenContext): String\n</code></pre> <p><code>doProduce</code> is part of the CodegenSupport abstraction.</p>  <p><code>doProduce</code> doProduceWithoutKeys when this aggregate operator uses no grouping keys. Otherwise, <code>doProduce</code> doProduceWithKeys.</p>","text":""},{"location":"physical-operators/AggregateCodegenSupport/#doproducewithoutkeys","title":"doProduceWithoutKeys <pre><code>doProduceWithoutKeys(\n  ctx: CodegenContext): String\n</code></pre> <p><code>doProduceWithoutKeys</code>...FIXME</p>","text":""},{"location":"physical-operators/AggregateCodegenSupport/#generating-java-source-code-for-consume-path","title":"Generating Java Source Code for Consume Path <pre><code>doConsume(\n  ctx: CodegenContext,\n  input: Seq[ExprCode],\n  row: ExprCode): String\n</code></pre> <p><code>doConsume</code> is part of the CodegenSupport abstraction.</p>  <p><code>doConsume</code> doConsumeWithoutKeys when this aggregate operator uses no grouping keys. Otherwise, <code>doConsume</code> doConsumeWithKeys.</p>","text":""},{"location":"physical-operators/AggregateCodegenSupport/#doconsumewithoutkeys","title":"doConsumeWithoutKeys <pre><code>doConsumeWithoutKeys(\n  ctx: CodegenContext,\n  input: Seq[ExprCode]): String\n</code></pre> <p><code>doConsumeWithoutKeys</code>...FIXME</p>","text":""},{"location":"physical-operators/AggregationIterator/","title":"AggregationIterators","text":"<p><code>AggregationIterator</code> is an abstraction of aggregation iterators of UnsafeRows.</p> <pre><code>abstract class AggregationIterator(...)\nextends Iterator[UnsafeRow]\n</code></pre> <p>From scala.collection.Iterator:</p> <p>Iterators are data structures that allow to iterate over a sequence of elements. They have a <code>hasNext</code> method for checking if there is a next element available, and a <code>next</code> method which returns the next element and discards it from the iterator.</p>"},{"location":"physical-operators/AggregationIterator/#implementations","title":"Implementations","text":"<ul> <li>ObjectAggregationIterator</li> <li>SortBasedAggregationIterator</li> <li>TungstenAggregationIterator</li> </ul>"},{"location":"physical-operators/AggregationIterator/#creating-instance","title":"Creating Instance","text":"<p><code>AggregationIterator</code> takes the following to be created:</p> <ul> <li> Partition ID <li> Grouping NamedExpressions <li> Input Attributes <li> AggregateExpressions <li> Aggregate Attributes <li> Initial input buffer offset <li> Result NamedExpressions <li> Function to create a new <code>MutableProjection</code> given expressions and attributes (<code>(Seq[Expression], Seq[Attribute]) =&gt; MutableProjection</code>) Abstract Class <p><code>AggregationIterator</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete AggregationIterators.</p>"},{"location":"physical-operators/AggregationIterator/#aggregatemodes","title":"AggregateModes <p>When created, <code>AggregationIterator</code> makes sure that there are at most 2 distinct <code>AggregateMode</code>s of the AggregateExpressions.</p> <p>The <code>AggregateMode</code>s have to be a subset of the following mode pairs:</p> <ul> <li><code>Partial</code> and <code>PartialMerge</code></li> <li><code>Final</code> and <code>Complete</code></li> </ul>","text":""},{"location":"physical-operators/AggregationIterator/#aggregatefunctions","title":"AggregateFunctions <pre><code>aggregateFunctions: Array[AggregateFunction]\n</code></pre> <p>When created, <code>AggregationIterator</code> initializes AggregateFunctions in the aggregateExpressions (with initialInputBufferOffset).</p>","text":""},{"location":"physical-operators/AggregationIterator/#initializeaggregatefunctions","title":"initializeAggregateFunctions <pre><code>initializeAggregateFunctions(\n  expressions: Seq[AggregateExpression],\n  startingInputBufferOffset: Int): Array[AggregateFunction]\n</code></pre> <p><code>initializeAggregateFunctions</code>...FIXME</p> <p><code>initializeAggregateFunctions</code> is used when:</p> <ul> <li><code>AggregationIterator</code> is requested for the aggregateFunctions</li> <li><code>ObjectAggregationIterator</code> is requested for the mergeAggregationBuffers</li> <li><code>TungstenAggregationIterator</code> is requested to switchToSortBasedAggregation</li> </ul>","text":""},{"location":"physical-operators/AggregationIterator/#generating-process-row-function","title":"Generating Process Row Function <pre><code>generateProcessRow(\n  expressions: Seq[AggregateExpression],\n  functions: Seq[AggregateFunction],\n  inputAttributes: Seq[Attribute]): (InternalRow, InternalRow) =&gt; Unit\n</code></pre>  <p><code>generateProcessRow</code> is a procedure</p> <p><code>generateProcessRow</code> creates a Scala function (procedure) that takes two InternalRows and produces no output.</p> <pre><code>def generateProcessRow(currentBuffer: InternalRow, row: InternalRow): Unit = {\n  ...\n}\n</code></pre>  <p><code>generateProcessRow</code> creates a mutable <code>JoinedRow</code> (of two InternalRows).</p> <p><code>generateProcessRow</code> branches off based on the given AggregateExpressions (<code>expressions</code>).</p> <p>With no AggregateExpressions (<code>expressions</code>), <code>generateProcessRow</code> creates a function that does nothing (and \"swallows\" the input).</p>  <p><code>functions</code> Argument</p> <p><code>generateProcessRow</code> works differently based on the type of the given AggregateFunctions:</p> <ul> <li>DeclarativeAggregate</li> <li>AggregateFunction</li> <li>ImperativeAggregate</li> </ul>  <p>Otherwise, with some AggregateExpressions (<code>expressions</code>), <code>generateProcessRow</code>...FIXME</p>  <p><code>generateProcessRow</code> is used when:</p> <ul> <li><code>AggregationIterator</code> is requested for the processRow function</li> <li><code>ObjectAggregationIterator</code> is requested for the mergeAggregationBuffers function</li> <li><code>TungstenAggregationIterator</code> is requested to switchToSortBasedAggregation</li> </ul>","text":""},{"location":"physical-operators/AggregationIterator/#generateoutput","title":"generateOutput <pre><code>generateOutput: (UnsafeRow, InternalRow) =&gt; UnsafeRow\n</code></pre> <p>When created, <code>AggregationIterator</code> creates a ResultProjection function.</p> <p><code>generateOutput</code> is used when:</p> <ul> <li><code>ObjectAggregationIterator</code> is requested for the next element and to outputForEmptyGroupingKeyWithoutInput</li> <li><code>SortBasedAggregationIterator</code> is requested for the next element and to outputForEmptyGroupingKeyWithoutInput</li> <li><code>TungstenAggregationIterator</code> is requested for the next element and to outputForEmptyGroupingKeyWithoutInput</li> </ul>","text":""},{"location":"physical-operators/AggregationIterator/#generateresultprojection","title":"generateResultProjection <pre><code>generateResultProjection(): (UnsafeRow, InternalRow) =&gt; UnsafeRow\n</code></pre> <p><code>generateResultProjection</code>...FIXME</p>","text":""},{"location":"physical-operators/AggregationIterator/#initializebuffer","title":"initializeBuffer <pre><code>initializeBuffer(\n  buffer: InternalRow): Unit\n</code></pre> <p><code>initializeBuffer</code> requests the expressionAggInitialProjection to store an execution result of an empty row in the given InternalRow (<code>buffer</code>).</p> <p><code>initializeBuffer</code> requests all the ImperativeAggregate functions to initialize with the <code>buffer</code> internal row.</p>  <p><code>initializeBuffer</code> is used when:</p> <ul> <li><code>MergingSessionsIterator</code> is requested to <code>newBuffer</code>, <code>initialize</code>, <code>next</code>, <code>outputForEmptyGroupingKeyWithoutInput</code></li> <li><code>SortBasedAggregationIterator</code> is requested to newBuffer, initialize, next and outputForEmptyGroupingKeyWithoutInput</li> </ul>","text":""},{"location":"physical-operators/AliasAwareOutputExpression/","title":"AliasAwareOutputExpression Unary Physical Operators","text":"<p><code>AliasAwareOutputExpression</code>\u00a0is an extension of the UnaryExecNode abstraction for unary physical operators with the output named expressions.</p>"},{"location":"physical-operators/AliasAwareOutputExpression/#contract","title":"Contract","text":""},{"location":"physical-operators/AliasAwareOutputExpression/#output-named-expressions","title":"Output Named Expressions <pre><code>outputExpressions: Seq[NamedExpression]\n</code></pre> <p>Output NamedExpressions</p> <p>Used when:</p> <ul> <li><code>AliasAwareOutputExpression</code> is requested for the aliasMap</li> </ul>","text":""},{"location":"physical-operators/AliasAwareOutputExpression/#implementations","title":"Implementations","text":"<ul> <li>AliasAwareOutputOrdering</li> <li>AliasAwareOutputPartitioning</li> </ul>"},{"location":"physical-operators/AliasAwareOutputExpression/#aliasmap","title":"aliasMap <pre><code>aliasMap: AttributeMap[Attribute]\n</code></pre> <p><code>aliasMap</code> is a collection of <code>AttributeReference</code> expressions (of the <code>Alias</code> expressions with the <code>AttributeReference</code> expressions in the output named expressions) and the <code>Alias</code>es converted to <code>Attribute</code>s.</p>  Lazy Value <p><code>aliasMap</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>aliasMap</code>\u00a0is used when:</p> <ul> <li><code>AliasAwareOutputExpression</code> is requested to hasAlias and normalizeExpression</li> </ul>","text":""},{"location":"physical-operators/AliasAwareOutputExpression/#hasalias","title":"hasAlias <pre><code>hasAlias: Boolean\n</code></pre> <p><code>hasAlias</code> is <code>true</code> when the aliasMap is not empty.</p> <p><code>hasAlias</code>\u00a0is used when:</p> <ul> <li><code>AliasAwareOutputPartitioning</code> is requested for the outputPartitioning</li> <li><code>AliasAwareOutputOrdering</code> is requested for the outputOrdering</li> </ul>","text":""},{"location":"physical-operators/AliasAwareOutputExpression/#normalizeexpression","title":"normalizeExpression <pre><code>normalizeExpression(\n  exp: Expression): Expression\n</code></pre> <p><code>normalizeExpression</code>...FIXME</p> <p><code>normalizeExpression</code>\u00a0is used when:</p> <ul> <li><code>AliasAwareOutputPartitioning</code> is requested for the outputPartitioning</li> <li><code>AliasAwareOutputOrdering</code> is requested for the outputOrdering</li> </ul>","text":""},{"location":"physical-operators/AliasAwareOutputOrdering/","title":"AliasAwareOutputOrdering Unary Physical Operators","text":"<p><code>AliasAwareOutputOrdering</code>\u00a0is an extension of the AliasAwareOutputExpression abstraction for unary physical operators with alias-aware ordering expressions.</p>"},{"location":"physical-operators/AliasAwareOutputOrdering/#contract","title":"Contract","text":""},{"location":"physical-operators/AliasAwareOutputOrdering/#ordering-expressions","title":"Ordering Expressions <pre><code>orderingExpressions: Seq[SortOrder]\n</code></pre> <p>SortOrder expressions</p> <p>Used when:</p> <ul> <li><code>AliasAwareOutputOrdering</code> is requested for the output ordering</li> </ul>","text":""},{"location":"physical-operators/AliasAwareOutputOrdering/#implementations","title":"Implementations","text":"<ul> <li>ProjectExec</li> <li>SortAggregateExec</li> </ul>"},{"location":"physical-operators/AliasAwareOutputOrdering/#output-data-ordering-requirements","title":"Output Data Ordering Requirements <pre><code>outputOrdering: Seq[SortOrder]\n</code></pre> <p><code>outputOrdering</code>...FIXME</p> <p><code>outputOrdering</code>\u00a0is part of the SparkPlan abstraction.</p>","text":""},{"location":"physical-operators/AliasAwareOutputPartitioning/","title":"AliasAwareOutputPartitioning Unary Physical Operators","text":"<p><code>AliasAwareOutputPartitioning</code>\u00a0is an extension of the AliasAwareOutputExpression abstraction for unary physical operators with alias-aware output partitioning.</p>"},{"location":"physical-operators/AliasAwareOutputPartitioning/#implementations","title":"Implementations","text":"<ul> <li>BaseAggregateExec</li> <li>ProjectExec</li> </ul>"},{"location":"physical-operators/AliasAwareOutputPartitioning/#output-data-partitioning-requirements","title":"Output Data Partitioning Requirements <pre><code>outputPartitioning: Partitioning\n</code></pre> <p><code>outputPartitioning</code>...FIXME</p> <p><code>outputPartitioning</code>\u00a0is part of the SparkPlan abstraction.</p>","text":""},{"location":"physical-operators/AllTuples/","title":"AllTuples","text":"<p><code>AllTuples</code> is a Distribution that indicates to use one partition only.</p>"},{"location":"physical-operators/AlterTableExec/","title":"AlterTableExec","text":"<p><code>AlterTableExec</code> is a V2CommandExec leaf physical command to represent AlterTableCommand logical operator at execution.</p>"},{"location":"physical-operators/AlterTableExec/#creating-instance","title":"Creating Instance","text":"<p><code>AlterTableExec</code> takes the following to be created:</p> <ul> <li> TableCatalog <li> Table Identifier <li> TableChanges <p><code>AlterTableExec</code> is created\u00a0when:</p> <ul> <li>DataSourceV2Strategy execution planning strategy is executed (to plan an AlterTableCommand logical operator)</li> </ul>"},{"location":"physical-operators/AlterTableExec/#executing-command","title":"Executing Command <pre><code>run(): Seq[InternalRow]\n</code></pre> <p><code>run</code>\u00a0is part of the V2CommandExec abstraction.</p>  <p><code>run</code> requests the TableCatalog to alter a table (with the identifier and TableChanges).</p>","text":""},{"location":"physical-operators/AtomicTableWriteExec/","title":"AtomicTableWriteExec Physical Commands","text":"<p><code>AtomicTableWriteExec</code>\u00a0is an extension of the V2TableWriteExec abstraction for physical commands that writeToStagedTable and support V1 write path (<code>SupportsV1Write</code>).</p>"},{"location":"physical-operators/AtomicTableWriteExec/#implementations","title":"Implementations","text":"<ul> <li><code>AtomicCreateTableAsSelectExec</code></li> <li><code>AtomicReplaceTableAsSelectExec</code></li> </ul>"},{"location":"physical-operators/AtomicTableWriteExec/#writetostagedtable","title":"writeToStagedTable <pre><code>writeToStagedTable(\n  stagedTable: StagedTable,\n  writeOptions: CaseInsensitiveStringMap,\n  ident: Identifier): Seq[InternalRow]\n</code></pre> <p><code>writeToStagedTable</code>...FIXME</p>","text":""},{"location":"physical-operators/BaseAggregateExec/","title":"BaseAggregateExec Unary Physical Operators","text":"<p><code>BaseAggregateExec</code>\u00a0is an extension of the UnaryExecNode abstraction for aggregate unary physical operators.</p>"},{"location":"physical-operators/BaseAggregateExec/#contract","title":"Contract","text":""},{"location":"physical-operators/BaseAggregateExec/#aggregate-attributes","title":"Aggregate Attributes <pre><code>aggregateAttributes: Seq[Attribute]\n</code></pre> <p>Aggregate Attributes</p> <p>Used when:</p> <ul> <li><code>AggregateCodegenSupport</code> is requested to doProduceWithoutKeys</li> <li><code>BaseAggregateExec</code> is requested to verboseStringWithOperatorId, producedAttributes, toSortAggregate</li> </ul>","text":""},{"location":"physical-operators/BaseAggregateExec/#aggregate-functions","title":"Aggregate Functions <pre><code>aggregateExpressions: Seq[AggregateExpression]\n</code></pre> <p>AggregateExpressions</p>","text":""},{"location":"physical-operators/BaseAggregateExec/#grouping-keys","title":"Grouping Keys <pre><code>groupingExpressions: Seq[NamedExpression]\n</code></pre> <p>NamedExpressions of the grouping keys</p>","text":""},{"location":"physical-operators/BaseAggregateExec/#initialinputbufferoffset","title":"initialInputBufferOffset <pre><code>initialInputBufferOffset: Int\n</code></pre>","text":""},{"location":"physical-operators/BaseAggregateExec/#isstreaming","title":"isStreaming <pre><code>isStreaming: Boolean\n</code></pre> <p>Used when:</p> <ul> <li><code>BaseAggregateExec</code> is requested to requiredChildDistribution, toSortAggregate</li> </ul>","text":""},{"location":"physical-operators/BaseAggregateExec/#numshufflepartitions","title":"numShufflePartitions <pre><code>numShufflePartitions: Option[Int]\n</code></pre> <p>Used when:</p> <ul> <li><code>BaseAggregateExec</code> is requested to requiredChildDistribution, toSortAggregate</li> </ul>","text":""},{"location":"physical-operators/BaseAggregateExec/#required-child-distribution-expressions","title":"Required Child Distribution Expressions <pre><code>requiredChildDistributionExpressions: Option[Seq[Expression]]\n</code></pre> <p>Expressions</p> <p>Used when:</p> <ul> <li><code>BaseAggregateExec</code> is requested for the requiredChildDistribution</li> <li>DisableUnnecessaryBucketedScan physical optimization is executed</li> </ul>","text":""},{"location":"physical-operators/BaseAggregateExec/#result-expressions","title":"Result Expressions <pre><code>resultExpressions: Seq[NamedExpression]\n</code></pre> <p>NamedExpressions of the result</p>","text":""},{"location":"physical-operators/BaseAggregateExec/#implementations","title":"Implementations","text":"<ul> <li>HashAggregateExec</li> <li>ObjectHashAggregateExec</li> <li>SortAggregateExec</li> </ul>"},{"location":"physical-operators/BaseAggregateExec/#aliasawareoutputpartitioning","title":"AliasAwareOutputPartitioning","text":"<p><code>BaseAggregateExec</code> is an AliasAwareOutputPartitioning.</p>"},{"location":"physical-operators/BaseAggregateExec/#detailed-description-with-operator-id","title":"Detailed Description (with Operator Id) <pre><code>verboseStringWithOperatorId(): String\n</code></pre> <p><code>verboseStringWithOperatorId</code>\u00a0is part of the QueryPlan abstraction.</p>  <p><code>verboseStringWithOperatorId</code> returns the following text (with the formattedNodeName and the others):</p> <pre><code>[formattedNodeName]\nInput [size]: [output]\nKeys [size]: [groupingExpressions]\nFunctions [size]: [aggregateExpressions]\nAggregate Attributes [size]: [aggregateAttributes]\nResults [size]: [resultExpressions]\n</code></pre>    Field Description     formattedNodeName <code>(operatorId) nodeName [codegen id : $id]</code>   Input Output schema of the single child operator   Keys Grouping Keys   Functions Aggregate Functions   Aggregate Attributes Aggregate Attributes   Results Result Expressions","text":""},{"location":"physical-operators/BaseAggregateExec/#required-child-output-distribution","title":"Required Child Output Distribution <pre><code>requiredChildDistribution: List[Distribution]\n</code></pre> <p><code>requiredChildDistribution</code>\u00a0is part of the SparkPlan abstraction.</p>  <p><code>requiredChildDistribution</code>...FIXME</p>","text":""},{"location":"physical-operators/BaseAggregateExec/#produced-attributes-schema","title":"Produced Attributes (Schema) <pre><code>producedAttributes: AttributeSet\n</code></pre> <p><code>producedAttributes</code>\u00a0is part of the QueryPlan abstraction.</p>  <p><code>producedAttributes</code> is the following:</p> <ul> <li>Aggregate Attributes</li> <li>Result Expressions that are not Grouping Keys</li> <li>Aggregate Buffer Attributes</li> <li>inputAggBufferAttributes without the output attributes of the single child operator</li> </ul>","text":""},{"location":"physical-operators/BaseAggregateExec/#aggregate-buffer-attributes-schema","title":"Aggregate Buffer Attributes (Schema) <pre><code>aggregateBufferAttributes: Seq[AttributeReference]\n</code></pre> <p><code>aggregateBufferAttributes</code> is the aggBufferAttributes of the AggregateFunctions of all the Aggregate Functions.</p>  <p><code>aggregateBufferAttributes</code> is used when:</p> <ul> <li><code>AggregateCodegenSupport</code> is requested to supportCodegen, doProduceWithoutKeys</li> <li><code>BaseAggregateExec</code> is requested for the produced attributes</li> </ul>","text":""},{"location":"physical-operators/BaseJoinExec/","title":"BaseJoinExec Physical Operators","text":"<p><code>BaseJoinExec</code>\u00a0is an extension of the <code>BinaryExecNode</code> abstraction for join physical operators.</p>"},{"location":"physical-operators/BaseJoinExec/#contract","title":"Contract","text":""},{"location":"physical-operators/BaseJoinExec/#join-condition","title":"Join Condition <pre><code>condition: Option[Expression]\n</code></pre>","text":""},{"location":"physical-operators/BaseJoinExec/#join-type","title":"Join Type <pre><code>joinType: JoinType\n</code></pre> <p>JoinType</p>","text":""},{"location":"physical-operators/BaseJoinExec/#left-keys","title":"Left Keys <pre><code>leftKeys: Seq[Expression]\n</code></pre>","text":""},{"location":"physical-operators/BaseJoinExec/#right-keys","title":"Right Keys <pre><code>rightKeys: Seq[Expression]\n</code></pre>","text":""},{"location":"physical-operators/BaseJoinExec/#implementations","title":"Implementations","text":"<ul> <li>BroadcastNestedLoopJoinExec</li> <li>CartesianProductExec</li> <li>HashJoin</li> <li>ShuffledJoin</li> </ul>"},{"location":"physical-operators/BaseSubqueryExec/","title":"BaseSubqueryExec Physical Operators","text":"<p><code>BaseSubqueryExec</code> is an extension of the SparkPlan abstraction for physical operators with a child subquery plan.</p>"},{"location":"physical-operators/BaseSubqueryExec/#contract","title":"Contract","text":""},{"location":"physical-operators/BaseSubqueryExec/#child-subquery-physical-plan","title":"Child Subquery Physical Plan <pre><code>child: SparkPlan\n</code></pre> <p>SparkPlan of the subquery</p> <p>Used when:</p> <ul> <li><code>BaseSubqueryExec</code> is requested to output, outputPartitioning and outputOrdering</li> <li><code>ExplainUtils</code> utility is used to <code>processPlan</code></li> </ul>","text":""},{"location":"physical-operators/BaseSubqueryExec/#name","title":"Name <pre><code>name: String\n</code></pre> <p>Used when:</p> <ul> <li><code>ReusedSubqueryExec</code> physical operator is requested for the name</li> <li><code>InSubqueryExec</code> expression is requested for the text representation</li> </ul>","text":""},{"location":"physical-operators/BaseSubqueryExec/#implementations","title":"Implementations","text":"<ul> <li>ReusedSubqueryExec</li> <li><code>SubqueryAdaptiveBroadcastExec</code></li> <li><code>SubqueryBroadcastExec</code></li> <li>SubqueryExec</li> </ul>"},{"location":"physical-operators/BaseSubqueryExec/#text-representation","title":"Text Representation <pre><code>generateTreeString(\n  depth: Int,\n  lastChildren: Seq[Boolean],\n  append: String =&gt; Unit,\n  verbose: Boolean,\n  prefix: String = \"\",\n  addSuffix: Boolean = false,\n  maxFields: Int,\n  printNodeId: Boolean): Unit\n</code></pre> <p><code>generateTreeString</code>...FIXME</p> <p><code>generateTreeString</code> is part of the TreeNode abstraction.</p>","text":""},{"location":"physical-operators/BatchScanExec/","title":"BatchScanExec Physical Operator","text":"<p><code>BatchScanExec</code> is a DataSourceV2ScanExecBase leaf physical operator for scanning a batch of data from a Scan.</p> <p><code>BatchScanExec</code> represents a data scan over a DataSourceV2ScanRelation relation at execution.</p>"},{"location":"physical-operators/BatchScanExec/#creating-instance","title":"Creating Instance","text":"<p><code>BatchScanExec</code> takes the following to be created:</p> <ul> <li> Output Schema (<code>Seq[AttributeReference]</code>) <li> Scan <li> Runtime Filters <li> Key Grouped Partitioning <li> Ordering <li> Table <li> Common Partition Values <li> <code>applyPartialClustering</code> flag (default: <code>false</code>) <li> <code>replicatePartitions</code> flag (default: <code>false</code>) <p><code>BatchScanExec</code> is created when:</p> <ul> <li>DataSourceV2Strategy execution planning strategy is executed (for physical operators with a DataSourceV2ScanRelation relation)</li> </ul>"},{"location":"physical-operators/BatchScanExec/#input-rdd","title":"Input RDD  Signature <pre><code>inputRDD: RDD[InternalRow]\n</code></pre> <p><code>inputRDD</code> is part of the DataSourceV2ScanExecBase abstraction.</p>  <p>For no filteredPartitions and the outputPartitioning to be <code>SinglePartition</code>, <code>inputRDD</code> creates an empty <code>RDD[InternalRow]</code> with 1 partition.</p> <p>Otherwise, <code>inputRDD</code> creates a DataSourceRDD as follows:</p>    DataSourceRDD's Attribute Value     InputPartitions filteredPartitions   PartitionReaderFactory readerFactory   columnarReads supportsColumnar   Custom Metrics customMetrics","text":""},{"location":"physical-operators/BatchScanExec/#filtered-input-partitions","title":"Filtered Input Partitions <pre><code>filteredPartitions: Seq[Seq[InputPartition]]\n</code></pre>  Lazy Value <p><code>filteredPartitions</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p>For non-empty runtimeFilters, <code>filteredPartitions</code>...FIXME</p> <p>Otherwise, <code>filteredPartitions</code> is the partitions (that usually is the input partitions of this <code>BatchScanExec</code>).</p>","text":""},{"location":"physical-operators/BatchScanExec/#input-partitions","title":"Input Partitions  Signature <pre><code>inputPartitions: Seq[InputPartition]\n</code></pre> <p><code>inputPartitions</code> is part of the DataSourceV2ScanExecBase abstraction.</p>  <p><code>inputPartitions</code> requests the Batch to plan input partitions.</p>","text":""},{"location":"physical-operators/BatchScanExec/#partitionreaderfactory","title":"PartitionReaderFactory  Signature <pre><code>readerFactory: PartitionReaderFactory\n</code></pre> <p><code>readerFactory</code> is part of the DataSourceV2ScanExecBase abstraction.</p>  <p><code>readerFactory</code> requests the Batch to createReaderFactory.</p>","text":""},{"location":"physical-operators/BatchScanExec/#batch","title":"Batch <pre><code>batch: Batch\n</code></pre> <p><code>batch</code> requests the Scan for the physical representation for batch query.</p>  <p><code>batch</code> is used when:</p> <ul> <li><code>BatchScanExec</code> is requested for partitions and readerFactory</li> </ul>","text":""},{"location":"physical-operators/BatchWriteHelper/","title":"BatchWriteHelper Physical Operators","text":"<p><code>BatchWriteHelper</code> is an abstraction of physical operators that build batch writes.</p>"},{"location":"physical-operators/BatchWriteHelper/#contract","title":"Contract","text":""},{"location":"physical-operators/BatchWriteHelper/#physical-query-plan","title":"Physical Query Plan <pre><code>query: SparkPlan\n</code></pre> <p>SparkPlan</p> <p>Used when:</p> <ul> <li><code>BatchWriteHelper</code> is requested for a WriteBuilder</li> </ul>","text":""},{"location":"physical-operators/BatchWriteHelper/#writable-table","title":"Writable Table <pre><code>table: SupportsWrite\n</code></pre> <p>SupportsWrite</p> <p>Used when:</p> <ul> <li><code>BatchWriteHelper</code> is requested for a WriteBuilder</li> </ul>","text":""},{"location":"physical-operators/BatchWriteHelper/#write-options","title":"Write Options <pre><code>writeOptions: CaseInsensitiveStringMap\n</code></pre> <p>Used when:</p> <ul> <li><code>BatchWriteHelper</code> is requested for a WriteBuilder</li> </ul>","text":""},{"location":"physical-operators/BatchWriteHelper/#implementations","title":"Implementations","text":"<ul> <li><code>AppendDataExec</code></li> <li>OverwriteByExpressionExec</li> <li><code>OverwritePartitionsDynamicExec</code></li> </ul>"},{"location":"physical-operators/BatchWriteHelper/#creating-writebuilder","title":"Creating WriteBuilder <pre><code>newWriteBuilder(): WriteBuilder\n</code></pre> <p><code>newWriteBuilder</code> requests the table for a WriteBuilder (with a new <code>LogicalWriteInfoImpl</code> with the query schema and write options).</p> <p><code>newWriteBuilder</code>\u00a0is used when:</p> <ul> <li><code>AppendDataExec</code>, OverwriteByExpressionExec and <code>OverwritePartitionsDynamicExec</code> physical operators are executed</li> </ul>","text":""},{"location":"physical-operators/BroadcastDistribution/","title":"BroadcastDistribution","text":"<p><code>BroadcastDistribution</code> is a data distribution requirement of the children of BroadcastHashJoinExec and BroadcastNestedLoopJoinExec physical operators.</p>"},{"location":"physical-operators/BroadcastDistribution/#creating-instance","title":"Creating Instance","text":"<p><code>BroadcastDistribution</code> takes the following to be created:</p> <ul> <li> BroadcastMode <p><code>BroadcastDistribution</code> is created when BroadcastHashJoinExec and BroadcastNestedLoopJoinExec physical operators are requested for the required child distribution.</p>"},{"location":"physical-operators/BroadcastDistribution/#required-number-of-partitions","title":"Required Number of Partitions <pre><code>requiredNumPartitions: Option[Int]\n</code></pre> <p><code>requiredNumPartitions</code> is always <code>1</code>.</p> <p><code>requiredNumPartitions</code> is part of the Distribution abstraction.</p>","text":""},{"location":"physical-operators/BroadcastDistribution/#creating-partitioning","title":"Creating Partitioning <pre><code>createPartitioning(\n  numPartitions: Int): Partitioning\n</code></pre> <p><code>createPartitioning</code> creates a BroadcastPartitioning (with the BroadcastMode).</p> <p><code>createPartitioning</code> throws an <code>AssertionError</code> when the given <code>numPartitions</code> is not <code>1</code>:</p> <pre><code>The default partitioning of BroadcastDistribution can only have 1 partition.\n</code></pre> <p><code>createPartitioning</code> is part of the Distribution abstraction.</p>","text":""},{"location":"physical-operators/BroadcastExchangeExec/","title":"BroadcastExchangeExec Unary Physical Operator for Broadcast Joins","text":"<p><code>BroadcastExchangeExec</code> is an BroadcastExchangeLike unary physical operator to collect and broadcast rows of a child relation (to worker nodes).</p> <p><code>BroadcastExchangeExec</code> is &lt;&gt; when EnsureRequirements physical optimization is executed (that can really be either BroadcastHashJoinExec.md[BroadcastHashJoinExec] or BroadcastNestedLoopJoinExec.md[BroadcastNestedLoopJoinExec] operators). <pre><code>val t1 = spark.range(5)\nval t2 = spark.range(5)\nval q = t1.join(t2).where(t1(\"id\") === t2(\"id\"))\n\nscala&gt; q.explain\n== Physical Plan ==\n*BroadcastHashJoin [id#19L], [id#22L], Inner, BuildRight\n:- *Range (0, 5, step=1, splits=Some(8))\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]))\n   +- *Range (0, 5, step=1, splits=Some(8))\n</code></pre> <p>[[outputPartitioning]] <code>BroadcastExchangeExec</code> uses BroadcastPartitioning partitioning scheme (with the input &lt;&gt;). <p>=== [[doExecuteBroadcast]] Waiting Until Relation Has Been Broadcast -- <code>doExecuteBroadcast</code> Method</p>"},{"location":"physical-operators/BroadcastExchangeExec/#source-scala","title":"[source, scala]","text":""},{"location":"physical-operators/BroadcastExchangeExec/#def-doexecutebroadcastt-broadcastbroadcastt","title":"def doExecuteBroadcastT: broadcast.Broadcast[T]","text":"<p><code>doExecuteBroadcast</code> waits until the &lt;&gt;. <p>NOTE: <code>doExecuteBroadcast</code> waits spark.sql.broadcastTimeout (defaults to 5 minutes).</p> <p>NOTE: <code>doExecuteBroadcast</code> is part of SparkPlan.md#doExecuteBroadcast[SparkPlan Contract] to return the result of a structured query as a broadcast variable.</p> <p>=== [[relationFuture]] Lazily-Once-Initialized Asynchronously-Broadcast <code>relationFuture</code> Internal Attribute</p>"},{"location":"physical-operators/BroadcastExchangeExec/#source-scala_1","title":"[source, scala]","text":""},{"location":"physical-operators/BroadcastExchangeExec/#relationfuture-futurebroadcastbroadcastany","title":"relationFuture: Future[broadcast.Broadcast[Any]]","text":"<p>When \"materialized\" (aka executed), <code>relationFuture</code> finds the current execution id and sets it to the <code>Future</code> thread.</p> <p><code>relationFuture</code> requests &lt;&gt; to SparkPlan.md#executeCollectIterator[executeCollectIterator]. <p><code>relationFuture</code> records the time for <code>executeCollectIterator</code> in &lt;&gt; metrics. <p>NOTE: <code>relationFuture</code> accepts a relation with up to 512 millions rows and 8GB in size, and reports a <code>SparkException</code> if the conditions are violated.</p> <p><code>relationFuture</code> requests the input &lt;&gt; to <code>transform</code> the internal rows to create a relation, e.g. HashedRelation or a <code>Array[InternalRow]</code>. <p><code>relationFuture</code> calculates the data size:</p> <ul> <li> <p>For a <code>HashedRelation</code>, <code>relationFuture</code> requests it to estimatedSize</p> </li> <li> <p>For a <code>Array[InternalRow]</code>, <code>relationFuture</code> transforms the <code>InternalRows</code> to UnsafeRow.md[UnsafeRows] and requests each to UnsafeRow.md#getSizeInBytes[getSizeInBytes] that it sums all up.</p> </li> </ul> <p><code>relationFuture</code> records the data size as the &lt;&gt; metric. <p><code>relationFuture</code> records the &lt;&gt; metric. <p><code>relationFuture</code> requests the SparkPlan.md#sparkContext[SparkContext] to <code>broadcast</code> the relation and records the time in &lt;&gt; metrics. <p>In the end, <code>relationFuture</code> requests <code>SQLMetrics</code> to post a SparkListenerDriverAccumUpdates (with the execution id and the SQL metrics) and returns the broadcast internal rows.</p> <p>NOTE: Since initialization of <code>relationFuture</code> happens on the driver, posting a SparkListenerDriverAccumUpdates is the only way how all the SQL metrics could be accessible to other subsystems using <code>SparkListener</code> listeners (incl. web UI).</p> <p>In case of <code>OutOfMemoryError</code>, <code>relationFuture</code> reports another <code>OutOfMemoryError</code> with the following message:</p>"},{"location":"physical-operators/BroadcastExchangeExec/#optionswrap","title":"[options=\"wrap\"]","text":""},{"location":"physical-operators/BroadcastExchangeExec/#not-enough-memory-to-build-and-broadcast-the-table-to-all-worker-nodes-as-a-workaround-you-can-either-disable-broadcast-by-setting-sparksqlautobroadcastjointhreshold-to-1-or-increase-the-spark-driver-memory-by-setting-sparkdrivermemory-to-a-higher-value","title":"Not enough memory to build and broadcast the table to all worker nodes. As a workaround, you can either disable broadcast by setting spark.sql.autoBroadcastJoinThreshold to -1 or increase the spark driver memory by setting spark.driver.memory to a higher value","text":"<p>[[executionContext]] NOTE: <code>relationFuture</code> is executed on a separate thread from a custom https://www.scala-lang.org/api/2.11.8/index.html#scala.concurrent.ExecutionContext[scala.concurrent.ExecutionContext] (built from a cached https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[java.util.concurrent.ThreadPoolExecutor] with the prefix broadcast-exchange and up to 128 threads).</p> <p>NOTE: <code>relationFuture</code> is used when <code>BroadcastExchangeExec</code> is requested to &lt;&gt; (that triggers asynchronous execution of the child operator and broadcasting the result) and &lt;&gt; (that waits until the broadcasting has finished). <p>=== [[doPrepare]] Broadcasting Relation (Rows) Asynchronously -- <code>doPrepare</code> Method</p>"},{"location":"physical-operators/BroadcastExchangeExec/#source-scala_2","title":"[source, scala]","text":""},{"location":"physical-operators/BroadcastExchangeExec/#doprepare-unit","title":"doPrepare(): Unit","text":"<p>NOTE: <code>doPrepare</code> is part of SparkPlan.md#doPrepare[SparkPlan Contract] to prepare a physical operator for execution.</p> <p><code>doPrepare</code> simply \"materializes\" the internal lazily-once-initialized &lt;&gt;. <p>=== [[creating-instance]] Creating BroadcastExchangeExec Instance</p> <p><code>BroadcastExchangeExec</code> takes the following when created:</p> <ul> <li>[[mode]] BroadcastMode</li> <li>[[child]] Child logical plan</li> </ul>"},{"location":"physical-operators/BroadcastExchangeExec/#performance-metrics","title":"Performance Metrics    Key Name (in web UI) Description     broadcastTime time to broadcast (ms)    buildTime time to build (ms)    collectTime time to collect (ms)    dataSize data size (bytes)","text":""},{"location":"physical-operators/BroadcastExchangeLike/","title":"BroadcastExchangeLike Physical Operators","text":"<p><code>BroadcastExchangeLike</code>\u00a0is an extension of the Exchange abstraction for physical operators that...FIXME</p>"},{"location":"physical-operators/BroadcastExchangeLike/#contract","title":"Contract","text":""},{"location":"physical-operators/BroadcastExchangeLike/#completionfuture","title":"completionFuture <pre><code>completionFuture: Future[Broadcast[Any]]\n</code></pre> <p>Used when:</p> <ul> <li><code>BroadcastQueryStageExec</code> physical operator is requested to materializeWithTimeout</li> </ul>","text":""},{"location":"physical-operators/BroadcastExchangeLike/#relationfuture","title":"relationFuture <pre><code>relationFuture: Future[Broadcast[Any]]\n</code></pre> <p>Used when:</p> <ul> <li><code>AQEPropagateEmptyRelation</code> adaptive logical optimization is executed</li> <li><code>BroadcastQueryStageExec</code> physical optimization is requested to cancel</li> </ul>","text":""},{"location":"physical-operators/BroadcastExchangeLike/#runid","title":"runId <pre><code>runId: UUID\n</code></pre> <p>Job group ID (for cancellation)</p> <p>Used when:</p> <ul> <li><code>BroadcastQueryStageExec</code> physical operator is requested to cancel</li> <li><code>BroadcastExchangeExec</code> physical operator is requested for the relationFuture and doExecuteBroadcast</li> </ul>","text":""},{"location":"physical-operators/BroadcastExchangeLike/#runtime-statistics","title":"Runtime Statistics <pre><code>runtimeStatistics: Statistics\n</code></pre> <p>Statistics with data size and row count</p> <p>See:</p> <ul> <li>BroadcastExchangeExec</li> </ul> <p>Used when:</p> <ul> <li><code>BroadcastQueryStageExec</code> physical operator is requested for runtime statistics</li> </ul>","text":""},{"location":"physical-operators/BroadcastExchangeLike/#implementations","title":"Implementations","text":"<ul> <li>BroadcastExchangeExec</li> </ul>"},{"location":"physical-operators/BroadcastHashJoinExec/","title":"BroadcastHashJoinExec Physical Operator","text":"<p><code>BroadcastHashJoinExec</code> is a hash-based join physical operator for broadcast hash join.</p> <p><code>BroadcastHashJoinExec</code> supports Java code generation (variable prefix: <code>bhj</code>).</p>"},{"location":"physical-operators/BroadcastHashJoinExec/#performance-metrics","title":"Performance Metrics    Key Name (in web UI) Description     numOutputRows number of output rows Number of output rows","text":""},{"location":"physical-operators/BroadcastHashJoinExec/#creating-instance","title":"Creating Instance <p><code>BroadcastHashJoinExec</code> takes the following to be created:</p> <ul> <li> Left Key Expressions <li> Right Key Expressions <li> Join Type <li> <code>BuildSide</code> <li> Optional Join Condition Expression <li> Left Child Physical Operator <li> Right Child Physical Operator <li>isNullAwareAntiJoin flag</li>  <p><code>BroadcastHashJoinExec</code> is created when:</p> <ul> <li>JoinSelection execution planning strategy is executed (createBroadcastHashJoin and ExtractSingleColumnNullAwareAntiJoin)</li> <li>LogicalQueryStageStrategy execution planning strategy is executed (ExtractEquiJoinKeys and ExtractSingleColumnNullAwareAntiJoin)</li> </ul>","text":""},{"location":"physical-operators/BroadcastHashJoinExec/#isnullawareantijoin-flag","title":"isNullAwareAntiJoin Flag <p><code>BroadcastHashJoinExec</code> can be given <code>isNullAwareAntiJoin</code> flag when created.</p> <p><code>isNullAwareAntiJoin</code> flag is <code>false</code> by default.</p> <p><code>isNullAwareAntiJoin</code> flag is <code>true</code> when:</p> <ul> <li>JoinSelection execution planning strategy is executed (for an ExtractSingleColumnNullAwareAntiJoin)</li> <li>LogicalQueryStageStrategy execution planning strategy is executed (for an ExtractSingleColumnNullAwareAntiJoin)</li> </ul> <p>If enabled, <code>BroadcastHashJoinExec</code> makes sure that the following all hold:</p> <ol> <li>There is one left key only</li> <li>There is one right key only</li> <li>Join Type is LeftAnti</li> <li>Build Side is <code>BuildRight</code></li> <li>Join condition is not defined</li> </ol> <p><code>isNullAwareAntiJoin</code> is used for the following:</p> <ul> <li>Required Child Output Distribution (and create a HashedRelationBroadcastMode)</li> <li>Executing Physical Operator</li> <li>Generating Java Code for Anti Join</li> </ul>","text":""},{"location":"physical-operators/BroadcastHashJoinExec/#required-child-output-distribution","title":"Required Child Output Distribution <pre><code>requiredChildDistribution: Seq[Distribution]\n</code></pre> <p><code>requiredChildDistribution</code> is part of the SparkPlan abstraction.</p>    BuildSide Left Child Right Child     BuildLeft BroadcastDistribution with HashedRelationBroadcastMode broadcast mode of build join keys UnspecifiedDistribution   BuildRight UnspecifiedDistribution BroadcastDistribution with HashedRelationBroadcastMode broadcast mode of build join keys","text":""},{"location":"physical-operators/BroadcastHashJoinExec/#output-data-partitioning-requirements","title":"Output Data Partitioning Requirements <pre><code>outputPartitioning: Partitioning\n</code></pre> <p><code>outputPartitioning</code> is part of the SparkPlan abstraction.</p> <p><code>outputPartitioning</code>...FIXME</p>","text":""},{"location":"physical-operators/BroadcastHashJoinExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p> <p><code>doExecute</code> requests the buildPlan to executeBroadcast (that gives a broadcast variable with a HashedRelation).</p> <p><code>doExecute</code> branches off based on isNullAwareAntiJoin flag: enabled or not.</p>","text":""},{"location":"physical-operators/BroadcastHashJoinExec/#isnullawareantijoin-enabled","title":"isNullAwareAntiJoin Enabled <p><code>doExecute</code>...FIXME</p>","text":""},{"location":"physical-operators/BroadcastHashJoinExec/#isnullawareantijoin-disabled","title":"isNullAwareAntiJoin Disabled <p><code>doExecute</code> requests the streamedPlan to execute (that gives an <code>RDD[InternalRow]</code>) and maps over partitions (<code>RDD.mapPartitions</code>):</p> <ol> <li>Takes the read-only copy of the HashedRelation (from the broadcast variable)</li> <li>Increment the peak execution memory (of the task) by the size of the <code>HashedRelation</code></li> <li>Joins the rows with the <code>HashedRelation</code> (with the numOutputRows metric)</li> </ol>","text":""},{"location":"physical-operators/BroadcastHashJoinExec/#generating-java-code-for-anti-join","title":"Generating Java Code for Anti Join <pre><code>codegenAnti(\n  ctx: CodegenContext,\n  input: Seq[ExprCode]): String\n</code></pre> <p><code>codegenAnti</code> is part of the HashJoin abstraction.</p> <p><code>codegenAnti</code>...FIXME</p>","text":""},{"location":"physical-operators/BroadcastHashJoinExec/#demo","title":"Demo <pre><code>val tokens = Seq(\n  (0, \"playing\"),\n  (1, \"with\"),\n  (2, \"BroadcastHashJoinExec\")\n).toDF(\"id\", \"token\")\n</code></pre> <pre><code>val q = tokens.join(tokens, Seq(\"id\"), \"inner\")\n</code></pre> <pre><code>scala&gt; println(q.queryExecution.executedPlan.numberedTreeString)\n00 AdaptiveSparkPlan isFinalPlan=false\n01 +- Project [id#18, token#19, token#25]\n02    +- BroadcastHashJoin [id#18], [id#24], Inner, BuildRight, false\n03       :- LocalTableScan [id#18, token#19]\n04       +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#16]\n05          +- LocalTableScan [id#24, token#25]\n</code></pre> <pre><code>import org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec\nval op = q\n  .queryExecution\n  .executedPlan\n  .collect { case op: AdaptiveSparkPlanExec =&gt; op }\n  .head\n</code></pre> <pre><code>scala&gt; println(op.treeString)\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [id#18, token#19, token#25]\n   +- BroadcastHashJoin [id#18], [id#24], Inner, BuildRight, false\n      :- LocalTableScan [id#18, token#19]\n      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#16]\n         +- LocalTableScan [id#24, token#25]\n</code></pre> <p>Execute the adaptive operator to generate the final execution plan.</p> <pre><code>op.executeTake(1)\n</code></pre> <p>Mind the isFinalPlan flag that is now enabled.</p> <pre><code>scala&gt; println(op.treeString)\nAdaptiveSparkPlan isFinalPlan=true\n+- == Final Plan ==\n   *(1) Project [id#18, token#19, token#25]\n   +- *(1) BroadcastHashJoin [id#18], [id#24], Inner, BuildRight, false\n      :- *(1) LocalTableScan [id#18, token#19]\n      +- BroadcastQueryStage 0\n         +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#16]\n            +- LocalTableScan [id#24, token#25]\n+- == Initial Plan ==\n   Project [id#18, token#19, token#25]\n   +- BroadcastHashJoin [id#18], [id#24], Inner, BuildRight, false\n      :- LocalTableScan [id#18, token#19]\n      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#16]\n         +- LocalTableScan [id#24, token#25]\n</code></pre> <p>With the isFinalPlan flag enabled, it is possible to print out the WholeStageCodegen subtrees.</p> <pre><code>scala&gt; q.queryExecution.debug.codegen\nFound 1 WholeStageCodegen subtrees.\n== Subtree 1 / 1 (maxMethodCodeSize:265; maxConstantPoolSize:146(0.22% used); numInnerClasses:0) ==\n*(1) Project [id#18, token#19, token#25]\n+- *(1) BroadcastHashJoin [id#18], [id#24], Inner, BuildRight, false\n   :- *(1) LocalTableScan [id#18, token#19]\n   +- BroadcastQueryStage 0\n      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#16]\n         +- LocalTableScan [id#24, token#25]\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=1\n/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private scala.collection.Iterator localtablescan_input_0;\n/* 010 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;\n/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] bhj_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n/* 012 */\n/* 013 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n/* 014 */     this.references = references;\n/* 015 */   }\n/* 016 */\n/* 017 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 018 */     partitionIndex = index;\n/* 019 */     this.inputs = inputs;\n/* 020 */     localtablescan_input_0 = inputs[0];\n/* 021 */\n/* 022 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[1] /* broadcast */).value()).asReadOnlyCopy();\n/* 023 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());\n/* 024 */\n/* 025 */     bhj_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 64);\n/* 026 */     bhj_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 64);\n/* 027 */\n/* 028 */   }\n...\n</code></pre> <p>Let's access the generated source code via WholeStageCodegenExec physical operator.</p> <pre><code>val aqe = op\nimport org.apache.spark.sql.execution.WholeStageCodegenExec\nval wsce = aqe.executedPlan\n  .collect { case op: WholeStageCodegenExec =&gt; op }\n  .head\nval (_, source) = wsce.doCodeGen\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatter\nval formattedCode = CodeFormatter.format(source)\n</code></pre> <pre><code>scala&gt; println(formattedCode)\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=1\n/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private scala.collection.Iterator localtablescan_input_0;\n/* 010 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;\n/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] bhj_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n/* 012 */\n/* 013 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n/* 014 */     this.references = references;\n/* 015 */   }\n...\n</code></pre>","text":""},{"location":"physical-operators/BroadcastMode/","title":"BroadcastModes","text":"<p><code>BroadcastMode</code> is an abstraction of broadcast modes that can transform internal rows (with optional size hint).</p> <p><code>BroadcastMode</code> is used to create:</p> <ul> <li>BroadcastDistribution</li> <li>BroadcastPartitioning</li> <li>BroadcastExchangeExec physical operator</li> </ul>"},{"location":"physical-operators/BroadcastMode/#contract","title":"Contract","text":""},{"location":"physical-operators/BroadcastMode/#canonicalized-form","title":"Canonicalized Form <pre><code>canonicalized: BroadcastMode\n</code></pre>","text":""},{"location":"physical-operators/BroadcastMode/#transforming-internalrows-into-hashedrelation-with-optional-size-hint","title":"Transforming InternalRows into HashedRelation (with Optional Size Hint) <pre><code>transform(\n  rows: Iterator[InternalRow],\n  sizeHint: Option[Long]): Any\n</code></pre> <p>Used when:</p> <ul> <li><code>BroadcastExchangeExec</code> physical operator is requested for relationFuture</li> </ul>","text":""},{"location":"physical-operators/BroadcastMode/#transform-rows","title":"Transform Rows <pre><code>transform(\n  rows: Array[InternalRow]): Any\n</code></pre>  <p>Note</p> <p><code>transform(rows)</code> does not seem to be used.</p>","text":""},{"location":"physical-operators/BroadcastMode/#implementations","title":"Implementations","text":"<ul> <li>HashedRelationBroadcastMode</li> <li><code>IdentityBroadcastMode</code></li> </ul>"},{"location":"physical-operators/BroadcastNestedLoopJoinExec/","title":"BroadcastNestedLoopJoinExec Binary Physical Operator","text":"<p><code>BroadcastNestedLoopJoinExec</code> is a binary physical operator that is &lt;&gt; (and converted to) when JoinSelection physical plan strategy finds a Join.md[Join] logical operator that meets either case: <ul> <li> <p>canBuildRight join type and <code>right</code> physical operator broadcastable</p> </li> <li> <p>canBuildLeft join type and <code>left</code> broadcastable</p> </li> <li> <p>non-<code>InnerLike</code> join type</p> </li> </ul> <p>Note</p> <p><code>BroadcastNestedLoopJoinExec</code> is the default physical operator when no other operators have matched selection requirements.</p>"},{"location":"physical-operators/BroadcastNestedLoopJoinExec/#note","title":"[NOTE]","text":"<p>canBuildRight join types are:</p> <ul> <li>CROSS, INNER, LEFT ANTI, LEFT OUTER, LEFT SEMI or Existence</li> </ul> <p>canBuildLeft join types are:</p>"},{"location":"physical-operators/BroadcastNestedLoopJoinExec/#cross-inner-right-outer","title":"* CROSS, INNER, RIGHT OUTER","text":"<pre><code>val nums = spark.range(2)\nval letters = ('a' to 'c').map(_.toString).toDF(\"letter\")\nval q = nums.crossJoin(letters)\n\nscala&gt; q.explain\n== Physical Plan ==\nBroadcastNestedLoopJoin BuildRight, Cross\n:- *Range (0, 2, step=1, splits=Some(8))\n+- BroadcastExchange IdentityBroadcastMode\n   +- LocalTableScan [letter#69]\n</code></pre> <p>[[requiredChildDistribution]] .BroadcastNestedLoopJoinExec's Required Child Output Distributions [cols=\"1m,2,2\",options=\"header\",width=\"100%\"] |=== | BuildSide | Left Child | Right Child</p> <p>| BuildLeft | BroadcastDistribution (uses <code>IdentityBroadcastMode</code> broadcast mode) | UnspecifiedDistribution</p> <p>| BuildRight | UnspecifiedDistribution | BroadcastDistribution (uses <code>IdentityBroadcastMode</code> broadcast mode) |===</p> <p>=== [[creating-instance]] Creating BroadcastNestedLoopJoinExec Instance</p> <p><code>BroadcastNestedLoopJoinExec</code> takes the following when created:</p> <ul> <li>[[left]] Left SparkPlan.md[physical operator]</li> <li>[[right]] Right SparkPlan.md[physical operator]</li> <li>[[buildSide]] <code>BuildSide</code></li> <li>[[joinType]] Join type</li> <li>[[condition]] Optional join condition expressions/Expression.md[expressions]</li> </ul>"},{"location":"physical-operators/BroadcastNestedLoopJoinExec/#performance-metrics","title":"Performance Metrics    Key Name (in web UI) Description     numOutputRows number of output rows Number of output rows","text":""},{"location":"physical-operators/BroadcastQueryStageExec/","title":"BroadcastQueryStageExec Physical Operator","text":"<p><code>BroadcastQueryStageExec</code> is a QueryStageExec.</p>"},{"location":"physical-operators/BroadcastQueryStageExec/#creating-instance","title":"Creating Instance","text":"<p><code>BroadcastQueryStageExec</code> takes the following to be created:</p> <ul> <li> ID <li> SparkPlan <p><code>BroadcastQueryStageExec</code> is created when:</p> <ul> <li> <p><code>AdaptiveSparkPlanExec</code> physical operator is requested to newQueryStage (for a BroadcastExchangeExec)</p> </li> <li> <p><code>BroadcastQueryStageExec</code> physical operator is requested to newReuseInstance</p> </li> </ul>"},{"location":"physical-operators/BroadcastQueryStageExec/#runtime-statistics","title":"Runtime Statistics <pre><code>getRuntimeStatistics: Statistics\n</code></pre> <p><code>getRuntimeStatistics</code> is part of the QueryStageExec abstraction.</p>  <p><code>getRuntimeStatistics</code> requests the BroadcastExchangeLike operator for the runtime statistics.</p>","text":""},{"location":"physical-operators/BroadcastQueryStageExec/#materializewithtimeout","title":"materializeWithTimeout <pre><code>materializeWithTimeout: Future[Any]\n</code></pre> <p><code>materializeWithTimeout</code> is...FIXME</p>","text":""},{"location":"physical-operators/BroadcastQueryStageExec/#broadcastexchangeexec-physical-operator","title":"BroadcastExchangeExec Physical Operator <pre><code>broadcast: BroadcastExchangeExec\n</code></pre> <p><code>BroadcastQueryStageExec</code> creates a BroadcastExchangeExec when created.</p>","text":""},{"location":"physical-operators/BroadcastQueryStageExec/#creating-broadcastquerystageexec-physical-operator","title":"Creating BroadcastQueryStageExec Physical Operator <pre><code>newReuseInstance(\n  newStageId: Int,\n  newOutput: Seq[Attribute]): QueryStageExec\n</code></pre> <p><code>newReuseInstance</code> creates a new <code>BroadcastQueryStageExec</code> with the given <code>newStageId</code> and a new ReusedExchangeExec (with the given <code>newOutput</code> and the broadcast).</p> <p><code>newReuseInstance</code> is part of the QueryStageExec abstraction.</p>","text":""},{"location":"physical-operators/ClusteredDistribution/","title":"ClusteredDistribution","text":"<p><code>ClusteredDistribution</code> is a Distribution that &lt;&gt; for the &lt;&gt; and a requested number of partitions. <p><code>ClusteredDistribution</code> requires that the &lt;&gt; should not be empty (i.e. <code>Nil</code>). <p><code>ClusteredDistribution</code> is &lt;&gt; when the following physical operators are requested for a required child distribution: <ul> <li> <p><code>MapGroupsExec</code>, HashAggregateExec, ObjectHashAggregateExec, SortAggregateExec, WindowExec</p> </li> <li> <p>Spark Structured Streaming's <code>FlatMapGroupsWithStateExec</code>, <code>StateStoreRestoreExec</code>, <code>StateStoreSaveExec</code>, <code>StreamingDeduplicateExec</code>, <code>StreamingSymmetricHashJoinExec</code>, <code>StreamingSymmetricHashJoinExec</code></p> </li> <li> <p>SparkR's <code>FlatMapGroupsInRExec</code></p> </li> <li> <p>PySpark's <code>FlatMapGroupsInPandasExec</code></p> </li> </ul> <p><code>ClusteredDistribution</code> is used when:</p> <ul> <li> <p><code>DataSourcePartitioning</code>, <code>SinglePartition</code>, <code>HashPartitioning</code>, and <code>RangePartitioning</code> are requested to <code>satisfies</code></p> </li> <li> <p>EnsureRequirements is executed for Adaptive Query Execution</p> </li> </ul> <p>=== [[createPartitioning]] <code>createPartitioning</code> Method</p>"},{"location":"physical-operators/ClusteredDistribution/#source-scala","title":"[source, scala]","text":""},{"location":"physical-operators/ClusteredDistribution/#createpartitioningnumpartitions-int-partitioning","title":"createPartitioning(numPartitions: Int): Partitioning","text":"<p><code>createPartitioning</code> creates a <code>HashPartitioning</code> for the &lt;&gt; and the input <code>numPartitions</code>. <p><code>createPartitioning</code> reports an <code>AssertionError</code> when the &lt;&gt; is not the input <code>numPartitions</code>. <p>[options=\"wrap\"] <pre><code>This ClusteredDistribution requires [requiredNumPartitions] partitions, but the actual number of partitions is [numPartitions].\n</code></pre></p> <p><code>createPartitioning</code> is part of the Distribution abstraction.</p>"},{"location":"physical-operators/ClusteredDistribution/#creating-instance","title":"Creating Instance","text":"<p><code>ClusteredDistribution</code> takes the following to be created:</p> <ul> <li>[[clustering]] Clustering expressions</li> <li>[[requiredNumPartitions]] Required number of partitions (default: <code>None</code>)</li> </ul> <p>Note</p> <p><code>None</code> for the required number of partitions indicates to use any number of partitions (possibly spark.sql.shuffle.partitions configuration property).</p>"},{"location":"physical-operators/CoalesceExec/","title":"CoalesceExec Unary Physical Operator","text":"<p><code>CoalesceExec</code> is a unary physical operator to...FIXME...with <code>numPartitions</code> number of partitions and a <code>child</code> spark plan.</p> <p><code>CoalesceExec</code> represents Repartition logical operator at execution (when <code>shuffle</code> was disabled -- see BasicOperators execution planning strategy). When executed, it executes the input <code>child</code> and calls spark-rdd-partitions.md#coalesce[coalesce] on the result RDD (with <code>shuffle</code> disabled).</p> <p>Please note that since physical operators present themselves without the suffix Exec, <code>CoalesceExec</code> is the <code>Coalesce</code> in the Physical Plan section in the following example:</p>"},{"location":"physical-operators/CoalesceExec/#source-scala","title":"[source, scala]","text":"<p>scala&gt; df.rdd.getNumPartitions res6: Int = 8</p> <p>scala&gt; df.coalesce(1).rdd.getNumPartitions res7: Int = 1</p> <p>scala&gt; df.coalesce(1).explain(extended = true) == Parsed Logical Plan == Repartition 1, false +- LocalRelation [value#1]</p> <p>== Analyzed Logical Plan == value: int Repartition 1, false +- LocalRelation [value#1]</p> <p>== Optimized Logical Plan == Repartition 1, false +- LocalRelation [value#1]</p> <p>== Physical Plan == Coalesce 1 +- LocalTableScan [value#1]</p> <p><code>output</code> collection of spark-sql-Expression-Attribute.md[Attribute] matches the <code>child</code>'s (since <code>CoalesceExec</code> is about changing the number of partitions not the internal representation).</p> <p><code>outputPartitioning</code> returns a SinglePartition when the input <code>numPartitions</code> is <code>1</code> while a UnknownPartitioning partitioning scheme for the other cases.</p>"},{"location":"physical-operators/CodegenSupport/","title":"CodegenSupport Physical Operators","text":"<p><code>CodegenSupport</code> is an extension of the SparkPlan abstraction for physical operators that support Whole-Stage Java Code Generation.</p>"},{"location":"physical-operators/CodegenSupport/#contract","title":"Contract","text":""},{"location":"physical-operators/CodegenSupport/#java-source-code-for-consume-path","title":"Java Source Code for Consume Path <pre><code>doConsume(\n  ctx: CodegenContext,\n  input: Seq[ExprCode],\n  row: ExprCode): String\n</code></pre> <p>Generates a Java source code (as a text) for this physical operator for the consume execution path in Whole-Stage Java Code Generation</p>  <p>UnsupportedOperationException</p> <p><code>doConsume</code> throws an <code>UnsupportedOperationException</code> by default.</p>  <p>Used when the physical operator is requested to generate the Java source code for consume code path (a Java code that consumers the generated columns or a row from a physical operator)</p>","text":""},{"location":"physical-operators/CodegenSupport/#java-source-code-for-produce-path","title":"Java Source Code for Produce Path <pre><code>doProduce(\n  ctx: CodegenContext): String\n</code></pre> <p>Generates a Java source code (as a text) for the physical operator to process the rows from the input RDDs for the whole-stage-codegen \"produce\" path.</p> <p>Used when the physical operator is requested to generate the Java source code for \"produce\" code path</p>","text":""},{"location":"physical-operators/CodegenSupport/#input-rdds","title":"Input RDDs <pre><code>inputRDDs(): Seq[RDD[InternalRow]]\n</code></pre> <p>Input RDDs of the physical operator</p>  <p>Important</p> <p>Whole-Stage Java Code Generation supports up to two input RDDs.</p>  <p>Used when WholeStageCodegenExec unary physical operator is executed</p>","text":""},{"location":"physical-operators/CodegenSupport/#implementations","title":"Implementations","text":"<ul> <li>BroadcastHashJoinExec</li> <li>ColumnarToRowExec</li> <li>DebugExec</li> <li>FilterExec</li> <li>GenerateExec</li> <li>ProjectExec</li> <li>RangeExec</li> <li>SerializeFromObjectExec</li> <li>SortMergeJoinExec</li> <li>WholeStageCodegenExec</li> <li>others</li> </ul>"},{"location":"physical-operators/CodegenSupport/#final-methods","title":"Final Methods","text":"<p>Final methods are used to generate the Java source code in different phases of Whole-Stage Java Code Generation.</p>"},{"location":"physical-operators/CodegenSupport/#generating-java-source-code-for-consume-code-path","title":"Generating Java Source Code for Consume Code Path <pre><code>consume(\n  ctx: CodegenContext,\n  outputVars: Seq[ExprCode],\n  row: String = null): String\n</code></pre> <p><code>consume</code> generates Java source code for consuming generated columns or a row from the physical operator</p> <p><code>consume</code> creates the <code>ExprCodes</code> for the input variables (<code>inputVars</code>).</p> <ul> <li> <p>If <code>outputVars</code> is defined, <code>consume</code> makes sure that their number is exactly the length of the output attributes and copies them. In other words, <code>inputVars</code> is exactly <code>outputVars</code>.</p> </li> <li> <p>If <code>outputVars</code> is not defined, <code>consume</code> makes sure that <code>row</code> is defined. <code>consume</code> sets currentVars of the <code>CodegenContext</code> to <code>null</code> while INPUT_ROW to the <code>row</code>. For every output attribute, <code>consume</code> creates a BoundReference and requests it to generate code for expression evaluation.</p> </li> </ul> <p><code>consume</code> creates a row variable.</p> <p><code>consume</code> sets the following in the <code>CodegenContext</code>:</p> <ul> <li> <p>currentVars as the <code>inputVars</code></p> </li> <li> <p>INPUT_ROW as <code>null</code></p> </li> <li> <p>freshNamePrefix as the &lt;&gt; of the &lt;&gt;.   <p><code>consume</code> &lt;&gt; (with the <code>output</code>, <code>inputVars</code> and &lt;&gt; of the &lt;&gt;) and creates so-called <code>evaluated</code>. <p><code>consume</code> creates a so-called <code>consumeFunc</code> by &lt;&gt; when the following are all met: <p>. spark.sql.codegen.splitConsumeFuncByOperator internal configuration property is enabled</p> <p>. &lt;&gt; of the &lt;&gt; contains all catalyst/QueryPlan.md#output[output attributes] <p>. <code>paramLength</code> is correct (FIXME)</p> <p>Otherwise, <code>consume</code> requests the &lt;&gt; to &lt;&gt;. <p>In the end, <code>consume</code> gives the plain Java source code with the comment <code>CONSUME: [parent]</code>:</p> <pre><code>[evaluated]\n[consumeFunc]\n</code></pre>  <p>Tip</p> <p>Enable spark.sql.codegen.comments Spark SQL property to have <code>CONSUME</code> markers in the generated Java source code.</p>  <pre><code>// ./bin/spark-shell --conf spark.sql.codegen.comments=true\nimport org.apache.spark.sql.execution.debug._\nval q = Seq((0 to 4).toList).toDF.\n  select(explode('value) as \"id\").\n  join(spark.range(1), \"id\")\nscala&gt; q.debugCodegen\nFound 2 WholeStageCodegen subtrees.\n...\n== Subtree 2 / 2 ==\n*Project [id#6]\n+- *BroadcastHashJoin [cast(id#6 as bigint)], [id#9L], Inner, BuildRight\n   :- Generate explode(value#1), false, false, [id#6]\n   :  +- LocalTableScan [value#1]\n   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]))\n      +- *Range (0, 1, step=1, splits=8)\n...\n/* 066 */     while (inputadapter_input.hasNext() &amp;&amp; !stopEarly()) {\n/* 067 */       InternalRow inputadapter_row = (InternalRow) inputadapter_input.next();\n/* 068 */       // CONSUME: BroadcastHashJoin [cast(id#6 as bigint)], [id#9L], Inner, BuildRight\n/* 069 */       // input[0, int, false]\n/* 070 */       int inputadapter_value = inputadapter_row.getInt(0);\n...\n/* 079 */       // find matches from HashedRelation\n/* 080 */       UnsafeRow bhj_matched = bhj_isNull ? null: (UnsafeRow)bhj_relation.getValue(bhj_value);\n/* 081 */       if (bhj_matched != null) {\n/* 082 */         {\n/* 083 */           bhj_numOutputRows.add(1);\n/* 084 */\n/* 085 */           // CONSUME: Project [id#6]\n/* 086 */           // CONSUME: WholeStageCodegen\n/* 087 */           project_rowWriter.write(0, inputadapter_value);\n/* 088 */           append(project_result);\n/* 089 */\n/* 090 */         }\n/* 091 */       }\n/* 092 */       if (shouldStop()) return;\n...\n</code></pre> <p><code>consume</code> is used when:</p> <ul> <li> <p>BroadcastHashJoinExec, <code>BaseLimitExec</code>, DeserializeToObjectExec, <code>ExpandExec</code>, &lt;&gt;, GenerateExec.md#doConsume[GenerateExec], ProjectExec.md#doConsume[ProjectExec], <code>SampleExec</code>, <code>SerializeFromObjectExec</code>, <code>MapElementsExec</code>, <code>DebugExec</code> physical operators are requested to generate the Java source code for \"consume\" path in whole-stage code generation  <li> <p>HashAggregateExec, InputAdapter, RowDataSourceScanExec, RangeExec, SortExec, SortMergeJoinExec physical operators are requested to generate the Java source code for the \"produce\" path in whole-stage code generation</p> </li>","text":""},{"location":"physical-operators/CodegenSupport/#data-producing-loop-condition","title":"Data-Producing Loop Condition <pre><code>limitNotReachedCond: String\n</code></pre> <p><code>limitNotReachedCond</code> is used as a loop condition by ColumnarToRowExec, SortExec, <code>InputRDDCodegen</code> and HashAggregateExec physical operators (when requested to doProduce).</p> <p><code>limitNotReachedCond</code> requests the parent physical operator for the limit-not-reached checks.</p> <p><code>limitNotReachedCond</code> returns an empty string for no limit-not-reached checks or concatenates them with <code>&amp;&amp;</code>.</p>","text":""},{"location":"physical-operators/CodegenSupport/#generating-java-source-code-for-produce-code-path","title":"Generating Java Source Code for Produce Code Path <pre><code>produce(\n  ctx: CodegenContext,\n  parent: CodegenSupport): String\n</code></pre> <p><code>produce</code> generates Java source code for whole-stage-codegen \"produce\" code path.</p> <p><code>produce</code> prepares a physical operator for query execution and then generates a Java source code with the result of doProduce.</p> <p><code>produce</code> annotates the code block with <code>PRODUCE</code> markers (that are simple descriptions of the physical operators in a structured query).</p> <p><code>produce</code> is used when:</p> <ul> <li> <p>(most importantly) <code>WholeStageCodegenExec</code> physical operator is requested to generate the Java source code for a subtree</p> </li> <li> <p>A physical operator (with <code>CodegenSupport</code>) is requested to generate a Java source code for the produce path in whole-stage Java code generation that usually looks as follows:</p> <pre><code>protected override def doProduce(ctx: CodegenContext): String = {\n  child.asInstanceOf[CodegenSupport].produce(ctx, this)\n}\n</code></pre> </li> </ul>  <p>spark.sql.codegen.comments Property</p> <p>Enable <code>spark.sql.codegen.comments</code> Spark SQL property for <code>PRODUCE</code> markers in the generated Java source code.</p>  <pre><code>// ./bin/spark-shell --conf spark.sql.codegen.comments=true\nimport org.apache.spark.sql.execution.debug._\nval q = Seq((0 to 4).toList).toDF.\n  select(explode('value) as \"id\").\n  join(spark.range(1), \"id\")\nscala&gt; q.debugCodegen\nFound 2 WholeStageCodegen subtrees.\n== Subtree 1 / 2 ==\n*Range (0, 1, step=1, splits=8)\n...\n/* 080 */   protected void processNext() throws java.io.IOException {\n/* 081 */     // PRODUCE: Range (0, 1, step=1, splits=8)\n/* 082 */     // initialize Range\n/* 083 */     if (!range_initRange) {\n...\n== Subtree 2 / 2 ==\n*Project [id#6]\n+- *BroadcastHashJoin [cast(id#6 as bigint)], [id#9L], Inner, BuildRight\n   :- Generate explode(value#1), false, false, [id#6]\n   :  +- LocalTableScan [value#1]\n   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]))\n      +- *Range (0, 1, step=1, splits=8)\n...\n/* 062 */   protected void processNext() throws java.io.IOException {\n/* 063 */     // PRODUCE: Project [id#6]\n/* 064 */     // PRODUCE: BroadcastHashJoin [cast(id#6 as bigint)], [id#9L], Inner, BuildRight\n/* 065 */     // PRODUCE: InputAdapter\n/* 066 */     while (inputadapter_input.hasNext() &amp;&amp; !stopEarly()) {\n...\n</code></pre>","text":""},{"location":"physical-operators/CodegenSupport/#supportcodegen-flag","title":"supportCodegen Flag <pre><code>supportCodegen: Boolean\n</code></pre> <p><code>supportCodegen</code> flag allows physical operators (that support Whole-Stage Java Code Generation) to disable Java code generation temporarily under certain conditions.</p> <p><code>supportCodegen</code> is enabled (<code>true</code>) by default.</p> <p><code>supportCodegen</code> can be disabled (<code>false</code>) in the following physical operators:</p> <ul> <li>AggregateCodegenSupport</li> <li>BroadcastNestedLoopJoinExec</li> <li>GenerateExec (based on supportCodegen of a Generator expression)</li> <li>ShuffledHashJoinExec (for all join types except <code>FullOuter</code> unless spark.sql.codegen.join.fullOuterShuffledHashJoin.enabled is enabled)</li> <li>SortAggregateExec</li> <li>SortMergeJoinExec</li> </ul> <p><code>supportCodegen</code> flag is used to select between <code>InputAdapter</code> or <code>WholeStageCodegenExec</code> physical operators when CollapseCodegenStages physical optimization is executed (and checks whether a physical operator meets the requirements of whole-stage Java code generation or not).</p>","text":""},{"location":"physical-operators/CodegenSupport/#preparerowvar-internal-method","title":"prepareRowVar Internal Method <pre><code>prepareRowVar(\n  ctx: CodegenContext,\n  row: String,\n  colVars: Seq[ExprCode]): ExprCode\n</code></pre> <p><code>prepareRowVar</code>...FIXME</p> <p><code>prepareRowVar</code> is used when <code>CodegenSupport</code> is requested to consume (and constructDoConsumeFunction with spark.sql.codegen.splitConsumeFuncByOperator enabled).</p>","text":""},{"location":"physical-operators/CodegenSupport/#constructdoconsumefunction-internal-method","title":"constructDoConsumeFunction Internal Method <pre><code>constructDoConsumeFunction(\n  ctx: CodegenContext,\n  inputVars: Seq[ExprCode],\n  row: String): String\n</code></pre> <p><code>constructDoConsumeFunction</code>...FIXME</p> <p><code>constructDoConsumeFunction</code> is used when <code>CodegenSupport</code> is requested to consume.</p>","text":""},{"location":"physical-operators/CodegenSupport/#used-input-attributes","title":"Used Input Attributes <pre><code>usedInputs: AttributeSet\n</code></pre> <p><code>usedInputs</code> returns the expression references.</p>  <p>Note</p> <p>Physical operators can mark it as empty to defer evaluation of attribute expressions until they are actually used (in the generated Java source code for consume path).</p>   <p><code>usedInputs</code> is used when:</p> <ul> <li><code>CodegenSupport</code> is requested to generate a Java source code for consume path</li> </ul>","text":""},{"location":"physical-operators/CodegenSupport/#parent-internal-variable-property","title":"parent Internal Variable Property <pre><code>parent: CodegenSupport\n</code></pre> <p><code>parent</code> is a physical operator that supports whole-stage Java code generation.</p> <p><code>parent</code> starts empty, (defaults to <code>null</code> value) and is assigned a physical operator (with <code>CodegenContext</code>) only when <code>CodegenContext</code> is requested to generate a Java source code for produce code path. The physical operator is passed in as an input argument for the produce code path.</p>","text":""},{"location":"physical-operators/CodegenSupport/#limitnotreachedchecks","title":"limitNotReachedChecks <pre><code>limitNotReachedChecks: Seq[String]\n</code></pre> <p><code>limitNotReachedChecks</code> is a sequence of checks which evaluate to true if the downstream Limit operators have not received enough records and reached the limit.</p> <p><code>limitNotReachedChecks</code> requests the parent physical operator for <code>limitNotReachedChecks</code>.</p>  <p><code>limitNotReachedChecks</code> is used when:</p> <ul> <li><code>RangeExec</code> physical operator is requested to doProduce</li> <li><code>BaseLimitExec</code> physical operator is requested to <code>limitNotReachedChecks</code></li> <li><code>CodegenSupport</code> physical operator is requested to limitNotReachedCond</li> </ul>","text":""},{"location":"physical-operators/CodegenSupport/#canchecklimitnotreached","title":"canCheckLimitNotReached <pre><code>canCheckLimitNotReached: Boolean\n</code></pre> <p><code>canCheckLimitNotReached</code> is <code>true</code> when there are no children.</p>  <p><code>canCheckLimitNotReached</code> is used when:</p> <ul> <li><code>CodegenSupport</code> physical operator is requested to limitNotReachedCond.</li> </ul>","text":""},{"location":"physical-operators/CodegenSupport/#variable-name-prefix","title":"Variable Name Prefix <pre><code>variablePrefix: String\n</code></pre> <p><code>variablePrefix</code> is the prefix of the variable names of this physical operator.</p>    Physical Operator Prefix     HashAggregateExec agg   BroadcastHashJoinExec bhj   ShuffledHashJoinExec shj   SortMergeJoinExec smj   RDDScanExec rdd   DataSourceScanExec scan   InMemoryTableScanExec memoryScan   WholeStageCodegenExec wholestagecodegen   others Lower-case node name    <p><code>variablePrefix</code> is used when:</p> <ul> <li><code>CodegenSupport</code> is requested to generate the Java source code for produce and consume code paths</li> </ul>","text":""},{"location":"physical-operators/CodegenSupport/#needcopyresult-flag","title":"needCopyResult Flag <pre><code>needCopyResult: Boolean\n</code></pre> <p><code>needCopyResult</code> controls whether <code>WholeStageCodegenExec</code> physical operator should copy result when requested for the Java source code for consume path.</p> <p><code>needCopyResult</code>...FIXME</p>","text":""},{"location":"physical-operators/CodegenSupport/#demo","title":"Demo <pre><code>val q = spark.range(1)\n\nimport org.apache.spark.sql.execution.debug._\nscala&gt; q.debugCodegen\nFound 1 WholeStageCodegen subtrees.\n== Subtree 1 / 1 ==\n*Range (0, 1, step=1, splits=8)\n\nGenerated code:\n...\n\n// The above is equivalent to the following method chain\nscala&gt; q.queryExecution.debug.codegen\nFound 1 WholeStageCodegen subtrees.\n== Subtree 1 / 1 ==\n*Range (0, 1, step=1, splits=8)\n\nGenerated code:\n...\n</code></pre>","text":""},{"location":"physical-operators/CollectLimitExec/","title":"CollectLimitExec Physical Operator","text":"<p><code>CollectLimitExec</code> is a unary physical operator that represents GlobalLimit unary logical operator at execution time.</p>"},{"location":"physical-operators/CollectLimitExec/#creating-instance","title":"Creating Instance","text":"<p><code>CollectLimitExec</code> takes the following to be created:</p> <ul> <li> Number of rows (to collect from the child operator) <li> Physical operator <p><code>CollectLimitExec</code> is created when SpecialLimits execution planning strategy is executed (and plans a GlobalLimit unary logical operator).</p>"},{"location":"physical-operators/CollectLimitExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> requests the child operator to execute and (maps over every partition to) takes the given number of rows from every partition. That gives a <code>RDD[InternalRow]</code>.</p> <p><code>doExecute</code> prepares a ShuffleDependency (for the <code>RDD[InternalRow]</code> and <code>SinglePartition</code> partitioning) and creates a ShuffledRowRDD.</p> <p>In the end, <code>doExecute</code> (maps over every partition to) takes the given number of rows from the single partition.</p> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p>","text":""},{"location":"physical-operators/CollectMetricsExec/","title":"CollectMetricsExec Physical Operator","text":"<p><code>CollectMetricsExec</code> is a unary physical operator.</p>"},{"location":"physical-operators/CollectMetricsExec/#creating-instance","title":"Creating Instance","text":"<p><code>CollectMetricsExec</code> takes the following to be created:</p> <ul> <li> Name <li> Metric NamedExpressions <li> Child physical operator <p><code>CollectMetricsExec</code> is created when BasicOperators execution planning strategy is executed (and plans a CollectMetrics logical operator).</p>"},{"location":"physical-operators/CollectMetricsExec/#collected-metrics-accumulator","title":"Collected metrics Accumulator <p><code>CollectMetricsExec</code> registers an AggregatingAccumulator under the name Collected metrics.</p> <p><code>AggregatingAccumulator</code> is created with the metric expressions and the output attributes of the child physical operator.</p>","text":""},{"location":"physical-operators/CollectMetricsExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p>  <p><code>doExecute</code> resets the Collected metrics Accumulator.</p> <p><code>doExecute</code> requests the child physical operator to execute and uses <code>RDD.mapPartitions</code> operator for the following:</p> <ul> <li>A new per-partition AggregatingAccumulator (called <code>updater</code>) is requested to copyAndReset</li> <li>The value of the accumulator is published only when a task is completed</li> <li>For every row, the per-partition <code>AggregatingAccumulator</code> is requested to add it (that updates ImperativeAggregates and TypedImperativeAggregates)</li> </ul>","text":""},{"location":"physical-operators/ColumnarToRowExec/","title":"ColumnarToRowExec Physical Operator","text":"<p><code>ColumnarToRowExec</code> is a ColumnarToRowTransition unary physical operator to translate an RDD of ColumnarBatches into an RDD of InternalRows in Columnar Processing.</p> <p><code>ColumnarToRowExec</code> supports Whole-Stage Java Code Generation.</p> <p><code>ColumnarToRowExec</code> requires that the child physical operator supports columnar processing.</p>"},{"location":"physical-operators/ColumnarToRowExec/#creating-instance","title":"Creating Instance","text":"<p><code>ColumnarToRowExec</code> takes the following to be created:</p> <ul> <li> Child physical operator <p><code>ColumnarToRowExec</code> is created when:</p> <ul> <li>ApplyColumnarRulesAndInsertTransitions physical optimization is executed</li> </ul>"},{"location":"physical-operators/ColumnarToRowExec/#performance-metrics","title":"Performance Metrics","text":""},{"location":"physical-operators/ColumnarToRowExec/#number-of-input-batches","title":"number of input batches <p>Number of input ColumnarBatches across all partitions (from columnar execution of the child physical operator that produces <code>RDD[ColumnarBatch]</code> and hence RDD partitions with rows \"compressed\" into <code>ColumnarBatch</code>es)</p> <p>The number of input ColumnarBatches is influenced by spark.sql.parquet.columnarReaderBatchSize configuration property.</p>","text":""},{"location":"physical-operators/ColumnarToRowExec/#number-of-output-rows","title":"number of output rows <p>Total of the number of rows in every ColumnarBatch across all partitions (of executeColumnar of the child physical operator)</p>","text":""},{"location":"physical-operators/ColumnarToRowExec/#executing-physical-operator","title":"Executing Physical Operator  Signature <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p>  <p><code>doExecute</code> requests the child physical operator to executeColumnar (which is valid since it does support columnar processing) and <code>RDD.mapPartitionsInternal</code> over partitions of ColumnarBatches (<code>Iterator[ColumnarBatch]</code>) to \"unpack\" / \"uncompress\" them to InternalRows.</p> <p>While \"unpacking\", <code>doExecute</code> updates the number of input batches and number of output rows performance metrics.</p>","text":""},{"location":"physical-operators/ColumnarToRowExec/#input-rdds","title":"Input RDDs  Signature <pre><code>inputRDDs(): Seq[RDD[InternalRow]]\n</code></pre> <p><code>inputRDDs</code> is part of the CodegenSupport abstraction.</p>  <p><code>inputRDDs</code> is the RDD of ColumnarBatches (<code>RDD[ColumnarBatch]</code>) from the child physical operator (when requested to executeColumnar).</p>","text":""},{"location":"physical-operators/ColumnarToRowExec/#canchecklimitnotreached-flag","title":"canCheckLimitNotReached Flag  Signature <pre><code>canCheckLimitNotReached: Boolean\n</code></pre> <p><code>canCheckLimitNotReached</code> is part of the CodegenSupport abstraction.</p>  <p><code>canCheckLimitNotReached</code> is always enabled (<code>true</code>).</p>","text":""},{"location":"physical-operators/ColumnarToRowTransition/","title":"ColumnarToRowTransition Unary Physical Operators","text":"<p><code>ColumnarToRowTransition</code> is a marker extension of the UnaryExecNode abstraction for unary physical operators that can transition from columns to rows (when executed).</p> <p>Found in the source code</p> <p>This allows plugins to replace the current ColumnarToRowExec with an optimized version.</p> <p><code>ColumnarToRowTransition</code> type is explicitly checked while ApplyColumnarRulesAndInsertTransitions physical optimization is executed (to skip insertTransitions).</p> <p><code>ColumnarToRowTransition</code> is used for <code>InMemoryRelation</code> to convertToColumnarIfPossible (when <code>CachedBatchSerializer</code> supports columnar input).</p>"},{"location":"physical-operators/ColumnarToRowTransition/#implementations","title":"Implementations","text":"<ul> <li>ColumnarToRowExec</li> </ul>"},{"location":"physical-operators/ColumnarToRowTransition/#whole-stage-code-generation","title":"Whole-Stage Code Generation","text":"<p><code>ColumnarToRowTransition</code> and ApplyColumnarRulesAndInsertTransitions physical optimization look similar to how Whole-Stage Code Generation works (with WholeStageCodegen and InputAdapter physical operators).</p>"},{"location":"physical-operators/CreateTableAsSelectExec/","title":"CreateTableAsSelectExec Physical Command","text":"<p><code>CreateTableAsSelectExec</code> is a TableWriteExecHelper that represents CreateTableAsSelect logical operator at execution time.</p>"},{"location":"physical-operators/CreateTableAsSelectExec/#creating-instance","title":"Creating Instance","text":"<p><code>CreateTableAsSelectExec</code> takes the following to be created:</p> <ul> <li> TableCatalog <li> <code>Identifier</code> <li> Partitioning Transforms <li> LogicalPlan <li> SparkPlan <li> Properties <li> Case-Insensitive Write Options <li> <code>ifNotExists</code> flag <p><code>CreateTableAsSelectExec</code> is created\u00a0when:</p> <ul> <li>DataSourceV2Strategy execution planning strategy is executed (for CreateTableAsSelect)</li> </ul>"},{"location":"physical-operators/CreateTableAsSelectExec/#executing-command","title":"Executing Command  Signature <pre><code>run(): Seq[InternalRow]\n</code></pre> <p><code>run</code>\u00a0is part of the V2CommandExec abstraction.</p>  <p><code>run</code>...FIXME</p>","text":""},{"location":"physical-operators/DataSourceScanExec/","title":"DataSourceScanExec Leaf Physical Operators","text":"<p><code>DataSourceScanExec</code>\u00a0is an extension of the <code>LeafExecNode</code> abstraction for leaf physical operators that represent scans over a BaseRelation.</p> <p><code>DataSourceScanExec</code> uses <code>scan</code> for the variable name prefix for Whole-Stage Java Code Generation.</p>"},{"location":"physical-operators/DataSourceScanExec/#contract","title":"Contract","text":""},{"location":"physical-operators/DataSourceScanExec/#input-rdds","title":"Input RDDs <pre><code>inputRDDs(): Seq[RDD[InternalRow]]\n</code></pre>  <p>Note</p> <p>This is to provide input to tests only.</p>","text":""},{"location":"physical-operators/DataSourceScanExec/#metadata","title":"Metadata <pre><code>metadata: Map[String, String]\n</code></pre>","text":""},{"location":"physical-operators/DataSourceScanExec/#baserelation","title":"BaseRelation <pre><code>relation: BaseRelation\n</code></pre> <p>BaseRelation</p>","text":""},{"location":"physical-operators/DataSourceScanExec/#tableidentifier","title":"TableIdentifier <pre><code>tableIdentifier: Option[TableIdentifier]\n</code></pre>","text":""},{"location":"physical-operators/DataSourceScanExec/#implementations","title":"Implementations","text":"<ul> <li>FileSourceScanExec</li> <li>RowDataSourceScanExec</li> </ul>"},{"location":"physical-operators/DataSourceScanExec/#node-name","title":"Node Name <pre><code>nodeName: String\n</code></pre> <p><code>nodeName</code>\u00a0is part of the TreeNode abstraction.</p> <p><code>nodeName</code> is the following text (with the relation and the tableIdentifier):</p> <pre><code>Scan [relation] [tableIdentifier]\n</code></pre>","text":""},{"location":"physical-operators/DataSourceScanExec/#simple-node-description","title":"Simple Node Description <pre><code>simpleString(\n  maxFields: Int): String\n</code></pre> <p><code>simpleString</code>\u00a0is part of the TreeNode abstraction.</p> <p><code>simpleString</code> is the following text (with the nodeNamePrefix, the nodeName and the metadata redacted and truncated to spark.sql.maxMetadataStringLength characters):</p> <pre><code>[nodeNamePrefix][nodeName][comma-separated output][metadata]\n</code></pre>","text":""},{"location":"physical-operators/DataSourceScanExec/#node-name-prefix","title":"Node Name Prefix <pre><code>nodeNamePrefix: String\n</code></pre> <p><code>nodeNamePrefix</code> is an empty text.</p>","text":""},{"location":"physical-operators/DataSourceV2ScanExecBase/","title":"DataSourceV2ScanExecBase Leaf Physical Operators","text":"<p><code>DataSourceV2ScanExecBase</code> is an extension of LeafExecNode abstraction for leaf physical operators that track number of output rows when executed (with or without support for columnar reads).</p>"},{"location":"physical-operators/DataSourceV2ScanExecBase/#contract","title":"Contract","text":""},{"location":"physical-operators/DataSourceV2ScanExecBase/#input-partitions","title":"Input Partitions <pre><code>partitions: Seq[InputPartition]\n</code></pre> <p>See:</p> <ul> <li>BatchScanExec</li> </ul> <p>Used when:</p> <ul> <li><code>DataSourceV2ScanExecBase</code> is requested for the partitions, groupedPartitions, supportsColumnar</li> </ul>","text":""},{"location":"physical-operators/DataSourceV2ScanExecBase/#input-rdd","title":"Input RDD <pre><code>inputRDD: RDD[InternalRow]\n</code></pre> <p>See:</p> <ul> <li>BatchScanExec</li> </ul>","text":""},{"location":"physical-operators/DataSourceV2ScanExecBase/#keygroupedpartitioning","title":"keyGroupedPartitioning <pre><code>keyGroupedPartitioning: Option[Seq[Expression]]\n</code></pre>","text":""},{"location":"physical-operators/DataSourceV2ScanExecBase/#partitionreaderfactory","title":"PartitionReaderFactory <pre><code>readerFactory: PartitionReaderFactory\n</code></pre> <p>PartitionReaderFactory for partition readers (of the input partitions)</p> <p>See:</p> <ul> <li>BatchScanExec</li> </ul> <p>Used when:</p> <ul> <li><code>BatchScanExec</code> physical operator is requested for an input RDD</li> <li><code>ContinuousScanExec</code> and <code>MicroBatchScanExec</code> physical operators (Spark Structured Streaming) are requested for an <code>inputRDD</code></li> <li><code>DataSourceV2ScanExecBase</code> physical operator is requested to outputPartitioning or supportsColumnar</li> </ul>","text":""},{"location":"physical-operators/DataSourceV2ScanExecBase/#scan","title":"Scan <pre><code>scan: Scan\n</code></pre> <p>Scan</p>","text":""},{"location":"physical-operators/DataSourceV2ScanExecBase/#implementations","title":"Implementations","text":"<ul> <li>BatchScanExec</li> <li><code>ContinuousScanExec</code> (Spark Structured Streaming)</li> <li><code>MicroBatchScanExec</code> (Spark Structured Streaming)</li> </ul>"},{"location":"physical-operators/DataSourceV2ScanExecBase/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p>  <p><code>doExecute</code>...FIXME</p>","text":""},{"location":"physical-operators/DataSourceV2ScanExecBase/#doexecutecolumnar","title":"doExecuteColumnar <pre><code>doExecuteColumnar(): RDD[ColumnarBatch]\n</code></pre> <p><code>doExecuteColumnar</code> is part of the SparkPlan abstraction.</p>  <p><code>doExecuteColumnar</code>...FIXME</p>","text":""},{"location":"physical-operators/DataSourceV2ScanExecBase/#performance-metrics","title":"Performance Metrics <pre><code>metrics: Map[String, SQLMetric]\n</code></pre> <p><code>metrics</code> is part of the SparkPlan abstraction.</p>  <p><code>metrics</code> is the following SQLMetrics with the customMetrics:</p>    Metric Name web UI     <code>numOutputRows</code> number of output rows","text":""},{"location":"physical-operators/DataSourceV2ScanExecBase/#output-data-partitioning-requirements","title":"Output Data Partitioning Requirements <pre><code>outputPartitioning: physical.Partitioning\n</code></pre> <p><code>outputPartitioning</code> is part of the SparkPlan abstraction.</p>  <p><code>outputPartitioning</code>...FIXME</p>","text":""},{"location":"physical-operators/DataSourceV2ScanExecBase/#simple-node-description","title":"Simple Node Description <pre><code>simpleString(\n    maxFields: Int): String\n</code></pre> <p><code>simpleString</code> is part of the TreeNode abstraction.</p>  <p><code>simpleString</code>...FIXME</p>","text":""},{"location":"physical-operators/DataSourceV2ScanExecBase/#supportscolumnar","title":"supportsColumnar <pre><code>supportsColumnar: Boolean\n</code></pre> <p><code>supportsColumnar</code> is part of the SparkPlan abstraction.</p>  <p><code>supportsColumnar</code> is <code>true</code> if the PartitionReaderFactory can supportColumnarReads for all the input partitions. Otherwise, <code>supportsColumnar</code> is <code>false</code>.</p>  <p><code>supportsColumnar</code> makes sure that either all the input partitions are supportColumnarReads or none, or throws an <code>IllegalArgumentException</code>:</p> <pre><code>Cannot mix row-based and columnar input partitions.\n</code></pre>","text":""},{"location":"physical-operators/DataSourceV2ScanExecBase/#custom-metrics","title":"Custom Metrics <pre><code>customMetrics: Map[String, SQLMetric]\n</code></pre>  Lazy Value <p><code>customMetrics</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>customMetrics</code> requests the Scan for supportedCustomMetrics that are then converted to SQLMetrics.</p>  <p><code>customMetrics</code> is used when:</p> <ul> <li><code>DataSourceV2ScanExecBase</code> is requested for the performance metrics</li> <li><code>BatchScanExec</code> is requested for the inputRDD</li> <li><code>ContinuousScanExec</code> is requested for the <code>inputRDD</code></li> <li><code>MicroBatchScanExec</code> is requested for the <code>inputRDD</code> (that creates a DataSourceRDD)</li> </ul>","text":""},{"location":"physical-operators/DataSourceV2ScanExecBase/#verbosestringwithoperatorid","title":"verboseStringWithOperatorId  Signature <pre><code>verboseStringWithOperatorId(): String\n</code></pre> <p><code>verboseStringWithOperatorId</code> is part of the QueryPlan abstraction.</p>  <p><code>verboseStringWithOperatorId</code> requests the Scan for one of the following (<code>metaDataStr</code>):</p> <ul> <li>Metadata when SupportsMetadata</li> <li>Description, otherwise</li> </ul> <p>In the end, <code>verboseStringWithOperatorId</code> is as follows (based on formattedNodeName and output):</p> <pre><code>[formattedNodeName]\nOutput: [output]\n[metaDataStr]\n</code></pre>","text":""},{"location":"physical-operators/DataWritingCommandExec/","title":"DataWritingCommandExec Physical Operator","text":"<p><code>DataWritingCommandExec</code> is a UnaryExecNode that is the execution environment for a DataWritingCommand logical command at execution time.</p>"},{"location":"physical-operators/DataWritingCommandExec/#creating-instance","title":"Creating Instance","text":"<p><code>DataWritingCommandExec</code> takes the following to be created:</p> <ul> <li> DataWritingCommand <li> Child SparkPlan <p><code>DataWritingCommandExec</code> is created when:</p> <ul> <li>BasicOperators execution planning strategy is requested to plan a DataWritingCommand logical command</li> </ul>"},{"location":"physical-operators/DataWritingCommandExec/#performance-metrics","title":"Performance Metrics <pre><code>metrics: Map[String, SQLMetric]\n</code></pre> <p><code>metrics</code> requests the DataWritingCommand for the metrics.</p> <p><code>metrics</code> is part of the SparkPlan abstraction.</p>","text":""},{"location":"physical-operators/DataWritingCommandExec/#sideeffectresult","title":"sideEffectResult <pre><code>sideEffectResult: Seq[InternalRow]\n</code></pre>  Lazy Value <p><code>sideEffectResult</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>sideEffectResult</code> requests the DataWritingCommand to run (with the active SparkSession and the child logical operator) that produces output Rows.</p> <p>In the end, <code>sideEffectResult</code> creates a Catalyst converter (for the schema) to convert the output rows.</p> <p>Used when:</p> <ul> <li><code>DataWritingCommandExec</code> is requested to executeCollect, executeToIterator, executeTake, executeTail and doExecute</li> </ul>","text":""},{"location":"physical-operators/DataWritingCommandExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> requests the SparkPlan to <code>parallelize</code> the sideEffectResult (with <code>1</code> partition).</p> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p>","text":""},{"location":"physical-operators/DebugExec/","title":"DebugExec Unary Physical Operator","text":"<p><code>DebugExec</code> is a unary physical operator.</p>"},{"location":"physical-operators/DeleteFromTableExec/","title":"DeleteFromTableExec","text":"<p><code>DeleteFromTableExec</code> is a V2CommandExec for DeleteFromTable logical operators at execution.</p>"},{"location":"physical-operators/DeleteFromTableExec/#creating-instance","title":"Creating Instance","text":"<p><code>DeleteFromTableExec</code> takes the following to be created:</p> <ul> <li> Table that SupportsDelete <li> Condition Filters <li> Refresh Cache Function (<code>() =&gt; Unit</code>) <p><code>DeleteFromTableExec</code> is created\u00a0when:</p> <ul> <li>DataSourceV2Strategy execution planning strategy is executed (and plans DeleteFromTable logical operators)</li> </ul>"},{"location":"physical-operators/DeleteFromTableExec/#executing-command","title":"Executing Command <pre><code>run(): Seq[InternalRow]\n</code></pre> <p><code>run</code>\u00a0is part of the V2CommandExec abstraction.</p> <p><code>run</code> requests the table to deleteWhere with the condition.</p> <p>In the end, <code>run</code> calls the refresh cache function.</p>","text":""},{"location":"physical-operators/DescribeTableExec/","title":"DescribeTableExec Physical Command","text":"<p><code>DescribeTableExec</code> is a physical command that represents DescribeRelation logical command at execution time.</p>"},{"location":"physical-operators/DescribeTableExec/#creating-instance","title":"Creating Instance","text":"<p><code>DescribeTableExec</code> takes the following to be created:</p> <ul> <li> Output Attributes <li> Table <li> <code>isExtended</code> flag <p><code>DescribeTableExec</code> is created when DataSourceV2Strategy execution planning strategy is executed (and plans a DescribeRelation logical command).</p>"},{"location":"physical-operators/DeserializeToObjectExec/","title":"DeserializeToObjectExec","text":"<p><code>DeserializeToObjectExec</code> is a unary physical operator with CodegenSupport.</p> <p><code>DeserializeToObjectExec</code> is a ObjectProducerExec.</p>"},{"location":"physical-operators/DeserializeToObjectExec/#creating-instance","title":"Creating Instance","text":"<p><code>DeserializeToObjectExec</code> takes the following to be created:</p> <ul> <li> Deserializer Expression <li>Attribute</li> <li> Child physical operator <p><code>DeserializeToObjectExec</code> is created when:</p> <ul> <li>BasicOperators execution planning strategy is executed (with a logical query with a DeserializeToObject logical operator)</li> </ul>"},{"location":"physical-operators/DeserializeToObjectExec/#codegensupport","title":"CodegenSupport <p><code>DeserializeToObjectExec</code> is a CodegenSupport.</p>","text":""},{"location":"physical-operators/DeserializeToObjectExec/#doconsume","title":"doConsume <pre><code>doConsume(\n  ctx: CodegenContext,\n  input: Seq[ExprCode],\n  row: ExprCode): String\n</code></pre> <p><code>doConsume</code>...FIXME</p> <p><code>doConsume</code> is part of the CodegenSupport abstraction.</p>","text":""},{"location":"physical-operators/DeserializeToObjectExec/#doproduce","title":"doProduce <pre><code>doProduce(\n  ctx: CodegenContext): String\n</code></pre> <p><code>doProduce</code>...FIXME</p> <p><code>doProduce</code> is part of the CodegenSupport abstraction.</p>","text":""},{"location":"physical-operators/DeserializeToObjectExec/#inputrdds","title":"inputRDDs <pre><code>inputRDDs(): Seq[RDD[InternalRow]]\n</code></pre> <p><code>inputRDDs</code> requests the child (that is supposed to be a physical operator with CodegenSupport) for the inputRDDs.</p> <p><code>inputRDDs</code> is part of the CodegenSupport abstraction.</p>","text":""},{"location":"physical-operators/DeserializeToObjectExec/#objectproducerexec","title":"ObjectProducerExec <p><code>DeserializeToObjectExec</code> is an ObjectProducerExec.</p>","text":""},{"location":"physical-operators/DeserializeToObjectExec/#outputobjattr","title":"outputObjAttr <p><code>DeserializeToObjectExec</code> is given an <code>outputObjAttr</code> when created.</p> <p><code>outputObjAttr</code> is part of the ObjectProducerExec abstraction.</p>","text":""},{"location":"physical-operators/DeserializeToObjectExec/#sparkplan","title":"SparkPlan <p><code>DeserializeToObjectExec</code> is a SparkPlan.</p>","text":""},{"location":"physical-operators/DeserializeToObjectExec/#execution","title":"Execution <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code>...FIXME</p> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p>","text":""},{"location":"physical-operators/Distribution/","title":"Distribution","text":"<p><code>Distribution</code> is an abstraction of the data distribution requirements of physical operators.</p> <p><code>Distribution</code> is enforced by EnsureRequirements physical optimization.</p>"},{"location":"physical-operators/Distribution/#contract","title":"Contract","text":""},{"location":"physical-operators/Distribution/#creating-partitioning","title":"Creating Partitioning <pre><code>createPartitioning(\n  numPartitions: Int): Partitioning\n</code></pre> <p>Creates a Partitioning for the given number of partitions</p> <p>Used when:</p> <ul> <li>EnsureRequirements physical optimization is executed</li> </ul>","text":""},{"location":"physical-operators/Distribution/#required-number-of-partitions","title":"Required Number of Partitions <pre><code>requiredNumPartitions: Option[Int]\n</code></pre> <p>Required number of partitions for this data distribution</p> <p>When defined, only Partitionings with the same number of partitions can satisfy the distribution requirement.</p> <p>When undefined (<code>None</code>), indicates to use any number of partitions (possibly spark.sql.shuffle.partitions).</p> <p>Used when:</p> <ul> <li>EnsureRequirements physical optimization is executed</li> </ul>","text":""},{"location":"physical-operators/Distribution/#implementations","title":"Implementations","text":"sealed abstract class <p><code>Distribution</code> is a Scala sealed abstract class which means that all possible implementations (<code>Distribution</code>s) are all in the same compilation unit (file).</p> <ul> <li>AllTuples</li> <li>BroadcastDistribution</li> <li>ClusteredDistribution</li> <li>HashClusteredDistribution</li> <li>OrderedDistribution</li> <li>UnspecifiedDistribution</li> </ul>"},{"location":"physical-operators/Distribution/#physical-operators-distribution-requirements","title":"Physical Operators' Distribution Requirements","text":"<p>Physical operators use <code>Distribution</code>s to specify the required child distribution for every child operator.</p> <p>The default <code>Distribution</code>s are UnspecifiedDistributions for all the children.</p> Physical Operator Required Child Distribution AdaptiveSparkPlanExec UnspecifiedDistribution or AQEUtils.getRequiredDistribution BaseAggregateExec One of AllTuples, ClusteredDistribution and UnspecifiedDistribution BroadcastHashJoinExec BroadcastDistribution with UnspecifiedDistribution or vice versa BroadcastNestedLoopJoinExec BroadcastDistribution with UnspecifiedDistribution or vice versa CoGroupExec HashClusteredDistributions GlobalLimitExec AllTuples ShuffledJoin UnspecifiedDistributions or HashClusteredDistributions SortExec OrderedDistribution or UnspecifiedDistribution others"},{"location":"physical-operators/DropNamespaceExec/","title":"DropNamespaceExec Physical Command","text":"<p><code>DropNamespaceExec</code> is a V2CommandExec.</p>"},{"location":"physical-operators/EvalPythonExec/","title":"EvalPythonExec Physical Operators","text":"<p><code>EvalPythonExec</code> is an extension of the UnaryExecNode abstraction for unary physical operators that can evaluate PythonUDFs.</p>"},{"location":"physical-operators/EvalPythonExec/#contract","title":"Contract","text":""},{"location":"physical-operators/EvalPythonExec/#evaluating-pythonudfs","title":"Evaluating PythonUDFs <pre><code>evaluate(\n  funcs: Seq[ChainedPythonFunctions],\n  argOffsets: Array[Array[Int]],\n  iter: Iterator[InternalRow],\n  schema: StructType,\n  context: TaskContext): Iterator[InternalRow]\n</code></pre> <p>Evaluates PythonUDFs (and produces internal binary rows)</p> <p>Used when <code>EvalPythonExec</code> physical operator is executed</p>","text":""},{"location":"physical-operators/EvalPythonExec/#implementations","title":"Implementations","text":"<ul> <li>ArrowEvalPythonExec</li> <li>BatchEvalPythonExec</li> </ul>"},{"location":"physical-operators/EvalPythonExec/#creating-instance","title":"Creating Instance","text":"<p><code>EvalPythonExec</code> takes the following to be created:</p> <ul> <li> <code>PythonUDF</code>s <li> Result Attributes <li> Child physical operator <p>Abstract Class</p> <p><code>EvalPythonExec</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete EvalPythonExecs.</p>"},{"location":"physical-operators/Exchange/","title":"Exchange Unary Physical Operators","text":"<p><code>Exchange</code> is an extension of the UnaryExecNode abstraction for unary physical operators to exchange data (among tasks).</p> <p>Adaptive Query Execution</p> <p><code>Exchange</code> operators are target of Adaptive Query Execution.</p>"},{"location":"physical-operators/Exchange/#implementations","title":"Implementations","text":"<ul> <li>BroadcastExchangeLike</li> <li>ShuffleExchangeLike</li> </ul>"},{"location":"physical-operators/Exchange/#output-attributes","title":"Output Attributes <pre><code>output: Seq[Attribute]\n</code></pre> <p><code>output</code> is part of the QueryPlan abstraction.</p> <p><code>output</code> requests the child operator for the output attributes.</p>","text":""},{"location":"physical-operators/Exchange/#arguments","title":"Arguments <pre><code>stringArgs: Iterator[Any]\n</code></pre> <p><code>stringArgs</code> is part of the TreeNode abstraction.</p> <p><code>stringArgs</code> adds [id=#[id]] to the default stringArgs.</p>","text":""},{"location":"physical-operators/ExecutedCommandExec/","title":"ExecutedCommandExec Physical Operator","text":"<p><code>ExecutedCommandExec</code> is a leaf physical operator for executing runnable logical commands.</p>"},{"location":"physical-operators/ExecutedCommandExec/#creating-instance","title":"Creating Instance","text":"<p><code>ExecutedCommandExec</code> takes the following to be created:</p> <ul> <li> RunnableCommand <p><code>ExecutedCommandExec</code> is created\u00a0when BasicOperators execution planning strategy is executed (for RunnableCommand logical operators).</p>"},{"location":"physical-operators/ExecutedCommandExec/#node-name","title":"Node Name <pre><code>nodeName: String\n</code></pre> <p><code>nodeName</code> is the following (using the node name of the RunnableCommand):</p> <pre><code>Execute [nodeName]\n</code></pre> <p><code>nodeName</code>\u00a0is part of the TreeNode abstraction.</p>","text":""},{"location":"physical-operators/ExecutedCommandExec/#executing-command","title":"Executing Command <p>As a physical operator, <code>ExecutedCommandExec</code> use the sideEffectResult for execution:</p> <ul> <li>executeCollect</li> <li>executeToIterator</li> <li>executeTake</li> <li>executeTail</li> <li>doExecute</li> </ul>","text":""},{"location":"physical-operators/ExecutedCommandExec/#side-effect-of-executing-command","title":"Side Effect of Executing Command <pre><code>sideEffectResult: Seq[InternalRow]\n</code></pre> <p><code>sideEffectResult</code> creates a Catalyst converter for the schema.</p> <p><code>sideEffectResult</code> requests the RunnableCommand to execute and maps over the result using the Catalyst converter.</p>  Lazy Value <p><code>sideEffectResult</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards.</p>  <p><code>sideEffectResult</code> is used for executeCollect, executeToIterator, executeTake, executeTail, and doExecute.</p>","text":""},{"location":"physical-operators/ExternalRDDScanExec/","title":"ExternalRDDScanExec Leaf Physical Operator","text":"<p><code>ExternalRDDScanExec</code> is a leaf physical operator.</p>"},{"location":"physical-operators/FileSourceScanExec/","title":"FileSourceScanExec Physical Operator","text":"<p><code>FileSourceScanExec</code> is a DataSourceScanExec that represents a scan over a HadoopFsRelation.</p>"},{"location":"physical-operators/FileSourceScanExec/#creating-instance","title":"Creating Instance","text":"<p><code>FileSourceScanExec</code> takes the following to be created:</p> <ul> <li> HadoopFsRelation <li> Output Attributes <li> Required Schema <li>Partition Filters</li> <li> <code>optionalBucketSet</code> <li> <code>optionalNumCoalescedBuckets</code> <li>Data Filters</li> <li> <code>TableIdentifier</code> <li> <code>disableBucketedScan</code> flag (default: <code>false</code>) <p><code>FileSourceScanExec</code> is created\u00a0when:</p> <ul> <li>FileSourceStrategy execution planning strategy is executed (for LogicalRelations over a HadoopFsRelation)</li> </ul>"},{"location":"physical-operators/FileSourceScanExec/#data-filters","title":"Data Filters <p><code>FileSourceScanExec</code> is given Data Filters (Expressions) when created.</p> <p>The Data Filters are data columns of the HadoopFsRelation (that are not partition columns that are part of Partition Filters) in FileSourceStrategy execution planning strategy.</p>","text":""},{"location":"physical-operators/FileSourceScanExec/#partition-filters","title":"Partition Filters <p><code>FileSourceScanExec</code> is given Partition Filters (Expressions) when created.</p> <p>The Partition Filters are the PushedDownFilters (based on the partition columns of the HadoopFsRelation) in FileSourceStrategy execution planning strategy.</p>","text":""},{"location":"physical-operators/FileSourceScanExec/#node-name-prefix","title":"Node Name Prefix  Signature <pre><code>nodeNamePrefix: String\n</code></pre> <p><code>nodeNamePrefix</code>\u00a0is part of the DataSourceScanExec abstraction.</p>  <p><code>nodeNamePrefix</code>\u00a0is always File.</p> <pre><code>val fileScanExec: FileSourceScanExec = ... // see the example earlier\nassert(fileScanExec.nodeNamePrefix == \"File\")\n\nscala&gt; println(fileScanExec.simpleString)\nFileScan csv [id#20,name#21,city#22] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/jacek/dev/oss/datasets/people.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:string,name:string,city:string&gt;\n</code></pre>","text":""},{"location":"physical-operators/FileSourceScanExec/#performance-metrics","title":"Performance Metrics    Key Name (in web UI) Description     filesSize size of files read    metadataTime metadata time (ms)    numFiles number of files    numOutputRows number of output rows","text":""},{"location":"physical-operators/FileSourceScanExec/#columnar-scan-metrics","title":"Columnar Scan Metrics <p>The following performance metrics are available only with supportsColumnar enabled.</p>    Key Name (in web UI) Description     scanTime scan time","text":""},{"location":"physical-operators/FileSourceScanExec/#partition-scan-metrics","title":"Partition Scan Metrics <p>The following performance metrics are available only when partitions are used</p>    Key Name (in web UI) Description     numPartitions number of partitions read    pruningTime dynamic partition pruning time","text":""},{"location":"physical-operators/FileSourceScanExec/#dynamic-partition-pruning-scan-metrics","title":"Dynamic Partition Pruning Scan Metrics <p>The following performance metrics are available only for isDynamicPruningFilter among the partition filters.</p>    Key Name (in web UI) Description     staticFilesNum static number of files read    staticFilesSize static size of files read","text":""},{"location":"physical-operators/FileSourceScanExec/#metadata","title":"Metadata <pre><code>metadata: Map[String, String]\n</code></pre> <p><code>metadata</code>\u00a0is part of the DataSourceScanExec abstraction.</p> <p><code>metadata</code>...FIXME</p>","text":""},{"location":"physical-operators/FileSourceScanExec/#inputrdds","title":"inputRDDs <pre><code>inputRDDs(): Seq[RDD[InternalRow]]\n</code></pre> <p><code>inputRDDs</code>\u00a0is part of the DataSourceScanExec abstraction.</p> <p><code>inputRDDs</code>\u00a0is the single input RDD.</p>","text":""},{"location":"physical-operators/FileSourceScanExec/#input-rdd","title":"Input RDD <pre><code>inputRDD: RDD[InternalRow]\n</code></pre>  <p>lazy value</p> <p><code>inputRDD</code> is a Scala lazy value which is computed once when accessed and never changes afterwards.</p>  <p><code>inputRDD</code> is an input <code>RDD</code> that is used when <code>FileSourceScanExec</code> physical operator is requested for inputRDDs and to execute.</p> <p>When created, <code>inputRDD</code> requests HadoopFsRelation to get the underlying FileFormat that is in turn requested to build a data reader with partition column values appended (with the input parameters from the properties of <code>HadoopFsRelation</code> and pushedDownFilters).</p> <p>In case the <code>HadoopFsRelation</code> has bucketing specification specified and bucketing support is enabled, <code>inputRDD</code> creates a FileScanRDD with bucketing (with the bucketing specification, the reader, selectedPartitions and the <code>HadoopFsRelation</code> itself). Otherwise, <code>inputRDD</code> createNonBucketedReadRDD.</p>","text":""},{"location":"physical-operators/FileSourceScanExec/#creating-rdd-for-non-bucketed-read","title":"Creating RDD for Non-Bucketed Read <pre><code>createReadRDD(\n  readFile: (PartitionedFile) =&gt; Iterator[InternalRow],\n  selectedPartitions: Array[PartitionDirectory],\n  fsRelation: HadoopFsRelation): RDD[InternalRow]\n</code></pre> <p><code>createReadRDD</code> prints out the following INFO message to the logs (with maxSplitBytes hint and openCostInBytes):</p> <pre><code>Planning scan with bin packing, max size: [maxSplitBytes] bytes,\nopen cost is considered as scanning [openCostInBytes] bytes.\n</code></pre> <p><code>createReadRDD</code> determines whether Bucketing is enabled or not (based on spark.sql.sources.bucketing.enabled) for bucket pruning.</p>  Bucket Pruning <p>Bucket Pruning is an optimization to filter out data files from scanning (based on optionalBucketSet).</p> <p>With Bucketing disabled or optionalBucketSet undefined, all files are included in scanning.</p>  <p><code>createReadRDD</code> splits files to be scanned (in the given <code>selectedPartitions</code>), possibly applying bucket pruning (with Bucketing enabled). <code>createReadRDD</code> uses the following:</p> <ul> <li>isSplitable property of the FileFormat of the HadoopFsRelation</li> <li>maxSplitBytes hint</li> </ul> <p><code>createReadRDD</code> sorts the split files (by length in reverse order).</p> <p>In the end, creates a FileScanRDD with the following:</p>    Property Value     readFunction Input <code>readFile</code> function   filePartitions Partitions   readSchema requiredSchema with partitionSchema of the input HadoopFsRelation   metadataColumns metadataColumns","text":""},{"location":"physical-operators/FileSourceScanExec/#dynamically-selected-partitions","title":"Dynamically Selected Partitions <pre><code>dynamicallySelectedPartitions: Array[PartitionDirectory]\n</code></pre>  <p>lazy value</p> <p><code>dynamicallySelectedPartitions</code> is a Scala lazy value which is computed once when accessed and cached afterwards.</p>  <p><code>dynamicallySelectedPartitions</code>...FIXME</p>","text":""},{"location":"physical-operators/FileSourceScanExec/#selected-partitions","title":"Selected Partitions <pre><code>selectedPartitions: Seq[PartitionDirectory]\n</code></pre>  <p>lazy value</p> <p><code>selectedPartitions</code> is a Scala lazy value which is computed once when accessed and cached afterwards.</p>  <p><code>selectedPartitions</code>...FIXME</p>","text":""},{"location":"physical-operators/FileSourceScanExec/#bucketedscan-flag","title":"bucketedScan Flag <pre><code>bucketedScan: Boolean\n</code></pre>  <p>lazy value</p> <p><code>selectedPartitions</code> is a Scala lazy value which is computed once when accessed and cached afterwards.</p>  <p><code>bucketedScan</code>...FIXME</p> <p><code>bucketedScan</code> is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"physical-operators/FileSourceScanExec/#output-data-ordering-requirements","title":"Output Data Ordering Requirements <pre><code>outputOrdering: Seq[SortOrder]\n</code></pre> <p><code>outputOrdering</code>\u00a0is part of the SparkPlan abstraction.</p>  <p>Danger</p> <p>Review Me</p>  <p><code>outputOrdering</code> is a SortOrder expression for every sort column in <code>Ascending</code> order only when the following all hold:</p> <ul> <li>bucketing is enabled</li> <li>HadoopFsRelation has a bucketing specification defined</li> <li>All the buckets have a single file in it</li> </ul> <p>Otherwise, <code>outputOrdering</code> is simply empty (<code>Nil</code>).</p>","text":""},{"location":"physical-operators/FileSourceScanExec/#output-data-partitioning-requirements","title":"Output Data Partitioning Requirements <pre><code>outputPartitioning: Partitioning\n</code></pre> <p><code>outputPartitioning</code>\u00a0is part of the SparkPlan abstraction.</p>  <p>Danger</p> <p>Review Me</p>  <p><code>outputPartitioning</code> can be one of the following:</p> <ul> <li> <p>HashPartitioning (with the bucket column names and the number of buckets of the bucketing specification of the HadoopFsRelation) when bucketing is enabled and the HadoopFsRelation has a bucketing specification defined</p> </li> <li> <p>UnknownPartitioning (with <code>0</code> partitions) otherwise</p> </li> </ul>","text":""},{"location":"physical-operators/FileSourceScanExec/#fully-qualified-class-names-of-columnvectors","title":"Fully-Qualified Class Names of ColumnVectors <pre><code>vectorTypes: Option[Seq[String]]\n</code></pre> <p><code>vectorTypes</code>\u00a0is part of the SparkPlan abstraction.</p>  <p>Danger</p> <p>Review Me</p>  <p><code>vectorTypes</code> simply requests the FileFormat of the HadoopFsRelation for vectorTypes.</p>","text":""},{"location":"physical-operators/FileSourceScanExec/#doexecutecolumnar","title":"doExecuteColumnar <pre><code>doExecuteColumnar(): RDD[ColumnarBatch]\n</code></pre> <p><code>doExecuteColumnar</code> is part of the SparkPlan abstraction.</p>","text":""},{"location":"physical-operators/FileSourceScanExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p>   <p>Danger</p> <p>Review Me</p>  <p><code>doExecute</code> branches off per supportsBatch flag.</p>  <p>Note</p> <p>supportsBatch flag can be enabled for ParquetFileFormat and <code>OrcFileFormat</code> built-in file formats (under certain conditions).</p>  <p>With supportsBatch flag enabled, <code>doExecute</code> creates a WholeStageCodegenExec physical operator (with the <code>FileSourceScanExec</code> as the child physical operator and codegenStageId as <code>0</code>) and executes it right after.</p> <p>With supportsBatch flag disabled, <code>doExecute</code> creates an <code>unsafeRows</code> RDD to scan over which is different per needsUnsafeRowConversion flag.</p> <p>If needsUnsafeRowConversion flag is on, <code>doExecute</code> takes the input RDD and creates a new RDD by applying a function to each partition (using <code>RDD.mapPartitionsWithIndexInternal</code>):</p> <ol> <li> <p>Creates a UnsafeProjection for the schema</p> </li> <li> <p>Initializes the UnsafeProjection</p> </li> <li> <p>Maps over the rows in a partition iterator using the <code>UnsafeProjection</code> projection</p> </li> </ol> <p>Otherwise, <code>doExecute</code> simply takes the input RDD as the <code>unsafeRows</code> RDD (with no changes).</p> <p><code>doExecute</code> takes the <code>numOutputRows</code> metric and creates a new RDD by mapping every element in the <code>unsafeRows</code> and incrementing the <code>numOutputRows</code> metric.</p>  <p>Tip</p> <p>Use <code>RDD.toDebugString</code> to review the RDD lineage and \"reverse-engineer\" the values of the supportsBatch and needsUnsafeRowConversion flags given the number of RDDs.</p> <p>With supportsBatch off and needsUnsafeRowConversion on you should see two more RDDs in the RDD lineage.</p>","text":""},{"location":"physical-operators/FileSourceScanExec/#creating-filescanrdd-with-bucketing-support","title":"Creating FileScanRDD with Bucketing Support <pre><code>createBucketedReadRDD(\n  bucketSpec: BucketSpec,\n  readFile: (PartitionedFile) =&gt; Iterator[InternalRow],\n  selectedPartitions: Array[PartitionDirectory],\n  fsRelation: HadoopFsRelation): RDD[InternalRow]\n</code></pre>  <p>Danger</p> <p>Review Me</p>  <p><code>createBucketedReadRDD</code> prints the following INFO message to the logs:</p> <pre><code>Planning with [numBuckets] buckets\n</code></pre> <p><code>createBucketedReadRDD</code> maps the available files of the input <code>selectedPartitions</code> into PartitionedFiles. For every file, <code>createBucketedReadRDD</code> getBlockLocations and getBlockHosts.</p> <p><code>createBucketedReadRDD</code> then groups the <code>PartitionedFiles</code> by bucket ID.</p>  <p>Note</p> <p>Bucket ID is of the format _0000n, i.e. the bucket ID prefixed with up to four <code>0</code>s.</p>  <p><code>createBucketedReadRDD</code> prunes (filters out) the bucket files for the bucket IDs that are not listed in the bucket IDs for bucket pruning.</p> <p><code>createBucketedReadRDD</code> creates a FilePartition (file block) for every bucket ID and the (pruned) bucket <code>PartitionedFiles</code>.</p> <p>In the end, <code>createBucketedReadRDD</code> creates a FileScanRDD (with the input <code>readFile</code> for the read function and the file blocks (<code>FilePartitions</code>) for every bucket ID for partitions)</p>  <p>Tip</p> <p>Use <code>RDD.toDebugString</code> to see <code>FileScanRDD</code> in the RDD execution plan (aka RDD lineage).</p> <pre><code>// Create a bucketed table\nspark.range(8).write.bucketBy(4, \"id\").saveAsTable(\"b1\")\n\nscala&gt; sql(\"desc extended b1\").where($\"col_name\" like \"%Bucket%\").show\n+--------------+---------+-------+\n|      col_name|data_type|comment|\n+--------------+---------+-------+\n|   Num Buckets|        4|       |\n|Bucket Columns|   [`id`]|       |\n+--------------+---------+-------+\n\nval bucketedTable = spark.table(\"b1\")\n\nval lineage = bucketedTable.queryExecution.toRdd.toDebugString\nscala&gt; println(lineage)\n(4) MapPartitionsRDD[26] at toRdd at &lt;console&gt;:26 []\n|  FileScanRDD[25] at toRdd at &lt;console&gt;:26 []\n</code></pre>  <p><code>createBucketedReadRDD</code> is used when:</p> <ul> <li><code>FileSourceScanExec</code> physical operator is requested for the input RDD (and the optional bucketing specification of the HadoopFsRelation is defined and bucketing is enabled)</li> </ul>","text":""},{"location":"physical-operators/FileSourceScanExec/#needsunsaferowconversion-flag","title":"needsUnsafeRowConversion Flag <pre><code>needsUnsafeRowConversion: Boolean\n</code></pre> <p><code>needsUnsafeRowConversion</code> is enabled (i.e. <code>true</code>) when the following conditions all hold:</p> <ol> <li> <p>FileFormat of the HadoopFsRelation is ParquetFileFormat</p> </li> <li> <p>spark.sql.parquet.enableVectorizedReader configuration property is enabled</p> </li> </ol> <p>Otherwise, <code>needsUnsafeRowConversion</code> is disabled (i.e. <code>false</code>).</p> <p><code>needsUnsafeRowConversion</code> is used when:</p> <ul> <li><code>FileSourceScanExec</code> is executed (and supportsBatch flag is off)</li> </ul>","text":""},{"location":"physical-operators/FileSourceScanExec/#supportscolumnar-flag","title":"supportsColumnar Flag <pre><code>supportsColumnar: Boolean\n</code></pre> <p><code>supportsColumnar</code>\u00a0is part of the SparkPlan abstraction.</p> <p><code>supportsColumnar</code>...FIXME</p>","text":""},{"location":"physical-operators/FileSourceScanExec/#demo","title":"Demo <pre><code>// Create a bucketed data source table\n// It is one of the most complex examples of a LogicalRelation with a HadoopFsRelation\nval tableName = \"bucketed_4_id\"\nspark\n  .range(100)\n  .withColumn(\"part\", $\"id\" % 2)\n  .write\n  .partitionBy(\"part\")\n  .bucketBy(4, \"id\")\n  .sortBy(\"id\")\n  .mode(\"overwrite\")\n  .saveAsTable(tableName)\nval q = spark.table(tableName)\n\nval sparkPlan = q.queryExecution.executedPlan\nscala&gt; :type sparkPlan\norg.apache.spark.sql.execution.SparkPlan\n\nscala&gt; println(sparkPlan.numberedTreeString)\n00 *(1) FileScan parquet default.bucketed_4_id[id#7L,part#8L] Batched: true, Format: Parquet, Location: CatalogFileIndex[file:/Users/jacek/dev/oss/spark/spark-warehouse/bucketed_4_id], PartitionCount: 2, PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt;, SelectedBucketsCount: 4 out of 4\n\nimport org.apache.spark.sql.execution.FileSourceScanExec\nval scan = sparkPlan.collectFirst { case exec: FileSourceScanExec =&gt; exec }.get\n\nscala&gt; :type scan\norg.apache.spark.sql.execution.FileSourceScanExec\n\nscala&gt; scan.metadata.toSeq.sortBy(_._1).map { case (k, v) =&gt; s\"$k -&gt; $v\" }.foreach(println)\nBatched -&gt; true\nFormat -&gt; Parquet\nLocation -&gt; CatalogFileIndex[file:/Users/jacek/dev/oss/spark/spark-warehouse/bucketed_4_id]\nPartitionCount -&gt; 2\nPartitionFilters -&gt; []\nPushedFilters -&gt; []\nReadSchema -&gt; struct&lt;id:bigint&gt;\nSelectedBucketsCount -&gt; 4 out of 4\n</code></pre> <p>As a DataSourceScanExec, <code>FileSourceScanExec</code> uses Scan for the prefix of the node name.</p> <pre><code>val fileScanExec: FileSourceScanExec = ... // see the example earlier\nassert(fileScanExec.nodeName startsWith \"Scan\")\n</code></pre> <p>When executed, <code>FileSourceScanExec</code> operator creates a FileScanRDD (for bucketed and non-bucketed reads).</p> <pre><code>scala&gt; :type scan\norg.apache.spark.sql.execution.FileSourceScanExec\n\nval rdd = scan.execute\nscala&gt; println(rdd.toDebugString)\n(6) MapPartitionsRDD[7] at execute at &lt;console&gt;:28 []\n |  FileScanRDD[2] at execute at &lt;console&gt;:27 []\n\nimport org.apache.spark.sql.execution.datasources.FileScanRDD\nassert(rdd.dependencies.head.rdd.isInstanceOf[FileScanRDD])\n</code></pre> <p><code>FileSourceScanExec</code> supports bucket pruning so it only scans the bucket files required for a query.</p> <pre><code>scala&gt; :type scan\norg.apache.spark.sql.execution.FileSourceScanExec\n\nimport org.apache.spark.sql.execution.datasources.FileScanRDD\nval rdd = scan.inputRDDs.head.asInstanceOf[FileScanRDD]\n\nimport org.apache.spark.sql.execution.datasources.FilePartition\nval bucketFiles = for {\n  FilePartition(bucketId, files) &lt;- rdd.filePartitions\n  f &lt;- files\n} yield s\"Bucket $bucketId =&gt; $f\"\n\nscala&gt; println(bucketFiles.size)\n51\n\nscala&gt; bucketFiles.foreach(println)\nBucket 0 =&gt; path: file:///Users/jacek/dev/oss/spark/spark-warehouse/bucketed_4_id/part=0/part-00004-5301d371-01c3-47d4-bb6b-76c3c94f3699_00000.c000.snappy.parquet, range: 0-423, partition values: [0]\nBucket 0 =&gt; path: file:///Users/jacek/dev/oss/spark/spark-warehouse/bucketed_4_id/part=0/part-00001-5301d371-01c3-47d4-bb6b-76c3c94f3699_00000.c000.snappy.parquet, range: 0-423, partition values: [0]\n...\nBucket 3 =&gt; path: file:///Users/jacek/dev/oss/spark/spark-warehouse/bucketed_4_id/part=1/part-00005-5301d371-01c3-47d4-bb6b-76c3c94f3699_00003.c000.snappy.parquet, range: 0-423, partition values: [1]\nBucket 3 =&gt; path: file:///Users/jacek/dev/oss/spark/spark-warehouse/bucketed_4_id/part=1/part-00000-5301d371-01c3-47d4-bb6b-76c3c94f3699_00003.c000.snappy.parquet, range: 0-431, partition values: [1]\nBucket 3 =&gt; path: file:///Users/jacek/dev/oss/spark/spark-warehouse/bucketed_4_id/part=1/part-00007-5301d371-01c3-47d4-bb6b-76c3c94f3699_00003.c000.snappy.parquet, range: 0-423, partition values: [1]\n</code></pre> <p><code>FileSourceScanExec</code> uses a <code>HashPartitioning</code> or the default <code>UnknownPartitioning</code> as the output partitioning scheme.</p> <p><code>FileSourceScanExec</code> supports data source filters that are printed out to the console (at INFO logging level) and available as metadata (e.g. in web UI or explain).</p> <pre><code>Pushed Filters: [pushedDownFilters]\n</code></pre>","text":""},{"location":"physical-operators/FileSourceScanExec/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.FileSourceScanExec</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.FileSourceScanExec=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"physical-operators/FilterExec/","title":"FilterExec Unary Physical Operator","text":"<p><code>FilterExec</code> is a unary physical operator that represents <code>Filter</code> and <code>TypedFilter</code> unary logical operators at execution time.</p> <p><code>FilterExec</code> supports Java code generation (aka codegen) as follows:</p> <ul> <li> <p>&lt;&gt; is an empty <code>AttributeSet</code> (to defer evaluation of attribute expressions until they are actually used, i.e. in the generated Java source code for consume path) <li> <p>Uses whatever the &lt;&gt; physical operator uses for the input RDDs <li> <p>Generates a Java source code for the &lt;&gt; and &lt;&gt; paths in whole-stage code generation <p><code>FilterExec</code> is &lt;&gt; when: <ul> <li> <p>BasicOperators execution planning strategy is executed (and plans Filter and TypedFilter unary logical operators</p> </li> <li> <p>hive/HiveTableScans.md[HiveTableScans] execution planning strategy is executed (and plans hive/HiveTableRelation.md[HiveTableRelation] leaf logical operators and requests <code>SparkPlanner</code> to pruneFilterProject)</p> </li> <li> <p>InMemoryScans execution planning strategy is executed (and plans InMemoryRelation leaf logical operators and requests <code>SparkPlanner</code> to pruneFilterProject)</p> </li> <li> <p><code>DataSourceStrategy</code> execution planning strategy is requested to create a RowDataSourceScanExec physical operator (possibly under FilterExec and ProjectExec operators)</p> </li> <li> <p>FileSourceStrategy execution planning strategy is executed (on &lt;&gt; with a HadoopFsRelation) <li> <p>ExtractPythonUDFs physical optimization is executed</p> </li> <p>[[inputRDDs]] [[outputOrdering]] [[outputPartitioning]] <code>FilterExec</code> uses whatever the &lt;&gt; physical operator uses for the input RDDs, the outputOrdering and the outputPartitioning. <p><code>FilterExec</code> uses PredicateHelper.</p> <p>[[internal-registries]] .FilterExec's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| <code>notNullAttributes</code> | [[notNullAttributes]] FIXME</p> <p>Used when...FIXME</p> <p>| <code>notNullPreds</code> | [[notNullPreds]] FIXME</p> <p>Used when...FIXME</p> <p>| <code>otherPreds</code> | [[otherPreds]] FIXME</p> <p>Used when...FIXME |===</p> <p>=== [[creating-instance]] Creating FilterExec Instance</p> <p><code>FilterExec</code> takes the following when created:</p> <ul> <li>[[condition]] &lt;&gt; for the filter condition <li>[[child]] Child &lt;&gt; <p><code>FilterExec</code> initializes the &lt;&gt;. <p>=== [[isNullIntolerant]] <code>isNullIntolerant</code> Internal Method</p>"},{"location":"physical-operators/FilterExec/#source-scala","title":"[source, scala]","text":""},{"location":"physical-operators/FilterExec/#isnullintolerantexpr-expression-boolean","title":"isNullIntolerant(expr: Expression): Boolean","text":"<p><code>isNullIntolerant</code>...FIXME</p> <p>NOTE: <code>isNullIntolerant</code> is used when...FIXME</p> <p>=== [[doConsume]] Generating Java Source Code for Consume Path in Whole-Stage Code Generation -- <code>doConsume</code> Method</p>"},{"location":"physical-operators/FilterExec/#source-scala_1","title":"[source, scala]","text":""},{"location":"physical-operators/FilterExec/#doconsumectx-codegencontext-input-seqexprcode-row-exprcode-string","title":"doConsume(ctx: CodegenContext, input: Seq[ExprCode], row: ExprCode): String","text":"<p><code>doConsume</code> creates a new metric term for the &lt;&gt; metric. <p><code>doConsume</code>...FIXME</p> <p>In the end, <code>doConsume</code> uses consume and FIXME to generate a Java source code (as a plain text) inside a <code>do {...} while(false);</code> code block.</p> <p><code>doConsume</code> is part of the CodegenSupport abstraction.</p> <p>==== [[doConsume-genPredicate]] <code>genPredicate</code> Internal Method</p>"},{"location":"physical-operators/FilterExec/#source-scala_2","title":"[source, scala]","text":""},{"location":"physical-operators/FilterExec/#genpredicatec-expression-in-seqexprcode-attrs-seqattribute-string","title":"genPredicate(c: Expression, in: Seq[ExprCode], attrs: Seq[Attribute]): String","text":"<p>NOTE: <code>genPredicate</code> is an internal method of &lt;&gt;. <p><code>genPredicate</code>...FIXME</p> <p>=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- <code>doExecute</code> Method</p>"},{"location":"physical-operators/FilterExec/#source-scala_3","title":"[source, scala]","text":""},{"location":"physical-operators/FilterExec/#doexecute-rddinternalrow","title":"doExecute(): RDD[InternalRow]","text":"<p><code>doExecute</code> is part of the SparkPlan abstraction.</p> <p><code>doExecute</code> executes the &lt;&gt; physical operator and creates a new <code>MapPartitionsRDD</code> that does the filtering."},{"location":"physical-operators/FilterExec/#source-scala_4","title":"[source, scala]","text":""},{"location":"physical-operators/FilterExec/#demo-show-the-rdd-lineage-with-the-new-mappartitionsrdd-after-filterexec","title":"// DEMO Show the RDD lineage with the new MapPartitionsRDD after FilterExec","text":"<p>Internally, <code>doExecute</code> takes the &lt;&gt; metric. <p>In the end, <code>doExecute</code> requests the &lt;&gt; physical operator to &lt;&gt; (that triggers physical query planning and generates an <code>RDD[InternalRow]</code>) and transforms it by executing the following function on internal rows per partition with index (using <code>RDD.mapPartitionsWithIndexInternal</code> that creates another RDD): <p>. Creates a partition filter as a new &lt;&gt; (for the &lt;&gt; expression and the &lt;&gt; of the &lt;&gt; physical operator) <p>. Requests the generated partition filter <code>Predicate</code> to <code>initialize</code> (with <code>0</code> partition index)</p> <p>. Filters out elements from the partition iterator (<code>Iterator[InternalRow]</code>) by requesting the generated partition filter <code>Predicate</code> to evaluate for every <code>InternalRow</code> .. Increments the &lt;&gt; metric for positive evaluations (i.e. that returned <code>true</code>) <p>NOTE: <code>doExecute</code> (by <code>RDD.mapPartitionsWithIndexInternal</code>) adds a new <code>MapPartitionsRDD</code> to the RDD lineage. Use <code>RDD.toDebugString</code> to see the additional <code>MapPartitionsRDD</code>.</p>"},{"location":"physical-operators/FilterExec/#performance-metrics","title":"Performance Metrics    Key Name (in web UI) Description     numOutputRows number of output rows Number of output rows","text":""},{"location":"physical-operators/GenerateExec/","title":"GenerateExec Unary Physical Operator","text":"<p><code>GenerateExec</code> is a unary physical operator that is &lt;&gt; exclusively when BasicOperators execution planning strategy is executed. <pre><code>val nums = Seq((0 to 4).toArray).toDF(\"nums\")\nval q = nums.withColumn(\"explode\", explode($\"nums\"))\n\nscala&gt; q.explain\n== Physical Plan ==\nGenerate explode(nums#3), true, false, [explode#12]\n+- LocalTableScan [nums#3]\n\nval sparkPlan = q.queryExecution.executedPlan\nimport org.apache.spark.sql.execution.GenerateExec\nval ge = sparkPlan.asInstanceOf[GenerateExec]\n\nscala&gt; :type ge\norg.apache.spark.sql.execution.GenerateExec\n\nval rdd = ge.execute\n\nscala&gt; rdd.toDebugString\nres1: String =\n(1) MapPartitionsRDD[2] at execute at &lt;console&gt;:26 []\n |  MapPartitionsRDD[1] at execute at &lt;console&gt;:26 []\n |  ParallelCollectionRDD[0] at execute at &lt;console&gt;:26 []\n</code></pre> <p>When &lt;&gt;, <code>GenerateExec</code> expressions/Generator.md#eval[executes] (aka evaluates) the &lt;&gt; expression on every row in a RDD partition. <p></p> <p>NOTE: &lt;&gt; physical operator has to support CodegenSupport. <p><code>GenerateExec</code> supports Java code generation (aka codegen).</p> <p>[[supportCodegen]] <code>GenerateExec</code> does not support Java code generation (aka whole-stage codegen), i.e. supportCodegen flag is turned off.</p> <pre><code>scala&gt; :type ge\norg.apache.spark.sql.execution.GenerateExec\n\nscala&gt; ge.supportCodegen\nres2: Boolean = false\n</code></pre> <pre><code>// Turn spark.sql.codegen.comments on to see comments in the code\n// ./bin/spark-shell --conf spark.sql.codegen.comments=true\n// inline function gives Inline expression\nval q = spark.range(1)\n  .selectExpr(\"inline(array(struct(1, 'a'), struct(2, 'b')))\")\n\nscala&gt; q.explain\n== Physical Plan ==\nGenerate inline([[1,a],[2,b]]), false, false, [col1#47, col2#48]\n+- *Project\n   +- *Range (0, 1, step=1, splits=8)\n\nval sparkPlan = q.queryExecution.executedPlan\nimport org.apache.spark.sql.execution.GenerateExec\nval ge = sparkPlan.asInstanceOf[GenerateExec]\n\nimport org.apache.spark.sql.execution.WholeStageCodegenExec\nval wsce = ge.child.asInstanceOf[WholeStageCodegenExec]\nval (_, code) = wsce.doCodeGen\nimport org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatter\nval formattedCode = CodeFormatter.format(code)\nscala&gt; println(formattedCode)\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIterator(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ /**\n * Codegend pipeline for\n * Project\n * +- Range (0, 1, step=1, splits=8)\n */\n/* 006 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private org.apache.spark.sql.execution.metric.SQLMetric range_numOutputRows;\n/* 010 */   private boolean range_initRange;\n/* 011 */   private long range_number;\n/* 012 */   private TaskContext range_taskContext;\n/* 013 */   private InputMetrics range_inputMetrics;\n/* 014 */   private long range_batchEnd;\n/* 015 */   private long range_numElementsTodo;\n/* 016 */   private scala.collection.Iterator range_input;\n/* 017 */   private UnsafeRow range_result;\n/* 018 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder range_holder;\n/* 019 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter range_rowWriter;\n/* 020 */\n/* 021 */   public GeneratedIterator(Object[] references) {\n/* 022 */     this.references = references;\n/* 023 */   }\n/* 024 */\n/* 025 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 026 */     partitionIndex = index;\n/* 027 */     this.inputs = inputs;\n/* 028 */     range_numOutputRows = (org.apache.spark.sql.execution.metric.SQLMetric) references[0];\n/* 029 */     range_initRange = false;\n/* 030 */     range_number = 0L;\n/* 031 */     range_taskContext = TaskContext.get();\n/* 032 */     range_inputMetrics = range_taskContext.taskMetrics().inputMetrics();\n/* 033 */     range_batchEnd = 0;\n/* 034 */     range_numElementsTodo = 0L;\n/* 035 */     range_input = inputs[0];\n/* 036 */     range_result = new UnsafeRow(1);\n/* 037 */     range_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(range_result, 0);\n/* 038 */     range_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(range_holder, 1);\n/* 039 */\n/* 040 */   }\n/* 041 */\n/* 042 */   private void initRange(int idx) {\n/* 043 */     java.math.BigInteger index = java.math.BigInteger.valueOf(idx);\n/* 044 */     java.math.BigInteger numSlice = java.math.BigInteger.valueOf(8L);\n/* 045 */     java.math.BigInteger numElement = java.math.BigInteger.valueOf(1L);\n/* 046 */     java.math.BigInteger step = java.math.BigInteger.valueOf(1L);\n/* 047 */     java.math.BigInteger start = java.math.BigInteger.valueOf(0L);\n/* 048 */     long partitionEnd;\n/* 049 */\n/* 050 */     java.math.BigInteger st = index.multiply(numElement).divide(numSlice).multiply(step).add(start);\n/* 051 */     if (st.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) &gt; 0) {\n/* 052 */       range_number = Long.MAX_VALUE;\n/* 053 */     } else if (st.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) &lt; 0) {\n/* 054 */       range_number = Long.MIN_VALUE;\n/* 055 */     } else {\n/* 056 */       range_number = st.longValue();\n/* 057 */     }\n/* 058 */     range_batchEnd = range_number;\n/* 059 */\n/* 060 */     java.math.BigInteger end = index.add(java.math.BigInteger.ONE).multiply(numElement).divide(numSlice)\n/* 061 */     .multiply(step).add(start);\n/* 062 */     if (end.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) &gt; 0) {\n/* 063 */       partitionEnd = Long.MAX_VALUE;\n/* 064 */     } else if (end.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) &lt; 0) {\n/* 065 */       partitionEnd = Long.MIN_VALUE;\n/* 066 */     } else {\n/* 067 */       partitionEnd = end.longValue();\n/* 068 */     }\n/* 069 */\n/* 070 */     java.math.BigInteger startToEnd = java.math.BigInteger.valueOf(partitionEnd).subtract(\n/* 071 */       java.math.BigInteger.valueOf(range_number));\n/* 072 */     range_numElementsTodo  = startToEnd.divide(step).longValue();\n/* 073 */     if (range_numElementsTodo &lt; 0) {\n/* 074 */       range_numElementsTodo = 0;\n/* 075 */     } else if (startToEnd.remainder(step).compareTo(java.math.BigInteger.valueOf(0L)) != 0) {\n/* 076 */       range_numElementsTodo++;\n/* 077 */     }\n/* 078 */   }\n/* 079 */\n/* 080 */   protected void processNext() throws java.io.IOException {\n/* 081 */     // PRODUCE: Project\n/* 082 */     // PRODUCE: Range (0, 1, step=1, splits=8)\n/* 083 */     // initialize Range\n/* 084 */     if (!range_initRange) {\n/* 085 */       range_initRange = true;\n/* 086 */       initRange(partitionIndex);\n/* 087 */     }\n/* 088 */\n/* 089 */     while (true) {\n/* 090 */       long range_range = range_batchEnd - range_number;\n/* 091 */       if (range_range != 0L) {\n/* 092 */         int range_localEnd = (int)(range_range / 1L);\n/* 093 */         for (int range_localIdx = 0; range_localIdx &lt; range_localEnd; range_localIdx++) {\n/* 094 */           long range_value = ((long)range_localIdx * 1L) + range_number;\n/* 095 */\n/* 096 */           // CONSUME: Project\n/* 097 */           // CONSUME: WholeStageCodegen\n/* 098 */           append(unsafeRow);\n/* 099 */\n/* 100 */           if (shouldStop()) { range_number = range_value + 1L; return; }\n/* 101 */         }\n/* 102 */         range_number = range_batchEnd;\n/* 103 */       }\n/* 104 */\n/* 105 */       range_taskContext.killTaskIfInterrupted();\n/* 106 */\n/* 107 */       long range_nextBatchTodo;\n/* 108 */       if (range_numElementsTodo &gt; 1000L) {\n/* 109 */         range_nextBatchTodo = 1000L;\n/* 110 */         range_numElementsTodo -= 1000L;\n/* 111 */       } else {\n/* 112 */         range_nextBatchTodo = range_numElementsTodo;\n/* 113 */         range_numElementsTodo = 0;\n/* 114 */         if (range_nextBatchTodo == 0) break;\n/* 115 */       }\n/* 116 */       range_numOutputRows.add(range_nextBatchTodo);\n/* 117 */       range_inputMetrics.incRecordsRead(range_nextBatchTodo);\n/* 118 */\n/* 119 */       range_batchEnd += range_nextBatchTodo * 1L;\n/* 120 */     }\n/* 121 */   }\n/* 122 */\n/* 123 */ }\n</code></pre> <p>[[output]] The catalyst/QueryPlan.md#output[output schema] of a <code>GenerateExec</code> is...FIXME</p> <p>[[producedAttributes]] <code>producedAttributes</code>...FIXME</p> <p>[[outputPartitioning]] <code>outputPartitioning</code>...FIXME</p> <p>[[boundGenerator]] <code>boundGenerator</code>...FIXME</p> <p>[[inputRDDs]] <code>GenerateExec</code> gives &lt;&gt;'s input RDDs (when <code>WholeStageCodegenExec</code> is WholeStageCodegenExec.md#doExecute[executed]). <p>[[needCopyResult]] <code>GenerateExec</code> requires that...FIXME</p> <p>=== [[doProduce]] Generating Java Source Code for Produce Path in Whole-Stage Code Generation -- <code>doProduce</code> Method</p>"},{"location":"physical-operators/GenerateExec/#source-scala","title":"[source, scala]","text":""},{"location":"physical-operators/GenerateExec/#doproducectx-codegencontext-string","title":"doProduce(ctx: CodegenContext): String","text":"<p><code>doProduce</code>...FIXME</p> <p><code>doProduce</code> is part of the CodegenSupport abstraction.</p> <p>=== [[doConsume]] Generating Java Source Code for Consume Path in Whole-Stage Code Generation -- <code>doConsume</code> Method</p>"},{"location":"physical-operators/GenerateExec/#source-scala_1","title":"[source, scala]","text":""},{"location":"physical-operators/GenerateExec/#doconsumectx-codegencontext-input-seqexprcode-row-exprcode-string","title":"doConsume(ctx: CodegenContext, input: Seq[ExprCode], row: ExprCode): String","text":"<p><code>doConsume</code>...FIXME</p> <p><code>doConsume</code> is part of the CodegenSupport abstraction.</p> <p>=== [[codeGenCollection]] <code>codeGenCollection</code> Internal Method</p>"},{"location":"physical-operators/GenerateExec/#source-scala_2","title":"[source, scala]","text":"<p>codeGenCollection(   ctx: CodegenContext,   e: CollectionGenerator,   input: Seq[ExprCode],   row: ExprCode): String</p> <p><code>codeGenCollection</code>...FIXME</p> <p>NOTE: <code>codeGenCollection</code> is used exclusively when <code>GenerateExec</code> is requested to &lt;&gt; (when &lt;&gt; is a <code>CollectionGenerator</code>). <p>=== [[codeGenTraversableOnce]] <code>codeGenTraversableOnce</code> Internal Method</p>"},{"location":"physical-operators/GenerateExec/#source-scala_3","title":"[source, scala]","text":"<p>codeGenTraversableOnce(   ctx: CodegenContext,   e: Expression,   input: Seq[ExprCode],   row: ExprCode): String</p> <p><code>codeGenTraversableOnce</code>...FIXME</p> <p>NOTE: <code>codeGenTraversableOnce</code> is used exclusively when <code>GenerateExec</code> is requested to &lt;&gt; (when &lt;&gt; is not a <code>CollectionGenerator</code>). <p>=== [[codeGenAccessor]] <code>codeGenAccessor</code> Internal Method</p>"},{"location":"physical-operators/GenerateExec/#source-scala_4","title":"[source, scala]","text":"<p>codeGenAccessor(   ctx: CodegenContext,   source: String,   name: String,   index: String,   dt: DataType,   nullable: Boolean,   initialChecks: Seq[String]): ExprCode</p> <p><code>codeGenAccessor</code>...FIXME</p> <p>NOTE: <code>codeGenAccessor</code> is used...FIXME</p> <p>=== [[creating-instance]] Creating GenerateExec Instance</p> <p><code>GenerateExec</code> takes the following when created:</p> <ul> <li>[[generator]] expressions/Generator.md[Generator]</li> <li>[[join]] <code>join</code> flag</li> <li>[[outer]] <code>outer</code> flag</li> <li>[[generatorOutput]] Generator's output schema</li> <li>[[child]] Child SparkPlan.md[physical operator]</li> </ul> <p>=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- <code>doExecute</code> Method</p>"},{"location":"physical-operators/GenerateExec/#source-scala_5","title":"[source, scala]","text":""},{"location":"physical-operators/GenerateExec/#doexecute-rddinternalrow","title":"doExecute(): RDD[InternalRow]","text":"<p><code>doExecute</code>...FIXME</p> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p>"},{"location":"physical-operators/GenerateExec/#performance-metrics","title":"Performance Metrics    Key Name (in web UI) Description     numOutputRows number of output rows Number of output rows","text":""},{"location":"physical-operators/HashAggregateExec/","title":"HashAggregateExec Physical Operator","text":"<p><code>HashAggregateExec</code> is a unary physical operator for hash-based aggregation.</p> <p></p> <p><code>HashAggregateExec</code> is a <code>BlockingOperatorWithCodegen</code>.</p> <p><code>HashAggregateExec</code> is an AliasAwareOutputPartitioning.</p> <p>Note</p> <p><code>HashAggregateExec</code> is the preferred aggregate physical operator for Aggregation execution planning strategy (over ObjectHashAggregateExec and SortAggregateExec).</p> <p><code>HashAggregateExec</code> supports Java code generation (aka codegen).</p> <p><code>HashAggregateExec</code> uses TungstenAggregationIterator (to iterate over <code>UnsafeRows</code> in partitions) when executed.</p> <p>Note</p> <p><code>HashAggregateExec</code> uses <code>TungstenAggregationIterator</code> that can (theoretically) switch to a sort-based aggregation when the hash-based approach is unable to acquire enough memory.</p> <p>See testFallbackStartsAt internal property and spark.sql.TungstenAggregate.testFallbackStartsAt configuration property.</p> <p>Search logs for the following INFO message to know whether the switch has happened.</p> <pre><code>falling back to sort based aggregation.\n</code></pre>"},{"location":"physical-operators/HashAggregateExec/#creating-instance","title":"Creating Instance","text":"<p><code>HashAggregateExec</code> takes the following to be created:</p> <ul> <li> Required child distribution expressions <li> Grouping Keys (as NamedExpressions) <li> AggregateExpressions <li> Aggregate attributes <li> Initial input buffer offset <li> Output named expressions <li> Child physical operator <p><code>HashAggregateExec</code> is created when (indirectly through AggUtils.createAggregate) when:</p> <ul> <li> <p>Aggregation execution planning strategy is executed (to select the aggregate physical operator for an Aggregate logical operator)</p> </li> <li> <p><code>StatefulAggregationStrategy</code> (Spark Structured Streaming) execution planning strategy creates plan for streaming <code>EventTimeWatermark</code> or Aggregate logical operators</p> </li> </ul>"},{"location":"physical-operators/HashAggregateExec/#performance-metrics","title":"Performance Metrics","text":""},{"location":"physical-operators/HashAggregateExec/#avg-hash-probe-bucket-list-iters","title":"avg hash probe bucket list iters <p>Average hash map probe per lookup (i.e. <code>numProbes</code> / <code>numKeyLookups</code>)</p> <p><code>numProbes</code> and <code>numKeyLookups</code> are used in <code>BytesToBytesMap</code> (Spark Core) append-only hash map for the number of iteration to look up a single key and the number of all the lookups in total, respectively.</p>","text":""},{"location":"physical-operators/HashAggregateExec/#number-of-output-rows","title":"number of output rows <p>Average hash map probe per lookup (i.e. <code>numProbes</code> / <code>numKeyLookups</code>)</p> <p>Number of groups (per partition) that (depending on the number of partitions and the side of ShuffleExchangeExec.md[ShuffleExchangeExec] operator) is the number of groups</p> <ul> <li> <p><code>0</code> for no input with a grouping expression, e.g. <code>spark.range(0).groupBy($\"id\").count.show</code></p> </li> <li> <p><code>1</code> for no grouping expression and no input, e.g. <code>spark.range(0).groupBy().count.show</code></p> </li> </ul>  <p>Tip</p> <p>Use different number of elements and partitions in <code>range</code> operator to observe the difference in <code>numOutputRows</code> metric, e.g.</p>  <pre><code>spark.\n  range(0, 10, 1, numPartitions = 1).\n  groupBy($\"id\" % 5 as \"gid\").\n  count.\n  show\n\nspark.\n  range(0, 10, 1, numPartitions = 5).\n  groupBy($\"id\" % 5 as \"gid\").\n  count.\n  show\n</code></pre>","text":""},{"location":"physical-operators/HashAggregateExec/#number-of-sort-fallback-tasks","title":"number of sort fallback tasks","text":""},{"location":"physical-operators/HashAggregateExec/#peak-memory","title":"peak memory","text":""},{"location":"physical-operators/HashAggregateExec/#spill-size","title":"spill size <p>Used to create a TungstenAggregationIterator when doExecute</p>","text":""},{"location":"physical-operators/HashAggregateExec/#time-in-aggregation-build","title":"time in aggregation build","text":""},{"location":"physical-operators/HashAggregateExec/#required-child-distribution","title":"Required Child Distribution <pre><code>requiredChildDistribution: List[Distribution]\n</code></pre> <p><code>requiredChildDistribution</code> is part of the SparkPlan abstraction.</p> <p><code>requiredChildDistribution</code> varies per the input required child distribution expressions:</p> <ul> <li>AllTuples when defined, but empty</li> <li>ClusteredDistribution for non-empty expressions</li> <li>UnspecifiedDistribution when undefined</li> </ul>  <p>Note</p> <p><code>requiredChildDistributionExpressions</code> is exactly <code>requiredChildDistributionExpressions</code> from AggUtils.createAggregate and is undefined by default.</p>  <p>(No distinct in aggregation) <code>requiredChildDistributionExpressions</code> is undefined when <code>HashAggregateExec</code> is created for partial aggregations (i.e. <code>mode</code> is <code>Partial</code> for aggregate expressions).</p> <p><code>requiredChildDistributionExpressions</code> is defined, but could possibly be empty, when <code>HashAggregateExec</code> is created for final aggregations (i.e. <code>mode</code> is <code>Final</code> for aggregate expressions).</p>  <p>(one distinct in aggregation) <code>requiredChildDistributionExpressions</code> is undefined when <code>HashAggregateExec</code> is created for partial aggregations (i.e. <code>mode</code> is <code>Partial</code> for aggregate expressions) with one distinct in aggregation.</p> <p><code>requiredChildDistributionExpressions</code> is defined, but could possibly be empty, when <code>HashAggregateExec</code> is created for partial merge aggregations (i.e. <code>mode</code> is <code>PartialMerge</code> for aggregate expressions).</p> <p>FIXME for the following two cases in aggregation with one distinct.</p>  <p>NOTE: The prefix for variable names for <code>HashAggregateExec</code> operators in CodegenSupport-generated code is agg.</p>","text":""},{"location":"physical-operators/HashAggregateExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p>  <p><code>doExecute</code> requests the child physical operator to execute (that triggers physical query planning and generates an <code>RDD[InternalRow]</code>).</p> <p><code>doExecute</code> transforms the <code>RDD[InternalRow]</code> with a transformation function for every partition.</p>","text":""},{"location":"physical-operators/HashAggregateExec/#processing-partition-rows","title":"Processing Partition Rows <p><code>doExecute</code> uses RDD.mapPartitionsWithIndex to process InternalRows per partition (with a partition ID).</p> <p>For every partition, <code>mapPartitionsWithIndex</code> records the start execution time (<code>beforeAgg</code>).</p> <p><code>mapPartitionsWithIndex</code> branches off based on whether there are input rows and grouping keys.</p> <p>For a grouped aggregate (grouping keys defined) with no input rows (an empty partition), <code>doExecute</code> returns an empty iterator.</p> <p>Otherwise, <code>doExecute</code> creates a TungstenAggregationIterator.</p> <p>With no grouping keys defined and no input rows (an empty partition), <code>doExecute</code> increments the numOutputRows metric and returns a single-element <code>Iterator[UnsafeRow]</code> from the TungstenAggregationIterator.</p> <p>With input rows available (and regardless of grouping keys), <code>doExecute</code> returns the <code>TungstenAggregationIterator</code>.</p> <p>In the end, <code>doExecute</code> calculates the aggTime metric and returns an <code>Iterator[UnsafeRow]</code> that can be as follows:</p> <ul> <li>Empty</li> <li>A single-element one</li> <li>The TungstenAggregationIterator</li> </ul>","text":""},{"location":"physical-operators/HashAggregateExec/#mappartitionswithindex","title":"mapPartitionsWithIndex <pre><code>mapPartitionsWithIndex[U: ClassTag](\n  f: (Int, Iterator[T]) =&gt; Iterator[U],\n  preservesPartitioning: Boolean = false): RDD[U]\n</code></pre> <p><code>RDD.mapPartitionsWithIndex</code> adds a new <code>MapPartitionsRDD</code> to the RDD lineage.</p> <pre><code>val ids = spark.range(1)\nscala&gt; println(ids.queryExecution.toRdd.toDebugString)\n(8) MapPartitionsRDD[12] at toRdd at &lt;console&gt;:29 []\n|  MapPartitionsRDD[11] at toRdd at &lt;console&gt;:29 []\n|  ParallelCollectionRDD[10] at toRdd at &lt;console&gt;:29 []\n\n// Use groupBy that gives HashAggregateExec operator\nval q = ids.groupBy('id).count\nscala&gt; q.explain\n== Physical Plan ==\n*(2) HashAggregate(keys=[id#30L], functions=[count(1)])\n+- Exchange hashpartitioning(id#30L, 200)\n  +- *(1) HashAggregate(keys=[id#30L], functions=[partial_count(1)])\n      +- *(1) Range (0, 1, step=1, splits=8)\n\nval rdd = q.queryExecution.toRdd\nscala&gt; println(rdd.toDebugString)\n(200) MapPartitionsRDD[18] at toRdd at &lt;console&gt;:28 []\n  |   ShuffledRowRDD[17] at toRdd at &lt;console&gt;:28 []\n  +-(8) MapPartitionsRDD[16] at toRdd at &lt;console&gt;:28 []\n    |  MapPartitionsRDD[15] at toRdd at &lt;console&gt;:28 []\n    |  MapPartitionsRDD[14] at toRdd at &lt;console&gt;:28 []\n    |  ParallelCollectionRDD[13] at toRdd at &lt;console&gt;:28 []\n</code></pre>","text":""},{"location":"physical-operators/HashAggregateExec/#generating-java-code-for-consume-path","title":"Generating Java Code for Consume Path <pre><code>doConsume(\n  ctx: CodegenContext,\n  input: Seq[ExprCode],\n  row: ExprCode): String\n</code></pre> <p><code>doConsume</code> is part of the CodegenSupport abstraction.</p>  <p><code>doConsume</code> doConsumeWithoutKeys when no named expressions for the grouping keys were specified for the <code>HashAggregateExec</code> or doConsumeWithKeys otherwise.</p>","text":""},{"location":"physical-operators/HashAggregateExec/#doconsumewithkeys","title":"doConsumeWithKeys <pre><code>doConsumeWithKeys(\n  ctx: CodegenContext,\n  input: Seq[ExprCode]): String\n</code></pre> <p><code>doConsumeWithKeys</code>...FIXME</p>","text":""},{"location":"physical-operators/HashAggregateExec/#doconsumewithoutkeys","title":"doConsumeWithoutKeys <pre><code>doConsumeWithoutKeys(\n  ctx: CodegenContext,\n  input: Seq[ExprCode]): String\n</code></pre> <p><code>doConsumeWithoutKeys</code>...FIXME</p>","text":""},{"location":"physical-operators/HashAggregateExec/#generating-java-code-for-produce-path","title":"Generating Java Code for Produce Path <pre><code>doProduce(\n  ctx: CodegenContext): String\n</code></pre> <p><code>doProduce</code> is part of the CodegenSupport abstraction.</p>  <p><code>doProduce</code> executes doProduceWithoutKeys when no named expressions for the grouping keys were specified for the <code>HashAggregateExec</code> or doProduceWithKeys otherwise.</p>","text":""},{"location":"physical-operators/HashAggregateExec/#doproducewithoutkeys","title":"doProduceWithoutKeys <pre><code>doProduceWithoutKeys(\n  ctx: CodegenContext): String\n</code></pre> <p><code>doProduceWithoutKeys</code>...FIXME</p>","text":""},{"location":"physical-operators/HashAggregateExec/#generateresultfunction","title":"generateResultFunction <pre><code>generateResultFunction(\n  ctx: CodegenContext): String\n</code></pre> <p><code>generateResultFunction</code>...FIXME</p>","text":""},{"location":"physical-operators/HashAggregateExec/#finishaggregate","title":"finishAggregate <pre><code>finishAggregate(\n  hashMap: UnsafeFixedWidthAggregationMap,\n  sorter: UnsafeKVExternalSorter,\n  peakMemory: SQLMetric,\n  spillSize: SQLMetric,\n  avgHashProbe: SQLMetric): KVIterator[UnsafeRow, UnsafeRow]\n</code></pre> <p><code>finishAggregate</code>...FIXME</p>","text":""},{"location":"physical-operators/HashAggregateExec/#doproducewithkeys","title":"doProduceWithKeys <pre><code>doProduceWithKeys(\n  ctx: CodegenContext): String\n</code></pre> <p><code>doProduceWithKeys</code> is part of the AggregateCodegenSupport abstraction.</p>  <p><code>doProduceWithKeys</code> uses the following configuration properties:</p> <ul> <li>spark.sql.codegen.aggregate.map.twolevel.enabled</li> <li>spark.sql.codegen.aggregate.map.vectorized.enable</li> <li>spark.sql.codegen.aggregate.fastHashMap.capacityBit</li> </ul> <p><code>doProduceWithKeys</code>...FIXME</p> <p>In the end, <code>doProduceWithKeys</code> generates the following Java code (with the <code>[]</code>-marked sections filled out):</p> <pre><code>if (![initAgg]) {\n  [initAgg] = true;\n  [createFastHashMap]\n  [addHookToCloseFastHashMap]\n  [hashMapTerm] = [thisPlan].createHashMap();\n  long [beforeAgg] = System.nanoTime();\n  [doAggFuncName]();\n  [aggTime].add((System.nanoTime() - [beforeAgg]) / [NANOS_PER_MILLIS]);\n}\n// output the result\n[outputFromFastHashMap]\n[outputFromRegularHashMap]\n</code></pre>","text":""},{"location":"physical-operators/HashAggregateExec/#creating-hashmap","title":"Creating HashMap <pre><code>createHashMap(): UnsafeFixedWidthAggregationMap\n</code></pre> <p><code>createHashMap</code> requests all the DeclarativeAggregate functions for the Catalyst expressions to initialize aggregation buffers.</p> <p><code>createHashMap</code> creates an UnsafeProjection for the expressions and executes it (with an \"empty\" <code>null</code> row).</p>  <p>Note</p> <p>Executing an UnsafeProjection produces an UnsafeRow that becomes an empty aggregation buffer of an UnsafeFixedWidthAggregationMap to be created.</p>  <p>In the end, <code>createHashMap</code> creates an UnsafeFixedWidthAggregationMap with the following:</p>    UnsafeFixedWidthAggregationMap Value     emptyAggregationBuffer The <code>UnsafeRow</code> after executing the <code>UnsafeProjection</code> to initialize aggregation buffers   aggregationBufferSchema bufferSchema   groupingKeySchema groupingKeySchema","text":""},{"location":"physical-operators/HashAggregateExec/#demo","title":"Demo","text":""},{"location":"physical-operators/HashAggregateExec/#aggregation-query","title":"Aggregation Query <pre><code>val data = spark.range(10)\nval q = data\n  .groupBy('id % 2 as \"group\")\n  .agg(sum(\"id\") as \"sum\")\n</code></pre> <p><code>HashAggregateExec</code> operator should be selected due to:</p> <ol> <li><code>sum</code> uses mutable types for aggregate expression</li> <li>just a single <code>id</code> column reference of <code>LongType</code> data type</li> </ol> <pre><code>scala&gt; println(q.queryExecution.executedPlan.numberedTreeString)\n00 AdaptiveSparkPlan isFinalPlan=false\n01 +- HashAggregate(keys=[_groupingexpression#8L], functions=[sum(id#0L)], output=[group#2L, sum#5L])\n02    +- Exchange hashpartitioning(_groupingexpression#8L, 200), ENSURE_REQUIREMENTS, [plan_id=15]\n03       +- HashAggregate(keys=[_groupingexpression#8L], functions=[partial_sum(id#0L)], output=[_groupingexpression#8L, sum#10L])\n04          +- Project [id#0L, (id#0L % 2) AS _groupingexpression#8L]\n05             +- Range (0, 10, step=1, splits=16)\n</code></pre>","text":""},{"location":"physical-operators/HashAggregateExec/#generate-final-plan","title":"Generate Final Plan <p><code>isFinalPlan</code> flag is <code>false</code>. Let's execute it and access the final plan.</p> <pre><code>import org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec\nval op = q\n  .queryExecution\n  .executedPlan\n  .collect { case op: AdaptiveSparkPlanExec =&gt; op }\n  .head\n</code></pre> <p>Execute the adaptive operator to generate the final execution plan.</p> <pre><code>op.executeTake(1)\n</code></pre> <p><code>isFinalPlan</code> flag should now be <code>true</code>.</p> <pre><code>scala&gt; println(op.treeString)\nAdaptiveSparkPlan isFinalPlan=true\n+- == Final Plan ==\n   *(2) HashAggregate(keys=[_groupingexpression#8L], functions=[sum(id#0L)], output=[group#2L, sum#5L])\n   +- AQEShuffleRead coalesced\n      +- ShuffleQueryStage 0\n         +- Exchange hashpartitioning(_groupingexpression#8L, 200), ENSURE_REQUIREMENTS, [plan_id=25]\n            +- *(1) HashAggregate(keys=[_groupingexpression#8L], functions=[partial_sum(id#0L)], output=[_groupingexpression#8L, sum#10L])\n               +- *(1) Project [id#0L, (id#0L % 2) AS _groupingexpression#8L]\n                  +- *(1) Range (0, 10, step=1, splits=16)\n+- == Initial Plan ==\n   HashAggregate(keys=[_groupingexpression#8L], functions=[sum(id#0L)], output=[group#2L, sum#5L])\n   +- Exchange hashpartitioning(_groupingexpression#8L, 200), ENSURE_REQUIREMENTS, [plan_id=15]\n      +- HashAggregate(keys=[_groupingexpression#8L], functions=[partial_sum(id#0L)], output=[_groupingexpression#8L, sum#10L])\n         +- Project [id#0L, (id#0L % 2) AS _groupingexpression#8L]\n            +- Range (0, 10, step=1, splits=16)\n</code></pre> <p>With the <code>isFinalPlan</code> flag <code>true</code>, it is possible to print out the WholeStageCodegen subtrees.</p> <pre><code>scala&gt; q.queryExecution.debug.codegen\nFound 2 WholeStageCodegen subtrees.\n== Subtree 1 / 2 (maxMethodCodeSize:284; maxConstantPoolSize:337(0.51% used); numInnerClasses:2) ==\n*(1) HashAggregate(keys=[_groupingexpression#8L], functions=[partial_sum(id#0L)], output=[_groupingexpression#8L, sum#10L])\n+- *(1) Project [id#0L, (id#0L % 2) AS _groupingexpression#8L]\n   +- *(1) Range (0, 10, step=1, splits=16)\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=1\n/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean hashAgg_initAgg_0;\n/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n/* 014 */   private boolean hashAgg_hashAgg_isNull_2_0;\n...\n</code></pre> <p>Let's access the generated source code via WholeStageCodegenExec physical operator.</p> <pre><code>val aqe = op\nimport org.apache.spark.sql.execution.WholeStageCodegenExec\nval wsce = aqe.executedPlan\n  .collect { case op: WholeStageCodegenExec =&gt; op }\n  .head\nval (_, source) = wsce.doCodeGen\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatter\nval formattedCode = CodeFormatter.format(source)\n</code></pre> <pre><code>scala&gt; println(formattedCode)\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=2\n/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean hashAgg_initAgg_0;\n/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n/* 014 */   private boolean hashAgg_hashAgg_isNull_2_0;\n...\n</code></pre> <pre><code>val execPlan = q.queryExecution.sparkPlan\n</code></pre> <pre><code>scala&gt; println(execPlan.numberedTreeString)\n00 HashAggregate(keys=[_groupingexpression#8L], functions=[sum(id#0L)], output=[group#2L, sum#5L])\n01 +- HashAggregate(keys=[_groupingexpression#8L], functions=[partial_sum(id#0L)], output=[_groupingexpression#8L, sum#10L])\n02    +- Project [id#0L, (id#0L % 2) AS _groupingexpression#8L]\n03       +- Range (0, 10, step=1, splits=16)\n</code></pre> <p>Going low level. Watch your steps :)</p> <pre><code>import org.apache.spark.sql.catalyst.plans.logical.Aggregate\nval aggLog = q.queryExecution.optimizedPlan.asInstanceOf[Aggregate]\n\nimport org.apache.spark.sql.catalyst.planning.PhysicalAggregation\nimport org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression\nval (_, aggregateExpressions: Seq[AggregateExpression], _, _) = PhysicalAggregation.unapply(aggLog).get\nval aggregateBufferAttributes =\n  aggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes)\n</code></pre> <p>Here comes the very reason why <code>HashAggregateExec</code> was selected. Aggregation execution planning strategy prefers <code>HashAggregateExec</code> when <code>aggregateBufferAttributes</code> are supported.</p> <pre><code>import org.apache.spark.sql.catalyst.plans.logical.Aggregate\nassert(Aggregate.supportsHashAggregate(aggregateBufferAttributes))\n</code></pre> <pre><code>import org.apache.spark.sql.execution.aggregate.HashAggregateExec\nval hashAggExec = execPlan.asInstanceOf[HashAggregateExec]\n</code></pre> <pre><code>scala&gt; println(execPlan.numberedTreeString)\n00 HashAggregate(keys=[_groupingexpression#8L], functions=[sum(id#0L)], output=[group#2L, sum#5L])\n01 +- HashAggregate(keys=[_groupingexpression#8L], functions=[partial_sum(id#0L)], output=[_groupingexpression#8L, sum#10L])\n02    +- Project [id#0L, (id#0L % 2) AS _groupingexpression#8L]\n03       +- Range (0, 10, step=1, splits=16)\n</code></pre>","text":""},{"location":"physical-operators/HashAggregateExec/#execute-hashaggregateexec","title":"Execute HashAggregateExec <pre><code>val hashAggExecRDD = hashAggExec.execute\n</code></pre> <pre><code>println(hashAggExecRDD.toDebugString)\n</code></pre> <pre><code>(16) MapPartitionsRDD[4] at execute at &lt;console&gt;:1 []\n |   MapPartitionsRDD[3] at execute at &lt;console&gt;:1 []\n |   MapPartitionsRDD[2] at execute at &lt;console&gt;:1 []\n |   MapPartitionsRDD[1] at execute at &lt;console&gt;:1 []\n |   ParallelCollectionRDD[0] at execute at &lt;console&gt;:1 []\n</code></pre>","text":""},{"location":"physical-operators/HashAggregateExec/#java-code-for-produce-execution-path","title":"Java Code for Produce Execution Path <pre><code>import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\nval ctx = new CodegenContext\nval parent = hashAggExec\nval doProduceWithKeysCode = hashAggExec.produce(ctx, parent)\n</code></pre> <pre><code>scala&gt; println(doProduceWithKeysCode)\nif (!hashAgg_initAgg_0) {\n  hashAgg_initAgg_0 = true;\n\n\n  hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n  long hashAgg_beforeAgg_1 = System.nanoTime();\n  hashAgg_doAggregateWithKeys_0();\n  ((org.apache.spark.sql.execution.metric.SQLMetric) references[16] /* aggTime */).add((System.nanoTime() - hashAgg_beforeAgg_1) / 1000000);\n}\n// output the result\n\n\nwhile ( hashAgg_mapIter_0.next()) {\n  UnsafeRow hashAgg_aggKey_1 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n  UnsafeRow hashAgg_aggBuffer_1 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n  hashAgg_doAggregateWithKeysOutput_1(hashAgg_aggKey_1, hashAgg_aggBuffer_1);\n  if (shouldStop()) return;\n}\nhashAgg_mapIter_0.close();\nif (hashAgg_sorter_0 == null) {\n  hashAgg_hashMap_0.free();\n}\n</code></pre>","text":""},{"location":"physical-operators/HashClusteredDistribution/","title":"HashClusteredDistribution","text":"<p><code>HashClusteredDistribution</code> is a Distribution.md[Distribution] that &lt;&gt; for the &lt;&gt; and a requested number of partitions. <p>[[requiredNumPartitions]] <code>HashClusteredDistribution</code> specifies <code>None</code> for the Distribution.md#requiredNumPartitions[required number of partitions].</p> <p>Note</p> <p><code>None</code> for the required number of partitions indicates to use any number of partitions (possibly spark.sql.shuffle.partitions configuration property).</p> <p><code>HashClusteredDistribution</code> is &lt;&gt; when the following physical operators are requested for the SparkPlan.md#requiredChildDistribution[required partition requirements of the child operator(s)] (e.g. <code>CoGroupExec</code>, ShuffledHashJoinExec.md[ShuffledHashJoinExec], SortMergeJoinExec.md[SortMergeJoinExec] and Spark Structured Streaming's <code>StreamingSymmetricHashJoinExec</code>). <p>[[creating-instance]][[expressions]] <code>HashClusteredDistribution</code> takes hash expressions/Expression.md[expressions] when created.</p> <p><code>HashClusteredDistribution</code> requires that the &lt;&gt; should not be empty (i.e. <code>Nil</code>). <p><code>HashClusteredDistribution</code> is used when:</p> <ul> <li> <p>EnsureRequirements is executed (for Adaptive Query Execution)</p> </li> <li> <p><code>HashPartitioning</code> is requested to <code>satisfies</code></p> </li> </ul> <p>=== [[createPartitioning]] <code>createPartitioning</code> Method</p>"},{"location":"physical-operators/HashClusteredDistribution/#source-scala","title":"[source, scala]","text":"<p>createPartitioning(   numPartitions: Int): Partitioning</p> <p><code>createPartitioning</code> creates a <code>HashPartitioning</code> for the &lt;&gt; and the input <code>numPartitions</code>. <p><code>createPartitioning</code> is part of the Distribution abstraction.</p>"},{"location":"physical-operators/HashJoin/","title":"HashJoin \u2014 Hash-Based Join Physical Operators","text":"<p><code>HashJoin</code> is an extension of the JoinCodegenSupport abstraction for hash-based join physical operators with support for Java code generation.</p>"},{"location":"physical-operators/HashJoin/#contract","title":"Contract","text":""},{"location":"physical-operators/HashJoin/#buildside","title":"BuildSide <pre><code>buildSide: BuildSide\n</code></pre>","text":""},{"location":"physical-operators/HashJoin/#preparing-hashedrelation","title":"Preparing HashedRelation <pre><code>prepareRelation(\n  ctx: CodegenContext): HashedRelationInfo\n</code></pre> <p>Used when:</p> <ul> <li><code>HashJoin</code> is requested to codegenInner, codegenOuter, codegenSemi, codegenAnti, codegenExistence</li> </ul>","text":""},{"location":"physical-operators/HashJoin/#implementations","title":"Implementations","text":"<ul> <li>BroadcastHashJoinExec</li> <li>ShuffledHashJoinExec</li> </ul>"},{"location":"physical-operators/HashJoin/#build-keys","title":"Build Keys <pre><code>buildKeys: Seq[Attribute]\n</code></pre>  Lazy Value <p><code>buildKeys</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>buildKeys</code> is the left keys for the BuildSide as <code>BuildLeft</code> while the right keys as <code>BuildRight</code>.</p>  <p>Important</p> <p><code>HashJoin</code> assumes that the number of the left and the right keys are the same and are of the same types (position-wise).</p>","text":""},{"location":"physical-operators/HashJoin/#streamed-keys","title":"Streamed Keys <pre><code>streamedKeys: Seq[Attribute]\n</code></pre>  Lazy Value <p><code>streamedKeys</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>streamedKeys</code> is the opposite of the build keys.</p> <p><code>streamedKeys</code> is the right keys for the BuildSide as <code>BuildLeft</code> while the left keys as <code>BuildRight</code>.</p>","text":""},{"location":"physical-operators/HashJoin/#output-attributes","title":"Output Attributes <pre><code>output: Seq[Attribute]\n</code></pre> <p><code>output</code> is a collection of Attributes based on the joinType.</p>    joinType Output Schema     <code>InnerLike</code> output of the left followed by the right operator's   <code>LeftOuter</code> output of the left followed by the right operator's (with <code>nullability</code> on)   <code>RightOuter</code> output of the left (with <code>nullability</code> on) followed by the right operator's   <code>ExistenceJoin</code> output of the left followed by the <code>exists</code> attribute   <code>LeftSemi</code> output of the left operator   <code>LeftAnti</code> output of the left operator    <p><code>output</code> is part of the QueryPlan abstraction.</p>","text":""},{"location":"physical-operators/HashJoin/#codegensupport-generating-java-source-code","title":"CodegenSupport \u2014 Generating Java Source Code <p><code>HashJoin</code> is a CodegenSupport (indirectly as a JoinCodegenSupport) and supports generating Java source code for consume and produce execution paths.</p>","text":""},{"location":"physical-operators/HashJoin/#consume-path","title":"Consume Path <pre><code>doConsume(\n  ctx: CodegenContext,\n  input: Seq[ExprCode],\n  row: ExprCode): String\n</code></pre> <p><code>doConsume</code> generates a Java source code for \"consume\" execution path based on the given join type.</p>    joinType doConsume     <code>InnerLike</code> codegenInner   <code>LeftOuter</code> codegenOuter   <code>RightOuter</code> codegenOuter   <code>LeftSemi</code> codegenSemi   <code>LeftAnti</code> codegenAnti   <code>ExistenceJoin</code> codegenExistence    <p><code>doConsume</code> is part of the CodegenSupport abstraction.</p>","text":""},{"location":"physical-operators/HashJoin/#anti-join","title":"Anti Join <pre><code>codegenAnti(\n  ctx: CodegenContext,\n  input: Seq[ExprCode]): String\n</code></pre> <p><code>codegenAnti</code>...FIXME</p> <p><code>codegenAnti</code> is used when:</p> <ul> <li><code>BroadcastHashJoinExec</code> physical operator is requested to codegenAnti (with the isNullAwareAntiJoin flag off)</li> <li><code>HashJoin</code> is requested to doConsume</li> </ul>","text":""},{"location":"physical-operators/HashJoin/#inner-join","title":"Inner Join <pre><code>codegenInner(\n  ctx: CodegenContext,\n  input: Seq[ExprCode]): String\n</code></pre> <p><code>codegenInner</code> prepares a HashedRelation (with the given CodegenContext).</p>  <p>Note</p> <p>Preparing a HashedRelation is implementation-specific.</p>  <p><code>codegenInner</code> genStreamSideJoinKey and getJoinCondition.</p> <p>For <code>isEmptyHashedRelation</code>, <code>codegenInner</code> returns the following text (which is simply a comment with no executable code):</p> <pre><code>// If HashedRelation is empty, hash inner join simply returns nothing.\n</code></pre> <p>For <code>keyIsUnique</code>, <code>codegenInner</code> returns the following code:</p> <pre><code>// generate join key for stream side\n[keyEv.code]\n// find matches from HashedRelation\nUnsafeRow [matched] = [anyNull] ? null: (UnsafeRow)[relationTerm].getValue([keyEv.value]);\nif ([matched] != null) {\n  [checkCondition] {\n    [numOutput].add(1);\n    [consume(ctx, resultVars)]\n  }\n}\n</code></pre> <p>For all other cases, <code>codegenInner</code> returns the following code:</p> <pre><code>// generate join key for stream side\n[keyEv.code]\n// find matches from HashRelation\nIterator[UnsafeRow] [matches] = [anyNull] ?\n  null : (Iterator[UnsafeRow])[relationTerm].get([keyEv.value]);\nif ([matches] != null) {\n  while ([matches].hasNext()) {\n    UnsafeRow [matched] = (UnsafeRow) [matches].next();\n    [checkCondition] {\n      [numOutput].add(1);\n      [consume(ctx, resultVars)]\n    }\n  }\n}\n</code></pre>","text":""},{"location":"physical-operators/HashJoin/#produce-path","title":"Produce Path <pre><code>doProduce(\n  ctx: CodegenContext): String\n</code></pre> <p><code>doProduce</code> assumes that the streamedPlan is a CodegenSupport and requests it to generate a Java source code for \"produce\" execution path.</p> <p><code>doProduce</code> is part of the CodegenSupport abstraction.</p>","text":""},{"location":"physical-operators/HashJoin/#join","title":"join <pre><code>join(\n  streamedIter: Iterator[InternalRow],\n  hashed: HashedRelation,\n  numOutputRows: SQLMetric): Iterator[InternalRow]\n</code></pre> <p><code>join</code> branches off per JoinType to create an joined rows iterator (off the rows from the input <code>streamedIter</code> and <code>hashed</code>):</p> <ul> <li> <p>innerJoin for a InnerLike join</p> </li> <li> <p>outerJoin for a LeftOuter or a RightOuter join</p> </li> <li> <p>semiJoin for a LeftSemi join</p> </li> <li> <p>antiJoin for a LeftAnti join</p> </li> <li> <p>existenceJoin for a ExistenceJoin join</p> </li> </ul> <p><code>join</code> creates a result projection.</p> <p>In the end, for every row in the joined rows iterator <code>join</code> increments the input <code>numOutputRows</code> SQL metric and applies the result projection.</p> <p><code>join</code> reports an <code>IllegalArgumentException</code> for unsupported JoinType:</p> <pre><code>HashJoin should not take [joinType] as the JoinType\n</code></pre> <p><code>join</code> is used when:</p> <ul> <li>BroadcastHashJoinExec and ShuffledHashJoinExec physical operators are executed</li> </ul>","text":""},{"location":"physical-operators/HashedRelation/","title":"HashedRelation","text":"<p><code>HashedRelation</code> is an extension of the KnownSizeEstimation abstraction for relations with values hashed by some key.</p> sealed trait <p><code>HashedRelation</code> is a Scala sealed trait which means that all possible implementations (<code>HashedRelation</code>s) are all in the same compilation unit (file).</p>"},{"location":"physical-operators/HashedRelation/#contract","title":"Contract","text":""},{"location":"physical-operators/HashedRelation/#asreadonlycopy","title":"asReadOnlyCopy <pre><code>asReadOnlyCopy(): HashedRelation\n</code></pre> <p>A read-only copy of this <code>HashedRelation</code> to be safely used in a separate thread</p> <p>Used when:</p> <ul> <li><code>BroadcastHashJoinExec</code> physical operator is executed</li> </ul>","text":""},{"location":"physical-operators/HashedRelation/#close","title":"close <pre><code>close(): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>ShuffledHashJoinExec</code> physical operator is requested to buildHashedRelation</li> </ul>","text":""},{"location":"physical-operators/HashedRelation/#get","title":"get <pre><code>get(\n  key: InternalRow): Iterator[InternalRow]\nget(\n  key: Long): Iterator[InternalRow]\n</code></pre> <p>Gets internal rows for the given key or <code>null</code></p> <p>Used when:</p> <ul> <li><code>HashJoin</code> is requested to innerJoin, [outerJoin]/HashJoin.md#outerJoin), semiJoin, existenceJoin and antiJoin</li> <li><code>LongHashedRelation</code> is requested to get a value for a key</li> </ul>","text":""},{"location":"physical-operators/HashedRelation/#getvalue","title":"getValue <pre><code>getValue(\n  key: InternalRow): InternalRow\ngetValue(\n  key: Long): InternalRow\n</code></pre> <p>Gives the value internal row for the given key</p> <p>Used when:</p> <ul> <li><code>LongHashedRelation</code> is requested to get a value for a key</li> </ul>","text":""},{"location":"physical-operators/HashedRelation/#keyisunique","title":"keyIsUnique <pre><code>keyIsUnique: Boolean\n</code></pre> <p>Used when:</p> <ul> <li><code>BroadcastHashJoinExec</code> physical operator is requested to multipleOutputForOneInput, codegenInner, codegenOuter, codegenSemi, codegenAnti, codegenExistence</li> </ul>","text":""},{"location":"physical-operators/HashedRelation/#keys","title":"keys <pre><code>keys(): Iterator[InternalRow]\n</code></pre> <p>Used when:</p> <ul> <li><code>SubqueryBroadcastExec</code> physical operator is requested for <code>relationFuture</code></li> </ul>","text":""},{"location":"physical-operators/HashedRelation/#implementations","title":"Implementations","text":"<ul> <li>LongHashedRelation</li> <li>UnsafeHashedRelation</li> </ul>"},{"location":"physical-operators/HashedRelation/#creating-hashedrelation","title":"Creating HashedRelation <pre><code>apply(\n  input: Iterator[InternalRow],\n  key: Seq[Expression],\n  sizeEstimate: Int = 64,\n  taskMemoryManager: TaskMemoryManager = null,\n  isNullAware: Boolean = false,\n  allowsNullKey: Boolean = false): HashedRelation\n</code></pre> <p>With no elements in the <code>input</code> iterator and with the <code>allowsNullKey</code> flag disabled, <code>apply</code> returns the <code>EmptyHashedRelation</code>.</p> <p>For exactly one <code>key</code> expression of type <code>LongType</code> and with the <code>allowsNullKey</code> flag disabled, <code>apply</code> returns a LongHashedRelation.</p> <p>For all other cases, <code>apply</code> returns an UnsafeHashedRelation.</p>  <p>The input <code>key</code> expressions can be the following:</p> <ul> <li>Build join keys of <code>ShuffledHashJoinExec</code> physical operator</li> <li>Canonicalized build-side join keys of <code>HashedRelationBroadcastMode</code> (of BroadcastHashJoinExec physical operator)</li> </ul>  <p><code>apply</code> is used when:</p> <ul> <li><code>ShuffledHashJoinExec</code> physical operator is requested to build a HashedRelation from InternalRows</li> <li><code>HashedRelationBroadcastMode</code> is requested to transform InternalRows into a HashedRelation</li> </ul>","text":""},{"location":"physical-operators/HashedRelationBroadcastMode/","title":"HashedRelationBroadcastMode","text":"<p><code>HashedRelationBroadcastMode</code> is a BroadcastMode.</p>"},{"location":"physical-operators/HashedRelationBroadcastMode/#creating-instance","title":"Creating Instance","text":"<p><code>HashedRelationBroadcastMode</code> takes the following to be created:</p> <ul> <li> Key Expressions <li> <code>isNullAware</code> flag (default: <code>false</code>) <p><code>HashedRelationBroadcastMode</code> is created when:</p> <ul> <li><code>PlanAdaptiveDynamicPruningFilters</code> physical optimization is executed (to optimize query plans with DynamicPruningExpression)</li> <li><code>PlanDynamicPruningFilters</code> physical optimization is requested to broadcastMode</li> <li><code>BroadcastHashJoinExec</code> physical operator is requested for requiredChildDistribution</li> </ul>"},{"location":"physical-operators/HashedRelationBroadcastMode/#transforming-internalrows-into-hashedrelation","title":"Transforming InternalRows into HashedRelation <pre><code>transform(\n  rows: Iterator[InternalRow],\n  sizeHint: Option[Long]): HashedRelation\n</code></pre> <p><code>transform</code> creates a HashedRelation with or without <code>sizeEstimate</code> based on the given <code>sizeHint</code>.</p> <p><code>transform</code> is part of the BroadcastMode abstraction.</p>","text":""},{"location":"physical-operators/InMemoryTableScanExec/","title":"InMemoryTableScanExec Leaf Physical Operator","text":"<p><code>InMemoryTableScanExec</code> is a leaf physical operator that represents an InMemoryRelation logical operator at execution time.</p>"},{"location":"physical-operators/InMemoryTableScanExec/#performance-metrics","title":"Performance Metrics    Key Name (in web UI) Description     numOutputRows number of output rows Number of output rows","text":""},{"location":"physical-operators/InMemoryTableScanExec/#demo","title":"Demo  <p>FIXME Do the below code blocks work still?</p>  <pre><code>// Example to show produceBatches to generate a Java source code\n\n// Create a DataFrame\nval ids = spark.range(10)\n// Cache it (and trigger the caching since it is lazy)\nids.cache.foreach(_ =&gt; ())\n\nimport org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\n// we need executedPlan with WholeStageCodegenExec physical operator\n// this will make sure the code generation starts at the right place\nval plan = ids.queryExecution.executedPlan\nval scan = plan.collectFirst { case e: InMemoryTableScanExec =&gt; e }.get\n\nassert(scan.supportsBatch, \"supportsBatch flag should be on to trigger produceBatches\")\n\nimport org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\nval ctx = new CodegenContext\n\n// produceBatches is private so we have to trigger it from \"outside\"\n// It could be doProduce with supportsBatch flag on but it is protected\n// (doProduce will also take care of the extra input `input` parameter)\n// let's do this the only one right way\nimport org.apache.spark.sql.execution.CodegenSupport\nval parent = plan.p(0).asInstanceOf[CodegenSupport]\nval produceCode = scan.produce(ctx, parent)\n\nscala&gt; println(produceCode)\n\n\n\nif (inmemorytablescan_mutableStateArray1[1] == null) {\n  inmemorytablescan_nextBatch1();\n}\nwhile (inmemorytablescan_mutableStateArray1[1] != null) {\n  int inmemorytablescan_numRows1 = inmemorytablescan_mutableStateArray1[1].numRows();\n  int inmemorytablescan_localEnd1 = inmemorytablescan_numRows1 - inmemorytablescan_batchIdx1;\n  for (int inmemorytablescan_localIdx1 = 0; inmemorytablescan_localIdx1 &lt; inmemorytablescan_localEnd1; inmemorytablescan_localIdx1++) {\n    int inmemorytablescan_rowIdx1 = inmemorytablescan_batchIdx1 + inmemorytablescan_localIdx1;\n    long inmemorytablescan_value2 = inmemorytablescan_mutableStateArray2[1].getLong(inmemorytablescan_rowIdx1);\ninmemorytablescan_mutableStateArray5[1].write(0, inmemorytablescan_value2);\nappend(inmemorytablescan_mutableStateArray3[1]);\n    if (shouldStop()) { inmemorytablescan_batchIdx1 = inmemorytablescan_rowIdx1 + 1; return; }\n  }\n  inmemorytablescan_batchIdx1 = inmemorytablescan_numRows1;\n  inmemorytablescan_mutableStateArray1[1] = null;\n  inmemorytablescan_nextBatch1();\n}\n((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* scanTime */).add(inmemorytablescan_scanTime1 / (1000 * 1000));\ninmemorytablescan_scanTime1 = 0;\n\n// the code does not look good and begs for some polishing\n// (You can only imagine how the Polish me looks when I say \"polishing\" :))\n\nimport org.apache.spark.sql.execution.WholeStageCodegenExec\nval wsce = plan.asInstanceOf[WholeStageCodegenExec]\n\n// Trigger code generation of the entire query plan tree\nval (ctx, code) = wsce.doCodeGen\n\n// CodeFormatter can pretty-print the code\nimport org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatter\nprintln(CodeFormatter.format(code))\n</code></pre> <pre><code>// Let's create a query with a InMemoryTableScanExec physical operator that supports batch decoding\nval q = spark.range(4).cache\nval plan = q.queryExecution.executedPlan\n\nimport org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\nval inmemoryScan = plan.collectFirst { case exec: InMemoryTableScanExec =&gt; exec }.get\n\nassert(inmemoryScan.supportsBatch)\n\nimport org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\nval ctx = new CodegenContext\nimport org.apache.spark.sql.execution.CodegenSupport\nval parent = plan.asInstanceOf[CodegenSupport]\nval code = inmemoryScan.produce(ctx, parent)\nscala&gt; println(code)\n\n\n\nif (inmemorytablescan_mutableStateArray1[1] == null) {\n  inmemorytablescan_nextBatch1();\n}\nwhile (inmemorytablescan_mutableStateArray1[1] != null) {\n  int inmemorytablescan_numRows1 = inmemorytablescan_mutableStateArray1[1].numRows();\n  int inmemorytablescan_localEnd1 = inmemorytablescan_numRows1 - inmemorytablescan_batchIdx1;\n  for (int inmemorytablescan_localIdx1 = 0; inmemorytablescan_localIdx1 &lt; inmemorytablescan_localEnd1; inmemorytablescan_localIdx1++) {\n    int inmemorytablescan_rowIdx1 = inmemorytablescan_batchIdx1 + inmemorytablescan_localIdx1;\n    long inmemorytablescan_value2 = inmemorytablescan_mutableStateArray2[1].getLong(inmemorytablescan_rowIdx1);\ninmemorytablescan_mutableStateArray5[1].write(0, inmemorytablescan_value2);\nappend(inmemorytablescan_mutableStateArray3[1]);\n    if (shouldStop()) { inmemorytablescan_batchIdx1 = inmemorytablescan_rowIdx1 + 1; return; }\n  }\n  inmemorytablescan_batchIdx1 = inmemorytablescan_numRows1;\n  inmemorytablescan_mutableStateArray1[1] = null;\n  inmemorytablescan_nextBatch1();\n}\n((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* scanTime */).add(inmemorytablescan_scanTime1 / (1000 * 1000));\ninmemorytablescan_scanTime1 = 0;\n</code></pre> <pre><code>val q = Seq(Seq(1,2,3)).toDF(\"ids\").cache\nval plan = q.queryExecution.executedPlan\n\nimport org.apache.spark.sql.execution.columnar.InMemoryTableScanExec\nval inmemoryScan = plan.collectFirst { case exec: InMemoryTableScanExec =&gt; exec }.get\n\nassert(inmemoryScan.supportsBatch == false)\n\n// NOTE: The following codegen won't work since supportsBatch is off and so is codegen\nimport org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\nval ctx = new CodegenContext\nimport org.apache.spark.sql.execution.CodegenSupport\nval parent = plan.asInstanceOf[CodegenSupport]\nscala&gt; val code = inmemoryScan.produce(ctx, parent)\njava.lang.UnsupportedOperationException\n  at org.apache.spark.sql.execution.CodegenSupport$class.doConsume(WholeStageCodegenExec.scala:315)\n  at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doConsume(InMemoryTableScanExec.scala:33)\n  at org.apache.spark.sql.execution.CodegenSupport$class.constructDoConsumeFunction(WholeStageCodegenExec.scala:208)\n  at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:179)\n  at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.consume(InMemoryTableScanExec.scala:33)\n  at org.apache.spark.sql.execution.ColumnarBatchScan$class.produceRows(ColumnarBatchScan.scala:166)\n  at org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:80)\n  at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doProduce(InMemoryTableScanExec.scala:33)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n  at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.produce(InMemoryTableScanExec.scala:33)\n  ... 49 elided\n</code></pre> <pre><code>val q = spark.read.text(\"README.md\")\n\nval plan = q.queryExecution.executedPlan\nimport org.apache.spark.sql.execution.FileSourceScanExec\nval scan = plan.collectFirst { case exec: FileSourceScanExec =&gt; exec }.get\n\n// 2. supportsBatch is off\nassert(scan.supportsBatch == false)\n\n// 3. InMemoryTableScanExec.produce\nimport org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\nval ctx = new CodegenContext\nimport org.apache.spark.sql.execution.CodegenSupport\n\nimport org.apache.spark.sql.execution.WholeStageCodegenExec\nval wsce = plan.collectFirst { case exec: WholeStageCodegenExec =&gt; exec }.get\n\nval code = scan.produce(ctx, parent = wsce)\nscala&gt; println(code)\n// blank lines removed\nwhile (scan_mutableStateArray[2].hasNext()) {\n  InternalRow scan_row2 = (InternalRow) scan_mutableStateArray[2].next();\n  ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* numOutputRows */).add(1);\n  append(scan_row2);\n  if (shouldStop()) return;\n}\n</code></pre>","text":""},{"location":"physical-operators/InputAdapter/","title":"InputAdapter Unary Physical Operator","text":"<p><code>InputAdapter</code> is a unary physical operator that is an adapter for the &lt;&gt; physical operator that does not meet the requirements of whole-stage Java code generation (possibly due to supportCodegen flag turned off) but is between operators that participate in whole-stage Java code generation optimization. <p>.InputAdapter's doProduce image::images/spark-sql-InputAdapter-doProduce.png[align=\"center\"]</p> <p>[[child]] [[creating-instance]] <code>InputAdapter</code> takes a single <code>child</code> SparkPlan.md[physical plan] when created.</p> <p><code>InputAdapter</code> is &lt;&gt; exclusively when CollapseCodegenStages physical optimization is executed (and requested to insert InputAdapters into a physical query plan with whole-stage Java code generation enabled). <p>[[generateTreeString]] <code>InputAdapter</code> makes sure that the prefix in the text representation of a physical plan tree is an empty string (and so it removes the star from the tree representation that WholeStageCodegenExec.md#generateTreeString[WholeStageCodegenExec] adds), e.g. for spark-sql-dataset-operators.md#explain[explain] or TreeNode.numberedTreeString operators.</p> <p>TIP: The number of <code>InputAdapters</code> is exactly the number of subtrees in a physical query plan that do not have stars.</p>"},{"location":"physical-operators/InputAdapter/#source-scala","title":"[source, scala]","text":"<p>scala&gt; println(plan.numberedTreeString) 00 *(1) Project [id#117L] 01 +- *(1) BroadcastHashJoin [id#117L], [cast(id#115 as bigint)], Inner, BuildRight 02    :- *(1) Range (0, 1, step=1, splits=8) 03    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint))) 04       +- Generate explode(ids#112), false, [id#115] 05          +- LocalTableScan [ids#112]</p> <p>[[needCopyResult]] <code>InputAdapter</code> requires that...FIXME, i.e. <code>needCopyResult</code> flag is turned off.</p> <p>[[inputRDDs]] <code>InputAdapter</code> SparkPlan.md#execute[executes] the &lt;&gt; physical operator to get the one and only one <code>RDD[InternalRow]</code> as its own input RDDs for &lt;&gt;. <pre><code>// explode expression (that uses Generate operator) does not support codegen\nval ids = Seq(Seq(0,1,2,3)).toDF(\"ids\").select(explode($\"ids\") as \"id\")\nval q = spark.range(1).join(ids, \"id\")\n// Use executedPlan\n// This is after the whole-stage Java code generation optimization is applied to a physical plan\nval plan = q.queryExecution.executedPlan\nscala&gt; println(plan.numberedTreeString)\n00 *(1) Project [id#117L]\n01 +- *(1) BroadcastHashJoin [id#117L], [cast(id#115 as bigint)], Inner, BuildRight\n02    :- *(1) Range (0, 1, step=1, splits=8)\n03    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))\n04       +- Generate explode(ids#112), false, [id#115]\n05          +- LocalTableScan [ids#112]\n\n// Find all InputAdapters in the physical query plan\nimport org.apache.spark.sql.execution.InputAdapter\nscala&gt; plan.collect { case a: InputAdapter =&gt; a }.zipWithIndex.map { case (op, idx) =&gt; s\"$idx) $op\" }.foreach(println)\n0) BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))\n+- Generate explode(ids#112), false, [id#115]\n   +- LocalTableScan [ids#112]\n</code></pre> <p>=== [[doProduce]] Generating Java Source Code for Produce Path in Whole-Stage Code Generation -- <code>doProduce</code> Method</p>"},{"location":"physical-operators/InputAdapter/#source-scala_1","title":"[source, scala]","text":""},{"location":"physical-operators/InputAdapter/#doproducectx-codegencontext-string","title":"doProduce(ctx: CodegenContext): String","text":"<p><code>doProduce</code> generates a Java source code that consumes InternalRow of a single input <code>RDD</code> one at a time (in a <code>while</code> loop).</p> <p>NOTE: <code>doProduce</code> supports one input RDD only (that the single &lt;&gt; physical operator creates when SparkPlan.md#execute[executed]). <p>Internally, <code>doProduce</code> generates two <code>input</code> and <code>row</code> \"fresh\" terms and registers <code>input</code> as a mutable state (in the generated class).</p> <p><code>doProduce</code> gives a plain Java source code that uses <code>input</code> and <code>row</code> terms as well as the code from consume code generator to iterate over the InternalRows from the first &lt;&gt; only. <p><code>doProduce</code> is part of the CodegenSupport abstraction.</p> <pre><code>val q = spark.range(1)\n  .select(explode(lit((0 to 1).toArray)) as \"n\")  // &lt;-- explode expression does not support codegen\n  .join(spark.range(2))\n  .where($\"n\" === $\"id\")\nscala&gt; q.explain\n== Physical Plan ==\n*BroadcastHashJoin [cast(n#4 as bigint)], [id#7L], Inner, BuildRight\n:- *Filter isnotnull(n#4)\n:  +- Generate explode([0,1]), false, false, [n#4]\n:     +- *Project\n:        +- *Range (0, 1, step=1, splits=8)\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]))\n   +- *Range (0, 2, step=1, splits=8)\n\nval plan = q.queryExecution.executedPlan\nimport org.apache.spark.sql.execution.InputAdapter\n// there are two InputAdapters (for Generate and BroadcastExchange operators) so get is safe\nval adapter = plan.collectFirst { case a: InputAdapter =&gt; a }.get\n\nimport org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\nval ctx = new CodegenContext\n\nimport org.apache.spark.sql.execution.CodegenSupport\nval code = adapter.produce(ctx, plan.asInstanceOf[CodegenSupport])\nscala&gt; println(code)\n\n/*inputadapter_c5*/\n\n while (inputadapter_input2.hasNext() &amp;&amp; !stopEarly()) {\n   InternalRow inputadapter_row2 = (InternalRow) inputadapter_input2.next();\n   /*wholestagecodegen_c1*/\n\nappend(inputadapter_row2);\n   if (shouldStop()) return;\n }\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.plans.logical.Range\nval r = Range(start = 0, end = 1, step = 1, numSlices = 1)\nimport org.apache.spark.sql.execution.RangeExec\nval re = RangeExec(r)\n\nimport org.apache.spark.sql.execution.InputAdapter\nval ia = InputAdapter(re)\n\nimport org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\nval ctx = new CodegenContext\n\n// You cannot call doProduce directly\n// CodegenSupport.parent is not set up\n// and so consume will throw NPE (that's used in doProduce)\n// That's why you're supposed to call produce final method that does this\nimport org.apache.spark.sql.execution.CodegenSupport\nia.produce(ctx, parent = ia.asInstanceOf[CodegenSupport])\n\n// produce however will lead to java.lang.UnsupportedOperationException\n// which is due to doConsume throwing it by default\n// and InputAdapter does not override it!\n// That's why InputAdapter has to be under a WholeStageCodegenExec-enabled physical operator\n//    which happens in CollapseCodegenStages.insertWholeStageCodegen\n//    when a physical operator is CodegenSupport and meets codegen requirements\n//    CollapseCodegenStages.supportCodegen\n//    Most importantly it is CodegenSupport with supportCodegen flag on\n//    The following physical operators turn supportCodegen flag off (and require InputAdapter wrapper)\n//    1. GenerateExec\n//    1. HashAggregateExec with a ImperativeAggregate aggregate function expression\n//    1. SortMergeJoinExec with InnerLike joins, i.e. CROSS and INNER\n//    1. InMemoryTableScanExec with output schema with primitive types only,\n//       i.e. BooleanType, ByteType, ShortType, IntegerType, LongType, FloatType, DoubleType\n\nFIXME Make the code working\n</code></pre>"},{"location":"physical-operators/JoinCodegenSupport/","title":"JoinCodegenSupport","text":"<p><code>JoinCodegenSupport</code> is...FIXME</p>"},{"location":"physical-operators/LocalTableScanExec/","title":"LocalTableScanExec Leaf Physical Operator","text":"<p><code>LocalTableScanExec</code> is a leaf physical operator that represents the following at execution:</p> <ul> <li>LocalRelation logical operator</li> <li><code>LocalScan</code> (under DataSourceV2ScanRelation)</li> <li><code>MemoryPlan</code> (Spark Structured Streaming)</li> <li><code>NoopCommand</code></li> </ul> <p></p>"},{"location":"physical-operators/LocalTableScanExec/#creating-instance","title":"Creating Instance","text":"<p><code>LocalTableScanExec</code> takes the following to be created:</p> <ul> <li> Output Attributes <li> Rows (InternalRows) <p><code>LocalTableScanExec</code> is created when:</p> <ul> <li>BasicOperators execution planning strategy is executed (to plan a <code>MemoryPlan</code>, a LocalRelation, DataSourceV2ScanRelation and <code>NoopCommand</code> logical operators)</li> </ul>"},{"location":"physical-operators/LocalTableScanExec/#performance-metrics","title":"Performance Metrics","text":""},{"location":"physical-operators/LocalTableScanExec/#number-of-output-rows","title":"number of output rows","text":""},{"location":"physical-operators/LocalTableScanExec/#inputrddcodegen","title":"InputRDDCodegen <p><code>LocalTableScanExec</code> is a <code>InputRDDCodegen</code>.</p>","text":""},{"location":"physical-operators/LocalTableScanExec/#collapsecodegenstages","title":"CollapseCodegenStages <p>CollapseCodegenStages physical optimization considers <code>LocalTableScanExec</code> special when insertWholeStageCodegen (so it won't be the root of WholeStageCodegen to support the fast driver-local collect/take paths).</p>","text":""},{"location":"physical-operators/LocalTableScanExec/#demo","title":"Demo <pre><code>val names = Seq(\"Jacek\", \"Agata\").toDF(\"name\")\nval optimizedPlan = names.queryExecution.optimizedPlan\n\nscala&gt; println(optimizedPlan.numberedTreeString)\n00 LocalRelation [name#9]\n\n// Physical plan with LocalTableScanExec operator (shown as LocalTableScan)\nscala&gt; names.explain\n== Physical Plan ==\nLocalTableScan [name#9]\n\n// Going fairly low-level...you've been warned\n\nval plan = names.queryExecution.executedPlan\nimport org.apache.spark.sql.execution.LocalTableScanExec\nval ltse = plan.asInstanceOf[LocalTableScanExec]\n\nval ltseRDD = ltse.execute()\nscala&gt; :type ltseRDD\norg.apache.spark.rdd.RDD[org.apache.spark.sql.catalyst.InternalRow]\n\nscala&gt; println(ltseRDD.toDebugString)\n(2) MapPartitionsRDD[1] at execute at &lt;console&gt;:30 []\n |  ParallelCollectionRDD[0] at execute at &lt;console&gt;:30 []\n\n// no computation on the source dataset has really occurred yet\n// Let's trigger a RDD action\nscala&gt; ltseRDD.first\nres6: org.apache.spark.sql.catalyst.InternalRow = [0,1000000005,6b6563614a]\n\n// Low-level \"show\"\nscala&gt; ltseRDD.foreach(println)\n[0,1000000005,6b6563614a]\n[0,1000000005,6174616741]\n\n// High-level show\nscala&gt; names.show\n+-----+\n| name|\n+-----+\n|Jacek|\n|Agata|\n+-----+\n</code></pre>","text":""},{"location":"physical-operators/LongHashedRelation/","title":"LongHashedRelation","text":"<p><code>LongHashedRelation</code> is a HashedRelation that is used when <code>HashedRelation</code> is requested for a concrete HashedRelation instance when the single key is of type long.</p> <p><code>LongHashedRelation</code> is also a Java <code>Externalizable</code>, i.e. when persisted, only the identity is written in the serialization stream and it is the responsibility of the class to &lt;&gt; and &lt;&gt; the contents of its instances. <p><code>LongHashedRelation</code> is &lt;&gt; when: <p>. <code>HashedRelation</code> is requested for a concrete HashedRelation (and &lt;&gt; factory method is used) <p>. <code>LongHashedRelation</code> is requested for a &lt;&gt; (when <code>BroadcastHashJoinExec</code> is requested to BroadcastHashJoinExec.md#doExecute[execute]) <p>=== [[writeExternal]] <code>writeExternal</code> Method</p>"},{"location":"physical-operators/LongHashedRelation/#source-scala","title":"[source, scala]","text":""},{"location":"physical-operators/LongHashedRelation/#writeexternalout-objectoutput-unit","title":"writeExternal(out: ObjectOutput): Unit","text":"<p>NOTE: <code>writeExternal</code> is part of Java's ++https://docs.oracle.com/javase/8/docs/api/java/io/Externalizable.html#writeExternal-java.io.ObjectOutput-++[Externalizable Contract] to...FIXME.</p> <p><code>writeExternal</code>...FIXME</p> <p>NOTE: <code>writeExternal</code> is used when...FIXME</p> <p>=== [[readExternal]] <code>readExternal</code> Method</p>"},{"location":"physical-operators/LongHashedRelation/#source-scala_1","title":"[source, scala]","text":""},{"location":"physical-operators/LongHashedRelation/#readexternalin-objectinput-unit","title":"readExternal(in: ObjectInput): Unit","text":"<p>NOTE: <code>readExternal</code> is part of Java's ++https://docs.oracle.com/javase/8/docs/api/java/io/Externalizable.html#readExternal-java.io.ObjectInput-++[Externalizable Contract] to...FIXME.</p> <p><code>readExternal</code>...FIXME</p> <p>NOTE: <code>readExternal</code> is used when...FIXME</p> <p>=== [[creating-instance]] Creating LongHashedRelation Instance</p> <p><code>LongHashedRelation</code> takes the following when created:</p> <ul> <li>[[nFields]] Number of fields</li> <li>[[map]] <code>LongToUnsafeRowMap</code></li> </ul> <p><code>LongHashedRelation</code> initializes the &lt;&gt;. <p>=== [[asReadOnlyCopy]] Creating Read-Only Copy of LongHashedRelation -- <code>asReadOnlyCopy</code> Method</p>"},{"location":"physical-operators/LongHashedRelation/#source-scala_2","title":"[source, scala]","text":""},{"location":"physical-operators/LongHashedRelation/#asreadonlycopy-longhashedrelation","title":"asReadOnlyCopy(): LongHashedRelation","text":"<p><code>asReadOnlyCopy</code>...FIXME</p> <p><code>asReadOnlyCopy</code> is part of the HashedRelation abstraction.</p> <p>=== [[getValue]] Getting Value Row for Given Key -- <code>getValue</code> Method</p>"},{"location":"physical-operators/LongHashedRelation/#source-scala_3","title":"[source, scala]","text":""},{"location":"physical-operators/LongHashedRelation/#getvaluekey-internalrow-internalrow","title":"getValue(key: InternalRow): InternalRow","text":"<p><code>getValue</code> checks if the input <code>key</code> is null at <code>0</code> position and if so gives <code>null</code>. Otherwise, <code>getValue</code> takes the long value at position <code>0</code> and &lt;&gt;. <p><code>getValue</code> is part of the HashedRelation abstraction.</p> <p>=== [[apply]] Creating LongHashedRelation Instance -- <code>apply</code> Factory Method</p>"},{"location":"physical-operators/LongHashedRelation/#source-scala_4","title":"[source, scala]","text":"<p>apply(   input: Iterator[InternalRow],   key: Seq[Expression],   sizeEstimate: Int,   taskMemoryManager: TaskMemoryManager): LongHashedRelation</p> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is used when <code>HashedRelation</code> is requested for a concrete HashedRelation.</p>"},{"location":"physical-operators/ObjectAggregationIterator/","title":"ObjectAggregationIterator","text":"<p><code>ObjectAggregationIterator</code> is an AggregationIterator for ObjectHashAggregateExec physical operator.</p>"},{"location":"physical-operators/ObjectAggregationIterator/#creating-instance","title":"Creating Instance","text":"<p><code>ObjectAggregationIterator</code> takes the following to be created:</p> <ul> <li> Partition ID <li> Output Attributes (unused) <li> Grouping NamedExpressions <li> AggregateExpressions <li> Aggregate Attributes <li> Initial input buffer offset <li> Result NamedExpressions <li> Function to create a new <code>MutableProjection</code> given expressions and attributes (<code>(Seq[Expression], Seq[Attribute]) =&gt; MutableProjection</code>) <li> Original Input Attributes <li> Input InternalRows <li> spark.sql.objectHashAggregate.sortBased.fallbackThreshold <li> numOutputRows metric <li> spillSize metric <li> numTasksFallBacked metric <p>While being created, <code>ObjectAggregationIterator</code> starts processing input rows.</p> <p><code>ObjectAggregationIterator</code> is created\u00a0when:</p> <ul> <li><code>ObjectHashAggregateExec</code> physical operator is requested to doExecute</li> </ul>"},{"location":"physical-operators/ObjectAggregationIterator/#outputforemptygroupingkeywithoutinput","title":"outputForEmptyGroupingKeyWithoutInput <pre><code>outputForEmptyGroupingKeyWithoutInput(): UnsafeRow\n</code></pre> <p><code>outputForEmptyGroupingKeyWithoutInput</code>...FIXME</p>  <p><code>outputForEmptyGroupingKeyWithoutInput</code> is used when:</p> <ul> <li><code>ObjectHashAggregateExec</code> physical operator is executed (with no input rows and no groupingExpressions)</li> </ul>","text":""},{"location":"physical-operators/ObjectAggregationIterator/#processing-input-rows","title":"Processing Input Rows <pre><code>processInputs(): Unit\n</code></pre> <p><code>processInputs</code> creates an ObjectAggregationMap.</p> <p>For no groupingExpressions, <code>processInputs</code> uses the groupingProjection to generate a grouping key (for <code>null</code> row) and finds the aggregation buffer that is used to process all input rows (of a partition).</p> <p>Otherwise, <code>processInputs</code> uses the sortBased flag to determine whether to use the <code>ObjectAggregationMap</code> or switch to a <code>SortBasedAggregator</code>.</p> <p><code>processInputs</code> uses the groupingProjection to generate a grouping key for an input row and finds the aggregation buffer that is used to process the row (of a partition). <code>processInputs</code> continues processing input rows until there are no more rows available or the size of the <code>ObjectAggregationMap</code> reaches spark.sql.objectHashAggregate.sortBased.fallbackThreshold.</p> <p>When the size of the <code>ObjectAggregationMap</code> reaches spark.sql.objectHashAggregate.sortBased.fallbackThreshold and there are still input rows in the partition, <code>processInputs</code> prints out the following INFO message to the logs, turns the sortBased flag on and increments the numTasksFallBacked metric.</p> <pre><code>Aggregation hash map size [size] reaches threshold capacity ([fallbackCountThreshold] entries),\nspilling and falling back to sort based aggregation.\nYou may change the threshold by adjusting the option spark.sql.objectHashAggregate.sortBased.fallbackThreshold\n</code></pre> <p>For sort-based aggregation (the sortBased flag is enabled), <code>processInputs</code> requests the <code>ObjectAggregationMap</code> to dumpToExternalSorter and create a <code>KVSorterIterator</code>. <code>processInputs</code> creates a <code>SortBasedAggregator</code>, uses the groupingProjection to generate a grouping key for every input row and adds them to the <code>SortBasedAggregator</code>.</p> <p>In the end, <code>processInputs</code> creates the aggBufferIterator (from the <code>ObjectAggregationMap</code> or <code>SortBasedAggregator</code> based on the sortBased flag).</p>  <p><code>processInputs</code> is used when:</p> <ul> <li><code>ObjectAggregationIterator</code> is created</li> </ul>","text":""},{"location":"physical-operators/ObjectAggregationIterator/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"physical-operators/ObjectAggregationMap/","title":"ObjectAggregationMap","text":"<p><code>ObjectAggregationMap</code> is an in-memory map to store aggregation buffer for hash-based aggregation (using ObjectAggregationIterator).</p>"},{"location":"physical-operators/ObjectConsumerExec/","title":"ObjectConsumerExec -- Unary Physical Operators with Child Physical Operator with One-Attribute Output Schema","text":"<p><code>ObjectConsumerExec</code> is the &lt;&gt; of &lt;&gt; with the child physical operator using a one-attribute &lt;&gt;. <p>[[contract]] [source, scala]</p> <p>package org.apache.spark.sql.execution</p> <p>trait ObjectConsumerExec extends UnaryExecNode {   // No properties (vals and methods) that have no implementation }</p> <p>[[references]] <code>ObjectConsumerExec</code> requests the child physical operator for the &lt;&gt; when requested for the &lt;&gt;. <p>[[implementations]] .ObjectConsumerExecs [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ObjectConsumerExec | Description</p> <p>| <code>AppendColumnsWithObjectExec</code> | [[AppendColumnsWithObjectExec]]</p> <p>| <code>MapPartitionsExec</code> | [[MapPartitionsExec]]</p> <p>| &lt;&gt; | [[SerializeFromObjectExec]] |=== <p>=== [[inputObjectType]] <code>inputObjectType</code> Method</p>"},{"location":"physical-operators/ObjectConsumerExec/#source-scala","title":"[source, scala]","text":""},{"location":"physical-operators/ObjectConsumerExec/#inputobjecttype-datatype","title":"inputObjectType: DataType","text":"<p><code>inputObjectType</code> simply returns the &lt;&gt; of the single &lt;&gt; of the child physical operator. <p>NOTE: <code>inputObjectType</code> is used when...FIXME</p>"},{"location":"physical-operators/ObjectHashAggregateExec/","title":"ObjectHashAggregateExec Physical Operator","text":"<p><code>ObjectHashAggregateExec</code> is an aggregate unary physical operator for object aggregation.</p> <p><code>ObjectHashAggregateExec</code> uses ObjectAggregationIterator for aggregation (one per partition).</p> <p></p>"},{"location":"physical-operators/ObjectHashAggregateExec/#creating-instance","title":"Creating Instance","text":"<p><code>ObjectHashAggregateExec</code> takes the following to be created:</p> <ul> <li> Required Child Distribution Expressions <li>isStreaming flag</li> <li> Number of Shuffle Partitions (always <code>None</code>) <li> Grouping NamedExpressions <li> AggregateExpressions <li> Aggregate Attributes <li> Initial Input Buffer Offset <li> Result NamedExpressions <li> Child Physical Operator <p><code>ObjectHashAggregateExec</code> is created when:</p> <ul> <li><code>AggUtils</code> is requested to create a physical operator for aggregation</li> </ul>"},{"location":"physical-operators/ObjectHashAggregateExec/#isstreaming-flag","title":"isStreaming Flag <p><code>ObjectHashAggregateExec</code> is given <code>isStreaming</code> flag when created.</p> <p>The <code>isStreaming</code> is always <code>false</code> but when <code>AggUtils</code> is requested to create a streaming aggregate physical operator.</p>","text":""},{"location":"physical-operators/ObjectHashAggregateExec/#performance-metrics","title":"Performance Metrics","text":""},{"location":"physical-operators/ObjectHashAggregateExec/#time-in-aggregation-build","title":"time in aggregation build <p>Time to execute a single partition</p>","text":""},{"location":"physical-operators/ObjectHashAggregateExec/#number-of-output-rows","title":"number of output rows <ul> <li><code>1</code> when there is neither input rows in a partition nor grouping expressions</li> </ul> <p>Used to create an ObjectAggregationIterator</p>","text":""},{"location":"physical-operators/ObjectHashAggregateExec/#number-of-sort-fallback-tasks","title":"number of sort fallback tasks <p>Number of tasks that crossed spark.sql.objectHashAggregate.sortBased.fallbackThreshold</p> <p>Used to create an ObjectAggregationIterator</p>","text":""},{"location":"physical-operators/ObjectHashAggregateExec/#spill-size","title":"spill size <p>Used to create a ObjectAggregationIterator</p>","text":""},{"location":"physical-operators/ObjectHashAggregateExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p>  <p><code>doExecute</code> requests the child physical operator to execute (and generate an <code>RDD[InternalRow]</code>) that is then mapPartitionsWithIndexInternal to process partitions.</p>  <p>Note</p> <p><code>doExecute</code> adds a new <code>MapPartitionsRDD</code> (Spark Core) to the RDD lineage.</p>","text":""},{"location":"physical-operators/ObjectHashAggregateExec/#processing-partition","title":"Processing Partition <p>While processing a partition, <code>mapPartitionsWithIndexInternal</code> branches off based on availability of input rows and the grouping expressions:</p> <ol> <li>No input rows but there are grouping expressions</li> <li>No input rows and no grouping expressions</li> <li>Input rows are available (regardless of the grouping expressions)</li> </ol>","text":""},{"location":"physical-operators/ObjectHashAggregateExec/#no-input-rows-with-grouping-expression","title":"No Input Rows with Grouping Expression <p>For no input records (in a partition) and non-empty grouping expressions, <code>doExecute</code> returns an empty <code>Iterator</code>.</p>","text":""},{"location":"physical-operators/ObjectHashAggregateExec/#no-input-rows-and-no-grouping-expression","title":"No Input Rows and No Grouping Expression <p>Otherwise, <code>doExecute</code> creates an ObjectAggregationIterator.</p> <p>For no input records (in a partition) and no grouping expressions, <code>doExecute</code> increments the numOutputRows metric (to be <code>1</code>) and requests the <code>ObjectAggregationIterator</code> for outputForEmptyGroupingKeyWithoutInput (that is the only output row).</p>","text":""},{"location":"physical-operators/ObjectHashAggregateExec/#input-rows-available","title":"Input Rows Available <p>Otherwise, <code>doExecute</code> returns the <code>ObjectAggregationIterator</code>.</p>","text":""},{"location":"physical-operators/ObjectHashAggregateExec/#selection-requirements","title":"Selection Requirements <pre><code>supportsAggregate(\n  aggregateExpressions: Seq[AggregateExpression]): Boolean\n</code></pre> <p><code>supportsAggregate</code> is enabled (<code>true</code>) when there is a TypedImperativeAggregate aggregate function among the AggregateFunctions of the given AggregateExpressions.</p>  <p><code>supportsAggregate</code> is used when:</p> <ul> <li><code>AggUtils</code> utility is used to select an aggregate physical operator</li> </ul>","text":""},{"location":"physical-operators/ObjectHashAggregateExec/#demo","title":"Demo <p><code>ObjectHashAggregateExec</code> is selected when spark.sql.execution.useObjectHashAggregateExec configuration property is enabled (and HashAggregateExec could not be used).</p> <pre><code>assert(spark.sessionState.conf.useObjectHashAggregation)\n</code></pre> <p>Use immutable data types for <code>aggregateBufferAttributes</code>.</p> <pre><code>val dataset = Seq(\n  (0, Seq.empty[Int]),\n  (1, Seq(1, 1)),\n  (2, Seq(2, 2))).toDF(\"id\", \"nums\")\n</code></pre> <pre><code>import org.apache.spark.sql.functions.size\nval q = dataset.\n  groupBy(size($\"nums\") as \"group\"). // &lt;-- size over array\n  agg(collect_list(\"id\") as \"ids\")\n</code></pre> <pre><code>scala&gt; q.explain\n== Physical Plan ==\nObjectHashAggregate(keys=[size(nums#8, true)#18], functions=[collect_list(id#7, 0, 0)])\n+- Exchange hashpartitioning(size(nums#8, true)#18, 200), ENSURE_REQUIREMENTS, [id=#10]\n   +- ObjectHashAggregate(keys=[size(nums#8, true) AS size(nums#8, true)#18], functions=[partial_collect_list(id#7, 0, 0)])\n      +- LocalTableScan [id#7, nums#8]\n</code></pre> <pre><code>scala&gt; println(q.queryExecution.sparkPlan.numberedTreeString)\n00 ObjectHashAggregate(keys=[size(nums#8, true)#18], functions=[collect_list(id#7, 0, 0)], output=[group#11, ids#15])\n01 +- ObjectHashAggregate(keys=[size(nums#8, true) AS size(nums#8, true)#18], functions=[partial_collect_list(id#7, 0, 0)], output=[size(nums#8, true)#18, buf#20])\n02    +- LocalTableScan [id#7, nums#8]\n</code></pre> <p>Going low level. Watch your steps :)</p> <pre><code>// copied from HashAggregateExec as it is the preferred aggreate physical operator\n// and HashAggregateExec is checked first\n// When the check fails, ObjectHashAggregateExec is then checked\nimport q.queryExecution.optimizedPlan\nimport org.apache.spark.sql.catalyst.plans.logical.Aggregate\nval aggLog = optimizedPlan.asInstanceOf[Aggregate]\nimport org.apache.spark.sql.catalyst.planning.PhysicalAggregation\nimport org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression\n// groupingExpressions, aggregateExpressions, resultExpressions, child\nval (_, aggregateExpressions: Seq[AggregateExpression], _, _) = PhysicalAggregation.unapply(aggLog).get\nval aggregateBufferAttributes =\n  aggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes)\n</code></pre> <p>One of the reasons why <code>ObjectHashAggregateExec</code> was selected is that <code>HashAggregateExec</code> did not meet the requirements.</p> <pre><code>import org.apache.spark.sql.execution.aggregate.HashAggregateExec\nassert(HashAggregateExec.supportsAggregate(aggregateBufferAttributes) == false)\n</code></pre> <pre><code>// collect_list aggregate function uses CollectList TypedImperativeAggregate under the covers\nimport org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec\nassert(ObjectHashAggregateExec.supportsAggregate(aggregateExpressions))\n</code></pre> <pre><code>val aggExec = q.queryExecution.sparkPlan.children.head.asInstanceOf[ObjectHashAggregateExec]\n</code></pre> <pre><code>scala&gt; println(aggExec.aggregateExpressions.head.numberedTreeString)\n00 partial_collect_list(id#7, 0, 0)\n01 +- collect_list(id#7, 0, 0)\n02    +- id#7: int\n</code></pre>","text":""},{"location":"physical-operators/ObjectProducerExec/","title":"ObjectProducerExec","text":"<p><code>ObjectProducerExec</code> is...FIXME</p>"},{"location":"physical-operators/OrderedDistribution/","title":"OrderedDistribution","text":"<p><code>OrderedDistribution</code> is a Distribution for ordered data distribution requirement (of SortExec physical operator with global sort).</p>"},{"location":"physical-operators/OrderedDistribution/#creating-instance","title":"Creating Instance","text":"<p><code>OrderedDistribution</code> takes the following to be created:</p> <ul> <li> SortOrder expressions (for ordering) <p><code>OrderedDistribution</code> is created when:</p> <ul> <li><code>SortExec</code> physical operator is requested for the requiredChildDistribution (with global sort)</li> </ul>"},{"location":"physical-operators/OrderedDistribution/#required-number-of-partitions","title":"Required Number of Partitions <pre><code>requiredNumPartitions: Option[Int]\n</code></pre> <p><code>requiredNumPartitions</code> is undefined (<code>None</code>).</p> <p><code>requiredNumPartitions</code> is part of the Distribution abstraction.</p>","text":""},{"location":"physical-operators/OrderedDistribution/#creating-partitioning","title":"Creating Partitioning <pre><code>createPartitioning(\n  numPartitions: Int): Partitioning\n</code></pre> <p><code>createPartitioning</code> creates a <code>RangePartitioning</code> expression.</p> <p><code>createPartitioning</code> is part of the Distribution abstraction.</p>","text":""},{"location":"physical-operators/OverwriteByExpressionExec/","title":"OverwriteByExpressionExec Physical Command","text":"<p><code>OverwriteByExpressionExec</code> is a V2TableWriteExec with BatchWriteHelper.</p>"},{"location":"physical-operators/OverwriteByExpressionExec/#creating-instance","title":"Creating Instance","text":"<p><code>OverwriteByExpressionExec</code> takes the following to be created:</p> <ul> <li> SupportsWrite <li> Delete Filters (<code>Array[Filter]</code>) <li> Write Options <li> Physical Query Plan <p><code>OverwriteByExpressionExec</code> is created\u00a0when DataSourceV2Strategy execution planning strategy is executed for OverwriteByExpression with DataSourceV2Relation over writable tables (tables with SupportsWrite except with V1_BATCH_WRITE capability).</p>"},{"location":"physical-operators/Partitioning/","title":"Partitioning (Catalyst)","text":"<p><code>Partitioning</code> is an abstraction of partitioning specifications that hint the Spark Physical Optimizer about the number of partitions and data distribution of the output of a physical operator.</p>"},{"location":"physical-operators/Partitioning/#contract","title":"Contract","text":""},{"location":"physical-operators/Partitioning/#number-of-partitions","title":"Number of Partitions <pre><code>numPartitions: Int\n</code></pre> <p>Used when:</p> <ul> <li><code>AliasAwareOutputPartitioning</code> unary physical operators are requested for the outputPartitioning</li> <li>EnsureRequirements physical optimization is executed</li> <li><code>ShuffleExchangeExec</code> utility is used to prepareShuffleDependency</li> <li><code>ValidateRequirements</code> utility is used to <code>validateInternal</code></li> <li><code>ShuffledJoin</code> physical operators are requested for the outputPartitioning (for <code>FullOuter</code> join type)</li> <li><code>Partitioning</code> is requested to satisfies, satisfies0</li> </ul>","text":""},{"location":"physical-operators/Partitioning/#implementations","title":"Implementations","text":""},{"location":"physical-operators/Partitioning/#broadcastpartitioning","title":"BroadcastPartitioning <pre><code>BroadcastPartitioning(\n  mode: BroadcastMode)\n</code></pre> <p>numPartitions: 1</p> <p>satisfies:</p> <ul> <li>UnspecifiedDistribution</li> <li>BroadcastDistribution with the same BroadcastMode</li> </ul> <p>Created when:</p> <ul> <li><code>BroadcastDistribution</code> is requested to create a Partitioning</li> <li><code>BroadcastExchangeExec</code> physical operator is requested for the outputPartitioning</li> </ul>","text":""},{"location":"physical-operators/Partitioning/#hashpartitioning","title":"HashPartitioning <p>HashPartitioning</p> <p>numPartitions: the given numPartitions</p> <p>satisfies:</p> <ul> <li>UnspecifiedDistribution</li> <li>ClusteredDistribution with all the hashing expressions included in <code>clustering</code> expressions</li> </ul> <p>createShuffleSpec: <code>HashShuffleSpec</code></p>","text":""},{"location":"physical-operators/Partitioning/#keygroupedpartitioning","title":"KeyGroupedPartitioning <pre><code>KeyGroupedPartitioning(\n  expressions: Seq[Expression],\n  numPartitions: Int,\n  partitionValues: Seq[InternalRow] = Seq.empty)\n</code></pre> <p>satisfies0: ClusteredDistribution</p> <p>createShuffleSpec: <code>KeyGroupedShuffleSpec</code></p>","text":""},{"location":"physical-operators/Partitioning/#partitioningcollection","title":"PartitioningCollection <pre><code>PartitioningCollection(\n  partitionings: Seq[Partitioning])\n</code></pre> <p>compatibleWith: Any <code>Partitioning</code> that is compatible with one of the input <code>partitionings</code></p> <p>guarantees: Any <code>Partitioning</code> that is guaranteed by any of the input <code>partitionings</code></p> <p>numPartitions: Number of partitions of the first <code>Partitioning</code> in the input <code>partitionings</code></p> <p>satisfies: Any <code>Distribution</code> that is satisfied by any of the input <code>partitionings</code></p> <p>createShuffleSpec: <code>ShuffleSpecCollection</code></p>","text":""},{"location":"physical-operators/Partitioning/#rangepartitioning","title":"RangePartitioning <p>compatibleWith: <code>RangePartitioning</code> when semantically equal (i.e. underlying expressions are deterministic and canonically equal)</p> <p>guarantees: <code>RangePartitioning</code> when semantically equal (i.e. underlying expressions are deterministic and canonically equal)</p> <p>numPartitions: the given <code>numPartitions</code></p> <p>satisfies:</p> <ul> <li>UnspecifiedDistribution</li> <li>OrderedDistribution with <code>requiredOrdering</code> that matches the input <code>ordering</code></li> <li>ClusteredDistribution with all the children of the input <code>ordering</code> semantically equal to one of the <code>clustering</code> expressions</li> </ul> <p>createShuffleSpec: <code>RangeShuffleSpec</code></p>","text":""},{"location":"physical-operators/Partitioning/#roundrobinpartitioning","title":"RoundRobinPartitioning <pre><code>RoundRobinPartitioning(\n  numPartitions: Int)\n</code></pre> <p>compatibleWith: Always <code>false</code></p> <p>guarantees: Always <code>false</code></p> <p>numPartitions: the given <code>numPartitions</code></p> <p>satisfies: UnspecifiedDistribution</p>","text":""},{"location":"physical-operators/Partitioning/#singlepartition","title":"SinglePartition <p>compatibleWith: Any <code>Partitioning</code> with one partition</p> <p>guarantees: Any <code>Partitioning</code> with one partition</p> <p>numPartitions: <code>1</code></p> <p>satisfies: Any <code>Distribution</code> except BroadcastDistribution</p> <p>createShuffleSpec: <code>SinglePartitionShuffleSpec</code></p>","text":""},{"location":"physical-operators/Partitioning/#unknownpartitioning","title":"UnknownPartitioning <pre><code>UnknownPartitioning(\n  numPartitions: Int)\n</code></pre> <p>compatibleWith: Always <code>false</code></p> <p>guarantees: Always <code>false</code></p> <p>numPartitions: the given <code>numPartitions</code></p> <p>satisfies: UnspecifiedDistribution</p>","text":""},{"location":"physical-operators/Partitioning/#satisfying-distribution","title":"Satisfying Distribution <pre><code>satisfies(\n  required: Distribution): Boolean\n</code></pre> <p><code>satisfies</code> is <code>true</code> when  all the following hold:</p> <ol> <li>The optional required number of partitions of the given Distribution is the number of partitions of this <code>Partitioning</code></li> <li>satisfies0 holds</li> </ol>  Final Method <p><code>satisfies</code> is a Scala final method and may not be overridden in subclasses.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>satisfies</code>\u00a0is used when:</p> <ul> <li>RemoveRedundantSorts physical optimization is executed</li> <li>EnsureRequirements physical optimization is executed</li> <li>AdaptiveSparkPlanExec leaf physical operator is executed</li> </ul>","text":""},{"location":"physical-operators/Partitioning/#satisfies0","title":"satisfies0 <pre><code>satisfies0(\n  required: Distribution): Boolean\n</code></pre> <p><code>satisfies0</code> is <code>true</code> when either holds:</p> <ul> <li>The given Distribution is a UnspecifiedDistribution</li> <li>The given Distribution is a AllTuples and the number of partitions is <code>1</code>.</li> </ul>  <p>Note</p> <p><code>satisfies0</code> can be overriden by subclasses if needed (to influence the final satisfies).</p>","text":""},{"location":"physical-operators/Partitioning/#createshufflespec","title":"createShuffleSpec <pre><code>createShuffleSpec(\n  distribution: ClusteredDistribution): ShuffleSpec\n</code></pre> <p><code>createShuffleSpec</code> gives a ShuffleSpec.</p> <p><code>createShuffleSpec</code> throws an <code>IllegalStateException</code> by default (and is supposed to be overriden by subclasses if needed):</p> <pre><code>Unexpected partitioning: [className]\n</code></pre>  <p><code>createShuffleSpec</code> is used when:</p> <ul> <li>EnsureRequirements physical optimization is executed</li> <li><code>ValidateRequirements</code> is requested to <code>validate</code> (for OptimizeSkewedJoin physical optimization and AdaptiveSparkPlanExec physical operator)</li> </ul>","text":""},{"location":"physical-operators/ProjectExec/","title":"ProjectExec Unary Physical Operator","text":"<p><code>ProjectExec</code> is a unary physical operator with support for Java code generation that represents Project logical operator at execution.</p>"},{"location":"physical-operators/ProjectExec/#creating-instance","title":"Creating Instance","text":"<p><code>ProjectExec</code> takes the following to be created:</p> <ul> <li> NamedExpressions <li> Child Physical Operator <p><code>ProjectExec</code> is created\u00a0when:</p> <ul> <li>BasicOperators execution planning strategy is executed (to plan Project logical operator)</li> <li><code>SparkPlanner</code> is requested to pruneFilterProject</li> <li>DataSourceStrategy execution planning strategy is executed</li> <li>FileSourceStrategy execution planning strategy is executed</li> <li>DataSourceV2Strategy execution planning strategy is executed</li> <li><code>FileFormatWriter</code> is requested to write</li> </ul>"},{"location":"physical-operators/ProjectExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code>\u00a0is part of the SparkPlan abstraction.</p> <p><code>doExecute</code> requests the child physical plan to execute and mapPartitionsWithIndexInternal.</p>","text":""},{"location":"physical-operators/ProjectExec/#mappartitionswithindexinternal","title":"mapPartitionsWithIndexInternal <p><code>doExecute</code> uses <code>RDD.mapPartitionsWithIndexInternal</code>.</p> <pre><code>mapPartitionsWithIndexInternal[U](\n  f: (Int, Iterator[T]) =&gt; Iterator[U],\n  preservesPartitioning: Boolean = false)\n</code></pre> <p><code>doExecute</code> creates an UnsafeProjection for the named expressions and (the output of) the child physical operator.</p> <p><code>doExecute</code> requests the <code>UnsafeProjection</code> to initialize and maps over the internal rows (of a partition) using the projection.</p>","text":""},{"location":"physical-operators/ProjectExec/#output-attributes","title":"Output Attributes <pre><code>output: Seq[Attribute]\n</code></pre> <p><code>output</code>\u00a0is part of the QueryPlan abstraction.</p> <p><code>output</code> is the NamedExpressions converted to Attributes.</p>","text":""},{"location":"physical-operators/QueryStageExec/","title":"QueryStageExec Leaf Physical Operators","text":"<p><code>QueryStageExec</code> is an extension of the LeafExecNode abstraction for leaf physical operators for Adaptive Query Execution.</p>"},{"location":"physical-operators/QueryStageExec/#contract","title":"Contract","text":""},{"location":"physical-operators/QueryStageExec/#cancelling","title":"Cancelling <pre><code>cancel(): Unit\n</code></pre> <p>Cancels the stage materialization if in progress; otherwise does nothing.</p> <p>Used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested to cleanUpAndThrowException</li> </ul>","text":""},{"location":"physical-operators/QueryStageExec/#materializing","title":"Materializing <pre><code>doMaterialize(): Future[Any]\n</code></pre> <p>Used when:</p> <ul> <li><code>QueryStageExec</code> is requested to materialize</li> </ul>","text":""},{"location":"physical-operators/QueryStageExec/#runtime-statistics","title":"Runtime Statistics <pre><code>getRuntimeStatistics: Statistics\n</code></pre> <p>Statistics after stage materialization</p> <p>See:</p> <ul> <li>BroadcastQueryStageExec</li> <li>ShuffleQueryStageExec</li> </ul> <p>Used when:</p> <ul> <li><code>AQEPropagateEmptyRelation</code> logical optimization is requested for an estimated row count</li> <li><code>QueryStageExec</code> is requested to compute statistics</li> </ul>","text":""},{"location":"physical-operators/QueryStageExec/#query-stage-id","title":"Query Stage ID <pre><code>id: Int\n</code></pre> <p>Used when:</p> <ul> <li>CoalesceShufflePartitions adaptive physical optimization is executed</li> </ul>","text":""},{"location":"physical-operators/QueryStageExec/#new-shufflequerystageexec-instance-for-reuse","title":"New ShuffleQueryStageExec Instance for Reuse <pre><code>newReuseInstance(\n  newStageId: Int,\n  newOutput: Seq[Attribute]): QueryStageExec\n</code></pre> <p>Used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested to reuseQueryStage</li> </ul>","text":""},{"location":"physical-operators/QueryStageExec/#physical-query-plan","title":"Physical Query Plan <pre><code>plan: SparkPlan\n</code></pre> <p>The sub-tree of the main query plan of this query stage (that acts like a child operator, but <code>QueryStageExec</code> is a LeafExecNode and has no children)</p>","text":""},{"location":"physical-operators/QueryStageExec/#implementations","title":"Implementations","text":"<ul> <li> BroadcastQueryStageExec <li> ShuffleQueryStageExec"},{"location":"physical-operators/QueryStageExec/#result","title":"Result <pre><code>_resultOption: AtomicReference[Option[Any]]\n</code></pre> <p><code>QueryStageExec</code> uses a <code>_resultOption</code> transient volatile internal variable (of type AtomicReference) for the result of a successful materialization of this <code>QueryStageExec</code> operator (when preparing for query execution):</p> <ul> <li>Broadcast variable (broadcasting data) for BroadcastQueryStageExec</li> <li>MapOutputStatistics (submitting map stages) for ShuffleQueryStageExec</li> </ul> <p>As <code>AtomicReference</code> is mutable that is enough to update the value.</p> <p><code>_resultOption</code> is set when <code>AdaptiveSparkPlanExec</code> physical operator is requested for the final physical plan.</p> <p><code>_resultOption</code> is available using resultOption.</p>","text":""},{"location":"physical-operators/QueryStageExec/#resultoption","title":"resultOption <pre><code>resultOption: AtomicReference[Option[Any]]\n</code></pre> <p><code>resultOption</code> returns the current value of the _resultOption registry.</p>  <p><code>resultOption</code> is used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> is requested to getFinalPhysicalPlan (to set the value)</li> <li><code>QueryStageExec</code> is requested to isMaterialized</li> <li><code>ShuffleQueryStageExec</code> is requested for the MapOutputStatistics</li> </ul>","text":""},{"location":"physical-operators/QueryStageExec/#computing-runtime-statistics","title":"Computing Runtime Statistics <pre><code>computeStats(): Option[Statistics]\n</code></pre> <p>Only when this <code>QueryStageExec</code> has been materialized, <code>computeStats</code> gives a new Statistics based on the runtime statistics (and flips the isRuntime flag to <code>true</code>).</p> <p>Otherwise, <code>computeStats</code> returns no statistics (<code>None</code>).</p>  <p><code>computeStats</code> is used when:</p> <ul> <li><code>LogicalQueryStage</code> logical operator is requested for the Statistics</li> </ul>","text":""},{"location":"physical-operators/QueryStageExec/#ismaterialized","title":"isMaterialized <pre><code>isMaterialized: Boolean\n</code></pre> <p><code>isMaterialized</code> checks whether or not the resultOption has a value.</p>  <p><code>isMaterialized</code> is used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> is requested to createQueryStages</li> <li><code>AQEPropagateEmptyRelation</code> logical optimization is requested for an estimated row count and isRelationWithAllNullKeys</li> <li><code>DynamicJoinSelection</code> logical optimization is requested to selectJoinStrategy</li> <li><code>ShuffleStage</code> is requested to extract a materialized <code>ShuffleQueryStageExec</code> (for OptimizeSkewedJoin physical optimization)</li> <li><code>QueryStageExec</code> is requested to computeStats</li> </ul>","text":""},{"location":"physical-operators/QueryStageExec/#materializing-query-stage","title":"Materializing Query Stage <pre><code>materialize(): Future[Any]\n</code></pre> <p><code>materialize</code> prints out the following DEBUG message to the logs (with the id):</p> <pre><code>Materialize query stage [simpleName]: [id]\n</code></pre> <p><code>materialize</code> doMaterialize.</p>  Final Method <p><code>materialize</code> is a Scala final method and may not be overridden in subclasses.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>materialize</code> is used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested to getFinalPhysicalPlan</li> </ul>","text":""},{"location":"physical-operators/QueryStageExec/#text-representation","title":"Text Representation <pre><code>generateTreeString(\n  depth: Int,\n  lastChildren: Seq[Boolean],\n  append: String =&gt; Unit,\n  verbose: Boolean,\n  prefix: String = \"\",\n  addSuffix: Boolean = false,\n  maxFields: Int,\n  printNodeId: Boolean,\n  indent: Int = 0): Unit\n</code></pre> <p><code>generateTreeString</code> is part of the TreeNode abstraction.</p> <p><code>generateTreeString</code> generateTreeString (the default) followed by another generateTreeString (with the depth incremented).</p>","text":""},{"location":"physical-operators/QueryStageExec/#logging","title":"Logging <p><code>QueryStageExec</code> is an abstract class and logging is configured using the logger of the implementations.</p>","text":""},{"location":"physical-operators/RangeExec/","title":"RangeExec Leaf Physical Operator","text":"<p><code>RangeExec</code> is a leaf physical operator.</p>"},{"location":"physical-operators/ReusedExchangeExec/","title":"ReusedExchangeExec Leaf Physical Operator","text":"<p><code>ReusedExchangeExec</code> is a leaf physical operator.</p>"},{"location":"physical-operators/ReusedSubqueryExec/","title":"ReusedSubqueryExec Physical Operator","text":"<p><code>ReusedSubqueryExec</code> is a BaseSubqueryExec and a <code>LeafExecNode</code> with a child BaseSubqueryExec physical operator.</p> <p><code>ReusedSubqueryExec</code> is a wrapper and delegates all activity (as a physical operator) to the child BaseSubqueryExec.</p>"},{"location":"physical-operators/ReusedSubqueryExec/#creating-instance","title":"Creating Instance","text":"<p><code>ReusedSubqueryExec</code> takes the following to be created:</p> <ul> <li> Child BaseSubqueryExec physical operator <p><code>ReusedSubqueryExec</code> is created\u00a0when:</p> <ul> <li>ReuseAdaptiveSubquery physical optimization is executed</li> <li>ReuseExchangeAndSubquery physical optimization is executed</li> </ul>"},{"location":"physical-operators/RowDataSourceScanExec/","title":"RowDataSourceScanExec Leaf Physical Operator","text":"<p><code>RowDataSourceScanExec</code> is a DataSourceScanExec (and so indirectly a leaf physical operator) for scanning data from a BaseRelation.</p> <p><code>RowDataSourceScanExec</code> is an <code>InputRDDCodegen</code>.</p>"},{"location":"physical-operators/RowDataSourceScanExec/#performance-metrics","title":"Performance Metrics    Key Name (in web UI) Description     numOutputRows number of output rows Number of output rows","text":""},{"location":"physical-operators/RowDataSourceScanExec/#creating-instance","title":"Creating Instance <p><code>RowDataSourceScanExec</code> takes the following to be created:</p> <ul> <li> Output Schema (Attributes) <li> Required Schema (StructType) <li> Data Source Filter Predicates <li> Handled Data Source Filter Predicates <li> <code>RDD[InternalRow]</code> <li> BaseRelation <li> Optional <code>TableIdentifier</code>  <p><code>RowDataSourceScanExec</code> is created\u00a0when:</p> <ul> <li>DataSourceStrategy execution planning strategy is executed (for LogicalRelation logical operators)</li> </ul>","text":""},{"location":"physical-operators/RowDataSourceScanExec/#metadata","title":"Metadata <pre><code>metadata: Map[String, String]\n</code></pre> <p><code>metadata</code>\u00a0is part of the DataSourceScanExec abstraction.</p> <p><code>metadata</code> marks the filter predicates that are included in the handled filters predicates with <code>*</code> (star).</p>  <p>Note</p> <p>Filter predicates with <code>*</code> (star) are to denote filters that are pushed down to a relation (aka data source).</p>  <p>In the end, <code>metadata</code> creates the following mapping:</p> <ul> <li>ReadSchema with the required schema converted to catalog representation</li> <li>PushedFilters with the marked and unmarked filter predicates</li> </ul>","text":""},{"location":"physical-operators/RowDataSourceScanExec/#createunsafeprojection","title":"createUnsafeProjection <pre><code>createUnsafeProjection: Boolean\n</code></pre> <p><code>createUnsafeProjection</code> is <code>true</code>.</p> <p><code>createUnsafeProjection</code>\u00a0is part of the <code>InputRDDCodegen</code> abstraction.</p>","text":""},{"location":"physical-operators/RowDataSourceScanExec/#input-rdd","title":"Input RDD <pre><code>inputRDD: RDD[InternalRow]\n</code></pre> <p><code>inputRDD</code> is the RDD.</p> <p><code>inputRDD</code>\u00a0is part of the <code>InputRDDCodegen</code> abstraction.</p>","text":""},{"location":"physical-operators/RowToColumnarExec/","title":"RowToColumnarExec Physical Operator","text":"<p><code>RowToColumnarExec</code> is a <code>RowToColumnarTransition</code> for Columnar Execution.</p> <p><code>RowToColumnarExec</code> is the opposite of ColumnarToRowExec physical operator.</p>"},{"location":"physical-operators/RowToColumnarExec/#creating-instance","title":"Creating Instance","text":"<p><code>RowToColumnarExec</code> takes the following to be created:</p> <ul> <li> Child SparkPlan <p><code>RowToColumnarExec</code> is created when:</p> <ul> <li>ApplyColumnarRulesAndInsertTransitions physical optimization is executed</li> </ul>"},{"location":"physical-operators/SerializeFromObjectExec/","title":"SerializeFromObjectExec Unary Physical Operator","text":"<p><code>SerializeFromObjectExec</code> is a unary physical operator that supports Java code generation.</p> <p><code>SerializeFromObjectExec</code> supports Java code generation with the &lt;&gt;, &lt;&gt; and &lt;&gt; methods. <p><code>SerializeFromObjectExec</code> is a &lt;&gt;. <p><code>SerializeFromObjectExec</code> is &lt;&gt; exclusively when BasicOperators execution planning strategy is executed. <p>[[inputRDDs]] [[outputPartitioning]] <code>SerializeFromObjectExec</code> uses the &lt;&gt; physical operator when requested for the input RDDs and the &lt;&gt;. <p>[[output]] <code>SerializeFromObjectExec</code> uses the &lt;&gt; for the &lt;&gt;. <p>=== [[creating-instance]] Creating SerializeFromObjectExec Instance</p> <p><code>SerializeFromObjectExec</code> takes the following when created:</p> <ul> <li>[[serializer]] Serializer (as <code>Seq[NamedExpression]</code>)</li> <li>[[child]] Child &lt;&gt; (that supports Java code generation) <p>=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- <code>doExecute</code> Method</p>"},{"location":"physical-operators/SerializeFromObjectExec/#source-scala","title":"[source, scala]","text":""},{"location":"physical-operators/SerializeFromObjectExec/#doexecute-rddinternalrow","title":"doExecute(): RDD[InternalRow]","text":"<p><code>doExecute</code> is part of the SparkPlan abstraction.</p> <p><code>doExecute</code> requests the &lt;&gt; physical operator to &lt;&gt; (that triggers physical query planning and generates an <code>RDD[InternalRow]</code>) and transforms it by executing the following function on internal rows per partition with index (using <code>RDD.mapPartitionsWithIndexInternal</code> that creates another RDD): <p>. Creates an UnsafeProjection for the &lt;&gt; <p>. Requests the <code>UnsafeProjection</code> to initialize (for the partition index)</p> <p>. Executes the <code>UnsafeProjection</code> on all internal binary rows in the partition</p> <p>NOTE: <code>doExecute</code> (by <code>RDD.mapPartitionsWithIndexInternal</code>) adds a new <code>MapPartitionsRDD</code> to the RDD lineage. Use <code>RDD.toDebugString</code> to see the additional <code>MapPartitionsRDD</code>.</p>"},{"location":"physical-operators/SetCatalogAndNamespaceExec/","title":"SetCatalogAndNamespaceExec Physical Command","text":"<p><code>SetCatalogAndNamespaceExec</code> is a physical command that represents SetCatalogAndNamespace logical command at execution time.</p> <pre><code>val ns = \"my_space\"\nsql(s\"CREATE NAMESPACE IF NOT EXISTS $ns\")\n\nsql(s\"USE NAMESPACE $ns\")\n\nsql(\"SHOW CURRENT NAMESPACE\").show(truncate = false)\n// +-------------+---------+\n// |catalog      |namespace|\n// +-------------+---------+\n// |spark_catalog|my_space |\n// +-------------+---------+\n</code></pre>"},{"location":"physical-operators/SetCatalogAndNamespaceExec/#creating-instance","title":"Creating Instance","text":"<p><code>SetCatalogAndNamespaceExec</code> takes the following to be created:</p> <ul> <li> CatalogManager <li> Optional Catalog Name <li> Optional Namespace <p><code>SetCatalogAndNamespaceExec</code> is created when DataSourceV2Strategy execution planning strategy is executed (and plans a SetCatalogAndNamespace logical command).</p>"},{"location":"physical-operators/ShowCreateTableExec/","title":"ShowCreateTableExec Physical Command","text":"<p><code>ShowCreateTableExec</code> is a V2CommandExec physical command that represents ShowCreateTable logical operator at execution.</p> <p><code>ShowCreateTableExec</code> is a <code>LeafExecNode</code>.</p>"},{"location":"physical-operators/ShowCreateTableExec/#creating-instance","title":"Creating Instance","text":"<p><code>ShowCreateTableExec</code> takes the following to be created:</p> <ul> <li> Output Attributes <li> Table <p><code>ShowCreateTableExec</code> is created when:</p> <ul> <li>DataSourceV2Strategy execution planning strategy is executed (to plan a logical query plan with a ShowCreateTable logical operator)</li> </ul>"},{"location":"physical-operators/ShowCreateTableExec/#executing-command","title":"Executing Command  Signature <pre><code>run(): Seq[InternalRow]\n</code></pre> <p><code>run</code> is part of the V2CommandExec abstraction.</p>   <p><code>run</code> showCreateTable.</p>","text":""},{"location":"physical-operators/ShowCreateTableExec/#showcreatetable","title":"showCreateTable <pre><code>showCreateTable(\n  table: Table,\n  builder: StringBuilder): Unit\n</code></pre> <p><code>showCreateTable</code> adds the following (to the given <code>StringBuilder</code>):</p> <pre><code>CREATE TABLE [tableName]\n</code></pre> <p><code>showCreateTable</code> then does the following:</p> <ul> <li>showTableDataColumns</li> <li>showTableUsing</li> <li>showTableOptions</li> <li>showTablePartitioning</li> <li>showTableComment</li> <li>showTableLocation</li> <li>showTableProperties</li> </ul>","text":""},{"location":"physical-operators/ShowCreateTableExec/#showtabledatacolumns","title":"showTableDataColumns <pre><code>showTableDataColumns(\n  table: Table,\n  builder: StringBuilder): Unit\n</code></pre> <p><code>showTableDataColumns</code> requests the given Table for the columns that are converted to DDL format.</p>","text":""},{"location":"physical-operators/ShowCreateTableExec/#showtablepartitioning","title":"showTablePartitioning <pre><code>showTablePartitioning(\n  table: Table,\n  builder: StringBuilder): Unit\n</code></pre>  Noop for a non-partitioned table <p><code>showTablePartitioning</code> does nothing (noop) when the given table has no partitioning.</p>  <p>For a <code>BucketTransform</code> (among partitioning transforms), <code>showTablePartitioning</code> creates a BucketSpec to add the following (to the given <code>StringBuilder</code>):</p> <pre><code>CLUSTERED BY [bucketColumnNames]\nSORTED BY [sortColumnNames]\nINTO [numBuckets] BUCKETS\n</code></pre>  One BucketTransform Supported <p>In case there are more <code>BucketTransform</code>s, the last wins.</p>  <p>For all other Transforms, <code>showTablePartitioning</code> requests them to describe and adds the following (to the given <code>StringBuilder</code>):</p> <pre><code>PARTITIONED BY [transforms]\n</code></pre>","text":""},{"location":"physical-operators/ShowTablePropertiesExec/","title":"ShowTablePropertiesExec Physical Command","text":"<p><code>ShowTablePropertiesExec</code> is a physical command that represents ShowTableProperties logical command at execution time (for non-ShowTablePropertiesCommand cases).</p>"},{"location":"physical-operators/ShowTablePropertiesExec/#creating-instance","title":"Creating Instance","text":"<p><code>ShowTablePropertiesExec</code> takes the following to be created:</p> <ul> <li> Output Attributes <li> Table <li> (optional) Property Key <p><code>ShowTablePropertiesExec</code> is created when:</p> <ul> <li>DataSourceV2Strategy execution planning strategy is executed (and plans a ShowTableProperties logical command)</li> </ul>"},{"location":"physical-operators/ShowTablesExec/","title":"ShowTablesExec Physical Command","text":"<p><code>ShowTablesExec</code> is a physical command that represents a ShowTables logical command at execution time.</p> <p><code>ShowTablesExec</code> is a leaf physical operator.</p>"},{"location":"physical-operators/ShowTablesExec/#creating-instance","title":"Creating Instance","text":"<p><code>ShowTablesExec</code> takes the following to be created:</p> <ul> <li> Output Attributes <li> TableCatalog <li> Multi-Part Namespace <li> Optional Pattern (of tables to show) <p><code>ShowTablesExec</code> is created when DataSourceV2Strategy execution planning strategy is requested to plan a ShowTables logical command.</p>"},{"location":"physical-operators/ShowTablesExec/#executing-command","title":"Executing Command <pre><code>run(): Seq[InternalRow]\n</code></pre> <p><code>run</code> requests the TableCatalog to listTables in the namespace.</p> <p><code>run</code> returns tables that match the pattern if defined or all the tables available.</p> <p><code>run</code> is part of the V2CommandExec abstraction.</p>","text":""},{"location":"physical-operators/ShuffleExchangeExec/","title":"ShuffleExchangeExec Physical Operator","text":"<p><code>ShuffleExchangeExec</code> is an Exchange (indirectly as a ShuffleExchangeLike) unary physical operator that is used to perform a shuffle.</p>"},{"location":"physical-operators/ShuffleExchangeExec/#creating-instance","title":"Creating Instance","text":"<p><code>ShuffleExchangeExec</code> takes the following to be created:</p> <ul> <li> Output Partitioning <li> Child physical operator <li> ShuffleOrigin (default: ENSURE_REQUIREMENTS) <p><code>ShuffleExchangeExec</code> is created when:</p> <ul> <li>BasicOperators execution planning strategy is executed and plans the following:</li> <li>Repartition with the shuffle flag enabled</li> <li>RepartitionByExpression</li> <li>EnsureRequirements physical optimization is executed</li> </ul>"},{"location":"physical-operators/ShuffleExchangeExec/#node-name","title":"Node Name <pre><code>nodeName: String\n</code></pre> <p><code>nodeName</code> is part of the TreeNode abstraction.</p>  <p><code>nodeName</code> is Exchange.</p>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#performance-metrics","title":"Performance Metrics","text":""},{"location":"physical-operators/ShuffleExchangeExec/#data-size","title":"data size","text":""},{"location":"physical-operators/ShuffleExchangeExec/#number-of-partitions","title":"number of partitions <p>Number of partitions (of the <code>Partitioner</code> of this ShuffleDependency)</p> <p>Posted as the only entry in <code>accumUpdates</code> of a <code>SparkListenerDriverAccumUpdates</code></p>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#read-metrics","title":"Read Metrics <p>Used to create a ShuffledRowRDD when:</p> <ul> <li>Executing</li> <li>getShuffleRDD</li> </ul>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#fetch-wait-time","title":"fetch wait time","text":""},{"location":"physical-operators/ShuffleExchangeExec/#local-blocks-read","title":"local blocks read","text":""},{"location":"physical-operators/ShuffleExchangeExec/#local-bytes-read","title":"local bytes read","text":""},{"location":"physical-operators/ShuffleExchangeExec/#records-read","title":"records read","text":""},{"location":"physical-operators/ShuffleExchangeExec/#remote-blocks-read","title":"remote blocks read","text":""},{"location":"physical-operators/ShuffleExchangeExec/#remote-bytes-read","title":"remote bytes read","text":""},{"location":"physical-operators/ShuffleExchangeExec/#remote-bytes-read-to-disk","title":"remote bytes read to disk","text":""},{"location":"physical-operators/ShuffleExchangeExec/#write-metrics","title":"Write Metrics <p>The write metrics are used to (passed directly to) create a ShuffleDependency (that in turn is used to create a ShuffleWriteProcessor)</p>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#shuffle-bytes-written","title":"shuffle bytes written","text":""},{"location":"physical-operators/ShuffleExchangeExec/#shuffle-records-written","title":"shuffle records written","text":""},{"location":"physical-operators/ShuffleExchangeExec/#shuffle-write-time","title":"shuffle write time","text":""},{"location":"physical-operators/ShuffleExchangeExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p>  <p><code>doExecute</code> gives a ShuffledRowRDD (with the ShuffleDependency and read performance metrics).</p> <p><code>doExecute</code> uses cachedShuffleRDD to avoid multiple execution.</p>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#unsaferowserializer","title":"UnsafeRowSerializer <pre><code>serializer: Serializer\n</code></pre> <p><code>serializer</code> is an <code>UnsafeRowSerializer</code> with the following properties:</p> <ul> <li>Number of fields is the number of the output attributes of the child physical operator</li> <li>dataSize performance metric</li> </ul> <p><code>serializer</code> is used when <code>ShuffleExchangeExec</code> operator is requested for a ShuffleDependency.</p>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#shuffledrowrdd","title":"ShuffledRowRDD <pre><code>cachedShuffleRDD: ShuffledRowRDD\n</code></pre> <p><code>cachedShuffleRDD</code> is an internal registry for the ShuffledRowRDD that <code>ShuffleExchangeExec</code> operator creates when executed.</p> <p>The purpose of <code>cachedShuffleRDD</code> is to avoid multiple executions of <code>ShuffleExchangeExec</code> operator when it is reused in a query plan:</p> <ul> <li><code>cachedShuffleRDD</code> is uninitialized (<code>null</code>) when <code>ShuffleExchangeExec</code> operator is created</li> <li><code>cachedShuffleRDD</code> is assigned a <code>ShuffledRowRDD</code> when <code>ShuffleExchangeExec</code> operator is executed for the first time</li> </ul>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#shuffledependency","title":"ShuffleDependency <pre><code>shuffleDependency: ShuffleDependency[Int, InternalRow, InternalRow]\n</code></pre> <p><code>shuffleDependency</code> is a <code>ShuffleDependency</code> (Spark Core).</p>  Lazy Value <p><code>shuffleDependency</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>ShuffleExchangeExec</code> operator creates a ShuffleDependency for the following:</p> <ul> <li>Input RDD</li> <li>Output attributes of the child physical operator</li> <li>Output partitioning</li> <li>UnsafeRowSerializer</li> <li>writeMetrics</li> </ul>  <p><code>shuffleDependency</code> is used when:</p> <ul> <li><code>CustomShuffleReaderExec</code> physical operator is executed</li> <li>OptimizeShuffleWithLocalRead is requested to <code>getPartitionSpecs</code></li> <li>OptimizeSkewedJoin physical optimization is executed</li> <li><code>ShuffleExchangeExec</code> physical operator is executed and requested for MapOutputStatistics</li> </ul>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#mapoutputstatisticsfuture","title":"mapOutputStatisticsFuture <pre><code>mapOutputStatisticsFuture: Future[MapOutputStatistics]\n</code></pre> <p><code>mapOutputStatisticsFuture</code> requests the inputRDD for the number of partitions:</p> <ul> <li> <p>If there are zero partitions, <code>mapOutputStatisticsFuture</code> simply creates an already completed <code>Future</code> (Scala) with <code>null</code> value</p> </li> <li> <p>Otherwise, <code>mapOutputStatisticsFuture</code> requests the operator's <code>SparkContext</code> to <code>submitMapStage</code> (Spark Core) with the ShuffleDependency.</p> </li> </ul>  Lazy Value <p><code>mapOutputStatisticsFuture</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>mapOutputStatisticsFuture</code>\u00a0is part of the ShuffleExchangeLike abstraction.</p>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#creating-shufflewriteprocessor","title":"Creating ShuffleWriteProcessor <pre><code>createShuffleWriteProcessor(\n  metrics: Map[String, SQLMetric]): ShuffleWriteProcessor\n</code></pre> <p><code>createShuffleWriteProcessor</code> creates a <code>ShuffleWriteProcessor</code> (Spark Core) to plug in <code>SQLShuffleWriteMetricsReporter</code> (as a <code>ShuffleWriteMetricsReporter</code> (Spark Core)) with the write metrics.</p>  <p><code>createShuffleWriteProcessor</code> is used when:</p> <ul> <li><code>ShuffleExchangeExec</code> operator is executed (and requested for a ShuffleDependency)</li> </ul>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#runtime-statistics","title":"Runtime Statistics <pre><code>runtimeStatistics: Statistics\n</code></pre> <p><code>runtimeStatistics</code> is part of the ShuffleExchangeLike abstraction.</p>  <p><code>runtimeStatistics</code> creates a Statistics with the value of the following metrics.</p>    Statistics Metric     Output size data size   Number of rows shuffle records written","text":""},{"location":"physical-operators/ShuffleExchangeExec/#creating-shuffledependency","title":"Creating ShuffleDependency <pre><code>prepareShuffleDependency(\n  rdd: RDD[InternalRow],\n  outputAttributes: Seq[Attribute],\n  newPartitioning: Partitioning,\n  serializer: Serializer,\n  writeMetrics: Map[String, SQLMetric]): ShuffleDependency[Int, InternalRow, InternalRow]\n</code></pre> <p><code>prepareShuffleDependency</code> is used when:</p> <ul> <li>CollectLimitExec, ShuffleExchangeExec and <code>TakeOrderedAndProjectExec</code> physical operators are executed</li> </ul>  <p><code>prepareShuffleDependency</code> creates a <code>ShuffleDependency</code> (Apache Spark) with the following:</p> <ul> <li>rddWithPartitionIds</li> <li><code>PartitionIdPassthrough</code> serializer (with the number of partitions as the Partitioner)</li> <li>The input <code>Serializer</code> (e.g., the UnsafeRowSerializer for <code>ShuffleExchangeExec</code> physical operators)</li> <li>Write Metrics</li> </ul>  Write Metrics for ShuffleExchangeExec <p>For <code>ShuffleExchangeExec</code>s, the write metrics are the following:</p> <ul> <li>shuffle bytes written</li> <li>shuffle records written</li> <li>shuffle write time</li> </ul>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#partitioner","title":"Partitioner <p><code>prepareShuffleDependency</code> determines a <code>Partitioner</code> based on the given <code>newPartitioning</code> Partitioning:</p> <ul> <li>For RoundRobinPartitioning, <code>prepareShuffleDependency</code> creates a <code>HashPartitioner</code> for the same number of partitions</li> <li>For HashPartitioning, <code>prepareShuffleDependency</code> creates a <code>Partitioner</code> for the same number of partitions and <code>getPartition</code> that is an \"identity\"</li> <li>For <code>RangePartitioning</code>, <code>prepareShuffleDependency</code> creates a <code>RangePartitioner</code> for the same number of partitions and <code>samplePointsPerPartitionHint</code> based on spark.sql.execution.rangeExchange.sampleSizePerPartition configuration property</li> <li>For SinglePartition, <code>prepareShuffleDependency</code> creates a <code>Partitioner</code> with <code>1</code> for the number of partitions and <code>getPartition</code> that always gives <code>0</code></li> </ul>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#getpartitionkeyextractor","title":"getPartitionKeyExtractor <pre><code>getPartitionKeyExtractor(): InternalRow =&gt; Any\n</code></pre> <p><code>getPartitionKeyExtractor</code> uses the given <code>newPartitioning</code> Partitioning:</p> <ul> <li>For RoundRobinPartitioning,...FIXME</li> <li>For HashPartitioning,...FIXME</li> <li>For <code>RangePartitioning</code>,...FIXME</li> <li>For SinglePartition,...FIXME</li> </ul>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#isroundrobin-flag","title":"isRoundRobin Flag <p><code>prepareShuffleDependency</code> determines whether \"this\" is <code>isRoundRobin</code> or not based on the given <code>newPartitioning</code> partitioning. It is <code>isRoundRobin</code> when the partitioning is a <code>RoundRobinPartitioning</code> with more than one partition.</p>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#rddwithpartitionids-rdd","title":"rddWithPartitionIds RDD <p><code>prepareShuffleDependency</code> creates a <code>rddWithPartitionIds</code>:</p> <ol> <li>Firstly, <code>prepareShuffleDependency</code> determines a <code>newRdd</code> based on <code>isRoundRobin</code> flag and spark.sql.execution.sortBeforeRepartition configuration property. When both are enabled (<code>true</code>), <code>prepareShuffleDependency</code> sorts partitions (using a <code>UnsafeExternalRowSorter</code>) Otherwise, <code>prepareShuffleDependency</code> returns the given <code>RDD[InternalRow]</code> (unchanged).</li> <li>Secondly, <code>prepareShuffleDependency</code> determines whether this is <code>isOrderSensitive</code> or not. This is <code>isOrderSensitive</code> when <code>isRoundRobin</code> flag is enabled (<code>true</code>) while spark.sql.execution.sortBeforeRepartition configuration property is not (<code>false</code>).</li> </ol> <p><code>prepareShuffleDependency</code>...FIXME</p>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#demo","title":"Demo","text":""},{"location":"physical-operators/ShuffleExchangeExec/#shuffleexchangeexec-and-repartition-logical-operator","title":"ShuffleExchangeExec and Repartition Logical Operator <pre><code>val q = spark.range(6).repartition(2)\nscala&gt; q.explain(extended = true)\n== Parsed Logical Plan ==\nRepartition 2, true\n+- Range (0, 6, step=1, splits=Some(16))\n\n== Analyzed Logical Plan ==\nid: bigint\nRepartition 2, true\n+- Range (0, 6, step=1, splits=Some(16))\n\n== Optimized Logical Plan ==\nRepartition 2, true\n+- Range (0, 6, step=1, splits=Some(16))\n\n== Physical Plan ==\nExchange RoundRobinPartitioning(2), false, [id=#8]\n+- *(1) Range (0, 6, step=1, splits=16)\n</code></pre>","text":""},{"location":"physical-operators/ShuffleExchangeExec/#shuffleexchangeexec-and-repartitionbyexpression-logical-operator","title":"ShuffleExchangeExec and RepartitionByExpression Logical Operator <pre><code>val q = spark.range(6).repartition(2, 'id % 2)\nscala&gt; q.explain(extended = true)\n== Parsed Logical Plan ==\n'RepartitionByExpression [('id % 2)], 2\n+- Range (0, 6, step=1, splits=Some(16))\n\n== Analyzed Logical Plan ==\nid: bigint\nRepartitionByExpression [(id#4L % cast(2 as bigint))], 2\n+- Range (0, 6, step=1, splits=Some(16))\n\n== Optimized Logical Plan ==\nRepartitionByExpression [(id#4L % 2)], 2\n+- Range (0, 6, step=1, splits=Some(16))\n\n== Physical Plan ==\nExchange hashpartitioning((id#4L % 2), 2), false, [id=#17]\n+- *(1) Range (0, 6, step=1, splits=16)\n</code></pre>","text":""},{"location":"physical-operators/ShuffleExchangeLike/","title":"ShuffleExchangeLike Physical Operators","text":"<p><code>ShuffleExchangeLike</code>\u00a0is an extension of the Exchange abstraction for physical operators.</p>"},{"location":"physical-operators/ShuffleExchangeLike/#contract","title":"Contract","text":""},{"location":"physical-operators/ShuffleExchangeLike/#getshufflerdd","title":"getShuffleRDD <pre><code>getShuffleRDD(\n  partitionSpecs: Array[ShufflePartitionSpec]): RDD[_]\n</code></pre> <p><code>RDD</code> (Spark Core)</p> <p>Used when:</p> <ul> <li><code>CustomShuffleReaderExec</code> physical operator is requested for the <code>shuffleRDD</code></li> </ul>","text":""},{"location":"physical-operators/ShuffleExchangeLike/#mapoutputstatisticsfuture","title":"mapOutputStatisticsFuture <pre><code>mapOutputStatisticsFuture: Future[MapOutputStatistics]\n</code></pre> <p><code>MapOutputStatistics</code> (Apache Spark)</p> <p>Used when:</p> <ul> <li><code>ShuffleQueryStageExec</code> physical operator is requested to doMaterialize and cancel</li> </ul>","text":""},{"location":"physical-operators/ShuffleExchangeLike/#number-of-mappers","title":"Number of Mappers <pre><code>numMappers: Int\n</code></pre> <p>Used when:</p> <ul> <li><code>OptimizeShuffleWithLocalRead</code> physical optimization is requested for the shuffle partition specification</li> </ul>","text":""},{"location":"physical-operators/ShuffleExchangeLike/#number-of-partitions","title":"Number of Partitions <pre><code>numPartitions: Int\n</code></pre> <p>Used when:</p> <ul> <li><code>OptimizeShuffleWithLocalRead</code> physical optimization is requested for the shuffle partition specification</li> </ul>","text":""},{"location":"physical-operators/ShuffleExchangeLike/#runtime-statistics","title":"Runtime Statistics <pre><code>runtimeStatistics: Statistics\n</code></pre> <p>Statistics with data size and row count</p> <p>See:</p> <ul> <li>ShuffleExchangeExec</li> </ul> <p>Used when:</p> <ul> <li><code>ShuffleQueryStageExec</code> physical operator is requested for the runtime statistics</li> </ul>","text":""},{"location":"physical-operators/ShuffleExchangeLike/#shuffleorigin","title":"ShuffleOrigin <pre><code>shuffleOrigin: ShuffleOrigin\n</code></pre> <p>ShuffleOrigin</p> <p>Used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested for the finalStageOptimizerRules</li> <li><code>CoalesceShufflePartitions</code> physical optimization is requested to supportCoalesce</li> <li><code>OptimizeShuffleWithLocalRead</code> physical optimization is requested to supportLocalReader</li> <li><code>ShuffleStage</code> utility is used to destructure a <code>SparkPlan</code> to a <code>ShuffleStageInfo</code></li> </ul>","text":""},{"location":"physical-operators/ShuffleExchangeLike/#implementations","title":"Implementations","text":"<ul> <li>ShuffleExchangeExec</li> </ul>"},{"location":"physical-operators/ShuffleExchangeLike/#submitting-shuffle-job","title":"Submitting Shuffle Job <pre><code>submitShuffleJob: Future[MapOutputStatistics]\n</code></pre> <p><code>submitShuffleJob</code> executes a query to materialize the mapOutputStatisticsFuture.</p>  Final Method <p><code>submitShuffleJob</code> is a Scala final method and may not be overridden in subclasses.</p> <p>Learn more in the Scala Language Specification.</p>   <p><code>submitShuffleJob</code>\u00a0is used when:</p> <ul> <li><code>ShuffleQueryStageExec</code> adaptive leaf physical operator is requested for the shuffleFuture</li> </ul>","text":""},{"location":"physical-operators/ShuffleOrigin/","title":"ShuffleOrigin","text":"<p><code>ShuffleOrigin</code> describes where a ShuffleExchangeLike comes from (where it was created).</p> <p><code>ShuffleOrigin</code> is used (supported) by AQEShuffleReadRules.</p>"},{"location":"physical-operators/ShuffleOrigin/#ensure_requirements","title":"ENSURE_REQUIREMENTS <p>The default <code>ShuffleOrigin</code> of ShuffleExchangeExec physical operator</p> <p>Supported by AQEShuffleReadRules:</p> <ul> <li>CoalesceShufflePartitions</li> <li>OptimizeShuffleWithLocalRead</li> <li>OptimizeSkewedJoin</li> </ul>","text":""},{"location":"physical-operators/ShuffleOrigin/#rebalance_partitions_by_col","title":"REBALANCE_PARTITIONS_BY_COL <p>Supported by AQEShuffleReadRules:</p> <ul> <li>CoalesceShufflePartitions</li> <li>OptimizeSkewInRebalancePartitions</li> </ul>","text":""},{"location":"physical-operators/ShuffleOrigin/#rebalance_partitions_by_none","title":"REBALANCE_PARTITIONS_BY_NONE <p>Supported by AQEShuffleReadRules:</p> <ul> <li>CoalesceShufflePartitions</li> <li>OptimizeShuffleWithLocalRead</li> <li>OptimizeSkewInRebalancePartitions</li> </ul>","text":""},{"location":"physical-operators/ShuffleOrigin/#repartition_by_col","title":"REPARTITION_BY_COL <p>Indicates a user-specified repartition operator</p> <p>Used to create a ShuffleExchangeExec physical operator when:</p> <ul> <li> <p>BasicOperators execution planning strategy is executed to plan the following logical operators:</p> </li> <li> <p>RepartitionByExpression with the partition expressions defined</p> </li> <li> <p>WithCTEStrategy execution planning strategy is executed to plan the following logical operators:</p> <ul> <li>CTERelationRef</li> </ul> </li> </ul> <p>Supported by AQEShuffleReadRules:</p> <ul> <li>CoalesceShufflePartitions</li> </ul>","text":""},{"location":"physical-operators/ShuffleOrigin/#repartition_by_num","title":"REPARTITION_BY_NUM <p>Indicates a user-specified repartition operator with a defined partition number</p> <p>Used to create a ShuffleExchangeExec physical operator when:</p> <ul> <li> <p>BasicOperators execution planning strategy is executed to plan the following logical operators:</p> <ul> <li>Repartition with <code>shuffle</code> enabled</li> <li>RepartitionByExpression with the number of partitions defined</li> </ul> </li> </ul>","text":""},{"location":"physical-operators/ShuffleQueryStageExec/","title":"ShuffleQueryStageExec Adaptive Leaf Physical Operator","text":"<p><code>ShuffleQueryStageExec</code> is a QueryStageExec with either ShuffleExchangeExec or ReusedExchangeExec child operators.</p>"},{"location":"physical-operators/ShuffleQueryStageExec/#creating-instance","title":"Creating Instance","text":"<p><code>ShuffleQueryStageExec</code> takes the following to be created:</p> <ul> <li> Query Stage ID <li> SparkPlan <li> Canonicalized SparkPlan <p><code>ShuffleQueryStageExec</code> is created when:</p> <ul> <li>AdaptiveSparkPlanExec physical operator is requested to newQueryStage (for a ShuffleExchangeExec)</li> <li><code>ShuffleQueryStageExec</code> physical operator is requested to newReuseInstance</li> </ul>"},{"location":"physical-operators/ShuffleQueryStageExec/#shuffleexchangelike","title":"ShuffleExchangeLike <pre><code>shuffle: ShuffleExchangeLike\n</code></pre> <p><code>ShuffleQueryStageExec</code> initializes the <code>shuffle</code> internal registry when created.</p> <p><code>ShuffleQueryStageExec</code> assumes that the given physical operator is either a ShuffleExchangeLike or a ReusedExchangeExec and extracts the <code>ShuffleExchangeLike</code>.</p> <p>If not, <code>ShuffleQueryStageExec</code> throws an <code>IllegalStateException</code>:</p> <pre><code>wrong plan for shuffle stage:\n[tree]\n</code></pre> <p><code>shuffle</code> is used when:</p> <ul> <li><code>AQEShuffleReadExec</code> unary physical operator is requested for the shuffleRDD</li> <li>CoalesceShufflePartitions physical optimization is executed</li> <li>OptimizeShuffleWithLocalRead physical optimization is executed</li> <li>OptimizeSkewedJoin physical optimization is executed</li> <li>OptimizeSkewInRebalancePartitions physical optimization is executed</li> <li><code>ShuffleQueryStageExec</code> leaf physical operator is requested for the shuffle MapOutputStatistics, newReuseInstance and getRuntimeStatistics</li> </ul>","text":""},{"location":"physical-operators/ShuffleQueryStageExec/#shuffle-mapoutputstatistics-future","title":"Shuffle MapOutputStatistics Future <pre><code>shuffleFuture: Future[MapOutputStatistics]\n</code></pre> <p><code>shuffleFuture</code> requests the ShuffleExchangeLike to submit a shuffle job (and eventually produce a <code>MapOutputStatistics</code> (Apache Spark)).</p>  Lazy Value <p><code>shuffleFuture</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>shuffleFuture</code> is used when:</p> <ul> <li><code>ShuffleQueryStageExec</code> is requested to materialize and cancel</li> </ul>","text":""},{"location":"physical-operators/ShuffleQueryStageExec/#materializing","title":"Materializing <pre><code>doMaterialize(): Future[Any]\n</code></pre> <p><code>doMaterialize</code> is part of the QueryStageExec abstraction.</p>  <p><code>doMaterialize</code> returns the Shuffle MapOutputStatistics Future.</p>","text":""},{"location":"physical-operators/ShuffleQueryStageExec/#cancelling","title":"Cancelling <pre><code>cancel(): Unit\n</code></pre> <p><code>cancel</code> is part of the QueryStageExec abstraction.</p>  <p><code>cancel</code> cancels the Shuffle MapOutputStatistics Future (unless already completed).</p>","text":""},{"location":"physical-operators/ShuffleQueryStageExec/#new-shufflequerystageexec-instance-for-reuse","title":"New ShuffleQueryStageExec Instance for Reuse <pre><code>newReuseInstance(\n  newStageId: Int,\n  newOutput: Seq[Attribute]): QueryStageExec\n</code></pre> <p><code>newReuseInstance</code> is part of the QueryStageExec abstraction.</p>  <p><code>newReuseInstance</code> creates a new <code>ShuffleQueryStageExec</code> with the following:</p>    Attribute Value     Query Stage ID The given <code>newStageId</code>   SparkPlan A new ReusedExchangeExec with the given <code>newOutput</code> and the ShuffleExchangeLike    <p><code>newReuseInstance</code> requests the new <code>ShuffleQueryStageExec</code> to use the _resultOption.</p>","text":""},{"location":"physical-operators/ShuffleQueryStageExec/#mapoutputstatistics","title":"MapOutputStatistics <pre><code>mapStats: Option[MapOutputStatistics]\n</code></pre> <p><code>mapStats</code> assumes that the MapOutputStatistics is already available or throws an <code>AssertionError</code>:</p> <pre><code>assertion failed: ShuffleQueryStageExec should already be ready\n</code></pre> <p><code>mapStats</code> returns the MapOutputStatistics.</p>  <p><code>mapStats</code> is used when:</p> <ul> <li><code>AQEShuffleReadExec</code> unary physical operator is requested for the partitionDataSizes</li> <li>DynamicJoinSelection adaptive optimization is executed (and selectJoinStrategy)</li> <li>OptimizeShuffleWithLocalRead adaptive physical optimization is executed (and canUseLocalShuffleRead)</li> <li>CoalesceShufflePartitions, OptimizeSkewedJoin and OptimizeSkewInRebalancePartitions adaptive physical optimizations are executed</li> </ul>","text":""},{"location":"physical-operators/ShuffleQueryStageExec/#runtime-statistics","title":"Runtime Statistics <pre><code>getRuntimeStatistics: Statistics\n</code></pre> <p><code>getRuntimeStatistics</code> is part of the QueryStageExec abstraction.</p>  <p><code>getRuntimeStatistics</code> requests the ShuffleExchangeLike for the runtime statistics.</p>","text":""},{"location":"physical-operators/ShuffleQueryStageExec/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.ShuffleQueryStageExec.name = org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec\nlogger.ShuffleQueryStageExec.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"physical-operators/ShuffleSpec/","title":"ShuffleSpec","text":"<p><code>ShuffleSpec</code> is an abstraction of shuffle specifications for the following physical optimizations:</p> <ul> <li>EnsureRequirements</li> <li>OptimizeSkewedJoin</li> <li>AdaptiveSparkPlanExec</li> </ul>"},{"location":"physical-operators/ShuffleSpec/#contract","title":"Contract","text":""},{"location":"physical-operators/ShuffleSpec/#cancreatepartitioning","title":"canCreatePartitioning <pre><code>canCreatePartitioning: Boolean\n</code></pre> <p>Used when:</p> <ul> <li>EnsureRequirements physical optimization is executed</li> </ul>","text":""},{"location":"physical-operators/ShuffleSpec/#iscompatiblewith","title":"isCompatibleWith <pre><code>isCompatibleWith(\n  other: ShuffleSpec): Boolean\n</code></pre> <p>Used when:</p> <ul> <li>EnsureRequirements physical optimization is executed</li> <li><code>ValidateRequirements</code> is requested to <code>validate</code> (for OptimizeSkewedJoin physical optimization and AdaptiveSparkPlanExec physical operator)</li> </ul>","text":""},{"location":"physical-operators/ShuffleSpec/#numpartitions","title":"numPartitions <pre><code>numPartitions: Int\n</code></pre> <p>Used when:</p> <ul> <li>EnsureRequirements physical optimization is executed</li> </ul>","text":""},{"location":"physical-operators/ShuffleSpec/#implementations","title":"Implementations","text":"<ul> <li><code>HashShuffleSpec</code></li> <li><code>KeyGroupedShuffleSpec</code></li> <li><code>RangeShuffleSpec</code></li> <li><code>ShuffleSpecCollection</code></li> <li><code>SinglePartitionShuffleSpec</code></li> </ul>"},{"location":"physical-operators/ShuffledHashJoinExec/","title":"ShuffledHashJoinExec Physical Operator","text":"<p><code>ShuffledHashJoinExec</code> is a ShuffledJoin and HashJoin for shuffle-hash join.</p> <p><code>ShuffledHashJoinExec</code> supports Java code generation for all the join types except FullOuter (variable prefix: <code>shj</code>).</p>"},{"location":"physical-operators/ShuffledHashJoinExec/#performance-metrics","title":"Performance Metrics    Key Name (in web UI) Description     numOutputRows number of output rows Number of output rows   buildDataSize data size of build side    buildTime time to build hash map","text":""},{"location":"physical-operators/ShuffledHashJoinExec/#creating-instance","title":"Creating Instance <p><code>ShuffledHashJoinExec</code> takes the following to be created:</p> <ul> <li> Left Key Expressions <li> Right Key Expressions <li> Join Type <li> <code>BuildSide</code> <li> Optional Join Condition Expression <li> Left Child Physical Operator <li> Right Child Physical Operator <li>isSkewJoin flag</li>  <p><code>ShuffledHashJoinExec</code> is created when:</p> <ul> <li>JoinSelection execution planning strategy is executed (createShuffleHashJoin)</li> </ul>","text":""},{"location":"physical-operators/ShuffledHashJoinExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p>  <p>Danger</p> <p>Review Me</p>  <p><code>doExecute</code> requests streamedPlan physical operator to execute (and generate a <code>RDD[InternalRow]</code>).</p> <p><code>doExecute</code> requests buildPlan physical operator to execute (and generate a <code>RDD[InternalRow]</code>).</p> <p><code>doExecute</code> requests streamedPlan physical operator's <code>RDD[InternalRow]</code> to zip partition-wise with buildPlan physical operator's <code>RDD[InternalRow]</code> (using <code>RDD.zipPartitions</code> method with <code>preservesPartitioning</code> flag disabled).</p> <p><code>doExecute</code> uses <code>RDD.zipPartitions</code> with a function applied to zipped partitions that takes two iterators of rows from the partitions of <code>streamedPlan</code> and <code>buildPlan</code>.</p> <p>For every partition (and pairs of rows from the RDD), the function buildHashedRelation on the partition of <code>buildPlan</code> and join the <code>streamedPlan</code> partition iterator, the HashedRelation, numOutputRows and avgHashProbe metrics.</p>","text":""},{"location":"physical-operators/ShuffledHashJoinExec/#building-hashedrelation","title":"Building HashedRelation <pre><code>buildHashedRelation(\n  iter: Iterator[InternalRow]): HashedRelation\n</code></pre>  <p>Danger</p> <p>Review Me</p>  <p><code>buildHashedRelation</code> creates a HashedRelation (for the input <code>iter</code> iterator of <code>InternalRows</code>, buildKeys and the current <code>TaskMemoryManager</code>).</p> <p><code>buildHashedRelation</code> records the time to create the <code>HashedRelation</code> as buildTime.</p> <p><code>buildHashedRelation</code> requests the <code>HashedRelation</code> for estimatedSize that is recorded as buildDataSize.</p> <p><code>buildHashedRelation</code> is used when:</p> <ul> <li><code>ShuffledHashJoinExec</code> is requested to execute (when streamedPlan and buildPlan physical operators are executed and their RDDs zipped partition-wise using <code>RDD.zipPartitions</code> method</li> </ul>","text":""},{"location":"physical-operators/ShuffledHashJoinExec/#supportcodegen","title":"supportCodegen <pre><code>supportCodegen: Boolean\n</code></pre> <p><code>supportCodegen</code> is part of the CodegenSupport abstraction.</p> <p><code>supportCodegen</code> is <code>true</code> for all the join types except FullOuter.</p>","text":""},{"location":"physical-operators/ShuffledHashJoinExec/#needcopyresult","title":"needCopyResult <pre><code>needCopyResult: Boolean\n</code></pre> <p><code>needCopyResult</code> is part of the CodegenSupport abstraction.</p> <p><code>needCopyResult</code> is <code>true</code>.</p>","text":""},{"location":"physical-operators/ShuffledHashJoinExec/#demo","title":"Demo <p>Enable <code>DEBUG</code> logging level for ExtractEquiJoinKeys logger to see the join condition and the left and right join keys.</p> <pre><code>// Use ShuffledHashJoinExec's selection requirements\n// 1. Disable auto broadcasting\n// JoinSelection (canBuildLocalHashMap specifically) requires that\n// plan.stats.sizeInBytes &lt; autoBroadcastJoinThreshold * numShufflePartitions\n// That gives that autoBroadcastJoinThreshold has to be at least 1\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 1)\n\nscala&gt; println(spark.sessionState.conf.numShufflePartitions)\n200\n\n// 2. Disable preference on SortMergeJoin\nspark.conf.set(\"spark.sql.join.preferSortMergeJoin\", false)\n\nval dataset = Seq(\n  (0, \"playing\"),\n  (1, \"with\"),\n  (2, \"ShuffledHashJoinExec\")\n).toDF(\"id\", \"token\")\n// Self LEFT SEMI join\nval q = dataset.join(dataset, Seq(\"id\"), \"leftsemi\")\n\nval sizeInBytes = q.queryExecution.optimizedPlan.stats.sizeInBytes\nscala&gt; println(sizeInBytes)\n72\n\n// 3. canBuildLeft is on for leftsemi\n\n// the right join side is at least three times smaller than the left side\n// Even though it's a self LEFT SEMI join there are two different join sides\n// How is that possible?\n\n// BINGO! ShuffledHashJoin is here!\n\n// Enable DEBUG logging level\nimport org.apache.log4j.{Level, Logger}\nval logger = \"org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys\"\nLogger.getLogger(logger).setLevel(Level.DEBUG)\n\n// ShuffledHashJoin with BuildRight\nscala&gt; q.explain\n== Physical Plan ==\nShuffledHashJoin [id#37], [id#41], LeftSemi, BuildRight\n:- Exchange hashpartitioning(id#37, 200)\n:  +- LocalTableScan [id#37, token#38]\n+- Exchange hashpartitioning(id#41, 200)\n   +- LocalTableScan [id#41]\n\nscala&gt; println(q.queryExecution.executedPlan.numberedTreeString)\n00 ShuffledHashJoin [id#37], [id#41], LeftSemi, BuildRight\n01 :- Exchange hashpartitioning(id#37, 200)\n02 :  +- LocalTableScan [id#37, token#38]\n03 +- Exchange hashpartitioning(id#41, 200)\n04    +- LocalTableScan [id#41]\n</code></pre> <p><code>doExecute</code> generates a <code>ZippedPartitionsRDD2</code> that you can see in a RDD lineage.</p> <pre><code>scala&gt; println(q.queryExecution.toRdd.toDebugString)\n(200) ZippedPartitionsRDD2[8] at toRdd at &lt;console&gt;:26 []\n  |   ShuffledRowRDD[3] at toRdd at &lt;console&gt;:26 []\n  +-(3) MapPartitionsRDD[2] at toRdd at &lt;console&gt;:26 []\n     |  MapPartitionsRDD[1] at toRdd at &lt;console&gt;:26 []\n     |  ParallelCollectionRDD[0] at toRdd at &lt;console&gt;:26 []\n  |   ShuffledRowRDD[7] at toRdd at &lt;console&gt;:26 []\n  +-(3) MapPartitionsRDD[6] at toRdd at &lt;console&gt;:26 []\n     |  MapPartitionsRDD[5] at toRdd at &lt;console&gt;:26 []\n     |  ParallelCollectionRDD[4] at toRdd at &lt;console&gt;:26 []\n</code></pre>","text":""},{"location":"physical-operators/ShuffledHashJoinExec/#hashjoin","title":"HashJoin <p><code>ShuffledHashJoinExec</code> is a HashJoin.</p>","text":""},{"location":"physical-operators/ShuffledHashJoinExec/#buildside","title":"BuildSide <pre><code>buildSide: BuildSide\n</code></pre> <p><code>ShuffledHashJoinExec</code> is given a <code>BuildSide</code> when created.</p> <p><code>buildSide</code> is part of the HashJoin abstraction.</p>","text":""},{"location":"physical-operators/ShuffledHashJoinExec/#preparerelation","title":"prepareRelation <pre><code>prepareRelation(\n  ctx: CodegenContext): HashedRelationInfo\n</code></pre> <p><code>prepareRelation</code> requests the given CodegenContext for a code to reference this <code>ShuffledHashJoinExec</code> (with <code>plan</code> name).</p> <p><code>prepareRelation</code> requests the given CodegenContext for a code with relationTerm mutable state (with the <code>HashedRelation</code> class name, <code>relation</code> variable name, etc.)</p> <p>In the end, <code>prepareRelation</code> creates a <code>HashedRelationInfo</code>.</p> <pre><code>\n</code></pre> <p><code>prepareRelation</code> is part of the HashJoin abstraction.</p>","text":""},{"location":"physical-operators/ShuffledHashJoinExec/#shuffledjoin","title":"ShuffledJoin <p><code>ShuffledHashJoinExec</code> is a ShuffledJoin and performs a hash join of two child relations by first shuffling the data using the join keys.</p>","text":""},{"location":"physical-operators/ShuffledHashJoinExec/#isskewjoin-flag","title":"isSkewJoin Flag <p><code>ShuffledHashJoinExec</code> can be given <code>isSkewJoin</code> flag when created. It is assumed disabled (<code>false</code>) by default.</p> <p><code>isSkewJoin</code> can only be enabled (<code>true</code>) when <code>OptimizeSkewedJoin</code> adaptive physical optimization is requested to optimize a skew join.</p> <p><code>isSkewJoin</code> is part of the ShuffledJoin abstraction.</p>","text":""},{"location":"physical-operators/ShuffledJoin/","title":"ShuffledJoin Physical Operators","text":"<p><code>ShuffledJoin</code> is an extension of the JoinCodegenSupport abstraction for join operators that shuffle two child relations using some join keys.</p>"},{"location":"physical-operators/ShuffledJoin/#contract","title":"Contract","text":""},{"location":"physical-operators/ShuffledJoin/#isskewjoin-flag","title":"isSkewJoin Flag <pre><code>isSkewJoin: Boolean\n</code></pre> <p>Used when:</p> <ul> <li><code>ShuffledJoin</code> is requested for node name and requiredChildDistribution</li> </ul>","text":""},{"location":"physical-operators/ShuffledJoin/#implementations","title":"Implementations","text":"<ul> <li>ShuffledHashJoinExec</li> <li>SortMergeJoinExec</li> </ul>"},{"location":"physical-operators/ShuffledJoin/#node-name","title":"Node Name <pre><code>nodeName: String\n</code></pre> <p><code>nodeName</code> adds <code>(skew=true)</code> suffix to the default node name when the isSkewJoin flag is enabled.</p> <p><code>nodeName</code> is part of the TreeNode abstraction.</p>","text":""},{"location":"physical-operators/ShuffledJoin/#required-child-output-distribution","title":"Required Child Output Distribution <pre><code>requiredChildDistribution: Seq[Distribution]\n</code></pre> <p><code>requiredChildDistribution</code> are HashClusteredDistributions for the left and right keys.</p> <p><code>requiredChildDistribution</code> is part of the SparkPlan abstraction.</p>","text":""},{"location":"physical-operators/SortAggregateExec/","title":"SortAggregateExec Aggregate Physical Operator","text":"<p><code>SortAggregateExec</code> is an aggregate unary physical operator for sort-based aggregation.</p> <p></p>"},{"location":"physical-operators/SortAggregateExec/#creating-instance","title":"Creating Instance","text":"<p><code>SortAggregateExec</code> takes the following to be created:</p> <ul> <li> (optional) Required Child Distribution Expressions <li> Grouping NamedExpressions <li> AggregateExpressions <li> Aggregate Attributes <li> Initial Input Buffer Offset <li> Result NamedExpressions <li> Child Physical Operator <p><code>SortAggregateExec</code> is created\u00a0when:</p> <ul> <li><code>AggUtils</code> utility is used to create a physical operator for aggregation</li> </ul>"},{"location":"physical-operators/SortAggregateExec/#performance-metrics","title":"Performance Metrics    Key Name (in web UI)     numOutputRows number of output rows","text":""},{"location":"physical-operators/SortAggregateExec/#demo","title":"Demo <p>Let's disable preference for ObjectHashAggregateExec physical operator (using the spark.sql.execution.useObjectHashAggregateExec configuration property).</p> <pre><code>spark.conf.set(\"spark.sql.execution.useObjectHashAggregateExec\", false)\nassert(spark.sessionState.conf.useObjectHashAggregation == false)\n</code></pre> <pre><code>val names = Seq(\n  (0, \"zero\"),\n  (1, \"one\"),\n  (2, \"two\")).toDF(\"num\", \"name\")\n</code></pre> <p>Let's use immutable data types for <code>aggregateBufferAttributes</code> (so HashAggregateExec physical operator will not be selected).</p> <pre><code>val q = names\n  .withColumn(\"initial\", substring('name, 0, 1))\n  .groupBy('initial)\n  .agg(collect_set('initial))\n</code></pre> <pre><code>scala&gt; q.explain\n== Physical Plan ==\nSortAggregate(key=[initial#160], functions=[collect_set(initial#160, 0, 0)])\n+- *(2) Sort [initial#160 ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(initial#160, 200), ENSURE_REQUIREMENTS, [id=#122]\n      +- SortAggregate(key=[initial#160], functions=[partial_collect_set(initial#160, 0, 0)])\n         +- *(1) Sort [initial#160 ASC NULLS FIRST], false, 0\n            +- *(1) LocalTableScan [initial#160]\n</code></pre> <pre><code>import q.queryExecution.optimizedPlan\nimport org.apache.spark.sql.catalyst.plans.logical.Aggregate\nval aggLog = optimizedPlan.asInstanceOf[Aggregate]\nimport org.apache.spark.sql.catalyst.planning.PhysicalAggregation\nimport org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression\nval (_, aggregateExpressions: Seq[AggregateExpression], _, _) = PhysicalAggregation.unapply(aggLog).get\nval aggregateBufferAttributes =\n  aggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes)\n</code></pre> <pre><code>import org.apache.spark.sql.execution.aggregate.HashAggregateExec\nassert(HashAggregateExec.supportsAggregate(aggregateBufferAttributes) == false)\n</code></pre>","text":""},{"location":"physical-operators/SortBasedAggregationIterator/","title":"SortBasedAggregationIterator","text":"<p><code>SortBasedAggregationIterator</code> is an AggregationIterator for SortAggregateExec physical operator.</p>"},{"location":"physical-operators/SortBasedAggregationIterator/#creating-instance","title":"Creating Instance","text":"<p><code>SortBasedAggregationIterator</code> takes the following to be created:</p> <ul> <li> Partition ID <li> Grouping NamedExpressions <li> Value Attributes <li> Input Iterator of InternalRows <li> AggregateExpressions <li> Aggregate Attributes <li> Initial input buffer offset <li> Result NamedExpressions <li> Function to create a new <code>MutableProjection</code> given expressions and attributes (<code>(Seq[Expression], Seq[Attribute]) =&gt; MutableProjection</code>) <li> <code>numOutputRows</code> SQLMetric <p><code>SortBasedAggregationIterator</code> is created\u00a0when:</p> <ul> <li><code>SortAggregateExec</code> physical operator is requested to doExecute</li> </ul>"},{"location":"physical-operators/SortExec/","title":"SortExec Physical Operator","text":"<p><code>SortExec</code> is a unary physical operator that (most importantly) represents Sort logical operator at execution.</p>"},{"location":"physical-operators/SortExec/#creating-instance","title":"Creating Instance","text":"<p><code>SortExec</code> takes the following to be created:</p> <ul> <li> SortOrder expressions <li> <code>global</code> flag <li> Child physical operator <li> <code>testSpillFrequency</code> <p><code>SortExec</code> is created\u00a0when:</p> <ul> <li>BasicOperators execution planning strategy is executed (with a Sort logical operator)</li> <li><code>FileFormatWriter</code> utility is used to write out a query result</li> <li>EnsureRequirements physical optimization is executed</li> </ul>"},{"location":"physical-operators/SortExec/#performance-metrics","title":"Performance Metrics","text":""},{"location":"physical-operators/SortExec/#peak-memory","title":"peak memory","text":""},{"location":"physical-operators/SortExec/#sort-time","title":"sort time","text":""},{"location":"physical-operators/SortExec/#spill-size","title":"spill size <p>Number of in-memory bytes spilled by this operator at execution (while an UnsafeExternalRowSorter was sorting the rows in a partition)</p> <p>The <code>spill size</code> metric is computed using <code>TaskMetrics</code> (Spark Core) and is a difference of the metric before and after sorting.</p>","text":""},{"location":"physical-operators/SortExec/#radix-sort","title":"Radix Sort <p><code>SortExec</code> operator uses the spark.sql.sort.enableRadixSort configuration property when creating an UnsafeExternalRowSorter.</p>","text":""},{"location":"physical-operators/SortExec/#blockingoperatorwithcodegen","title":"BlockingOperatorWithCodegen <p><code>SortExec</code> is a <code>BlockingOperatorWithCodegen</code>.</p>","text":""},{"location":"physical-operators/SortExec/#codegensupport","title":"CodegenSupport <p><code>SortExec</code> supports Java code generation (indirectly as a <code>BlockingOperatorWithCodegen</code>).</p>","text":""},{"location":"physical-operators/SortExec/#output-data-ordering-requirements","title":"Output Data Ordering Requirements <pre><code>outputOrdering: Seq[SortOrder]\n</code></pre> <p><code>outputOrdering</code> is the given SortOrder expressions.</p> <p><code>outputOrdering</code>\u00a0is part of the SparkPlan abstraction.</p>","text":""},{"location":"physical-operators/SortExec/#required-child-output-distribution","title":"Required Child Output Distribution <pre><code>requiredChildDistribution: Seq[Distribution]\n</code></pre> <p><code>requiredChildDistribution</code> is a OrderedDistribution (with the SortOrder expressions) with the global flag enabled or a UnspecifiedDistribution.</p> <p><code>requiredChildDistribution</code>\u00a0is part of the SparkPlan abstraction.</p>","text":""},{"location":"physical-operators/SortExec/#physical-optimizations","title":"Physical Optimizations","text":""},{"location":"physical-operators/SortExec/#optimizeskewedjoin","title":"OptimizeSkewedJoin <p>OptimizeSkewedJoin physical optimization is used to optimize skewed SortMergeJoinExecs (with <code>SortExec</code> operators) in Adaptive Query Execution.</p>","text":""},{"location":"physical-operators/SortExec/#removeredundantsorts","title":"RemoveRedundantSorts <p><code>SortExec</code> operators can be removed from a physical query plan by RemoveRedundantSorts physical optimization (with spark.sql.execution.removeRedundantSorts enabled).</p>","text":""},{"location":"physical-operators/SortExec/#creating-unsafeexternalrowsorter","title":"Creating UnsafeExternalRowSorter <pre><code>createSorter(): UnsafeExternalRowSorter\n</code></pre> <p><code>createSorter</code> creates a BaseOrdering for the sortOrders and the output schema.</p> <p><code>createSorter</code> uses spark.sql.sort.enableRadixSort configuration property to enable radix sort when possible.</p>  Radix Sort, Sort Order and Supported Data Types <p>Radix sort can be used when there is exactly one sortOrder that can be satisfied (based on the data type) with a radix sort on the prefix.</p> <p>The following data types are supported:</p> <ul> <li><code>AnsiIntervalType</code></li> <li><code>BooleanType</code></li> <li><code>ByteType</code></li> <li><code>DateType</code></li> <li><code>DecimalType</code> (up to <code>18</code> precision digits)</li> <li><code>DoubleType</code></li> <li><code>FloatType</code></li> <li><code>IntegerType</code></li> <li><code>LongType</code></li> <li><code>ShortType</code></li> <li><code>TimestampNTZType</code></li> <li><code>TimestampType</code></li> </ul>  <p><code>createSorter</code> creates an UnsafeExternalRowSorter with the following:</p> <ul> <li><code>spark.buffer.pageSize</code> (default: <code>64MB</code>) for a page size</li> <li>Whether radix sort can be used</li> <li>others</li> </ul>  <p><code>createSorter</code>\u00a0is used when:</p> <ul> <li><code>SortExec</code> is executed (one per partition)</li> <li><code>FileFormatWriter</code> utility is used to write out a query result</li> </ul>","text":""},{"location":"physical-operators/SortExec/#demo","title":"Demo <pre><code>val q = Seq((0, \"zero\"), (1, \"one\")).toDF(\"id\", \"name\").sort('id)\nval qe = q.queryExecution\n\nval logicalPlan = qe.analyzed\nscala&gt; println(logicalPlan.numberedTreeString)\n00 Sort [id#72 ASC NULLS FIRST], true\n01 +- Project [_1#69 AS id#72, _2#70 AS name#73]\n02    +- LocalRelation [_1#69, _2#70]\n\n// BasicOperators does the conversion of Sort logical operator to SortExec\nval sparkPlan = qe.sparkPlan\nscala&gt; println(sparkPlan.numberedTreeString)\n00 Sort [id#72 ASC NULLS FIRST], true, 0\n01 +- LocalTableScan [id#72, name#73]\n\n// SortExec supports Whole-Stage Code Generation\nval executedPlan = qe.executedPlan\nscala&gt; println(executedPlan.numberedTreeString)\n00 *(1) Sort [id#72 ASC NULLS FIRST], true, 0\n01 +- Exchange rangepartitioning(id#72 ASC NULLS FIRST, 200)\n02    +- LocalTableScan [id#72, name#73]\n\nimport org.apache.spark.sql.execution.SortExec\nval sortExec = executedPlan.collect { case se: SortExec =&gt; se }.head\nassert(sortExec.isInstanceOf[SortExec])\n</code></pre>","text":""},{"location":"physical-operators/SortMergeJoinExec/","title":"SortMergeJoinExec Physical Operator","text":"<p><code>SortMergeJoinExec</code> is a shuffle-based join physical operator for sort-merge join (with the left join keys being orderable).</p> <p><code>SortMergeJoinExec</code> supports Java code generation (variable prefix: <code>smj</code>) for inner and cross joins.</p>"},{"location":"physical-operators/SortMergeJoinExec/#performance-metrics","title":"Performance Metrics    Key Name (in web UI) Description     numOutputRows number of output rows Number of output rows","text":""},{"location":"physical-operators/SortMergeJoinExec/#creating-instance","title":"Creating Instance <p><code>ShuffledHashJoinExec</code> takes the following to be created:</p> <ul> <li> Left Join Key Expressions <li> Right Join Key Expressions <li> JoinType <li> Optional Join Expression <li> Left Physical Operator <li> Right Physical Operator <li>isSkewJoin flag</li>  <p><code>ShuffledHashJoinExec</code> is created\u00a0when:</p> <ul> <li>JoinSelection execution planning strategy is executed (for equi joins with the left join keys orderable).</li> </ul>","text":""},{"location":"physical-operators/SortMergeJoinExec/#physical-optimizations","title":"Physical Optimizations <ol> <li> <p>OptimizeSkewedJoin is used to optimize skewed sort-merge joins</p> </li> <li> <p>CoalesceBucketsInJoin physical optimization is used for...FIXME</p> </li> </ol>","text":""},{"location":"physical-operators/SortMergeJoinExec/#isskewjoin-flag","title":"isSkewJoin Flag <p><code>ShuffledHashJoinExec</code> can be given <code>isSkewJoin</code> flag when created.</p> <p><code>isSkewJoin</code> flag is <code>false</code> by default.</p> <p><code>isSkewJoin</code> flag is <code>true</code> when:</p> <ul> <li>FIXME</li> </ul> <p><code>isSkewJoin</code> is used for the following:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"physical-operators/SortMergeJoinExec/#node-name","title":"Node Name <pre><code>nodeName: String\n</code></pre> <p><code>nodeName</code>\u00a0is part of the TreeNode abstraction.</p> <p><code>nodeName</code>\u00a0adds <code>(skew=true)</code> suffix to the default node name for isSkewJoin flag on.</p>","text":""},{"location":"physical-operators/SortMergeJoinExec/#required-child-output-distribution","title":"Required Child Output Distribution <pre><code>requiredChildDistribution: Seq[Distribution]\n</code></pre> <p><code>requiredChildDistribution</code>\u00a0is part of the SparkPlan abstraction.</p> <p>HashClusteredDistributions of left and right join keys.</p>    Left Child Right Child     HashClusteredDistribution (per left join key expressions) HashClusteredDistribution (per right join key expressions)","text":""},{"location":"physical-operators/SortMergeJoinExec/#demo","title":"Demo <pre><code>// Disable auto broadcasting so Broadcast Hash Join won't take precedence\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n\nval tokens = Seq(\n  (0, \"playing\"),\n  (1, \"with\"),\n  (2, \"SortMergeJoinExec\")\n).toDF(\"id\", \"token\")\n\n// all data types are orderable\nscala&gt; tokens.printSchema\nroot\n |-- id: integer (nullable = false)\n |-- token: string (nullable = true)\n\n// Spark Planner prefers SortMergeJoin over Shuffled Hash Join\nscala&gt; println(spark.conf.get(\"spark.sql.join.preferSortMergeJoin\"))\ntrue\n\nval q = tokens.join(tokens, Seq(\"id\"), \"inner\")\nscala&gt; q.explain\n== Physical Plan ==\n*(3) Project [id#5, token#6, token#10]\n+- *(3) SortMergeJoin [id#5], [id#9], Inner\n   :- *(1) Sort [id#5 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(id#5, 200)\n   :     +- LocalTableScan [id#5, token#6]\n   +- *(2) Sort [id#9 ASC NULLS FIRST], false, 0\n      +- ReusedExchange [id#9, token#10], Exchange hashpartitioning(id#5, 200)\n</code></pre> <pre><code>scala&gt; q.queryExecution.debug.codegen\nFound 3 WholeStageCodegen subtrees.\n== Subtree 1 / 3 ==\n*Project [id#5, token#6, token#11]\n+- *SortMergeJoin [id#5], [id#10], Inner\n   :- *Sort [id#5 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(id#5, 200)\n   :     +- LocalTableScan [id#5, token#6]\n   +- *Sort [id#10 ASC NULLS FIRST], false, 0\n      +- ReusedExchange [id#10, token#11], Exchange hashpartitioning(id#5, 200)\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIterator(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 006 */   private Object[] references;\n/* 007 */   private scala.collection.Iterator[] inputs;\n/* 008 */   private scala.collection.Iterator smj_leftInput;\n/* 009 */   private scala.collection.Iterator smj_rightInput;\n/* 010 */   private InternalRow smj_leftRow;\n/* 011 */   private InternalRow smj_rightRow;\n/* 012 */   private int smj_value2;\n/* 013 */   private org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray smj_matches;\n/* 014 */   private int smj_value3;\n/* 015 */   private int smj_value4;\n/* 016 */   private UTF8String smj_value5;\n/* 017 */   private boolean smj_isNull2;\n/* 018 */   private org.apache.spark.sql.execution.metric.SQLMetric smj_numOutputRows;\n/* 019 */   private UnsafeRow smj_result;\n/* 020 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder smj_holder;\n/* 021 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter smj_rowWriter;\n...\n</code></pre>","text":""},{"location":"physical-operators/SortMergeJoinScanner/","title":"SortMergeJoinScanner","text":""},{"location":"physical-operators/SortMergeJoinScanner/#creating-instance","title":"Creating Instance","text":"<p><code>SortMergeJoinScanner</code> takes the following to be created:</p> <ul> <li> <code>streamedKeyGenerator</code> Projection <li> <code>bufferedKeyGenerator</code> Projection <li> Key <code>Ordering[InternalRow]</code> <li> Streamed <code>RowIterator</code> <li> Buffered <code>RowIterator</code> <li> inMemoryThreshold <li> spillThreshold <li> eagerCleanupResources (<code>() =&gt; Unit</code>) <li> <code>onlyBufferFirstMatch</code> flag (default: <code>false</code>) <p><code>SortMergeJoinScanner</code> is created when:</p> <ul> <li><code>SortMergeJoinExec</code> physical operator is requested to doExecute (for <code>InnerLike</code>, <code>LeftOuter</code>, <code>RightOuter</code>, <code>LeftSemi</code>, <code>LeftAnti</code>, <code>ExistenceJoin</code> joins)</li> </ul>"},{"location":"physical-operators/SortMergeJoinScanner/#externalappendonlyunsaferowarray","title":"ExternalAppendOnlyUnsafeRowArray <p><code>SortMergeJoinScanner</code> creates an ExternalAppendOnlyUnsafeRowArray (with the given inMemoryThreshold and spillThreshold thresholds) when created.</p> <p>The <code>ExternalAppendOnlyUnsafeRowArray</code> is requested to add an UnsafeRow in bufferMatchingRows.</p> <p>The <code>ExternalAppendOnlyUnsafeRowArray</code> is cleared in findNextInnerJoinRows, findNextOuterJoinRows and bufferMatchingRows.</p>","text":""},{"location":"physical-operators/SortMergeJoinScanner/#getbufferedmatches","title":"getBufferedMatches <pre><code>getBufferedMatches: ExternalAppendOnlyUnsafeRowArray\n</code></pre> <p><code>getBufferedMatches</code> returns the bufferedMatches.</p> <p><code>getBufferedMatches</code> is used when:</p> <ul> <li><code>SortMergeJoinExec</code> physical operator is requested to doExecute (for <code>InnerLike</code>, <code>LeftSemi</code>, <code>LeftAnti</code>, <code>ExistenceJoin</code> joins), advanceStream and advanceBufferUntilBoundConditionSatisfied</li> </ul>","text":""},{"location":"physical-operators/SortMergeJoinScanner/#buffermatchingrows","title":"bufferMatchingRows <pre><code>bufferMatchingRows(): Unit\n</code></pre> <p><code>bufferMatchingRows</code>...FIXME</p> <p><code>bufferMatchingRows</code> is used when:</p> <ul> <li><code>SortMergeJoinScanner</code> is requested to findNextInnerJoinRows and findNextOuterJoinRows</li> </ul>","text":""},{"location":"physical-operators/SparkPlan/","title":"SparkPlan \u2014 Physical Operators of Structured Query","text":"<p><code>SparkPlan</code> is an extension of the QueryPlan abstraction for physical operators that can be executed (to generate <code>RDD[InternalRow]</code> that Spark can execute).</p> <p><code>SparkPlan</code> can build a physical query plan (query execution plan).</p> <p><code>SparkPlan</code> is a recursive data structure in Spark SQL's Catalyst tree manipulation framework and as such represents a single physical operator in a physical execution query plan as well as a physical execution query plan itself (i.e. a tree of physical operators in a query plan of a structured query).</p> <p></p> High-Level Dataset API <p>A structured query can be expressed using Spark SQL's high-level Dataset API for Scala, Java, Python, R or good ol' SQL.</p> <p>A <code>SparkPlan</code> physical operator is a Catalyst tree node that may have zero or more child physical operators.</p> Catalyst Framework <p>A structured query is basically a single <code>SparkPlan</code> physical operator with child physical operators.</p> <p>Spark SQL uses Catalyst tree manipulation framework to compose nodes to build a tree of (logical or physical) operators that, in this particular case, is composing <code>SparkPlan</code> physical operator nodes to build the physical execution plan tree of a structured query.</p> explain Operator <p>Use explain operator to see the execution plan of a structured query.</p> <pre><code>val q = // your query here\nq.explain\n</code></pre> <p>You may also access the execution plan of a <code>Dataset</code> using its queryExecution property.</p> <pre><code>val q = // your query here\nq.queryExecution.sparkPlan\n</code></pre> <p><code>SparkPlan</code> assumes that concrete physical operators define doExecute (with optional hooks).</p>"},{"location":"physical-operators/SparkPlan/#contract","title":"Contract","text":""},{"location":"physical-operators/SparkPlan/#doexecute","title":"doExecute <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p>Generates a distributed computation (that is a runtime representation of the operator in particular and a structured query in general) as an RDD of InternalRows (<code>RDD[InternalRow]</code>) and thus execute.</p> <p>Part of execute</p>","text":""},{"location":"physical-operators/SparkPlan/#implementations","title":"Implementations","text":"<ul> <li>BaseSubqueryExec</li> <li>CodegenSupport</li> <li>UnaryExecNode</li> <li>V2CommandExec</li> <li>others</li> </ul>"},{"location":"physical-operators/SparkPlan/#final-methods","title":"Final Methods","text":"<p><code>SparkPlan</code> has the following <code>final</code> methods that prepare execution environment and pass calls to corresponding methods (that constitute SparkPlan abstraction).</p>"},{"location":"physical-operators/SparkPlan/#execute","title":"execute <pre><code>execute(): RDD[InternalRow]\n</code></pre> <p>\"Executes\" a physical operator (and its children) that triggers physical query planning and in the end generates an <code>RDD</code> of InternalRows (<code>RDD[InternalRow]</code>).</p> <p>Used mostly when <code>QueryExecution</code> is requested for the &lt;&gt; (that describes a distributed computation using Spark Core's RDD). <p>execute is called when <code>QueryExecution</code> is requested for the RDD that is Spark Core's physical execution plan (as a RDD lineage) that triggers query execution (i.e. physical planning, but not execution of the plan) and could be considered execution of a structured query.</p> <p>The could part above refers to the fact that the final execution of a structured query happens only when a RDD action is executed on the RDD of a structured query. And hence the need for Spark SQL's high-level Dataset API in which the Dataset operators simply execute a RDD action on the corresponding RDD. Easy, isn't it?</p> <p>Internally, <code>execute</code> first &lt;&gt; and eventually requests it to &lt;&gt;.  <p>Note</p> <p>Executing <code>doExecute</code> in a named scope happens only after the operator is &lt;&gt; followed by &lt;&gt;.","text":""},{"location":"physical-operators/SparkPlan/#executebroadcast","title":"executeBroadcast <pre><code>executeBroadcast[T](): broadcast.Broadcast[T]\n</code></pre> <p>Calls doExecuteBroadcast.</p>","text":""},{"location":"physical-operators/SparkPlan/#executecolumnar","title":"executeColumnar <pre><code>executeColumnar(): RDD[ColumnarBatch]\n</code></pre> <p><code>executeColumnar</code> executeQuery with doExecuteColumnar.</p> <p><code>executeColumnar</code> is used when:</p> <ul> <li>ColumnarToRowExec unary physical operator is executed and requested for the inputRDDs</li> </ul>","text":""},{"location":"physical-operators/SparkPlan/#executequery","title":"executeQuery <pre><code>executeQuery[T](\n  query: =&gt; T): T\n</code></pre> <p>Executes the physical operator in a single RDD scope (all RDDs created during execution of the physical operator have the same scope).</p> <p><code>executeQuery</code> executes the input <code>query</code> block after the following (in order):</p> <ol> <li>Preparing for Execution</li> <li>Waiting for Subqueries to Finish</li> </ol> <p><code>executeQuery</code> is used when:</p> <ul> <li><code>SparkPlan</code> is requested for the following:</li> <li>execute (the input <code>query</code> is doExecute)</li> <li>executeBroadcast (the input <code>query</code> is doExecuteBroadcast)</li> <li>executeColumnar (the input <code>query</code> is doExecuteColumnar())</li> <li><code>CodegenSupport</code> is requested to produce a Java source code of a physical operator (with the input <code>query</code> being doProduce)</li> <li><code>QueryStageExec</code> is requested to materialize (with the input <code>query</code> being doMaterialize)</li> </ul>","text":""},{"location":"physical-operators/SparkPlan/#executeWrite","title":"executeWrite <pre><code>executeWrite(\n  writeFilesSpec: WriteFilesSpec): RDD[WriterCommitMessage]\n</code></pre> <p><code>executeWrite</code> executeQuery followed by doExecuteWrite.</p> <p>Used when:</p> <ul> <li><code>FileFormatWriter</code> is requested to executeWrite</li> </ul>","text":""},{"location":"physical-operators/SparkPlan/#prepare","title":"prepare <pre><code>prepare(): Unit\n</code></pre> <p>Prepares a physical operator for execution</p> <p><code>prepare</code> is used mainly when a physical operator is requested to &lt;&gt; <p><code>prepare</code> is also used recursively for every child physical operator (down the physical plan) and when a physical operator is requested to &lt;&gt;.  <p>Note</p> <p><code>prepare</code> is idempotent, i.e. can be called multiple times with no change to the final result. It uses &lt;&gt; internal flag to execute the physical operator once only.  <p>Internally, <code>prepare</code> calls &lt;&gt; of its children before &lt;&gt; and &lt;&gt;.","text":""},{"location":"physical-operators/SparkPlan/#logicallink","title":"logicalLink <pre><code>logicalLink: Option[LogicalPlan]\n</code></pre> <p><code>logicalLink</code> gets the value of <code>logical_plan</code> node tag (if defined) or <code>logical_plan_inherited</code> node tag.</p> <p>In other words, <code>logicalLink</code> is the LogicalPlan this <code>SparkPlan</code> was planned from.</p> <p><code>logicalLink</code> is used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested for the final physical plan, setLogicalLinkForNewQueryStage and replaceWithQueryStagesInLogicalPlan</li> <li>InsertAdaptiveSparkPlan, PlanAdaptiveDynamicPruningFilters and DisableUnnecessaryBucketedScan physical optimizations are executed</li> </ul>","text":""},{"location":"physical-operators/SparkPlan/#extension-hooks","title":"Extension Hooks","text":""},{"location":"physical-operators/SparkPlan/#doexecutebroadcast","title":"doExecuteBroadcast <pre><code>doExecuteBroadcast[T](): broadcast.Broadcast[T]\n</code></pre> <p><code>doExecuteBroadcast</code> reports an <code>UnsupportedOperationException</code> by default:</p> <pre><code>[nodeName] does not implement doExecuteBroadcast\n</code></pre> <p>Part of executeBroadcast</p>","text":""},{"location":"physical-operators/SparkPlan/#doexecutecolumnar","title":"doExecuteColumnar <pre><code>doExecuteColumnar(): RDD[ColumnarBatch]\n</code></pre> <p><code>doExecuteColumnar</code> throws an <code>IllegalStateException</code> by default:</p> <pre><code>Internal Error [class] has column support mismatch:\n[this]\n</code></pre> <p>Part of Columnar Execution</p>","text":""},{"location":"physical-operators/SparkPlan/#doExecuteWrite","title":"doExecuteWrite <pre><code>doExecuteWrite(\n  writeFilesSpec: WriteFilesSpec): RDD[WriterCommitMessage]\n</code></pre>  <p>Throws SparkException by Default</p> <p><code>doExecuteColumnar</code> throws an <code>SparkException</code> by default and is supposed to be overriden by the implementations.</p> <pre><code>Internal Error [class] has write support mismatch:\n[this]\n</code></pre>  <p>Used by executeWrite</p> <p>See:</p> <ul> <li>WriteFilesExec</li> </ul>","text":""},{"location":"physical-operators/SparkPlan/#doprepare","title":"doPrepare <pre><code>doPrepare(): Unit\n</code></pre> <p><code>doPrepare</code> prepares the physical operator for execution</p> <p>Part of prepare</p>","text":""},{"location":"physical-operators/SparkPlan/#required-child-output-distribution","title":"Required Child Output Distribution <pre><code>requiredChildDistribution: Seq[Distribution]\n</code></pre> <p>Required Partition Requirements (child output distributions) of the input data, i.e. how child physical operators' output is split across partitions.</p> <p>Defaults to a UnspecifiedDistribution for all of the child operators.</p> <p>Used when:</p> <ul> <li>EnsureRequirements physical optimization is executed</li> </ul>","text":""},{"location":"physical-operators/SparkPlan/#required-child-ordering","title":"Required Child Ordering <pre><code>requiredChildOrdering: Seq[Seq[SortOrder]]\n</code></pre> <p>Specifies required sort ordering for each partition requirement (from child operators)</p> <p>Defaults to no sort ordering for all of the physical operator's child.</p> <p>Used when:</p> <ul> <li>EnsureRequirements physical optimization is executed</li> </ul>","text":""},{"location":"physical-operators/SparkPlan/#preparing-subqueries","title":"Preparing Subqueries <pre><code>prepareSubqueries(): Unit\n</code></pre> <p><code>prepareSubqueries</code>...FIXME</p> <p>Part of prepare</p>","text":""},{"location":"physical-operators/SparkPlan/#waiting-for-subqueries-to-finish","title":"Waiting for Subqueries to Finish <pre><code>waitForSubqueries(): Unit\n</code></pre> <p><code>waitForSubqueries</code> requests every subquery expression (in runningSubqueries registry) to update.</p> <p>In the end, <code>waitForSubqueries</code> clears up the runningSubqueries registry.</p> <p><code>waitForSubqueries</code> is used when:</p> <ul> <li>A physical operator is requested to executeQuery</li> </ul>","text":""},{"location":"physical-operators/SparkPlan/#naming-convention-exec-suffix","title":"Naming Convention (Exec Suffix) <p>The naming convention of physical operators in Spark's source code is to have their names end with the Exec prefix, e.g. <code>DebugExec</code> or LocalTableScanExec that is however removed when the operator is displayed, e.g. in web UI.</p>","text":""},{"location":"physical-operators/SparkPlan/#physical-operator-execution-pipeline","title":"Physical Operator Execution Pipeline <p>The entry point to Physical Operator Execution Pipeline is execute.</p> <p></p> <p>When &lt;&gt;, <code>SparkPlan</code> &lt;&gt; in a named scope (for visualization purposes, e.g. web UI) that triggers &lt;&gt; of the children physical operators first followed by &lt;&gt; and finally &lt;&gt; methods. After &lt;&gt;, &lt;&gt; method is eventually triggered. <p></p> <p>The result of &lt;&gt; a <code>SparkPlan</code> is an <code>RDD</code> of InternalRows (<code>RDD[InternalRow]</code>).  <p>Note</p> <p>Executing a structured query is simply a translation of the higher-level Dataset-based description to an RDD-based runtime representation that Spark will in the end execute (once an Dataset action is used).</p>  <p>CAUTION: FIXME Picture between Spark SQL's Dataset =&gt; Spark Core's RDD</p>","text":""},{"location":"physical-operators/SparkPlan/#decoding-byte-arrays-back-to-unsaferows","title":"Decoding Byte Arrays Back to UnsafeRows <pre><code>decodeUnsafeRows(\n  bytes: Array[Byte]): Iterator[InternalRow]\n</code></pre> <p><code>decodeUnsafeRows</code>...FIXME</p>  <p><code>decodeUnsafeRows</code> is used when:</p> <ul> <li><code>SparkPlan</code> is requested to executeCollect, executeCollectIterator, executeToIterator, and executeTake</li> </ul>","text":""},{"location":"physical-operators/SparkPlan/#compressing-rdd-partitions-of-unsaferows-to-byte-arrays","title":"Compressing RDD Partitions (of UnsafeRows) to Byte Arrays <pre><code>getByteArrayRdd(\n  n: Int = -1,\n  takeFromEnd: Boolean = false): RDD[(Long, Array[Byte])]\n</code></pre> <p><code>getByteArrayRdd</code> executes this operator and maps over partitions (Spark Core) using the partition processing function.</p>  <p>Note</p> <p><code>getByteArrayRdd</code> adds a <code>MapPartitionsRDD</code> (Spark Core) to the RDD lineage.</p>   <p><code>getByteArrayRdd</code> is used when:</p> <ul> <li><code>SparkPlan</code> is requested to executeCollect, executeCollectIterator, executeToIterator, and executeTake</li> </ul>","text":""},{"location":"physical-operators/SparkPlan/#partition-processing-function","title":"Partition Processing Function <p>The function creates a <code>CompressionCodec</code> (Spark Core) (to compress the output to a byte array).</p> <p>The function takes UnsafeRows from the partition (one at a time) and writes them out (compressed) to the output as a series of the size and the bytes of a single row.</p> <p>Once all rows have been processed, the function writes out <code>-1</code> to the output, flushes and closes it.</p> <p>In the end, the function returns the count of the rows written out and the byte array (with the rows).</p>","text":""},{"location":"physical-operators/SparkPlan/#preparing-sparkplan-for-query-execution","title":"Preparing SparkPlan for Query Execution <pre><code>executeQuery[T](\n  query: =&gt; T): T\n</code></pre> <p><code>executeQuery</code> executes the input <code>query</code> in a named scope (i.e. so that all RDDs created will have the same scope for visualization like web UI).</p> <p>Internally, <code>executeQuery</code> calls &lt;&gt; and &lt;&gt; followed by executing <code>query</code>. <p><code>executeQuery</code> is executed as part of &lt;&gt;, &lt;&gt; and when <code>CodegenSupport</code>-enabled physical operator produces a Java source code.","text":""},{"location":"physical-operators/SparkPlan/#broadcasting-result-of-structured-query","title":"Broadcasting Result of Structured Query <pre><code>executeBroadcast[T](): broadcast.Broadcast[T]\n</code></pre> <p><code>executeBroadcast</code> returns the result of a structured query as a broadcast variable.</p> <p>Internally, <code>executeBroadcast</code> calls doExecuteBroadcast inside executeQuery.</p> <p><code>executeBroadcast</code> is used when:</p> <ul> <li><code>SubqueryBroadcastExec</code> physical operator is requested for <code>relationFuture</code></li> <li>QueryStageExec physical operator is requested for doExecuteBroadcast</li> <li>DebugExec physical operator is requested for doExecuteBroadcast</li> <li>ReusedExchangeExec physical operator is requested for doExecuteBroadcast</li> <li>BroadcastHashJoinExec physical operator is requested to doExecute, multipleOutputForOneInput, prepareBroadcast and for doExecuteBroadcast</li> <li>BroadcastNestedLoopJoinExec physical operator is requested for doExecuteBroadcast</li> </ul>","text":""},{"location":"physical-operators/SparkPlan/#performance-metrics","title":"Performance Metrics <pre><code>metrics: Map[String, SQLMetric] = Map.empty\n</code></pre> <p><code>metrics</code> is the SQLMetrics by their names.</p> <p>By default, <code>metrics</code> contains no <code>SQLMetrics</code> (i.e. <code>Map.empty</code>).</p> <p><code>metrics</code> is used when...FIXME</p>","text":""},{"location":"physical-operators/SparkPlan/#taking-first-n-unsaferows","title":"Taking First N UnsafeRows <pre><code>executeTake(\n  n: Int): Array[InternalRow]\n</code></pre> <p><code>executeTake</code> gives an array of up to <code>n</code> first internal rows.</p> <p></p> <p><code>executeTake</code> gets an RDD of byte array of <code>n</code> unsafe rows and scans the RDD partitions one by one until <code>n</code> is reached or all partitions were processed.</p> <p><code>executeTake</code> runs Spark jobs that take all the elements from requested number of partitions, starting from the 0<sup>th</sup> partition and increasing their number by spark.sql.limit.scaleUpFactor property (but minimum twice as many).</p>  <p>Note</p> <p><code>executeTake</code> uses <code>SparkContext.runJob</code> to run a Spark job.</p>  <p>In the end, <code>executeTake</code> decodes the unsafe rows.</p>  <p>Note</p> <p><code>executeTake</code> gives an empty collection when <code>n</code> is 0 (and no Spark job is executed).</p>   <p>Note</p> <p><code>executeTake</code> may take and decode more unsafe rows than really needed since all unsafe rows from a partition are read (if the partition is included in the scan).</p>","text":""},{"location":"physical-operators/SparkPlan/#demo","title":"Demo <pre><code>import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS\nspark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 10)\n\n// 8 groups over 10 partitions\n// only 7 partitions are with numbers\nval nums = spark.\n  range(start = 0, end = 20, step = 1, numPartitions = 4).\n  repartition($\"id\" % 8)\n\nimport scala.collection.Iterator\nval showElements = (it: Iterator[java.lang.Long]) =&gt; {\n  val ns = it.toSeq\n  import org.apache.spark.TaskContext\n  val pid = TaskContext.get.partitionId\n  println(s\"[partition: $pid][size: ${ns.size}] ${ns.mkString(\" \")}\")\n}\n// ordered by partition id manually for demo purposes\nscala&gt; nums.foreachPartition(showElements)\n[partition: 0][size: 2] 4 12\n[partition: 1][size: 2] 7 15\n[partition: 2][size: 0]\n[partition: 3][size: 0]\n[partition: 4][size: 0]\n[partition: 5][size: 5] 0 6 8 14 16\n[partition: 6][size: 0]\n[partition: 7][size: 3] 3 11 19\n[partition: 8][size: 5] 2 5 10 13 18\n[partition: 9][size: 3] 1 9 17\n\nscala&gt; println(spark.sessionState.conf.limitScaleUpFactor)\n4\n\n// Think how many Spark jobs will the following queries run?\n// Answers follow\nscala&gt; nums.take(13)\nres0: Array[Long] = Array(4, 12, 7, 15, 0, 6, 8, 14, 16, 3, 11, 19, 2)\n\n// The number of Spark jobs = 3\n\nscala&gt; nums.take(5)\nres34: Array[Long] = Array(4, 12, 7, 15, 0)\n\n// The number of Spark jobs = 4\n\nscala&gt; nums.take(3)\nres38: Array[Long] = Array(4, 12, 7)\n\n// The number of Spark jobs = 2\n</code></pre>  <p><code>executeTake</code> is used when:</p> <ul> <li>CollectLimitExec physical operator is requested to executeCollect</li> <li>AnalyzeColumnCommand logical command is executed</li> </ul>","text":""},{"location":"physical-operators/SparkPlan/#executing-physical-operator-and-collecting-results","title":"Executing Physical Operator and Collecting Results <pre><code>executeCollect(): Array[InternalRow]\n</code></pre> <p><code>executeCollect</code> executes the physical operator and compresses partitions of UnsafeRows as byte arrays (that gives a <code>RDD[(Long, Array[Byte])]</code> and so no real Spark jobs may have been submitted).</p> <p><code>executeCollect</code> runs a Spark job to <code>collect</code> the elements of the RDD and for every pair in the result (of a count and bytes per partition) decodes the byte arrays back to UnsafeRows and stores the decoded arrays together as the final <code>Array[InternalRow]</code>.</p>  <p>Note</p> <p><code>executeCollect</code> runs a Spark job using Spark Core's <code>RDD.collect</code> operator.</p>   <p>Array[InternalRow]</p> <p><code>executeCollect</code> returns <code>Array[InternalRow]</code>, i.e. keeps the internal representation of rows unchanged and does not convert rows to JVM types.</p>  <p><code>executeCollect</code> is used when:</p> <ul> <li> <p><code>Dataset</code> is requested for the Dataset.md#logicalPlan[logical plan] (being a single Command.md[Command] or their <code>Union</code>)</p> </li> <li> <p>spark-sql-dataset-operators.md#explain[explain] and spark-sql-dataset-operators.md#count[count] operators are executed</p> </li> <li> <p><code>Dataset</code> is requested to <code>collectFromPlan</code></p> </li> <li> <p><code>SubqueryExec</code> is requested to SubqueryExec.md#doPrepare[prepare for execution] (and initializes SubqueryExec.md#relationFuture[relationFuture] for the first time)</p> </li> <li> <p><code>SparkPlan</code> is requested to &lt;&gt;  <li> <p><code>ScalarSubquery</code> and <code>InSubquery</code> plan expressions are requested to <code>updateResult</code></p> </li>","text":""},{"location":"physical-operators/SparkPlan/#output-data-partitioning-requirements","title":"Output Data Partitioning Requirements <pre><code>outputPartitioning: Partitioning\n</code></pre> <p><code>outputPartitioning</code> specifies the output data partitioning requirements, i.e. a hint for the Spark Physical Optimizer for the number of partitions the output of the physical operator should be split across.</p> <p><code>outputPartitioning</code> defaults to a <code>UnknownPartitioning</code> (with <code>0</code> partitions).</p>  <p><code>outputPartitioning</code> is used when:</p> <ul> <li> <p>EnsureRequirements physical optimization is executed</p> </li> <li> <p><code>Dataset</code> is requested to checkpoint</p> </li> </ul>","text":""},{"location":"physical-operators/SparkPlan/#output-data-ordering-requirements","title":"Output Data Ordering Requirements <pre><code>outputOrdering: Seq[SortOrder]\n</code></pre> <p><code>outputOrdering</code> specifies the output data ordering requirements of the physical operator, i.e. a hint for the Spark Physical Optimizer for the sorting (ordering) of the data (within and across partitions).</p> <p><code>outputOrdering</code> defaults to no ordering (<code>Nil</code>).</p>  <p><code>outputOrdering</code> is used when:</p> <ul> <li> <p>EnsureRequirements physical optimization is executed</p> </li> <li> <p><code>Dataset</code> is requested to checkpoint</p> </li> <li> <p><code>FileFormatWriter</code> is used to write out a query result</p> </li> </ul>","text":""},{"location":"physical-operators/SparkPlan/#checking-support-for-columnar-processing","title":"Checking Support for Columnar Processing <pre><code>supportsColumnar: Boolean\n</code></pre> <p><code>supportsColumnar</code> specifies whether the physical operator supports Columnar Execution.</p> <p><code>supportsColumnar</code> is <code>false</code> by default (and is expected to be overriden by implementations).</p> <p>See:</p> <ul> <li>AdaptiveSparkPlanExec</li> <li>FileSourceScanExec</li> <li>InMemoryTableScanExec</li> <li>DataSourceV2ScanExecBase</li> </ul>  <p><code>supportsColumnar</code> is used when:</p> <ul> <li>ApplyColumnarRulesAndInsertTransitions physical optimization is executed</li> <li>BatchScanExec physical operator is requested for an inputRDD</li> <li>ColumnarToRowExec physical operator is created and executed</li> <li>DataSourceV2Strategy execution planning strategy is executed</li> <li>FileSourceScanExec physical operator is requested for metadata and metrics</li> </ul>","text":""},{"location":"physical-operators/SparkPlan/#internal-properties","title":"Internal Properties","text":""},{"location":"physical-operators/SparkPlan/#prepared","title":"prepared <p>Flag that controls that prepare is executed only once.</p>","text":""},{"location":"physical-operators/SparkPlan/#subexpressioneliminationenabled","title":"subexpressionEliminationEnabled <p>Flag to control whether the subexpression elimination optimization is enabled or not.</p> <p>Used when the following physical operators are requested to execute (i.e. describe a distributed computation as an RDD of internal rows):</p> <ul> <li> <p>ProjectExec</p> </li> <li> <p>HashAggregateExec (and for finishAggregate)</p> </li> <li> <p>ObjectHashAggregateExec</p> </li> <li> <p>SortAggregateExec</p> </li> <li> <p>WindowExec (and creates a lookup table for WindowExpressions and factory functions for WindowFunctionFrame)</p> </li> </ul>","text":""},{"location":"physical-operators/SubqueryExec/","title":"SubqueryExec Unary Physical Operator","text":"<p><code>SubqueryExec</code> is a unary physical operator.</p> <p><code>SubqueryExec</code> uses &lt;&gt; that is lazily and executed only once when <code>SubqueryExec</code>  is first requested to &lt;&gt; that simply triggers execution of the &lt;&gt; operator asynchronously (i.e. on a separate thread) and to &lt;&gt; soon after (that makes <code>SubqueryExec</code> waiting indefinitely for the child operator to be finished). <p>CAUTION: FIXME When is <code>doPrepare</code> executed?</p> <p><code>SubqueryExec</code> is &lt;&gt; when PlanSubqueries physical optimization is executed (and transforms <code>ScalarSubquery</code> expressions in a physical plan)."},{"location":"physical-operators/SubqueryExec/#source-scala","title":"[source, scala]","text":"<p>val q = sql(\"select (select max(id) from t1) tt from t1\") scala&gt; q.explain == Physical Plan == *Project [Subquery subquery32 AS tt#33L] :  +- Subquery subquery32 :     +- *HashAggregate(keys=[], functions=[max(id#20L)]) :        +- Exchange SinglePartition :           +- *HashAggregate(keys=[], functions=[partial_max(id#20L)]) :              +- *FileScan parquet default.t1[id#20L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/jacek/dev/oss/spark/spark-warehouse/t1], PartitionFilters: [], PushedFilters: [], ReadSchema: struct +- *FileScan parquet default.t1[] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/jacek/dev/oss/spark/spark-warehouse/t1], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;&gt; <p>NOTE: <code>SubqueryExec</code> physical operator is almost an exact copy of BroadcastExchangeExec.md[BroadcastExchangeExec] physical operator.</p> <p>=== [[doPrepare]] Executing Child Operator Asynchronously -- <code>doPrepare</code> Method</p>"},{"location":"physical-operators/SubqueryExec/#source-scala_1","title":"[source, scala]","text":""},{"location":"physical-operators/SubqueryExec/#doprepare-unit","title":"doPrepare(): Unit","text":"<p>NOTE: <code>doPrepare</code> is part of SparkPlan.md#doPrepare[SparkPlan Contract] to prepare a physical operator for execution.</p> <p><code>doPrepare</code> simply triggers initialization of the internal lazily-once-initialized &lt;&gt; asynchronous computation. <p>=== [[relationFuture]] <code>relationFuture</code> Internal Lazily-Once-Initialized Property</p>"},{"location":"physical-operators/SubqueryExec/#source-scala_2","title":"[source, scala]","text":""},{"location":"physical-operators/SubqueryExec/#relationfuture-futurearrayinternalrow","title":"relationFuture: Future[Array[InternalRow]]","text":"<p>When \"materialized\" (aka executed), <code>relationFuture</code> spawns a new thread of execution that requests <code>SQLExecution</code> to execute an action (with the current execution id) on subquery &lt;&gt;. <p>NOTE: <code>relationFuture</code> uses Scala's https://docs.scala-lang.org/overviews/core/futures.html[scala.concurrent.Future] that spawns a new thread of execution once instantiated.</p> <p>The action tracks execution of the &lt;&gt; to SparkPlan.md#executeCollect[executeCollect] and collects &lt;&gt; and &lt;&gt; SQL metrics. <p>In the end, <code>relationFuture</code> posts metric updates and returns the internal rows.</p> <p>[[executionContext]] NOTE: <code>relationFuture</code> is executed on a separate thread from a custom https://www.scala-lang.org/api/2.11.8/index.html#scala.concurrent.ExecutionContext[scala.concurrent.ExecutionContext] (built from a cached https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[java.util.concurrent.ThreadPoolExecutor] with the prefix subquery and up to 16 threads).</p> <p>NOTE: <code>relationFuture</code> is used when <code>SubqueryExec</code> is requested to &lt;&gt; (that triggers execution of the child operator) and &lt;&gt; (that waits indefinitely until the child operator has finished). <p>=== [[creating-instance]] Creating SubqueryExec Instance</p> <p><code>SubqueryExec</code> takes the following when created:</p> <ul> <li>[[name]] Name of the subquery</li> <li>[[child]] Child SparkPlan.md[physical plan]</li> </ul> <p>=== [[executeCollect]] Collecting Internal Rows of Executing SubqueryExec Operator -- <code>executeCollect</code> Method</p>"},{"location":"physical-operators/SubqueryExec/#source-scala_3","title":"[source, scala]","text":""},{"location":"physical-operators/SubqueryExec/#executecollect-arrayinternalrow","title":"executeCollect(): Array[InternalRow]","text":"<p>NOTE: <code>executeCollect</code> is part of SparkPlan.md#executeCollect[SparkPlan Contract] to execute a physical operator and collect the results as collection of internal rows.</p> <p><code>executeCollect</code> waits till &lt;&gt; gives a result (as a <code>Array[InternalRow]</code>)."},{"location":"physical-operators/SubqueryExec/#performance-metrics","title":"Performance Metrics    Key Name (in web UI) Description     collectTime time to collect (ms)    dataSize data size (bytes)","text":""},{"location":"physical-operators/TableWriteExecHelper/","title":"TableWriteExecHelper Unary Physical Commands","text":"<p><code>TableWriteExecHelper</code>\u00a0is an extension of the V2TableWriteExec and <code>SupportsV1Write</code> abstractions for unary physical commands that can write to a table.</p>"},{"location":"physical-operators/TableWriteExecHelper/#implementations","title":"Implementations","text":"<ul> <li><code>AtomicCreateTableAsSelectExec</code></li> <li><code>AtomicReplaceTableAsSelectExec</code></li> <li>CreateTableAsSelectExec</li> <li><code>ReplaceTableAsSelectExec</code></li> </ul>"},{"location":"physical-operators/TableWriteExecHelper/#writetotable","title":"writeToTable <pre><code>writeToTable(\n  catalog: TableCatalog,\n  table: Table,\n  writeOptions: CaseInsensitiveStringMap,\n  ident: Identifier): Seq[InternalRow]\n</code></pre> <p><code>writeToTable</code>...FIXME</p> <p><code>writeToTable</code> is used when:</p> <ul> <li><code>CreateTableAsSelectExec</code> is requested to run</li> <li><code>AtomicCreateTableAsSelectExec</code> is requested to <code>run</code></li> <li><code>ReplaceTableAsSelectExec</code> is requested to <code>run</code></li> <li><code>AtomicReplaceTableAsSelectExec</code> is requested to <code>run</code></li> </ul>","text":""},{"location":"physical-operators/TruncateTableExec/","title":"TruncateTableExec Physical Operator","text":"<p><code>TruncateTableExec</code> is a <code>LeafV2CommandExec</code> physical operator that represents the following logical operators at execution:</p> <ul> <li>DeleteFromTable logical operator with a DataSourceV2ScanRelation over a TruncatableTable table with no <code>WHERE</code> clause</li> <li><code>TruncateTable</code></li> </ul>"},{"location":"physical-operators/TruncateTableExec/#creating-instance","title":"Creating Instance","text":"<p><code>TruncateTableExec</code> takes the following to be created:</p> <ul> <li> TruncatableTable <li> Refresh Cache Procedure (<code>() =&gt; Unit</code>) <p><code>TruncateTableExec</code> is created when:</p> <ul> <li>DataSourceV2Strategy execution planning strategy is executed</li> </ul>"},{"location":"physical-operators/TruncateTableExec/#output-schema","title":"Output Schema  Signature <pre><code>output: Seq[Attribute]\n</code></pre> <p><code>output</code> is part of the QueryPlan abstraction.</p>  <p><code>output</code> is empty.</p>","text":""},{"location":"physical-operators/TruncateTableExec/#run","title":"run  Signature <pre><code>run(): Seq[InternalRow]\n</code></pre> <p><code>run</code> is part of the V2CommandExec abstraction.</p>  <p><code>run</code> requests the TruncatableTable to truncateTable followed by executing the refreshCache procedure.</p>","text":""},{"location":"physical-operators/TungstenAggregationIterator/","title":"TungstenAggregationIterator","text":"<p><code>TungstenAggregationIterator</code> is an AggregationIterator for HashAggregateExec physical operator.</p> <p><code>TungstenAggregationIterator</code> prefers hash-based aggregation before switching to sort-based one.</p>"},{"location":"physical-operators/TungstenAggregationIterator/#creating-instance","title":"Creating Instance","text":"<p><code>TungstenAggregationIterator</code> takes the following to be created:</p> <ul> <li> Partition ID <li> Grouping NamedExpressions <li> AggregateExpressions <li> Aggregate Attributes <li> Initial input buffer offset <li> Result NamedExpressions <li> Function to create a new <code>MutableProjection</code> given expressions and attributes (<code>(Seq[Expression], Seq[Attribute]) =&gt; MutableProjection</code>) <li> Original Input Attributes <li> Input Iterator of InternalRows (from a single partition of the child of the HashAggregateExec physical operator) <li> (only for testing) Optional <code>HashAggregateExec</code>'s testFallbackStartsAt <li> <code>numOutputRows</code> SQLMetric <li> <code>peakMemory</code> SQLMetric <li> <code>spillSize</code> SQLMetric <li> <code>avgHashProbe</code> SQLMetric <p><code>TungstenAggregationIterator</code> is created when:</p> <ul> <li><code>HashAggregateExec</code> physical operator is executed</li> </ul> <p><code>TungstenAggregationIterator</code> starts processing input rows and pre-loads the first key-value pair from the UnsafeFixedWidthAggregationMap unless switched to a sort-based aggregation.</p>"},{"location":"physical-operators/TungstenAggregationIterator/#performance-metrics","title":"Performance Metrics <p>When created, <code>TungstenAggregationIterator</code> gets SQLMetrics from the HashAggregateExec aggregate physical operator being executed.</p> <ul> <li> <p>numOutputRows is used when <code>TungstenAggregationIterator</code> is requested for the next UnsafeRow (and it has one)</p> </li> <li> <p>peakMemory, spillSize and avgHashProbe are used at the end of every task (one per partition)</p> </li> </ul> <p>The metrics are displayed as part of HashAggregateExec aggregate physical operator (e.g. in web UI in Details for Query).</p> <p></p>","text":""},{"location":"physical-operators/TungstenAggregationIterator/#next-unsaferow","title":"Next UnsafeRow <pre><code>next(): UnsafeRow\n</code></pre> <p><code>next</code> is part of the <code>Iterator</code> (Scala) abstraction.</p>  <p><code>next</code>...FIXME</p>","text":""},{"location":"physical-operators/TungstenAggregationIterator/#processcurrentsortedgroup","title":"processCurrentSortedGroup <pre><code>processCurrentSortedGroup(): Unit\n</code></pre> <p><code>processCurrentSortedGroup</code>...FIXME</p>","text":""},{"location":"physical-operators/TungstenAggregationIterator/#unsafefixedwidthaggregationmap","title":"UnsafeFixedWidthAggregationMap <p>When created, <code>TungstenAggregationIterator</code> creates an UnsafeFixedWidthAggregationMap with the following:</p> <ul> <li>initialAggregationBuffer</li> <li>Schema built from the attributes of the aggregation buffers of all the AggregateFunctions</li> <li>Schema built from the attributes of all the grouping expressions</li> </ul> <p>Used when:</p> <ul> <li><code>TungstenAggregationIterator</code> is requested for the next UnsafeRow, to outputForEmptyGroupingKeyWithoutInput, process input rows, to initialize the aggregationBufferMapIterator and every time a partition has been processed</li> </ul>","text":""},{"location":"physical-operators/TungstenAggregationIterator/#taskcompletionlistener","title":"TaskCompletionListener <p><code>TungstenAggregationIterator</code> registers a <code>TaskCompletionListener</code> that is executed on task completion (for every task that processes a partition).</p> <p>When executed (once per partition), the <code>TaskCompletionListener</code> updates the following metrics:</p> <ul> <li>peakMemory</li> <li>spillSize</li> <li>avgHashProbe</li> </ul>","text":""},{"location":"physical-operators/TungstenAggregationIterator/#outputforemptygroupingkeywithoutinput","title":"outputForEmptyGroupingKeyWithoutInput <pre><code>outputForEmptyGroupingKeyWithoutInput(): UnsafeRow\n</code></pre> <p><code>outputForEmptyGroupingKeyWithoutInput</code>...FIXME</p>  <p><code>outputForEmptyGroupingKeyWithoutInput</code> is used when:</p> <ul> <li><code>HashAggregateExec</code> physical operator is requested to execute (with no input rows and grouping expressions)</li> </ul>","text":""},{"location":"physical-operators/TungstenAggregationIterator/#processing-input-rows","title":"Processing Input Rows <pre><code>processInputs(\n  fallbackStartsAt: (Int, Int)): Unit\n</code></pre> <p><code>processInputs</code>...FIXME</p>  <p><code>processInputs</code> is used when:</p> <ul> <li><code>TungstenAggregationIterator</code> is created</li> </ul>","text":""},{"location":"physical-operators/TungstenAggregationIterator/#hash-vs-sort-based-aggregations","title":"Hash- vs Sort-Based Aggregations <pre><code>sortBased: Boolean = false\n</code></pre> <p><code>TungstenAggregationIterator</code> creates and initializes <code>sortBased</code> flag to <code>false</code> when created.</p> <p>The flag is used to indicate whether <code>TungstenAggregationIterator</code> has switched (fall back) to sort-based aggregation while processing input rows.</p> <p><code>sortBased</code> flag is turned on (<code>true</code>) while switching to sort-based aggregation (and the numTasksFallBacked metric is incremented).</p> <p>Switching from hash-based to sort-based aggregation happens when the external sorter is initialized (that is used for sort-based aggregation).</p>","text":""},{"location":"physical-operators/TungstenAggregationIterator/#demo","title":"Demo <pre><code>val q = spark.range(10).\n  groupBy('id % 2 as \"group\").\n  agg(sum(\"id\") as \"sum\")\nval execPlan = q.queryExecution.sparkPlan\nscala&gt; println(execPlan.numberedTreeString)\n00 HashAggregate(keys=[(id#0L % 2)#11L], functions=[sum(id#0L)], output=[group#3L, sum#7L])\n01 +- HashAggregate(keys=[(id#0L % 2) AS (id#0L % 2)#11L], functions=[partial_sum(id#0L)], output=[(id#0L % 2)#11L, sum#13L])\n02    +- Range (0, 10, step=1, splits=8)\n\nimport org.apache.spark.sql.execution.aggregate.HashAggregateExec\nval hashAggExec = execPlan.asInstanceOf[HashAggregateExec]\nval hashAggExecRDD = hashAggExec.execute\n\n// MapPartitionsRDD is in private[spark] scope\n// Use :paste -raw for the following helper object\npackage org.apache.spark\nobject AccessPrivateSpark {\n  import org.apache.spark.rdd.RDD\n  def mapPartitionsRDD[T](hashAggExecRDD: RDD[T]) = {\n    import org.apache.spark.rdd.MapPartitionsRDD\n    hashAggExecRDD.asInstanceOf[MapPartitionsRDD[_, _]]\n  }\n}\n// END :paste -raw\n\nimport org.apache.spark.AccessPrivateSpark\nval mpRDD = AccessPrivateSpark.mapPartitionsRDD(hashAggExecRDD)\nval f = mpRDD.iterator(_, _)\n\nimport org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator\n// FIXME How to show that TungstenAggregationIterator is used?\n</code></pre>","text":""},{"location":"physical-operators/UnaryExecNode/","title":"UnaryExecNode Physical Operators","text":"<p><code>UnaryExecNode</code> is an extension of the SparkPlan abstraction for unary physical operators (with one child physical operator only).</p>"},{"location":"physical-operators/UnaryExecNode/#contract","title":"Contract","text":""},{"location":"physical-operators/UnaryExecNode/#child-physical-operator","title":"Child Physical Operator <pre><code>child: SparkPlan\n</code></pre> <p>The one and only child physical operator</p> <p>Used when:</p> <ul> <li><code>UnaryExecNode</code> is requested for the children</li> <li>others</li> </ul>","text":""},{"location":"physical-operators/UnaryExecNode/#implementations","title":"Implementations","text":"<ul> <li>AggregateInPandasExec (PySpark)</li> <li>AliasAwareOutputPartitioning</li> <li>BaseAggregateExec</li> <li>CoalesceExec</li> <li>CollectMetricsExec</li> <li>ColumnarToRowExec</li> <li>DataWritingCommandExec</li> <li>DebugExec</li> <li>EvalPythonExec</li> <li>Exchange</li> <li>FilterExec</li> <li>FlatMapGroupsInPandasExec (PySpark)</li> <li>FlatMapGroupsWithStateExec (Structured Streaming)</li> <li>GenerateExec</li> <li>InputAdapter</li> <li>ObjectConsumerExec</li> <li>ProjectExec</li> <li>SortExec</li> <li>SubqueryExec</li> <li>V2TableWriteExec</li> <li>WholeStageCodegenExec</li> <li>others</li> </ul>"},{"location":"physical-operators/UnspecifiedDistribution/","title":"UnspecifiedDistribution","text":"<p><code>UnspecifiedDistribution</code> is a Distribution.</p> <p>[[requiredNumPartitions]] <code>UnspecifiedDistribution</code> specifies <code>None</code> for the Distribution.md#requiredNumPartitions[required number of partitions].</p> <p>Note</p> <p><code>None</code> for the required number of partitions indicates to use any number of partitions (possibly spark.sql.shuffle.partitions configuration property).</p>"},{"location":"physical-operators/V2CommandExec/","title":"V2CommandExec Physical Commands","text":"<p><code>V2CommandExec</code> is an extension of the SparkPlan abstraction for physical commands that can be executed and cache the result to prevent multiple executions.</p>"},{"location":"physical-operators/V2CommandExec/#contract","title":"Contract","text":""},{"location":"physical-operators/V2CommandExec/#executing-command","title":"Executing Command <pre><code>run(): Seq[InternalRow]\n</code></pre> <p>Executing the command (and computing the result)</p> <p>Used when:</p> <ul> <li><code>V2CommandExec</code> physical command is requested for a result</li> </ul>","text":""},{"location":"physical-operators/V2CommandExec/#implementations","title":"Implementations","text":"<ul> <li><code>LeafV2CommandExec</code></li> <li><code>ShowCreateTableExec</code></li> <li><code>ShowNamespacesExec</code></li> <li><code>ShowPartitionsExec</code></li> <li>ShowTablesExec</li> <li>V2TableWriteExec</li> </ul>"},{"location":"physical-operators/V2CommandExec/#result","title":"result <pre><code>result: Seq[InternalRow]\n</code></pre> <p><code>result</code> is the cached result of executing the physical command.</p> <p><code>result</code> is used when <code>V2CommandExec</code> physical command is requested to doExecute, executeCollect, executeToIterator, executeTake or executeTail.</p>","text":""},{"location":"physical-operators/V2CommandExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code>...FIXME</p> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p>","text":""},{"location":"physical-operators/V2CommandExec/#executecollect","title":"executeCollect <pre><code>executeCollect(): Array[InternalRow]\n</code></pre> <p><code>executeCollect</code>...FIXME</p> <p><code>executeCollect</code> is part of the SparkPlan abstraction.</p>","text":""},{"location":"physical-operators/V2CommandExec/#executetoiterator","title":"executeToIterator <pre><code>executeToIterator: Iterator[InternalRow]\n</code></pre> <p><code>executeToIterator</code>...FIXME</p> <p><code>executeToIterator</code> is part of the SparkPlan abstraction.</p>","text":""},{"location":"physical-operators/V2CommandExec/#executetake","title":"executeTake <pre><code>executeTake(\n  limit: Int): Array[InternalRow]\n</code></pre> <p><code>executeTake</code>...FIXME</p> <p><code>executeTake</code> is part of the SparkPlan abstraction.</p>","text":""},{"location":"physical-operators/V2CommandExec/#executetail","title":"executeTail <pre><code>executeTail(\n  limit: Int): Array[InternalRow]\n</code></pre> <p><code>executeTail</code>...FIXME</p> <p><code>executeTail</code> is part of the SparkPlan abstraction.</p>","text":""},{"location":"physical-operators/V2ExistingTableWriteExec/","title":"V2ExistingTableWriteExec Unary Physical Commands","text":"<p><code>V2ExistingTableWriteExec</code> is an extension of the V2TableWriteExec abstraction for unary physical commands that refreshCache after writing data out to a writable table when executed.</p>"},{"location":"physical-operators/V2ExistingTableWriteExec/#contract","title":"Contract","text":""},{"location":"physical-operators/V2ExistingTableWriteExec/#refreshCache","title":"refreshCache","text":"<pre><code>refreshCache: () =&gt; Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>V2ExistingTableWriteExec</code> is executed</li> </ul>"},{"location":"physical-operators/V2ExistingTableWriteExec/#write","title":"write","text":"<pre><code>write: Write\n</code></pre> <p>Write-able table to write data out</p> <p>Used when:</p> <ul> <li><code>V2ExistingTableWriteExec</code> is requested for the customMetrics and to execute</li> </ul>"},{"location":"physical-operators/V2ExistingTableWriteExec/#implementations","title":"Implementations","text":"<ul> <li><code>AppendDataExec</code></li> <li>OverwriteByExpressionExec</li> <li><code>OverwritePartitionsDynamicExec</code></li> <li><code>ReplaceDataExec</code></li> <li><code>WriteDeltaExec</code></li> </ul>"},{"location":"physical-operators/V2ExistingTableWriteExec/#run","title":"Executing Command","text":"V2CommandExec <pre><code>run(): Seq[InternalRow]\n</code></pre> <p><code>run</code> is part of the V2CommandExec abstraction.</p> <p><code>run</code>...FIXME</p>"},{"location":"physical-operators/V2TableWriteExec/","title":"V2TableWriteExec Unary Physical Commands","text":"<p><code>V2TableWriteExec</code>\u00a0is an extension of the V2CommandExec abstraction for unary physical commands that writeWithV2.</p>"},{"location":"physical-operators/V2TableWriteExec/#contract","title":"Contract","text":""},{"location":"physical-operators/V2TableWriteExec/#physical-query-plan","title":"Physical Query Plan <pre><code>query: SparkPlan\n</code></pre> <p>SparkPlan for the data to be written out</p>","text":""},{"location":"physical-operators/V2TableWriteExec/#implementations","title":"Implementations","text":"<ul> <li>TableWriteExecHelper</li> <li>V2ExistingTableWriteExec</li> <li><code>WriteToDataSourceV2Exec</code> (Spark Structured Streaming)</li> </ul>"},{"location":"physical-operators/V2TableWriteExec/#writewithv2","title":"writeWithV2 <pre><code>writeWithV2(\n  batchWrite: BatchWrite): Seq[InternalRow]\n</code></pre> <p><code>writeWithV2</code> requests the physical query plan to execute (and produce a <code>RDD[InternalRow]</code>).</p> <p><code>writeWithV2</code> requests the given BatchWrite to create a DataWriterFactory (with the number of partitions of the <code>RDD</code>)</p> <p><code>writeWithV2</code> prints out the following INFO message to the logs:</p> <pre><code>Start processing data source write support: [batchWrite]. The input RDD has [n] partitions.\n</code></pre> <p><code>writeWithV2</code> runs a Spark job (Spark Core) with the DataWritingSparkTask for every partition. <code>writeWithV2</code> requests the <code>BatchWrite</code> to onDataWriterCommit (with the result <code>WriterCommitMessage</code>) after every partition has been processed successfully.</p> <p><code>writeWithV2</code> prints out the following INFO message to the logs:</p> <pre><code>Data source write support [batchWrite] is committing.\n</code></pre> <p><code>writeWithV2</code> requests the <code>BatchWrite</code> to commit (with all the result <code>WriterCommitMessage</code>s).</p> <p><code>writeWithV2</code> prints out the following INFO message to the logs:</p> <pre><code>Data source write support [batchWrite] committed.\n</code></pre> <p>In the end, <code>writeWithV2</code> returns an empty collection (of <code>InternalRow</code>s).</p>  <p><code>writeWithV2</code> is used when:</p> <ul> <li><code>TableWriteExecHelper</code> is requested to writeToTable</li> <li><code>V2ExistingTableWriteExec</code> is executed</li> <li><code>WriteToDataSourceV2Exec</code> (Spark Structured Streaming) is executed</li> </ul>","text":""},{"location":"physical-operators/V2TableWriteExec/#logging","title":"Logging <p><code>V2TableWriteExec</code> is a Scala trait and logging is configured using the logger of the implementations.</p>","text":""},{"location":"physical-operators/WholeStageCodegenExec/","title":"WholeStageCodegenExec Physical Operator","text":"<p><code>WholeStageCodegenExec</code> is a unary physical operator that (alongside InputAdapter) lays the foundation for the Whole-Stage Java Code Generation for a Codegened Execution Pipeline of a structured query.</p>"},{"location":"physical-operators/WholeStageCodegenExec/#creating-instance","title":"Creating Instance","text":"<p><code>WholeStageCodegenExec</code> takes the following to be created:</p> <ul> <li> Child SparkPlan (a physical subquery tree) <li> Codegen Stage Id <p><code>WholeStageCodegenExec</code> is created when:</p> <ul> <li>CollapseCodegenStages physical optimization is executed (with spark.sql.codegen.wholeStage configuration property enabled)</li> </ul>"},{"location":"physical-operators/WholeStageCodegenExec/#codegensupport","title":"CodegenSupport <p><code>WholeStageCodegenExec</code> is a CodegenSupport and, when executed, triggers code generation for the whole child physical plan subtree (stage) of a structured query.</p>","text":""},{"location":"physical-operators/WholeStageCodegenExec/#performance-metrics","title":"Performance Metrics","text":""},{"location":"physical-operators/WholeStageCodegenExec/#duration","title":"duration <p>How long (in ms) the whole-stage codegen pipeline has been running (i.e. the elapsed time since the underlying BufferedRowIterator had been created and the internal rows were all consumed).</p> <p></p>","text":""},{"location":"physical-operators/WholeStageCodegenExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p>  <p><code>doExecute</code> doCodeGen (that gives a CodegenContext and the Java source code).</p> <p><code>doExecute</code> tries to compile the code and, if fails and spark.sql.codegen.fallback is enabled, falls back to requesting the child physical operator to execute (and skipping codegen for the part of query).</p> <p>In case the compiled code has the maximum bytecode size of a single compiled Java function (generated by whole-stage codegen) above spark.sql.codegen.hugeMethodLimit threshold, <code>doExecute</code> prints out the following INFO message and requests the child physical operator to execute (and skipping codegen for the part of query):</p> <pre><code>Found too long generated codes and JIT optimization might not work:\nthe bytecode size ([maxMethodCodeSize]) is above the limit [hugeMethodLimit],\nand the whole-stage codegen was disabled for this plan (id=[codegenStageId]).\nTo avoid this, you can raise the limit `spark.sql.codegen.hugeMethodLimit`:\n[treeString]\n</code></pre> <p><code>doExecute</code> requests the <code>CodegenContext</code> for the references.</p> <p><code>doExecute</code> the child physical operator (that is supposed to be a CodegenSupport) for the inputRDDs.</p>  Up to two input RDDs are supported <p><code>doExecute</code> throws an <code>AssertionError</code> when the number of input RDDs is more than 2:</p> <pre><code>Up to two input RDDs can be supported\n</code></pre>","text":""},{"location":"physical-operators/WholeStageCodegenExec/#one-input-rdd","title":"One Input RDD <p>For one input RDD, <code>doExecute</code> uses <code>RDD.mapPartitionsWithIndex</code> operation.</p> <p>For every partition of the input RDD, <code>doExecute</code> does the following:</p> <ol> <li>Compiles the code (the compile code is cached the first time so it's almost a noop)</li> <li>Requests the compiled code to <code>generate</code> (with the references) to produce a BufferedRowIterator</li> <li>Requests the <code>BufferedRowIterator</code> to initialize</li> <li>Creates an <code>Iterator[InternalRow]</code> to track the rows until the last is consumed and the duration metric can be recorded</li> </ol>","text":""},{"location":"physical-operators/WholeStageCodegenExec/#two-input-rdds","title":"Two Input RDDs <p>For two input RDDs, <code>doExecute</code>...FIXME</p>","text":""},{"location":"physical-operators/WholeStageCodegenExec/#demo","title":"Demo <pre><code>val q = spark.range(9)\n\n// we need executedPlan with WholeStageCodegenExec physical operator \"injected\"\nval plan = q.queryExecution.executedPlan\nassert(plan.isInstanceOf[org.apache.spark.sql.execution.SparkPlan])\n</code></pre> <pre><code>// Note the star prefix of Range that marks WholeStageCodegenExec\nscala&gt; println(plan.numberedTreeString)\n00 *Range (0, 9, step=1, splits=8)\n</code></pre> <pre><code>// As a matter of fact, there are two physical operators in play here\n// i.e. WholeStageCodegenExec with Range as the child\nscala&gt; plan.foreach { op =&gt; println(op.getClass.getName) }\norg.apache.spark.sql.execution.WholeStageCodegenExec\norg.apache.spark.sql.execution.RangeExec\n</code></pre> <pre><code>// Let's access the parent WholeStageCodegenExec\nimport org.apache.spark.sql.execution.WholeStageCodegenExec\nval wsce = plan.asInstanceOf[WholeStageCodegenExec]\n</code></pre> <pre><code>// Trigger code generation of the entire query plan tree\nval (ctx, code) = wsce.doCodeGen\n</code></pre> <pre><code>// CodeFormatter can pretty-print the code\nimport org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatter\nprintln(CodeFormatter.format(code))\n</code></pre> <pre><code>/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=1\n/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean range_initRange_0;\n/* 010 */   private long range_nextIndex_0;\n/* 011 */   private TaskContext range_taskContext_0;\n/* 012 */   private InputMetrics range_inputMetrics_0;\n/* 013 */   private long range_batchEnd_0;\n/* 014 */   private long range_numElementsTodo_0;\n/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] range_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n/* 016 */\n/* 017 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n/* 018 */     this.references = references;\n/* 019 */   }\n/* 020 */\n/* 021 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 022 */     partitionIndex = index;\n/* 023 */     this.inputs = inputs;\n/* 024 */\n...\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.expressions.codegen._\n\nval ctx = new CodegenContext()\n\nval code: Block = CodeBlock(codeParts = Seq(\"valid_java_code\"), blockInputs = Seq.empty)\n\n// code will be evaluated to produce a value (that can be null)\nval isNull: ExprValue = FalseLiteral\nval value: ExprValue = new LiteralValue(value = \"valid_java_code_for_literal_value\", javaType = classOf[String])\nval exprCode = ExprCode(code, isNull, value)\n\nval consumeCode = wsce.doConsume(ctx, input = Seq.empty, row = exprCode)\n</code></pre> <pre><code>println(consumeCode)\n</code></pre> <pre><code>valid_java_code\nappend(valid_java_code_for_literal_value);\n</code></pre>","text":""},{"location":"physical-operators/WholeStageCodegenExec/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.WholeStageCodegenExec</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.WholeStageCodegenExec=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"physical-operators/WindowExec/","title":"WindowExec Unary Physical Operator","text":"<p><code>WindowExec</code> is a WindowExecBase unary physical operator for window function execution.</p> <p></p> <p><code>WindowExec</code> represents Window unary logical operator at execution.</p>"},{"location":"physical-operators/WindowExec/#creating-instance","title":"Creating Instance","text":"<p><code>WindowExec</code> takes the following to be created:</p> <ul> <li> Window NamedExpression <li> Partition Specification (Expressions) <li> SortOrders <li> Child SparkPlan <p><code>WindowExec</code> is created when:</p> <ul> <li>Window execution planning strategy plans a Window unary logical operator with a WindowFunction or an AggregateFunction</li> </ul>"},{"location":"physical-operators/WindowExec/#configuration-properties","title":"Configuration Properties","text":"<p><code>WindowExec</code> uses the following configuration properties when executed.</p>"},{"location":"physical-operators/WindowExec/#sparksqlwindowexecbufferinmemorythreshold","title":"spark.sql.windowExec.buffer.in.memory.threshold <p>spark.sql.windowExec.buffer.in.memory.threshold</p>","text":""},{"location":"physical-operators/WindowExec/#sparksqlwindowexecbufferspillthreshold","title":"spark.sql.windowExec.buffer.spill.threshold <p>spark.sql.windowExec.buffer.spill.threshold</p>","text":""},{"location":"physical-operators/WindowExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the SparkPlan abstraction.</p> <p><code>doExecute</code> requests the child physical operator to execute and maps over <code>InternalRow</code>s in partitions (using <code>RDD.mapPartitions</code> operator).</p>  <p>Note</p> <p>When executed, <code>doExecute</code> creates a new <code>MapPartitionsRDD</code> with the <code>RDD[InternalRow]</code> of the child physical operator.</p>  <pre><code>scala&gt; :type we\norg.apache.spark.sql.execution.window.WindowExec\n\nval windowRDD = we.execute\nscala&gt; :type windowRDD\norg.apache.spark.rdd.RDD[org.apache.spark.sql.catalyst.InternalRow]\n\nscala&gt; println(windowRDD.toDebugString)\n(200) MapPartitionsRDD[5] at execute at &lt;console&gt;:35 []\n  |   MapPartitionsRDD[4] at execute at &lt;console&gt;:35 []\n  |   ShuffledRowRDD[3] at execute at &lt;console&gt;:35 []\n  +-(7) MapPartitionsRDD[2] at execute at &lt;console&gt;:35 []\n     |  MapPartitionsRDD[1] at execute at &lt;console&gt;:35 []\n     |  ParallelCollectionRDD[0] at execute at &lt;console&gt;:35 []\n</code></pre> <p><code>doExecute</code> creates an <code>Iterator[InternalRow]</code> (of UnsafeRow exactly).</p>","text":""},{"location":"physical-operators/WindowExec/#demo","title":"Demo <pre><code>// arguably the most trivial example\n// just a dataset of 3 rows per group\n// to demo how partitions and frames work\n// note the rows per groups are not consecutive (in the middle)\nval metrics = Seq(\n  (0, 0, 0), (1, 0, 1), (2, 5, 2), (3, 0, 3), (4, 0, 1), (5, 5, 3), (6, 5, 0)\n).toDF(\"id\", \"device\", \"level\")\nscala&gt; metrics.show\n+---+------+-----+\n| id|device|level|\n+---+------+-----+\n|  0|     0|    0|\n|  1|     0|    1|\n|  2|     5|    2|  // &lt;-- this row for device 5 is among the rows of device 0\n|  3|     0|    3|  // &lt;-- as above but for device 0\n|  4|     0|    1|  // &lt;-- almost as above but there is a group of two rows for device 0\n|  5|     5|    3|\n|  6|     5|    0|\n+---+------+-----+\n\n// create windows of rows to use window aggregate function over every window\nimport org.apache.spark.sql.expressions.Window\nval rangeWithTwoDevicesById = Window.\n  partitionBy('device).\n  orderBy('id).\n  rangeBetween(start = -1, end = Window.currentRow) // &lt;-- demo rangeBetween first\nval sumOverRange = metrics.withColumn(\"sum\", sum('level) over rangeWithTwoDevicesById)\n\n// Logical plan with Window unary logical operator\nval optimizedPlan = sumOverRange.queryExecution.optimizedPlan\nscala&gt; println(optimizedPlan)\nWindow [sum(cast(level#9 as bigint)) windowspecdefinition(device#8, id#7 ASC NULLS FIRST, RANGE BETWEEN 1 PRECEDING AND CURRENT ROW) AS sum#15L], [device#8], [id#7 ASC NULLS FIRST]\n+- LocalRelation [id#7, device#8, level#9]\n\n// Physical plan with WindowExec unary physical operator (shown as Window)\nscala&gt; sumOverRange.explain\n== Physical Plan ==\nWindow [sum(cast(level#9 as bigint)) windowspecdefinition(device#8, id#7 ASC NULLS FIRST, RANGE BETWEEN 1 PRECEDING AND CURRENT ROW) AS sum#15L], [device#8], [id#7 ASC NULLS FIRST]\n+- *Sort [device#8 ASC NULLS FIRST, id#7 ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(device#8, 200)\n      +- LocalTableScan [id#7, device#8, level#9]\n\n// Going fairly low-level...you've been warned\n\nval plan = sumOverRange.queryExecution.executedPlan\nimport org.apache.spark.sql.execution.window.WindowExec\nval we = plan.asInstanceOf[WindowExec]\nval windowRDD = we.execute()\nscala&gt; :type windowRDD\norg.apache.spark.rdd.RDD[org.apache.spark.sql.catalyst.InternalRow]\n\nscala&gt; windowRDD.toDebugString\nres0: String =\n(200) MapPartitionsRDD[5] at execute at &lt;console&gt;:35 []\n  |   MapPartitionsRDD[4] at execute at &lt;console&gt;:35 []\n  |   ShuffledRowRDD[3] at execute at &lt;console&gt;:35 []\n  +-(7) MapPartitionsRDD[2] at execute at &lt;console&gt;:35 []\n     |  MapPartitionsRDD[1] at execute at &lt;console&gt;:35 []\n     |  ParallelCollectionRDD[0] at execute at &lt;console&gt;:35 []\n\n// no computation on the source dataset has really occurred\n// Let's trigger a RDD action\nscala&gt; windowRDD.first\nres0: org.apache.spark.sql.catalyst.InternalRow = [0,2,5,2,2]\n\nscala&gt; windowRDD.foreach(println)\n[0,2,5,2,2]\n[0,0,0,0,0]\n[0,5,5,3,3]\n[0,6,5,0,3]\n[0,1,0,1,1]\n[0,3,0,3,3]\n[0,4,0,1,4]\n\nscala&gt; sumOverRange.show\n+---+------+-----+---+\n| id|device|level|sum|\n+---+------+-----+---+\n|  2|     5|    2|  2|\n|  5|     5|    3|  3|\n|  6|     5|    0|  3|\n|  0|     0|    0|  0|\n|  1|     0|    1|  1|\n|  3|     0|    3|  3|\n|  4|     0|    1|  4|\n+---+------+-----+---+\n\n// use rowsBetween\nval rowsWithTwoDevicesById = Window.\n  partitionBy('device).\n  orderBy('id).\n  rowsBetween(start = -1, end = Window.currentRow)\nval sumOverRows = metrics.withColumn(\"sum\", sum('level) over rowsWithTwoDevicesById)\n\n// let's see the result first to have them close\n// and compare row- vs range-based windows\nscala&gt; sumOverRows.show\n+---+------+-----+---+\n| id|device|level|sum|\n+---+------+-----+---+\n|  2|     5|    2|  2|\n|  5|     5|    3|  5| &lt;-- a difference\n|  6|     5|    0|  3|\n|  0|     0|    0|  0|\n|  1|     0|    1|  1|\n|  3|     0|    3|  4| &lt;-- another difference\n|  4|     0|    1|  4|\n+---+------+-----+---+\n\nval rowsOptimizedPlan = sumOverRows.queryExecution.optimizedPlan\nscala&gt; println(rowsOptimizedPlan)\nWindow [sum(cast(level#901 as bigint)) windowspecdefinition(device#900, id#899 ASC NULLS FIRST, ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS sum#1458L], [device#900], [id#899 ASC NULLS FIRST]\n+- LocalRelation [id#899, device#900, level#901]\n\nscala&gt; sumOverRows.explain\n== Physical Plan ==\nWindow [sum(cast(level#901 as bigint)) windowspecdefinition(device#900, id#899 ASC NULLS FIRST, ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS sum#1458L], [device#900], [id#899 ASC NULLS FIRST]\n+- *Sort [device#900 ASC NULLS FIRST, id#899 ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(device#900, 200)\n      +- LocalTableScan [id#899, device#900, level#901]\n</code></pre> <pre><code>// a more involved example\nval dataset = spark.range(start = 0, end = 13, step = 1, numPartitions = 4)\n\nimport org.apache.spark.sql.expressions.Window\nval groupsOrderById = Window.partitionBy('group).rangeBetween(-2, Window.currentRow).orderBy('id)\nval query = dataset.\n  withColumn(\"group\", 'id % 4).\n  select('*, sum('id) over groupsOrderById as \"sum\")\n\nscala&gt; query.explain\n== Physical Plan ==\nWindow [sum(id#25L) windowspecdefinition(group#244L, id#25L ASC NULLS FIRST, RANGE BETWEEN 2 PRECEDING AND CURRENT ROW) AS sum#249L], [group#244L], [id#25L ASC NULLS FIRST]\n+- *Sort [group#244L ASC NULLS FIRST, id#25L ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(group#244L, 200)\n      +- *Project [id#25L, (id#25L % 4) AS group#244L]\n         +- *Range (0, 13, step=1, splits=4)\n\nval plan = query.queryExecution.executedPlan\nimport org.apache.spark.sql.execution.window.WindowExec\nval we = plan.asInstanceOf[WindowExec]\n</code></pre> <pre><code>// the whole schema is as follows\nval schema = query.queryExecution.executedPlan.output.toStructType\nscala&gt; println(schema.treeString)\nroot\n |-- id: long (nullable = false)\n |-- group: long (nullable = true)\n |-- sum: long (nullable = true)\n\n// Let's see ourselves how the schema is made up of\n\nscala&gt; :type we\norg.apache.spark.sql.execution.window.WindowExec\n\n// child's output\nscala&gt; println(we.child.output.toStructType.treeString)\nroot\n |-- id: long (nullable = false)\n |-- group: long (nullable = true)\n\n// window expressions' output\nval weExprSchema = we.windowExpression.map(_.toAttribute).toStructType\nscala&gt; println(weExprSchema.treeString)\nroot\n |-- sum: long (nullable = true)\n</code></pre>","text":""},{"location":"physical-operators/WindowExec/#rows-between","title":"ROWS BETWEEN <pre><code>val vs = Seq(\n  (1,0), (1,1), (1,3), (2, 0), (2,4)).toDF(\"gid\", \"v\")\n</code></pre> <pre><code>scala&gt; vs.show\n+---+---+\n|gid|  v|\n+---+---+\n|  1|  0|\n|  1|  1|\n|  1|  3|\n|  2|  0|\n|  2|  4|\n+---+---+\n</code></pre> <pre><code>import org.apache.spark.sql.expressions.Window\nval byRowsBetween = Window\n  .partitionBy('gid)\n  .orderBy('v)\n  .rowsBetween(-1, Window.currentRow)\n</code></pre> <pre><code>val q = vs.withColumn(\"vs\", collect_list('v).over(byRowsBetween))\n</code></pre> <pre><code>scala&gt; q.show\n+---+---+---------+\n|gid|  v|       vs|\n+---+---+---------+\n|  1|  0|[0, 1, 3]|\n|  1|  1|[0, 1, 3]|\n|  1|  3|[0, 1, 3]|\n|  2|  0|   [0, 4]|\n|  2|  4|   [0, 4]|\n+---+---+---------+\n</code></pre>","text":""},{"location":"physical-operators/WindowExec/#range-between","title":"RANGE BETWEEN <pre><code>val vs = Seq(\n  (1,0), (1,1), (1,3), (2, 0), (2,4)).toDF(\"gid\", \"v\")\n</code></pre> <pre><code>scala&gt; vs.show\n+---+---+\n|gid|  v|\n+---+---+\n|  1|  0|\n|  1|  1|\n|  1|  3|\n|  2|  0|\n|  2|  4|\n+---+---+\n</code></pre> <pre><code>import org.apache.spark.sql.expressions.Window\nval byRangeBetween = Window\n  .partitionBy('gid)\n  .orderBy('v)\n  .rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n</code></pre> <pre><code>val q = vs.withColumn(\"vs\", collect_list('v).over(byRangeBetween))\n</code></pre> <pre><code>scala&gt; q.show\n+---+---+---------+\n|gid|  v|       vs|\n+---+---+---------+\n|  1|  0|[0, 1, 3]|\n|  1|  1|[0, 1, 3]|\n|  1|  3|[0, 1, 3]|\n|  2|  0|   [0, 4]|\n|  2|  4|   [0, 4]|\n+---+---+---------+\n</code></pre>","text":""},{"location":"physical-operators/WindowExec/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.WindowExec</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.WindowExec=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"physical-operators/WindowExecBase/","title":"WindowExecBase Unary Physical Operators","text":"<p><code>WindowExecBase</code> is an extension of the UnaryExecNode abstraction for window unary physical operators.</p>"},{"location":"physical-operators/WindowExecBase/#contract","title":"Contract","text":""},{"location":"physical-operators/WindowExecBase/#orderspec","title":"orderSpec <pre><code>orderSpec: Seq[SortOrder]\n</code></pre> <p>Order Specification (SortOrder expressions)</p> <p>Used when:</p> <ul> <li><code>WindowExecBase</code> is requested to createBoundOrdering</li> </ul>","text":""},{"location":"physical-operators/WindowExecBase/#partitionspec","title":"partitionSpec <pre><code>partitionSpec: Seq[Expression]\n</code></pre> <p>Partition Specification (Expressions)</p>","text":""},{"location":"physical-operators/WindowExecBase/#windowexpression","title":"windowExpression <pre><code>windowExpression: Seq[NamedExpression]\n</code></pre> <p>Window Expressions (that are supposed to be WindowExpressions to build windowFrameExpressionFactoryPairs)</p> <p>Used when:</p> <ul> <li><code>WindowExecBase</code> is requested to createResultProjection and windowFrameExpressionFactoryPairs</li> </ul>","text":""},{"location":"physical-operators/WindowExecBase/#implementations","title":"Implementations","text":"<ul> <li>WindowExec</li> <li><code>WindowInPandasExec</code> (PySpark)</li> </ul>"},{"location":"physical-operators/WindowExecBase/#windowframeexpressionfactorypairs","title":"windowFrameExpressionFactoryPairs <pre><code>windowFrameExpressionFactoryPairs: Seq[(ExpressionBuffer, InternalRow =&gt; WindowFunctionFrame)]\n</code></pre> <p><code>windowFrameExpressionFactoryPairs</code> finds WindowExpressions in the given windowExpressions.</p> <p><code>windowFrameExpressionFactoryPairs</code> assumes that the WindowFrame (of the WindowSpecDefinition of the WindowExpression) is a <code>SpecifiedWindowFrame</code>.</p> <p><code>windowFrameExpressionFactoryPairs</code> branches off based on the window function (of the WindowExpression):</p> <ol> <li>AggregateExpression</li> <li><code>FrameLessOffsetWindowFunction</code></li> <li>OffsetWindowFunction</li> <li>AggregateWindowFunction</li> <li><code>PythonUDF</code> (PySpark)</li> </ol>  Lazy Value <p><code>windowFrameExpressionFactoryPairs</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>","text":""},{"location":"physical-operators/WindowExecBase/#createboundordering","title":"createBoundOrdering <pre><code>createBoundOrdering(\n  frame: FrameType,\n  bound: Expression,\n  timeZone: String): BoundOrdering\n</code></pre> <p><code>createBoundOrdering</code>...FIXME</p>","text":""},{"location":"physical-operators/WriteFilesExec/","title":"WriteFilesExec Unary Physical Operator","text":"<p><code>WriteFilesExec</code> is a UnaryExecNode physical operator that represents WriteFiles logical operator at execution time.</p>"},{"location":"physical-operators/WriteFilesExec/#creating-instance","title":"Creating Instance","text":"<p><code>WriteFilesExec</code> takes the following to be created:</p> <ul> <li> Child SparkPlan <li> FileFormat <li> Partition Columns (Attributes) <li> BucketSpec <li> Options <li> Static Partitions (<code>TablePartitionSpec</code>) <p><code>WriteFilesExec</code> is created when:</p> <ul> <li>BasicOperators execution planning strategy is executed (to plan a <code>WriteFiles</code> logical operator)</li> </ul>"},{"location":"physical-optimizations/AQEShuffleReadRule/","title":"AQEShuffleReadRule Physical Optimization Rules","text":"<p><code>AQEShuffleReadRule</code> is an extension of the Rule abstraction for physical optimization rules (<code>Rule[SparkPlan]</code>) that supportedShuffleOrigins.</p>"},{"location":"physical-optimizations/AQEShuffleReadRule/#contract","title":"Contract","text":""},{"location":"physical-optimizations/AQEShuffleReadRule/#supported-shuffleorigins","title":"Supported ShuffleOrigins <pre><code>supportedShuffleOrigins: Seq[ShuffleOrigin]\n</code></pre> <p>Used when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested for the final physical query plan</li> <li>others (FIXME: perhaps not as important?)</li> </ul>","text":""},{"location":"physical-optimizations/AQEShuffleReadRule/#implementations","title":"Implementations","text":"<ul> <li>CoalesceShufflePartitions</li> <li>OptimizeShuffleWithLocalRead</li> <li>OptimizeSkewedJoin</li> <li>OptimizeSkewInRebalancePartitions</li> </ul>"},{"location":"physical-optimizations/AdjustShuffleExchangePosition/","title":"AdjustShuffleExchangePosition Physical Optimization","text":"<p><code>AdjustShuffleExchangePosition</code> is...FIXME</p>"},{"location":"physical-optimizations/ApplyColumnarRulesAndInsertTransitions/","title":"ApplyColumnarRulesAndInsertTransitions Physical Optimization","text":"<p><code>ApplyColumnarRulesAndInsertTransitions</code> is a physical query plan optimization.</p> <p><code>ApplyColumnarRulesAndInsertTransitions</code> is a Catalyst rule for transforming physical plans (<code>Rule[SparkPlan]</code>).</p> <p><code>ApplyColumnarRulesAndInsertTransitions</code> is very similar (in how it optimizes physical query plans) to CollapseCodegenStages physical optimization for Whole-Stage Java Code Generation.</p>"},{"location":"physical-optimizations/ApplyColumnarRulesAndInsertTransitions/#creating-instance","title":"Creating Instance","text":"<p><code>ApplyColumnarRulesAndInsertTransitions</code> takes the following to be created:</p> <ul> <li> SQLConf <li> ColumnarRules <p><code>ApplyColumnarRulesAndInsertTransitions</code> is created when:</p> <ul> <li><code>QueryExecution</code> utility is requested for preparations optimizations</li> <li>AdaptiveSparkPlanExec physical operator is requested for adaptive optimization</li> </ul>"},{"location":"physical-optimizations/ApplyColumnarRulesAndInsertTransitions/#executing-rule","title":"Executing Rule  Signature <pre><code>apply(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p>  <p><code>apply</code>...FIXME</p>","text":""},{"location":"physical-optimizations/ApplyColumnarRulesAndInsertTransitions/#inserting-columnartorowexec-transitions","title":"Inserting ColumnarToRowExec Transitions <pre><code>insertTransitions(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>insertTransitions</code> creates a ColumnarToRowExec physical operator for the given SparkPlan that supportsColumnar. The child of the <code>ColumnarToRowExec</code> operator is created using insertRowToColumnar.</p>","text":""},{"location":"physical-optimizations/ApplyColumnarRulesAndInsertTransitions/#inserting-rowtocolumnarexec-transitions","title":"Inserting RowToColumnarExec Transitions <pre><code>insertRowToColumnar(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>insertRowToColumnar</code> does nothing (and returns the given SparkPlan) when the following all happen:</p> <ol> <li>The given physical operator supportsColumnar</li> <li>The given physical operator is <code>RowToColumnarTransition</code> (e.g., RowToColumnarExec)</li> </ol> <p>If the given physical operator does not supportsColumnar, <code>insertRowToColumnar</code> creates a RowToColumnarExec physical operator for the given SparkPlan. The child of the <code>RowToColumnarExec</code> operator is created using insertTransitions (with <code>outputsColumnar</code> flag disabled).</p> <p>If the given physical operator does supportsColumnar but it is not a <code>RowToColumnarTransition</code>, <code>insertRowToColumnar</code> replaces the child operators (of the physical operator) to insertRowToColumnar recursively.</p>","text":""},{"location":"physical-optimizations/CoalesceBucketsInJoin/","title":"CoalesceBucketsInJoin Physical Optimization","text":"<p><code>CoalesceBucketsInJoin</code> is a physical query optimization (aka physical query preparation rule or simply preparation rule).</p> <p><code>CollapseCodegenStages</code> is a Catalyst rule for transforming physical query plans (<code>Rule[SparkPlan]</code>).</p> <p><code>CoalesceBucketsInJoin</code> is part of the preparations batch of physical query plan rules and is executed when <code>QueryExecution</code> is requested for the optimized physical query plan (in executedPlan phase of a query execution).</p>"},{"location":"physical-optimizations/CoalesceBucketsInJoin/#sparksqlbucketingcoalescebucketsinjoinenabled","title":"spark.sql.bucketing.coalesceBucketsInJoin.enabled <p><code>CoalesceBucketsInJoin</code> uses the spark.sql.bucketing.coalesceBucketsInJoin.enabled configuration property.</p>","text":""},{"location":"physical-optimizations/CoalesceBucketsInJoin/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>apply</code>\u00a0is part of the Rule abstraction.</p> <p><code>apply</code> is a noop with the spark.sql.bucketing.coalesceBucketsInJoin.enabled configuration property turned off.</p> <p><code>apply</code> uses ExtractJoinWithBuckets to match on BaseJoinExec physical operators.</p> <p><code>apply</code>...FIXME</p>","text":""},{"location":"physical-optimizations/CoalesceShufflePartitions/","title":"CoalesceShufflePartitions Adaptive Physical Optimization","text":"<p><code>CoalesceShufflePartitions</code> is a physical optimization in Adaptive Query Execution.</p>"},{"location":"physical-optimizations/CoalesceShufflePartitions/#creating-instance","title":"Creating Instance","text":"<p><code>CoalesceShufflePartitions</code> takes the following to be created:</p> <ul> <li> SparkSession <p><code>CoalesceShufflePartitions</code> is created when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested for the QueryStage Optimizer Rules</li> </ul>"},{"location":"physical-optimizations/CoalesceShufflePartitions/#sparksqladaptivecoalescepartitionsenabled","title":"spark.sql.adaptive.coalescePartitions.enabled <p><code>CoalesceShufflePartitions</code> is enabled by default and can be turned off using spark.sql.adaptive.coalescePartitions.enabled configuration property.</p>","text":""},{"location":"physical-optimizations/CoalesceShufflePartitions/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p>  <p><code>apply</code> does nothing (and simply gives the input SparkPlan back unmodified) when one of the following holds:</p> <ol> <li> <p>spark.sql.adaptive.coalescePartitions.enabled configuration property is disabled</p> </li> <li> <p>The leaf physical operators are not all QueryStageExecs (as it's not safe to reduce the number of shuffle partitions, because it may break the assumption that all children of a spark plan have same number of output partitions).</p> </li> <li> <p>There is a ShuffleExchangeLike among the ShuffleQueryStageExecs (of the given SparkPlan) that is not supported</p> </li> </ol> <p><code>apply</code> coalesces the partitions (with the <code>MapOutputStatistics</code> and <code>ShufflePartitionSpec</code> of the <code>ShuffleStageInfo</code>s) based on the following settings:</p> <ul> <li> <p>The minimum number of partitions being the default Spark parallelism with spark.sql.adaptive.coalescePartitions.parallelismFirst enabled or <code>1</code></p> </li> <li> <p>spark.sql.adaptive.advisoryPartitionSizeInBytes as the advisory target size of partitions</p> </li> <li> <p>spark.sql.adaptive.coalescePartitions.minPartitionSize as the minimum size of partitions</p> </li> </ul>","text":""},{"location":"physical-optimizations/CoalesceShufflePartitions/#updateshufflereads","title":"updateShuffleReads <pre><code>updateShuffleReads(\n  plan: SparkPlan,\n  specsMap: Map[Int, Seq[ShufflePartitionSpec]]): SparkPlan\n</code></pre> <p><code>updateShuffleReads</code>...FIXME</p>","text":""},{"location":"physical-optimizations/CoalesceShufflePartitions/#supported-shuffleorigins","title":"Supported ShuffleOrigins <pre><code>supportedShuffleOrigins: Seq[ShuffleOrigin]\n</code></pre> <p><code>supportedShuffleOrigins</code> is the following <code>ShuffleOrigin</code>s:</p> <ul> <li><code>ENSURE_REQUIREMENTS</code></li> <li><code>REPARTITION_BY_COL</code></li> <li><code>REBALANCE_PARTITIONS_BY_NONE</code></li> <li><code>REBALANCE_PARTITIONS_BY_COL</code></li> </ul> <p><code>supportedShuffleOrigins</code> is part of the AQEShuffleReadRule abstraction.</p>","text":""},{"location":"physical-optimizations/CollapseCodegenStages/","title":"CollapseCodegenStages Physical Optimization","text":"<p><code>CollapseCodegenStages</code> is a physical query optimization (aka physical query preparation rule or simply preparation rule) that collapses physical operators and generates a Java source code for their execution.</p> <p>When executed (with whole-stage code generation enabled), <code>CollapseCodegenStages</code> inserts WholeStageCodegenExec or InputAdapter physical operators to the physical plan. <code>CollapseCodegenStages</code> uses so-called control gates before deciding whether a physical operator supports the whole-stage Java code generation or not (and what physical operator to insert):</p> <ol> <li> <p>Factors in physical operators with CodegenSupport only</p> </li> <li> <p>Enforces the supportCodegen custom requirements on a physical operator, i.e.</p> </li> <li>supportCodegen flag turned on</li> <li>No Catalyst expressions are CodegenFallback</li> <li>Output schema is neither wide nor deep and uses just enough fields (including nested fields)</li> <li>Children use output schema that is also neither wide nor deep</li> </ol> <p><code>CollapseCodegenStages</code> is a Catalyst rule for transforming physical query plans (<code>Rule[SparkPlan]</code>).</p> <p><code>CollapseCodegenStages</code> is part of the preparations batch of physical query plan rules and is executed when <code>QueryExecution</code> is requested for the optimized physical query plan (in executedPlan phase of a query execution).</p> <p>With spark.sql.codegen.wholeStage internal configuration property enabled, <code>CollapseCodegenStages</code> finds physical operators with CodegenSupport for which whole-stage codegen requirements hold and collapses them together as <code>WholeStageCodegenExec</code> physical operator (possibly with InputAdapter in-between for physical operators with no support for Java code generation).</p> Note <p><code>InputAdapter</code> shows itself with no star in the output of explain (or TreeNode.numberedTreeString).</p> <pre><code>val q = spark.range(1).groupBy(\"id\").count\nscala&gt; q.explain\n== Physical Plan ==\n*HashAggregate(keys=[id#16L], functions=[count(1)])\n+- Exchange hashpartitioning(id#16L, 200)\n   +- *HashAggregate(keys=[id#16L], functions=[partial_count(1)])\n      +- *Range (0, 1, step=1, splits=8)\n</code></pre>"},{"location":"physical-optimizations/CollapseCodegenStages/#creating-instance","title":"Creating Instance","text":"<p><code>CollapseCodegenStages</code> takes the following to be created:</p> <ul> <li> Codegen Stage Counter (Java's AtomicInteger) <p><code>CollapseCodegenStages</code> is created\u00a0when:</p> <ul> <li><code>QueryExecution</code> utility is used for the preparations batch</li> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested for the postStageCreationRules</li> </ul>"},{"location":"physical-optimizations/CollapseCodegenStages/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p> <p><code>apply</code> starts inserting WholeStageCodegenExec (with InputAdapter) in the input <code>plan</code> physical plan only when spark.sql.codegen.wholeStage configuration property.</p> <p>Otherwise, <code>apply</code> does nothing at all (i.e. passes the input physical plan through unchanged).</p>","text":""},{"location":"physical-optimizations/CollapseCodegenStages/#inserting-wholestagecodegenexec-physical-operators-for-codegen-stages","title":"Inserting WholeStageCodegenExec Physical Operators For Codegen Stages <pre><code>insertWholeStageCodegen(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>insertWholeStageCodegen</code> branches off per physical operator:</p> <ol> <li> <p>For physical operators with a single output schema attribute of type <code>ObjectType</code>, <code>insertWholeStageCodegen</code> requests the operator for the child physical operators and tries to insertWholeStageCodegen on them only.</p> </li> <li> <p>For physical operators that support Java code generation and meets the additional requirements for codegen, <code>insertWholeStageCodegen</code> insertInputAdapter (with the operator), requests <code>WholeStageCodegenId</code> for the <code>getNextStageId</code> and then uses both to return a new WholeStageCodegenExec physical operator.</p> </li> <li> <p>For any other physical operators, <code>insertWholeStageCodegen</code> requests the operator for the child physical operators and tries to insertWholeStageCodegen on them only.</p> </li> </ol> <p><code>insertWholeStageCodegen</code> skips physical operators with a single-attribute output schema with the type of the attribute being <code>ObjectType</code> type.</p>","text":""},{"location":"physical-optimizations/CollapseCodegenStages/#inserting-inputadapter-unary-physical-operator","title":"Inserting InputAdapter Unary Physical Operator <pre><code>insertInputAdapter(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>insertInputAdapter</code> inserts an InputAdapter physical operator in a physical plan.</p> <ul> <li> <p>For SortMergeJoinExec (with inner and outer joins) inserts an InputAdapter operator for both children physical operators individually</p> </li> <li> <p>For codegen-unsupported operators inserts an InputAdapter operator</p> </li> <li> <p>For other operators (except <code>SortMergeJoinExec</code> operator above or for which Java code cannot be generated) inserts a WholeStageCodegenExec operator for every child operator</p> </li> </ul>  <p>Fixme</p> <p>Examples for every case + screenshots from web UI</p>","text":""},{"location":"physical-optimizations/CollapseCodegenStages/#supportcodegen","title":"supportCodegen","text":""},{"location":"physical-optimizations/CollapseCodegenStages/#physical-operator","title":"Physical Operator <pre><code>supportCodegen(\n  plan: SparkPlan): Boolean\n</code></pre> <p><code>supportCodegen</code> is positive (<code>true</code>) when the input physical operator is as follows:</p> <ol> <li> <p>CodegenSupport and the supportCodegen flag is on (it is on by default)</p> </li> <li> <p>No Catalyst expressions are CodegenFallback (except LeafExpressions)</p> </li> <li> <p>Output schema is neither wide not deep, i.e. uses just enough fields (including nested fields)</p> </li> </ol> <p>. Children also have the output schema that is neither wide nor deep</p> <p>Otherwise, <code>supportCodegen</code> is negative (<code>false</code>).</p>","text":""},{"location":"physical-optimizations/CollapseCodegenStages/#expression","title":"Expression <pre><code>supportCodegen(\n  e: Expression): Boolean\n</code></pre> <p><code>supportCodegen</code> is positive (<code>true</code>) when the input Catalyst expression is the following (in the order of verification):</p> <ol> <li> <p>leaf</p> </li> <li> <p>non-CodegenFallback</p> </li> </ol> <p>Otherwise, <code>supportCodegen</code> is negative (<code>false</code>).</p>","text":""},{"location":"physical-optimizations/CollapseCodegenStages/#demo","title":"Demo <pre><code>// FIXME: DEMO\n// Step 1. The top-level physical operator is CodegenSupport with supportCodegen enabled\n// Step 2. The top-level operator is CodegenSupport with supportCodegen disabled\n// Step 3. The top-level operator is not CodegenSupport\n// Step 4. \"plan.output.length == 1 &amp;&amp; plan.output.head.dataType.isInstanceOf[ObjectType]\"\n</code></pre>","text":""},{"location":"physical-optimizations/CollapseCodegenStages/#demo_1","title":"Demo <pre><code>import org.apache.spark.sql.SparkSession\nval spark: SparkSession = ...\n// both where and select operators support codegen\n// the plan tree (with the operators and expressions) meets the requirements\n// That's why the plan has WholeStageCodegenExec inserted\n// That you can see as stars (*) in the output of explain\nval q = Seq((1,2,3)).toDF(\"id\", \"c0\", \"c1\").where('id === 0).select('c0)\nscala&gt; q.explain\n== Physical Plan ==\n*Project [_2#89 AS c0#93]\n+- *Filter (_1#88 = 0)\n   +- LocalTableScan [_1#88, _2#89, _3#90]\n\n// CollapseCodegenStages is only used in QueryExecution.executedPlan\n// Use sparkPlan then so we avoid CollapseCodegenStages\nval plan = q.queryExecution.sparkPlan\nimport org.apache.spark.sql.execution.ProjectExec\nval pe = plan.asInstanceOf[ProjectExec]\n\nscala&gt; pe.supportCodegen\nres1: Boolean = true\n\nscala&gt; pe.schema.fields.size\nres2: Int = 1\n\nscala&gt; pe.children.map(_.schema).map(_.size).sum\nres3: Int = 3\n</code></pre> <pre><code>import org.apache.spark.sql.SparkSession\nval spark: SparkSession = ...\n// both where and select support codegen\n// let's break the requirement of spark.sql.codegen.maxFields\nval newSpark = spark.newSession()\nimport org.apache.spark.sql.internal.SQLConf.WHOLESTAGE_MAX_NUM_FIELDS\nnewSpark.sessionState.conf.setConf(WHOLESTAGE_MAX_NUM_FIELDS, 2)\n\nscala&gt; println(newSpark.sessionState.conf.wholeStageMaxNumFields)\n2\n\nimport newSpark.implicits._\n// the same query as above but created in SparkSession with WHOLESTAGE_MAX_NUM_FIELDS as 2\nval q = Seq((1,2,3)).toDF(\"id\", \"c0\", \"c1\").where('id === 0).select('c0)\n\n// Note that there are no stars in the output of explain\n// No WholeStageCodegenExec operator in the plan =&gt; whole-stage codegen disabled\nscala&gt; q.explain\n== Physical Plan ==\nProject [_2#122 AS c0#126]\n+- Filter (_1#121 = 0)\n   +- LocalTableScan [_1#121, _2#122, _3#123]\n</code></pre>","text":""},{"location":"physical-optimizations/CollapseCodegenStages/#demo_2","title":"Demo <pre><code>val q = spark.range(3).groupBy('id % 2 as \"gid\").count\n</code></pre> <pre><code>// Let's see where and how many \"stars\" does this query get\nscala&gt; q.explain\n== Physical Plan ==\n*(2) HashAggregate(keys=[(id#0L % 2)#9L], functions=[count(1)])\n+- Exchange hashpartitioning((id#0L % 2)#9L, 200)\n   +- *(1) HashAggregate(keys=[(id#0L % 2) AS (id#0L % 2)#9L], functions=[partial_count(1)])\n      +- *(1) Range (0, 3, step=1, splits=8)\n\n// There are two stage IDs: 1 and 2 (see the round brackets)\n// Looks like Exchange physical operator does not support codegen\n// Let's walk through the query execution phases and see it ourselves\n\n// sparkPlan phase is just before CollapseCodegenStages physical optimization is applied\nval sparkPlan = q.queryExecution.sparkPlan\nscala&gt; println(sparkPlan.numberedTreeString)\n00 HashAggregate(keys=[(id#0L % 2)#12L], functions=[count(1)], output=[gid#2L, count#5L])\n01 +- HashAggregate(keys=[(id#0L % 2) AS (id#0L % 2)#12L], functions=[partial_count(1)], output=[(id#0L % 2)#12L, count#11L])\n02    +- Range (0, 3, step=1, splits=8)\n\n// Compare the above with the executedPlan phase\n// which happens to be after CollapseCodegenStages physical optimization\nscala&gt; println(q.queryExecution.executedPlan.numberedTreeString)\n00 *(2) HashAggregate(keys=[(id#0L % 2)#12L], functions=[count(1)], output=[gid#2L, count#5L])\n01 +- Exchange hashpartitioning((id#0L % 2)#12L, 200)\n02    +- *(1) HashAggregate(keys=[(id#0L % 2) AS (id#0L % 2)#12L], functions=[partial_count(1)], output=[(id#0L % 2)#12L, count#11L])\n03       +- *(1) Range (0, 3, step=1, splits=8)\n\n// Let's apply the CollapseCodegenStages rule ourselves\nimport org.apache.spark.sql.execution.CollapseCodegenStages\nval ccsRule = CollapseCodegenStages(spark.sessionState.conf)\nscala&gt; val planAfterCCS = ccsRule.apply(sparkPlan)\nplanAfterCCS: org.apache.spark.sql.execution.SparkPlan =\n*(1) HashAggregate(keys=[(id#0L % 2)#12L], functions=[count(1)], output=[gid#2L, count#5L])\n+- *(1) HashAggregate(keys=[(id#0L % 2) AS (id#0L % 2)#12L], functions=[partial_count(1)], output=[(id#0L % 2)#12L, count#11L])\n   +- *(1) Range (0, 3, step=1, splits=8)\n\n// The number of stage IDs do not match\n// Looks like the above misses one or more rules\n// EnsureRequirements optimization rule?\n// It is indeed executed before CollapseCodegenStages\nimport org.apache.spark.sql.execution.exchange.EnsureRequirements\nval erRule = EnsureRequirements(spark.sessionState.conf)\nval planAfterER = erRule.apply(sparkPlan)\nscala&gt; println(planAfterER.numberedTreeString)\n00 HashAggregate(keys=[(id#0L % 2)#12L], functions=[count(1)], output=[gid#2L, count#5L])\n01 +- Exchange hashpartitioning((id#0L % 2)#12L, 200)\n02    +- HashAggregate(keys=[(id#0L % 2) AS (id#0L % 2)#12L], functions=[partial_count(1)], output=[(id#0L % 2)#12L, count#11L])\n03       +- Range (0, 3, step=1, splits=8)\n\n// Time for CollapseCodegenStages\nval planAfterCCS = ccsRule.apply(planAfterER)\nscala&gt; println(planAfterCCS.numberedTreeString)\n00 *(2) HashAggregate(keys=[(id#0L % 2)#12L], functions=[count(1)], output=[gid#2L, count#5L])\n01 +- Exchange hashpartitioning((id#0L % 2)#12L, 200)\n02    +- *(1) HashAggregate(keys=[(id#0L % 2) AS (id#0L % 2)#12L], functions=[partial_count(1)], output=[(id#0L % 2)#12L, count#11L])\n03       +- *(1) Range (0, 3, step=1, splits=8)\n\nassert(planAfterCCS == q.queryExecution.executedPlan, \"Plan after ER and CCS rules should match the executedPlan plan\")\n\n// Bingo!\n// The result plan matches the executedPlan plan\n\n// HashAggregateExec and Range physical operators support codegen (is a CodegenSupport)\n// - HashAggregateExec disables codegen for ImperativeAggregate aggregate functions\n// ShuffleExchangeExec does not support codegen (is not a CodegenSupport)\n\n// The top-level physical operator should be WholeStageCodegenExec\nimport org.apache.spark.sql.execution.WholeStageCodegenExec\nval wsce = planAfterCCS.asInstanceOf[WholeStageCodegenExec]\n\n// The single child operator should be HashAggregateExec\nimport org.apache.spark.sql.execution.aggregate.HashAggregateExec\nval hae = wsce.child.asInstanceOf[HashAggregateExec]\n\n// Since ShuffleExchangeExec does not support codegen, the child of HashAggregateExec is InputAdapter\nimport org.apache.spark.sql.execution.InputAdapter\nval ia = hae.child.asInstanceOf[InputAdapter]\n\n// And it's only now when we can get at ShuffleExchangeExec\nimport org.apache.spark.sql.execution.exchange.ShuffleExchangeExec\nval se = ia.child.asInstanceOf[ShuffleExchangeExec]\n</code></pre>","text":""},{"location":"physical-optimizations/CollapseCodegenStages/#demo_3","title":"Demo <pre><code>import org.apache.spark.sql.SparkSession\nval spark: SparkSession = ...\n// Just a structured query with explode Generator expression that supports codegen \"partially\"\n// i.e. explode extends CodegenSupport but codegenSupport flag is off\nval q = spark.range(2)\n  .filter($\"id\" === 0)\n  .select(explode(lit(Array(0,1,2))) as \"exploded\")\n  .join(spark.range(2))\n  .where($\"exploded\" === $\"id\")\nscala&gt; q.show\n+--------+---+\n|exploded| id|\n+--------+---+\n|       0|  0|\n|       1|  1|\n+--------+---+\n\n// the final physical plan (after CollapseCodegenStages applied and the other optimization rules)\nscala&gt; q.explain\n== Physical Plan ==\n*BroadcastHashJoin [cast(exploded#34 as bigint)], [id#37L], Inner, BuildRight\n:- *Filter isnotnull(exploded#34)\n:  +- Generate explode([0,1,2]), false, false, [exploded#34]\n:     +- *Project\n:        +- *Filter (id#29L = 0)\n:           +- *Range (0, 2, step=1, splits=8)\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]))\n   +- *Range (0, 2, step=1, splits=8)\n\n// Control when CollapseCodegenStages is applied to a query plan\n// Take sparkPlan that is a physical plan before optimizations, incl. CollapseCodegenStages\nval plan = q.queryExecution.sparkPlan\n\n// Is wholeStageEnabled enabled?\n// It is by default\nscala&gt; println(spark.sessionState.conf.wholeStageEnabled)\ntrue\n\nimport org.apache.spark.sql.execution.CollapseCodegenStages\nval ccs = CollapseCodegenStages(conf = spark.sessionState.conf)\n\nscala&gt; ccs.ruleName\nres0: String = org.apache.spark.sql.execution.CollapseCodegenStages\n\n// Before CollapseCodegenStages\nscala&gt; println(plan.numberedTreeString)\n00 BroadcastHashJoin [cast(exploded#34 as bigint)], [id#37L], Inner, BuildRight\n01 :- Filter isnotnull(exploded#34)\n02 :  +- Generate explode([0,1,2]), false, false, [exploded#34]\n03 :     +- Project\n04 :        +- Filter (id#29L = 0)\n05 :           +- Range (0, 2, step=1, splits=8)\n06 +- Range (0, 2, step=1, splits=8)\n\n// After CollapseCodegenStages\n// Note the stars (that WholeStageCodegenExec.generateTreeString gives)\nval execPlan = ccs.apply(plan)\nscala&gt; println(execPlan.numberedTreeString)\n00 *BroadcastHashJoin [cast(exploded#34 as bigint)], [id#37L], Inner, BuildRight\n01 :- *Filter isnotnull(exploded#34)\n02 :  +- Generate explode([0,1,2]), false, false, [exploded#34]\n03 :     +- *Project\n04 :        +- *Filter (id#29L = 0)\n05 :           +- *Range (0, 2, step=1, splits=8)\n06 +- *Range (0, 2, step=1, splits=8)\n\n// The first star is from WholeStageCodegenExec physical operator\nimport org.apache.spark.sql.execution.WholeStageCodegenExec\nval wsc = execPlan(0).asInstanceOf[WholeStageCodegenExec]\nscala&gt; println(wsc.numberedTreeString)\n00 *BroadcastHashJoin [cast(exploded#34 as bigint)], [id#37L], Inner, BuildRight\n01 :- *Filter isnotnull(exploded#34)\n02 :  +- Generate explode([0,1,2]), false, false, [exploded#34]\n03 :     +- *Project\n04 :        +- *Filter (id#29L = 0)\n05 :           +- *Range (0, 2, step=1, splits=8)\n06 +- *Range (0, 2, step=1, splits=8)\n\n// Let's disable wholeStage codegen\n// CollapseCodegenStages becomes a noop\n// It is as if we were not applied Spark optimizations to a physical plan\n// We're selective as we only disable whole-stage codegen\nval newSpark = spark.newSession()\nimport org.apache.spark.sql.internal.SQLConf.WHOLESTAGE_CODEGEN_ENABLED\nnewSpark.sessionState.conf.setConf(WHOLESTAGE_CODEGEN_ENABLED, false)\nscala&gt; println(newSpark.sessionState.conf.wholeStageEnabled)\nfalse\n\n// Whole-stage codegen is disabled\n// So regardless whether you do apply Spark optimizations or not\n// Java code generation won't take place\nval ccsWholeStageDisabled = CollapseCodegenStages(conf = newSpark.sessionState.conf)\nval execPlan = ccsWholeStageDisabled.apply(plan)\n// Note no stars in the output\nscala&gt; println(execPlan.numberedTreeString)\n00 BroadcastHashJoin [cast(exploded#34 as bigint)], [id#37L], Inner, BuildRight\n01 :- Filter isnotnull(exploded#34)\n02 :  +- Generate explode([0,1,2]), false, false, [exploded#34]\n03 :     +- Project\n04 :        +- Filter (id#29L = 0)\n05 :           +- Range (0, 2, step=1, splits=8)\n06 +- Range (0, 2, step=1, splits=8)\n</code></pre>","text":""},{"location":"physical-optimizations/DisableUnnecessaryBucketedScan/","title":"DisableUnnecessaryBucketedScan Physical Optimization","text":"<p><code>DisableUnnecessaryBucketedScan</code> is a physical query plan optimization.</p> <p><code>DisableUnnecessaryBucketedScan</code> is a Catalyst Rule for transforming SparkPlans (<code>Rule[SparkPlan]</code>).</p> <p><code>DisableUnnecessaryBucketedScan</code> is used when:</p> <ul> <li><code>QueryExecution</code> utility is used for preparations rules</li> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested for the physical preparation rules</li> </ul>"},{"location":"physical-optimizations/DisableUnnecessaryBucketedScan/#creating-instance","title":"Creating Instance","text":"<p><code>DisableUnnecessaryBucketedScan</code> takes no input arguments to be created.</p>"},{"location":"physical-optimizations/DisableUnnecessaryBucketedScan/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>apply</code> finds FileSourceScanExec physical operators with bucketedScan enabled. If there are any, the given SparkPlan is considered to be a bucketed scan.</p> <p><code>apply</code> disableBucketWithInterestingPartition unless any of the configuration properties is <code>false</code> (and <code>apply</code> returns the given SparkPlan):</p> <ul> <li>spark.sql.sources.bucketing.enabled</li> <li>spark.sql.sources.bucketing.autoBucketedScan.enabled</li> </ul> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"physical-optimizations/EnsureRequirements/","title":"EnsureRequirements Physical Optimization","text":"<p>[[apply]] <code>EnsureRequirements</code> is a physical query optimization (aka physical query preparation rule or simply preparation rule) that <code>QueryExecution</code> uses to optimize the physical plan of a structured query by transforming the following physical operators (up the plan tree):</p> <p>. Removes two adjacent ShuffleExchangeExec.md[ShuffleExchangeExec] physical operators if the child partitioning scheme guarantees the parent's partitioning</p> <p>. For other non-<code>ShuffleExchangeExec</code> physical operators, &lt;&gt; (possibly adding new physical operators, e.g. BroadcastExchangeExec.md[BroadcastExchangeExec] and ShuffleExchangeExec.md[ShuffleExchangeExec] for distribution or SortExec.md[SortExec] for sorting) <p>Technically, <code>EnsureRequirements</code> is just a catalyst/Rule.md[Catalyst rule] for transforming SparkPlan.md[physical query plans], i.e. <code>Rule[SparkPlan]</code>.</p> <p><code>EnsureRequirements</code> is part of preparations batch of physical query plan rules and is executed when <code>QueryExecution</code> is requested for the optimized physical query plan (i.e. in executedPlan phase of a query execution).</p> <p>[[conf]] <code>EnsureRequirements</code> takes a SQLConf when created.</p>"},{"location":"physical-optimizations/EnsureRequirements/#source-scala","title":"[source, scala]","text":"<p>val q = ??? // FIXME val sparkPlan = q.queryExecution.sparkPlan</p> <p>import org.apache.spark.sql.execution.exchange.EnsureRequirements val plan = EnsureRequirements(spark.sessionState.conf).apply(sparkPlan)</p> <p>=== [[createPartitioning]] <code>createPartitioning</code> Internal Method</p> <p>CAUTION: FIXME</p> <p>=== [[defaultNumPreShufflePartitions]] <code>defaultNumPreShufflePartitions</code> Internal Method</p> <p>CAUTION: FIXME</p>"},{"location":"physical-optimizations/EnsureRequirements/#enforcing-partition-requirements-distribution-and-ordering-of-physical-operator","title":"Enforcing Partition Requirements (Distribution and Ordering) of Physical Operator <pre><code>ensureDistributionAndOrdering(\n  operator: SparkPlan): SparkPlan\n</code></pre> <p><code>ensureDistributionAndOrdering</code> takes the following from the input physical <code>operator</code>:</p> <ul> <li> <p>SparkPlan.md#requiredChildDistribution[required partition requirements] for the children</p> </li> <li> <p>SparkPlan.md#requiredChildOrdering[required sort ordering] per the required partition requirements per child</p> </li> <li> <p>child physical plans</p> </li> </ul> <p>NOTE: The number of requirements for partitions and their sort ordering has to match the number and the order of the child physical plans.</p> <p><code>ensureDistributionAndOrdering</code> matches the operator's required partition requirements of children (<code>requiredChildDistributions</code>) to the children's SparkPlan.md#outputPartitioning[output partitioning] and (in that order):</p> <p>. If the child satisfies the requested distribution, the child is left unchanged</p> <p>. For BroadcastDistribution, the child becomes the child of BroadcastExchangeExec.md[BroadcastExchangeExec] unary operator for BroadcastHashJoinExec.md[broadcast hash joins]</p> <p>. Any other pair of child and distribution leads to ShuffleExchangeExec.md[ShuffleExchangeExec] unary physical operator (with proper &lt;&gt; for distribution and with spark.sql.shuffle.partitions number of partitions) <p>NOTE: ShuffleExchangeExec.md[ShuffleExchangeExec] can appear in the physical plan when the children's output partitioning cannot satisfy the physical operator's required child distribution.</p> <p>If the input <code>operator</code> has multiple children and specifies child output distributions, then the children's SparkPlan.md#outputPartitioning[output partitionings] have to be compatible.</p> <p>If the children's output partitionings are not all compatible, then...FIXME</p> <p><code>ensureDistributionAndOrdering</code> &lt;&gt; (only when Adaptive Query Execution is enabled which is not by default). <p>NOTE: At this point in <code>ensureDistributionAndOrdering</code> the required child distributions are already handled.</p> <p><code>ensureDistributionAndOrdering</code> matches the operator's required sort ordering of children (<code>requiredChildOrderings</code>) to the children's SparkPlan.md#outputPartitioning[output partitioning] and if the orderings do not match, SortExec.md#creating-instance[SortExec] unary physical operator is created as a new child.</p> <p>In the end, <code>ensureDistributionAndOrdering</code> sets the new children for the input <code>operator</code>.</p> <p><code>ensureDistributionAndOrdering</code> is used when <code>EnsureRequirements</code> physical optimization is executed.</p> <p>=== [[reorderJoinPredicates]] <code>reorderJoinPredicates</code> Internal Method</p>","text":""},{"location":"physical-optimizations/EnsureRequirements/#source-scala_1","title":"[source, scala]","text":""},{"location":"physical-optimizations/EnsureRequirements/#reorderjoinpredicatesplan-sparkplan-sparkplan","title":"reorderJoinPredicates(plan: SparkPlan): SparkPlan <p><code>reorderJoinPredicates</code>...FIXME</p> <p>NOTE: <code>reorderJoinPredicates</code> is used when...FIXME</p>","text":""},{"location":"physical-optimizations/ExtractPythonUDFs/","title":"ExtractPythonUDFs Physical Query Optimization","text":"<p>[[apply]] <code>ExtractPythonUDFs</code> is a physical query optimization (aka physical query preparation rule or simply preparation rule) that <code>QueryExecution</code> uses to optimize the physical plan of a structured query by &lt;&gt; (excluding <code>FlatMapGroupsInPandasExec</code> operators that it simply skips over). <p>Technically, <code>ExtractPythonUDFs</code> is just a catalyst/Rule.md[Catalyst rule] for transforming SparkPlan.md[physical query plans], i.e. <code>Rule[SparkPlan]</code>.</p> <p><code>ExtractPythonUDFs</code> is part of preparations batch of physical query plan rules and is executed when <code>QueryExecution</code> is requested for the optimized physical query plan (i.e. in executedPlan phase of a query execution).</p> <p>=== [[extract]] Extracting Python UDFs from Physical Query Plan -- <code>extract</code> Internal Method</p>"},{"location":"physical-optimizations/ExtractPythonUDFs/#source-scala","title":"[source, scala]","text":""},{"location":"physical-optimizations/ExtractPythonUDFs/#extractplan-sparkplan-sparkplan","title":"extract(plan: SparkPlan): SparkPlan","text":"<p><code>extract</code>...FIXME</p> <p>NOTE: <code>extract</code> is used exclusively when <code>ExtractPythonUDFs</code> is requested to &lt;&gt;. <p>=== [[trySplitFilter]] <code>trySplitFilter</code> Internal Method</p>"},{"location":"physical-optimizations/ExtractPythonUDFs/#source-scala_1","title":"[source, scala]","text":""},{"location":"physical-optimizations/ExtractPythonUDFs/#trysplitfilterplan-sparkplan-sparkplan","title":"trySplitFilter(plan: SparkPlan): SparkPlan","text":"<p><code>trySplitFilter</code>...FIXME</p> <p>NOTE: <code>trySplitFilter</code> is used exclusively when <code>ExtractPythonUDFs</code> is requested to &lt;&gt;."},{"location":"physical-optimizations/InsertAdaptiveSparkPlan/","title":"InsertAdaptiveSparkPlan Physical Optimization","text":"<p><code>InsertAdaptiveSparkPlan</code> is a physical query plan optimization in Adaptive Query Execution that inserts AdaptiveSparkPlanExec operators.</p> <p><code>InsertAdaptiveSparkPlan</code> is a Rule to transform a SparkPlan (<code>Rule[SparkPlan]</code>).</p>"},{"location":"physical-optimizations/InsertAdaptiveSparkPlan/#creating-instance","title":"Creating Instance","text":"<p><code>InsertAdaptiveSparkPlan</code> takes the following to be created:</p> <ul> <li>AdaptiveExecutionContext</li> </ul> <p><code>InsertAdaptiveSparkPlan</code> is created when:</p> <ul> <li><code>QueryExecution</code> is requested for physical preparations rules</li> </ul>"},{"location":"physical-optimizations/InsertAdaptiveSparkPlan/#adaptiveexecutioncontext","title":"AdaptiveExecutionContext <p><code>InsertAdaptiveSparkPlan</code> is given an AdaptiveExecutionContext when created.</p> <p>The <code>AdaptiveExecutionContext</code> is used to create an AdaptiveSparkPlanExec physical operator (for a plan) when executed.</p>","text":""},{"location":"physical-optimizations/InsertAdaptiveSparkPlan/#adaptive-requirements","title":"Adaptive Requirements <pre><code>shouldApplyAQE(\n  plan: SparkPlan,\n  isSubquery: Boolean): Boolean\n</code></pre> <p><code>shouldApplyAQE</code> returns <code>true</code> when one of the following conditions holds:</p> <ol> <li>spark.sql.adaptive.forceApply configuration property is enabled</li> <li>The given <code>isSubquery</code> flag is <code>true</code> (a shortcut to continue since the input plan is from a sub-query and it was already decided to apply AQE for the main query)</li> <li>The given SparkPlan contains one of the following physical operators:<ol> <li>Exchange</li> <li>Operators with an UnspecifiedDistribution among the requiredChildDistribution (and the query may need to add exchanges)</li> <li>Operators with SubqueryExpression</li> </ol> </li> </ol>","text":""},{"location":"physical-optimizations/InsertAdaptiveSparkPlan/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p> <p><code>apply</code> applyInternal with the given SparkPlan and <code>isSubquery</code> flag disabled (<code>false</code>).</p>","text":""},{"location":"physical-optimizations/InsertAdaptiveSparkPlan/#applyinternal","title":"applyInternal <pre><code>applyInternal(\n  plan: SparkPlan,\n  isSubquery: Boolean): SparkPlan\n</code></pre> <p><code>applyInternal</code> returns the given SparkPlan unmodified when one of the following holds:</p> <ol> <li>spark.sql.adaptive.enabled configuration property is disabled (<code>false</code>)</li> <li>The given <code>SparkPlan</code> is either ExecutedCommandExec or <code>CommandResultExec</code></li> </ol> <p><code>applyInternal</code> skips processing the following parent physical operators and handles the children:</p> <ul> <li>DataWritingCommandExec</li> <li>V2CommandExec</li> </ul> <p>For all the other <code>SparkPlan</code>s, <code>applyInternal</code> checks out shouldApplyAQE condition. If holds, <code>applyInternal</code> checks out whether the physical plan supports Adaptive Query Execution or not.</p>","text":""},{"location":"physical-optimizations/InsertAdaptiveSparkPlan/#physical-plans-supporting-adaptive-query-execution","title":"Physical Plans Supporting Adaptive Query Execution <p><code>applyInternal</code> creates a new PlanAdaptiveSubqueries optimization (with subquery expressions) and executes it on the given <code>SparkPlan</code>.</p> <p><code>applyInternal</code> prints out the following DEBUG message to the logs:</p> <pre><code>Adaptive execution enabled for plan: [plan]\n</code></pre> <p>In the end, <code>applyInternal</code> creates an AdaptiveSparkPlanExec physical operator with the new pre-processed <code>SparkPlan</code>.</p> <p>In case of <code>SubqueryAdaptiveNotSupportedException</code>, <code>applyInternal</code> prints out the WARN message and returns the given physical plan.</p> <pre><code>spark.sql.adaptive.enabled is enabled but is not supported for sub-query: [subquery].\n</code></pre>","text":""},{"location":"physical-optimizations/InsertAdaptiveSparkPlan/#unsupported-physical-plans","title":"Unsupported Physical Plans <p><code>applyInternal</code> simply prints out the WARN message and returns the given physical plan.</p> <pre><code>spark.sql.adaptive.enabled is enabled but is not supported for query: [plan].\n</code></pre>","text":""},{"location":"physical-optimizations/InsertAdaptiveSparkPlan/#usage","title":"Usage <p><code>applyInternal</code> is used by <code>InsertAdaptiveSparkPlan</code> when requested for the following:</p> <ul> <li>Execute (with the <code>isSubquery</code> flag disabled)</li> <li>Compile a subquery (with the <code>isSubquery</code> flag enabled)</li> </ul>","text":""},{"location":"physical-optimizations/InsertAdaptiveSparkPlan/#collecting-subquery-expressions","title":"Collecting Subquery Expressions <pre><code>buildSubqueryMap(\n  plan: SparkPlan): Map[Long, SubqueryExec]\n</code></pre> <p><code>buildSubqueryMap</code> finds ScalarSubquery and ListQuery (in <code>InSubquery</code>) expressions (unique by expression ID to reuse the execution plan from another sub-query) in the given physical query plan.</p> <p>For every <code>ScalarSubquery</code> and <code>ListQuery</code> expressions, <code>buildSubqueryMap</code> compileSubquery, verifyAdaptivePlan and registers a new SubqueryExec operator.</p>","text":""},{"location":"physical-optimizations/InsertAdaptiveSparkPlan/#compilesubquery","title":"compileSubquery <pre><code>compileSubquery(\n  plan: LogicalPlan): SparkPlan\n</code></pre> <p><code>compileSubquery</code> requests the session-bound SparkPlanner (from the AdaptiveExecutionContext) to plan the given LogicalPlan (that produces a SparkPlan).</p> <p>In the end, <code>compileSubquery</code> applyInternal with <code>isSubquery</code> flag turned on.</p>","text":""},{"location":"physical-optimizations/InsertAdaptiveSparkPlan/#enforcing-adaptivesparkplanexec","title":"Enforcing AdaptiveSparkPlanExec <pre><code>verifyAdaptivePlan(\n  plan: SparkPlan,\n  logicalPlan: LogicalPlan): Unit\n</code></pre> <p><code>verifyAdaptivePlan</code> throws a <code>SubqueryAdaptiveNotSupportedException</code> when the given SparkPlan is not a AdaptiveSparkPlanExec.</p>","text":""},{"location":"physical-optimizations/InsertAdaptiveSparkPlan/#supportadaptive-condition","title":"supportAdaptive Condition <pre><code>supportAdaptive(\n  plan: SparkPlan): Boolean\n</code></pre> <p><code>supportAdaptive</code> returns <code>true</code> when the given SparkPlan and the children have all logical operator linked that are not streaming.</p>","text":""},{"location":"physical-optimizations/InsertAdaptiveSparkPlan/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.InsertAdaptiveSparkPlan.name = org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan\nlogger.InsertAdaptiveSparkPlan.level = all\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"physical-optimizations/OptimizeShuffleWithLocalRead/","title":"OptimizeShuffleWithLocalRead Adaptive Physical Optimization","text":"<p><code>OptimizeShuffleWithLocalRead</code> is a physical optimization in Adaptive Query Execution.</p> <p><code>OptimizeShuffleWithLocalRead</code> can be turned on and off using spark.sql.adaptive.localShuffleReader.enabled configuration property.</p>"},{"location":"physical-optimizations/OptimizeShuffleWithLocalRead/#supported-shuffleorigins","title":"Supported ShuffleOrigins <pre><code>supportedShuffleOrigins: Seq[ShuffleOrigin]\n</code></pre> <p><code>supportedShuffleOrigins</code> is the following ShuffleOrigins:</p> <ul> <li>ENSURE_REQUIREMENTS</li> <li>REBALANCE_PARTITIONS_BY_NONE</li> </ul> <p><code>supportedShuffleOrigins</code>\u00a0is part of the AQEShuffleReadRule abstraction.</p>","text":""},{"location":"physical-optimizations/OptimizeShuffleWithLocalRead/#issupported","title":"isSupported <pre><code>isSupported(\n  shuffle: ShuffleExchangeLike): Boolean\n</code></pre> <p><code>isSupported</code> is <code>true</code> when the following all hold:</p> <ul> <li>The outputPartitioning of the given ShuffleExchangeLike is not <code>SinglePartition</code></li> <li>The shuffleOrigin of the given ShuffleExchangeLike is supported</li> </ul> <p><code>isSupported</code>\u00a0is part of the AQEShuffleReadRule abstraction.</p>","text":""},{"location":"physical-optimizations/OptimizeShuffleWithLocalRead/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>apply</code> is a noop (and simply returns the given SparkPlan) with spark.sql.adaptive.localShuffleReader.enabled disabled.</p> <p>With canUseLocalShuffleRead <code>apply</code> createLocalRead. Otherwise, <code>apply</code> createProbeSideLocalRead.</p> <p><code>apply</code>\u00a0is part of the Rule abstraction.</p>","text":""},{"location":"physical-optimizations/OptimizeShuffleWithLocalRead/#canuselocalshuffleread","title":"canUseLocalShuffleRead <pre><code>canUseLocalShuffleRead(\n  plan: SparkPlan): Boolean\n</code></pre> <p><code>canUseLocalShuffleRead</code> is <code>true</code> when one of the following holds:</p> <ol> <li> <p>The given SparkPlan is a ShuffleQueryStageExec with the MapOutputStatistics available and the ShuffleExchangeLike is supported</p> </li> <li> <p>The given SparkPlan is a AQEShuffleReadExec with a ShuffleQueryStageExec with the above requirements met (the MapOutputStatistics is available and the ShuffleExchangeLike is supported) and the shuffleOrigin of the <code>ShuffleExchangeLike</code> is ENSURE_REQUIREMENTS</p> </li> </ol> <p><code>canUseLocalShuffleRead</code> is <code>false</code> otherwise.</p>","text":""},{"location":"physical-optimizations/OptimizeShuffleWithLocalRead/#createlocalread","title":"createLocalRead <pre><code>createLocalRead(\n  plan: SparkPlan): AQEShuffleReadExec\n</code></pre> <p><code>createLocalRead</code> branches off based on the type of the given physical operator and creates a new AQEShuffleReadExec (with or without advisory parallelism specified to determine ShufflePartitionSpecs):</p> <ul> <li> <p>For AQEShuffleReadExecs with a ShuffleQueryStageExec leaf physical operator, the advisory parallelism is the size of the ShufflePartitionSpec</p> </li> <li> <p>For ShuffleQueryStageExecs, the advisory parallelism is undefined</p> </li> </ul>","text":""},{"location":"physical-optimizations/OptimizeShuffleWithLocalRead/#createprobesidelocalread","title":"createProbeSideLocalRead <pre><code>createProbeSideLocalRead(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>createProbeSideLocalRead</code>...FIXME</p>","text":""},{"location":"physical-optimizations/OptimizeShuffleWithLocalRead/#getpartitionspecs","title":"getPartitionSpecs <pre><code>getPartitionSpecs(\n  shuffleStage: ShuffleQueryStageExec,\n  advisoryParallelism: Option[Int]): Seq[ShufflePartitionSpec]\n</code></pre> <p><code>createProbeSideLocalRead</code>...FIXME</p>","text":""},{"location":"physical-optimizations/OptimizeSkewInRebalancePartitions/","title":"OptimizeSkewInRebalancePartitions Adaptive Physical Optimization","text":"<p><code>OptimizeSkewInRebalancePartitions</code> is a physical optimization in Adaptive Query Execution.</p> <p><code>OptimizeSkewInRebalancePartitions</code> can be turned on and off using spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled configuration property.</p>"},{"location":"physical-optimizations/OptimizeSkewInRebalancePartitions/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>apply</code> works with ShuffleQueryStageExec leaf physical operators only (with the ShuffleExchangeLikes that are supported).</p> <p><code>apply</code> tryOptimizeSkewedPartitions of the <code>ShuffleQueryStageExec</code>.</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"physical-optimizations/OptimizeSkewInRebalancePartitions/#tryoptimizeskewedpartitions","title":"tryOptimizeSkewedPartitions <pre><code>tryOptimizeSkewedPartitions(\n  shuffle: ShuffleQueryStageExec): SparkPlan\n</code></pre> <p><code>tryOptimizeSkewedPartitions</code>...FIXME</p>","text":""},{"location":"physical-optimizations/OptimizeSkewedJoin/","title":"OptimizeSkewedJoin Adaptive Physical Optimization","text":"<p><code>OptimizeSkewedJoin</code> is a physical optimization to make data distribution more even in Adaptive Query Execution.</p> <p><code>OptimizeSkewedJoin</code> is also called skew join optimization.</p>"},{"location":"physical-optimizations/OptimizeSkewedJoin/#skew-threshold","title":"Skew Threshold <pre><code>getSkewThreshold(\n  medianSize: Long): Long\n</code></pre> <p><code>getSkewThreshold</code> is the maximum of the following:</p> <ol> <li>spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes</li> <li>spark.sql.adaptive.skewJoin.skewedPartitionFactor multiplied by the given <code>medianSize</code></li> </ol>  <p><code>getSkewThreshold</code> is used when:</p> <ul> <li><code>OptimizeSkewedJoin</code> is requested to tryOptimizeJoinChildren</li> </ul>","text":""},{"location":"physical-optimizations/OptimizeSkewedJoin/#supported-join-types","title":"Supported Join Types <p><code>OptimizeSkewedJoin</code> supports the following join types:</p> <ul> <li><code>Inner</code></li> <li><code>Cross</code></li> <li><code>LeftSemi</code></li> <li><code>LeftAnti</code></li> <li><code>LeftOuter</code></li> <li><code>RightOuter</code></li> </ul>","text":""},{"location":"physical-optimizations/OptimizeSkewedJoin/#configuration-properties","title":"Configuration Properties <p><code>OptimizeSkewedJoin</code> uses the following configuration properties:</p> <ul> <li>spark.sql.adaptive.skewJoin.enabled</li> <li>spark.sql.adaptive.skewJoin.skewedPartitionFactor</li> <li>spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes</li> <li>spark.sql.adaptive.advisoryPartitionSizeInBytes </li> </ul>","text":""},{"location":"physical-optimizations/OptimizeSkewedJoin/#creating-instance","title":"Creating Instance <p><code>OptimizeSkewedJoin</code> takes the following to be created:</p> <ul> <li> SQLConf  <p><code>OptimizeSkewedJoin</code> is created when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> physical operator is requested for the adaptive optimizations</li> </ul>","text":""},{"location":"physical-optimizations/OptimizeSkewedJoin/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>apply</code> uses spark.sql.adaptive.skewJoin.enabled configuration property to determine whether to apply any optimizations or not.</p> <p><code>apply</code> collects ShuffleQueryStageExec physical operators.</p>  <p>Note</p> <p><code>apply</code> does nothing and simply gives the query plan \"untouched\" when applied to a query plan with the number of ShuffleQueryStageExec physical operators different than <code>2</code>.</p>  <p><code>apply</code>...FIXME</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"physical-optimizations/OptimizeSkewedJoin/#optimizing-skewed-joins","title":"Optimizing Skewed Joins <pre><code>optimizeSkewJoin(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>optimizeSkewJoin</code> transforms the following physical operators:</p> <ul> <li>SortMergeJoinExec (of left and right SortExecs over <code>ShuffleStage</code> with ShuffleQueryStageExec and isSkewJoin disabled)</li> <li>ShuffledHashJoinExec (with left and right <code>ShuffleStage</code>s with ShuffleQueryStageExec and isSkewJoin disabled)</li> </ul> <p><code>optimizeSkewJoin</code> tryOptimizeJoinChildren and, if a new join left and right child operators are determined, replaces them in the physical operators (with the <code>isSkewJoin</code> flag enabled).</p>","text":""},{"location":"physical-optimizations/OptimizeSkewedJoin/#tryoptimizejoinchildren","title":"tryOptimizeJoinChildren <pre><code>tryOptimizeJoinChildren(\n  left: ShuffleQueryStageExec,\n  right: ShuffleQueryStageExec,\n  joinType: JoinType): Option[(SparkPlan, SparkPlan)]\n</code></pre> <p><code>tryOptimizeJoinChildren</code>...FIXME</p>","text":""},{"location":"physical-optimizations/OptimizeSkewedJoin/#target-partition-size","title":"Target Partition Size <pre><code>targetSize(\n  sizes: Seq[Long],\n  medianSize: Long): Long\n</code></pre> <p><code>targetSize</code> determines the target partition size (to optimize skewed join) and is the greatest value among the following:</p> <ul> <li>spark.sql.adaptive.advisoryPartitionSizeInBytes configuration property</li> <li>Average size of non-skewed partitions (based on the given <code>medianSize</code>)</li> </ul> <p><code>targetSize</code> throws an <code>AssertionError</code> when all partitions are skewed (no non-skewed partitions).</p>","text":""},{"location":"physical-optimizations/OptimizeSkewedJoin/#median-partition-size","title":"Median Partition Size <pre><code>medianSize(\n  sizes: Seq[Long]): Long\n</code></pre> <p><code>medianSize</code>...FIXME</p> <p><code>medianSize</code> is used when:</p> <ul> <li><code>OptimizeSkewedJoin</code> is requested to tryOptimizeJoinChildren</li> </ul>","text":""},{"location":"physical-optimizations/OptimizeSkewedJoin/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"physical-optimizations/PlanAdaptiveDynamicPruningFilters/","title":"PlanAdaptiveDynamicPruningFilters Physical Optimization","text":"<p><code>PlanAdaptiveDynamicPruningFilters</code> is a physical optimization in Adaptive Query Execution.</p> <p><code>PlanAdaptiveDynamicPruningFilters</code> is a Catalyst Rule for transforming physical plans (<code>Rule[SparkPlan]</code>).</p>"},{"location":"physical-optimizations/PlanAdaptiveDynamicPruningFilters/#creating-instance","title":"Creating Instance","text":"<p><code>PlanAdaptiveDynamicPruningFilters</code> takes the following to be created:</p> <ul> <li> Root AdaptiveSparkPlanExec <p><code>PlanAdaptiveDynamicPruningFilters</code> is created when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> leaf physical operator is requested for the adaptive optimizations</li> </ul>"},{"location":"physical-optimizations/PlanAdaptiveDynamicPruningFilters/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p>  <p><code>apply</code> is disabled (and simply returns the given SparkPlan unchanged) when the spark.sql.optimizer.dynamicPartitionPruning.enabled is disabled.</p> <p><code>apply</code> requests the given <code>SparkPlan</code> to transformAllExpressionsWithPruning in QueryPlans with the DYNAMIC_PRUNING_EXPRESSION and IN_SUBQUERY_EXEC tree patterns:</p> <ul> <li>DynamicPruningExpressions with InSubqueryExec expressions with SubqueryAdaptiveBroadcastExecs with AdaptiveSparkPlanExec leaf physical operators</li> </ul> <p>In the end, <code>apply</code> creates a new DynamicPruningExpression unary expression (with a InSubqueryExec or TrueLiteral).</p>","text":""},{"location":"physical-optimizations/PlanAdaptiveSubqueries/","title":"PlanAdaptiveSubqueries Physical Optimization","text":"<p><code>PlanAdaptiveSubqueries</code> is a physical query plan optimization in Adaptive Query Execution.</p> <p><code>PlanAdaptiveSubqueries</code> is a Catalyst Rule for transforming physical plans (<code>Rule[SparkPlan]</code>).</p>"},{"location":"physical-optimizations/PlanDynamicPruningFilters/","title":"PlanDynamicPruningFilters Physical Optimization","text":"<p><code>PlanDynamicPruningFilters</code> is a physical optimization and part of the default physical optimizations (preparations).</p> <p><code>PlanDynamicPruningFilters</code> transforms DynamicPruningSubquery expressions into DynamicPruningExpression expressions.</p> <p><code>PlanDynamicPruningFilters</code> is a noop when spark.sql.optimizer.dynamicPartitionPruning.enabled configuration property is disabled (<code>false</code>).</p> <p><code>PlanDynamicPruningFilters</code> is a Catalyst Rule for transforming physical operators (<code>Rule[SparkPlan]</code>).</p>"},{"location":"physical-optimizations/PlanDynamicPruningFilters/#creating-instance","title":"Creating Instance","text":"<p><code>PlanDynamicPruningFilters</code> takes the following to be created:</p> <ul> <li> SparkSession <p><code>PlanDynamicPruningFilters</code> is created when <code>QueryExecution</code> utility is requested for physical query optimizations.</p>"},{"location":"physical-optimizations/PlanDynamicPruningFilters/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>apply</code> transforms a <code>DynamicPruningSubquery</code> expression as follows:</p> <ol> <li> <p>Uses <code>QueryExecution.createSparkPlan</code> utility to plan the Build Logical Query (of the <code>DynamicPruningSubquery</code>)</p> </li> <li> <p>Finds whether to reuse an Exchange (based on spark.sql.exchange.reuse configuration property and other checks)</p> </li> <li> <p>Creates a DynamicPruningExpression expression</p> </li> </ol> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"physical-optimizations/PlanSubqueries/","title":"PlanSubqueries Physical Optimization","text":"<p><code>PlanSubqueries</code> is a physical query optimization (aka physical query preparation rule or simply preparation rule) that &lt;&gt; (as <code>ScalarSubquery ExecSubqueryExpression</code> expressions)."},{"location":"physical-optimizations/PlanSubqueries/#source-scala","title":"[source, scala]","text":"<p>import org.apache.spark.sql.SparkSession val spark: SparkSession = ...</p> <p>import org.apache.spark.sql.execution.PlanSubqueries val planSubqueries = PlanSubqueries(spark)</p> <p>Seq(   (0, 0),   (1, 0),   (2, 1) ).toDF(\"id\", \"gid\").createOrReplaceTempView(\"t\")</p> <p>Seq(   (0, 3),   (1, 20) ).toDF(\"gid\", \"lvl\").createOrReplaceTempView(\"v\")</p> <p>val sql = \"\"\"   select * from t where gid &gt; (select max(gid) from v) \"\"\" val q = spark.sql(sql)</p> <p>val sparkPlan = q.queryExecution.sparkPlan scala&gt; println(sparkPlan.numberedTreeString) 00 Project [_1#49 AS id#52, _2#50 AS gid#53] 01 +- Filter (_2#50 &gt; scalar-subquery#128 []) 02    :  +- Aggregate [max(gid#61) AS max(gid)#130] 03    :     +- LocalRelation [gid#61] 04    +- LocalTableScan [_1#49, _2#50]</p> <p>val optimizedPlan = planSubqueries(sparkPlan) scala&gt; println(optimizedPlan.numberedTreeString) 00 Project [_1#49 AS id#52, _2#50 AS gid#53] 01 +- Filter (_2#50 &gt; Subquery subquery128) 02    :  +- Subquery subquery128 03    :     +- *(2) HashAggregate(keys=[], functions=[max(gid#61)], output=[max(gid)#130]) 04    :        +- Exchange SinglePartition 05    :           +- *(1) HashAggregate(keys=[], functions=[partial_max(gid#61)], output=[max#134]) 06    :              +- LocalTableScan [gid#61] 07    +- LocalTableScan [_1#49, _2#50]</p> <p><code>PlanSubqueries</code> is part of preparations batch of physical query plan rules and is executed when <code>QueryExecution</code> is requested for the optimized physical query plan (i.e. in executedPlan phase of a query execution).</p> <p><code>PlanSubqueries</code> is a Catalyst rule for transforming physical query plans (<code>Rule[SparkPlan]</code>).</p>"},{"location":"physical-optimizations/PlanSubqueries/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p>For every ScalarSubquery expression in the input SparkPlan.md[physical plan], <code>apply</code> does the following:</p> <p>. Builds the optimized physical plan (aka <code>executedPlan</code>) of the subquery logical plan, i.e. creates a QueryExecution for the subquery logical plan and requests the optimized physical plan.</p> <p>. Plans the scalar subquery, i.e. creates a ScalarSubquery (ExecSubqueryExpression) expression with a new SubqueryExec.md#creating-instance[SubqueryExec] physical operator (with the name subquery[id] and the optimized physical plan) and the ScalarSubquery.md#exprId[ExprId].</p> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"physical-optimizations/RemoveRedundantProjects/","title":"RemoveRedundantProjects Physical Optimization","text":"<p><code>RemoveRedundantProjects</code> is...FIXME</p>"},{"location":"physical-optimizations/RemoveRedundantSorts/","title":"RemoveRedundantSorts Physical Optimization","text":"<p><code>RemoveRedundantSorts</code> is...FIXME</p>"},{"location":"physical-optimizations/ReplaceHashWithSortAgg/","title":"ReplaceHashWithSortAgg Physical Optimization","text":"<p><code>ReplaceHashWithSortAgg</code> is a physical optimization (<code>Rule[SparkPlan]</code>) to replace Hash Aggregate operators with grouping keys with SortAggregateExec operators.</p> <p><code>ReplaceHashWithSortAgg</code> can be enabled using spark.sql.execution.replaceHashWithSortAgg configuration property.</p> <p><code>ReplaceHashWithSortAgg</code> is part of the following optimizations:</p> <ul> <li>preparations</li> <li>queryStagePreparationRules</li> </ul>"},{"location":"physical-optimizations/ReplaceHashWithSortAgg/#executing-rule","title":"Executing Rule  Signature <pre><code>apply(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>apply</code> is part of the Rule abstraction.</p>   <p>Noop when spark.sql.execution.replaceHashWithSortAgg disabled</p> <p><code>apply</code> does nothing when spark.sql.execution.replaceHashWithSortAgg is disabled.</p>  <p><code>apply</code> replaceHashAgg.</p>","text":""},{"location":"physical-optimizations/ReplaceHashWithSortAgg/#replacehashagg","title":"replaceHashAgg <pre><code>replaceHashAgg(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>replaceHashAgg</code> finds BaseAggregateExec physical operators that are Hash Aggregate operators with grouping keys and converts them to SortAggregateExec when either is met:</p> <ol> <li>The child operator is again a Hash Aggregate operator with grouping keys with isPartialAgg and ordering is satisfied</li> <li>Ordering is satisfied</li> </ol>","text":""},{"location":"physical-optimizations/ReplaceHashWithSortAgg/#ishashbasedaggwithkeys","title":"isHashBasedAggWithKeys <pre><code>isHashBasedAggWithKeys(\n  agg: BaseAggregateExec): Boolean\n</code></pre> <p><code>isHashBasedAggWithKeys</code> is positive (<code>true</code>) when the given BaseAggregateExec is as follows:</p> <ol> <li>It is either HashAggregateExec or ObjectHashAggregateExec</li> <li>It has got grouping keys</li> </ol>","text":""},{"location":"physical-optimizations/ReplaceHashWithSortAgg/#ispartialagg","title":"isPartialAgg <pre><code>isPartialAgg(\n  partialAgg: BaseAggregateExec,\n  finalAgg: BaseAggregateExec): Boolean\n</code></pre> <p><code>isPartialAgg</code> is positive (<code>true</code>) when...FIXME</p>","text":""},{"location":"physical-optimizations/ReuseAdaptiveSubquery/","title":"ReuseAdaptiveSubquery Physical Optimization","text":"<p><code>ReuseAdaptiveSubquery</code> is a physical query plan optimization in Adaptive Query Execution.</p> <p><code>ReuseAdaptiveSubquery</code> is a Catalyst Rule for transforming physical plans (<code>Rule[SparkPlan]</code>).</p>"},{"location":"physical-optimizations/ReuseAdaptiveSubquery/#creating-instance","title":"Creating Instance","text":"<p><code>ReuseAdaptiveSubquery</code> takes the following to be created:</p> <ul> <li> Subquery Cache <p><code>ReuseAdaptiveSubquery</code> is created when:</p> <ul> <li><code>AdaptiveSparkPlanExec</code> leaf physical operator is requested for the adaptive optimizations</li> </ul>"},{"location":"physical-optimizations/ReuseAdaptiveSubquery/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>apply</code> is disabled (and returns the given SparkPlan) when the spark.sql.execution.reuseSubquery configuration property is <code>false</code>.</p> <p><code>apply</code> requests the given <code>SparkPlan</code> to transformAllExpressionsWithPruning with tree nodes with PLAN_EXPRESSION tree pattern:</p> <ul> <li>For a ExecSubqueryExpression expression, <code>apply</code> replaces the plan with a new ReusedSubqueryExec physical operator with a cached plan if found in the cache.</li> </ul> <p><code>apply</code> is part of the Rule abstraction.</p>","text":""},{"location":"physical-optimizations/ReuseExchange/","title":"ReuseExchange Physical Optimization","text":"<p><code>ReuseExchange</code> is a physical query optimization (aka physical query preparation rule or simply preparation rule) that <code>QueryExecution</code> uses to optimize the physical plan of a structured query.</p> <p>Technically, <code>ReuseExchange</code> is just a catalyst/Rule.md[Catalyst rule] for transforming SparkPlan.md[physical query plans], i.e. <code>Rule[SparkPlan]</code>.</p> <p><code>ReuseExchange</code> is part of preparations batch of physical query plan rules and is executed when <code>QueryExecution</code> is requested for the optimized physical query plan (i.e. in executedPlan phase of a query execution).</p> <p>=== [[apply]] <code>apply</code> Method</p>"},{"location":"physical-optimizations/ReuseExchange/#source-scala","title":"[source, scala]","text":""},{"location":"physical-optimizations/ReuseExchange/#applyplan-sparkplan-sparkplan","title":"apply(plan: SparkPlan): SparkPlan","text":"<p>NOTE: <code>apply</code> is part of catalyst/Rule.md#apply[Rule Contract] to apply a rule to a SparkPlan.md[physical plan].</p> <p><code>apply</code> finds all Exchange unary operators and...FIXME</p> <p><code>apply</code> does nothing and simply returns the input physical <code>plan</code> if spark.sql.exchange.reuse internal configuration property is disabled.</p>"},{"location":"physical-optimizations/ReuseExchangeAndSubquery/","title":"ReuseExchangeAndSubquery Physical Optimization","text":"<p><code>ReuseExchangeAndSubquery</code> is a physical query optimization.</p> <p><code>ReuseExchangeAndSubquery</code> is part of the preparations batch of physical query plan rules and is executed when <code>QueryExecution</code> is requested for the optimized physical query plan.</p> <p><code>ReuseExchangeAndSubquery</code> is a Catalyst rule for transforming physical query plans (<code>Rule[SparkPlan]</code>).</p>"},{"location":"physical-optimizations/ReuseExchangeAndSubquery/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: SparkPlan): SparkPlan\n</code></pre> <p><code>apply</code> is a noop (and simply returns the given SparkPlan unchanged) when neither the spark.sql.exchange.reuse nor the spark.sql.execution.reuseSubquery are enabled.</p> <p><code>apply</code> requests the <code>SparkPlan</code> to transformUpWithPruning any physical operator with EXCHANGE or PLAN_EXPRESSION tree pattern:</p> <ul> <li> <p>For an Exchange and the spark.sql.exchange.reuse enabled, <code>apply</code> may create a ReusedExchangeExec if there is a cached (found earlier) exchange.</p> </li> <li> <p>For other physical operators, <code>apply</code> requests the current <code>SparkPlan</code> to transformExpressionsUpWithPruning any physical operator with PLAN_EXPRESSION tree pattern and may create a ReusedSubqueryExec for a ExecSubqueryExpression if there is a cached (found earlier) subquery and the spark.sql.execution.reuseSubquery is enabled</p> </li> </ul> <p><code>apply</code>\u00a0is part of the Rule abstraction.</p>","text":""},{"location":"physical-optimizations/ReuseSubquery/","title":"ReuseSubquery Physical Optimization","text":"<p><code>ReuseSubquery</code> is a physical query optimization (aka physical query preparation rule or simply preparation rule) that <code>QueryExecution</code> uses to optimize the physical plan of a structured query.</p> <p>Technically, <code>ReuseSubquery</code> is just a catalyst/Rule.md[Catalyst rule] for transforming SparkPlan.md[physical query plans], i.e. <code>Rule[SparkPlan]</code>.</p> <p><code>ReuseSubquery</code> is part of preparations batch of physical query plan rules and is executed when <code>QueryExecution</code> is requested for the optimized physical query plan (i.e. in executedPlan phase of a query execution).</p> <p>=== [[apply]] <code>apply</code> Method</p>"},{"location":"physical-optimizations/ReuseSubquery/#source-scala","title":"[source, scala]","text":""},{"location":"physical-optimizations/ReuseSubquery/#applyplan-sparkplan-sparkplan","title":"apply(plan: SparkPlan): SparkPlan","text":"<p>NOTE: <code>apply</code> is part of catalyst/Rule.md#apply[Rule Contract] to apply a rule to a SparkPlan.md[physical plan].</p> <p><code>apply</code>...FIXME</p>"},{"location":"physical-optimizations/ValidateSparkPlan/","title":"ValidateSparkPlan Physical Optimization","text":"<p><code>ValidateSparkPlan</code> is...FIXME</p>"},{"location":"query-execution/","title":"Query Execution","text":"<p>QueryExecution is the query execution engine of Spark SQL.</p>"},{"location":"rdds/FileScanRDD/","title":"FileScanRDD","text":"<p><code>FileScanRDD</code> is the input RDD of FileSourceScanExec leaf physical operator (for Whole-Stage Java Code Generation).</p> RDD <p>Find out more on <code>RDD</code> abstraction in The Internals of Apache Spark.</p>"},{"location":"rdds/FileScanRDD/#creating-instance","title":"Creating Instance","text":"<p><code>FileScanRDD</code> takes the following to be created:</p> <ul> <li> SparkSession <li> Read Function of PartitionedFiles to InternalRows (<code>(PartitionedFile) =&gt; Iterator[InternalRow]</code>) <li> FilePartitions <li> Read Schema <li> Metadata Columns <p><code>FileScanRDD</code> is created when:</p> <ul> <li>FileSourceScanExec physical operator is requested to createBucketedReadRDD and createNonBucketedReadRDD (when <code>FileSourceScanExec</code> operator is requested for the input RDD when WholeStageCodegenExec physical operator is executed)</li> </ul>"},{"location":"rdds/FileScanRDD/#configuration-properties","title":"Configuration Properties","text":"<p><code>FileScanRDD</code> uses the following properties (when requested to compute a partition):</p> <ul> <li> spark.sql.files.ignoreCorruptFiles <li> spark.sql.files.ignoreMissingFiles"},{"location":"rdds/FileScanRDD/#filepartition","title":"FilePartition <p><code>FileScanRDD</code> is given FilePartitions when created that are custom RDD partitions with PartitionedFiles (file blocks).</p>","text":""},{"location":"rdds/FileScanRDD/#placement-preferences-of-partition-preferred-locations","title":"Placement Preferences of Partition (Preferred Locations) <pre><code>getPreferredLocations(\n  split: RDDPartition): Seq[String]\n</code></pre> <p><code>getPreferredLocations</code> is part of Spark Core's <code>RDD</code> abstraction.</p>  <p><code>getPreferredLocations</code> assumes that the given <code>RDDPartition</code> is actually a FilePartition and requests it for <code>preferredLocations</code>.</p>","text":""},{"location":"rdds/FileScanRDD/#rdd-partitions","title":"RDD Partitions <pre><code>getPartitions: Array[RDDPartition]\n</code></pre> <p><code>getPartitions</code> is part of Spark Core's <code>RDD</code> abstraction.</p>  <p><code>getPartitions</code> simply returns the FilePartitions.</p>","text":""},{"location":"rdds/FileScanRDD/#computing-partition","title":"Computing Partition <pre><code>compute(\n  split: RDDPartition,\n  context: TaskContext): Iterator[InternalRow]\n</code></pre>  <p>Note</p> <p>The <code>RDDPartition</code> given is actually a FilePartition with one or more PartitionedFiles (that getPartitions returned).</p>  <p><code>compute</code> is part of Spark Core's <code>RDD</code> abstraction.</p>","text":""},{"location":"rdds/FileScanRDD/#retrieving-next-element","title":"Retrieving Next Element <pre><code>next(): Object\n</code></pre> <p><code>next</code> takes the next element of the current iterator over elements of a file block (PartitionedFile).</p> <p><code>next</code> increments the metrics of bytes and number of rows read (that could be the number of rows in a ColumnarBatch for vectorized reads).</p> <p><code>next</code> is part of Scala's Iterator abstraction.</p>","text":""},{"location":"rdds/FileScanRDD/#demo","title":"Demo <pre><code>val q = spark.read.text(\"README.md\")\nval sparkPlan = q.queryExecution.executedPlan\nscala&gt; println(sparkPlan.numberedTreeString)\n00 FileScan text [value#0] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex[file:/Users/jacek/dev/oss/spark/README.md], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;value:string&gt;\n\nimport org.apache.spark.sql.execution.FileSourceScanExec\nval scan = sparkPlan.collectFirst { case exec: FileSourceScanExec =&gt; exec }.get\nval inputRDD = scan.inputRDDs.head\n\nimport org.apache.spark.sql.execution.datasources.FileScanRDD\nassert(inputRDD.isInstanceOf[FileScanRDD])\n\nval rdd = scan.execute\nscala&gt; println(rdd.toDebugString)\n(1) MapPartitionsRDD[1] at execute at &lt;console&gt;:27 []\n |  FileScanRDD[0] at inputRDDs at &lt;console&gt;:26 []\n\nval fileScanRDD = rdd.dependencies.head.rdd\nassert(fileScanRDD.isInstanceOf[FileScanRDD])\n</code></pre>","text":""},{"location":"rdds/FileScanRDD/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.FileScanRDD</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.datasources.FileScanRDD=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"sql/","title":"SQL Parsing Framework","text":"<p>Spark SQL supports SQL language using SQL Parser Framework.</p> <p>SQL Parser Framework translates SQL statements to corresponding relational entities using ANTLR<sup>1</sup>.</p> <p>What is ANTLR?</p> <p>ANTLR (ANother Tool for Language Recognition) is a parser generator used to build languages, tools, and frameworks. From a grammar, ANTLR generates a parser that can build and walk parse trees.</p> <p>The main abstraction is ParserInterface that is extended by AbstractSqlParser so SQL parsers can focus on a custom AstBuilder only.</p> <p>There are two concrete <code>AbstractSqlParsers</code>:</p> <ol> <li>SparkSqlParser that is the default parser of the SQL expressions into Spark SQL types.</li> <li>CatalystSqlParser that is used to parse data types from their canonical string representation.</li> </ol>"},{"location":"sql/#example","title":"Example","text":"<p>Let's take a look at <code>MERGE INTO</code> SQL statement to deep dive into how Spark SQL handles this and other SQL statements.</p> <p>MERGE INTO and UPDATE SQL Statements Not Supported</p> <p>Partial support for <code>MERGE INTO</code> went into Apache Spark 3.0.0 (as part of SPARK-28893).</p> <p>It is not finished yet since BasicOperators execution planning strategy throws an <code>UnsupportedOperationException</code> for <code>MERGE INTO</code> and <code>UPDATE</code> SQL statements.</p> <p><code>MERGE INTO</code> is described in SqlBaseParser.g4 grammar (in <code>#mergeIntoTable</code> labeled alternative).</p> <p><code>AstBuilder</code> translates a <code>MERGE INTO</code> SQL query into a MergeIntoTable logical command.</p> <p>ResolveReferences logical resolution rule is used to resolve references of <code>MergeIntoTables</code> (for a merge condition and matched and not-matched actions).</p> <p>In the end, BasicOperators execution planning strategy throws an <code>UnsupportedOperationException</code>:</p> <pre><code>MERGE INTO TABLE is not supported temporarily.\n</code></pre> <ol> <li> <p>ANTLR's home page \u21a9</p> </li> </ol>"},{"location":"sql/AbstractSqlParser/","title":"AbstractSqlParser","text":"<p><code>AbstractSqlParser</code> is an extension of the ParserInterface abstraction for SQL parsers that delegate parsing to an AstBuilder.</p>"},{"location":"sql/AbstractSqlParser/#astbuilder","title":"AstBuilder","text":"<pre><code>astBuilder: AstBuilder\n</code></pre> <p>AstBuilder for parsing SQL statements</p> <p>Used when:</p> <ul> <li><code>AbstractSqlParser</code> is requested to parseDataType, parseExpression, parseTableIdentifier, parseFunctionIdentifier, parseMultipartIdentifier, parseTableSchema, parsePlan</li> </ul>"},{"location":"sql/AbstractSqlParser/#implementations","title":"Implementations","text":"<ul> <li>CatalystSqlParser</li> <li>SparkSqlParser</li> </ul>"},{"location":"sql/AbstractSqlParser/#setting-up-sqlbaselexer-and-sqlbaseparser-for-parsing","title":"Setting Up SqlBaseLexer and SqlBaseParser for Parsing","text":"<pre><code>parse[T](\ncommand: String)(\ntoResult: SqlBaseParser =&gt; T): T\n</code></pre> <p><code>parse</code> sets up ANTLR parsing infrastructure with <code>SqlBaseLexer</code> and <code>SqlBaseParser</code> (which are the ANTLR-specific classes of Spark SQL that are auto-generated at build time from the SqlBaseParser.g4 grammar).</p> <p>Internally, <code>parse</code> first prints out the following INFO message to the logs:</p> <pre><code>Parsing command: [command]\n</code></pre> <p>Tip</p> <p>Enable <code>INFO</code> logging level for the custom <code>AbstractSqlParser</code>s to see the above and other INFO messages.</p> <p><code>parse</code> then creates and sets up a <code>SqlBaseLexer</code> and <code>SqlBaseParser</code> that in turn passes the latter on to the input <code>toResult</code> function where the parsing finally happens.</p> <p><code>parse</code> uses <code>SLL</code> prediction mode for parsing first before falling back to <code>LL</code> mode.</p> <p>In case of parsing errors, <code>parse</code> reports a <code>ParseException</code>.</p> <p><code>parse</code> is used in all the <code>parse</code> methods.</p>"},{"location":"sql/AbstractSqlParser/#parseplan","title":"parsePlan <pre><code>parsePlan(\n  sqlText: String): LogicalPlan\n</code></pre> <p><code>parsePlan</code> parses the given SQL text (that requests the AstBuilder to visitSingleStatement to produce a LogicalPlan).</p> <p><code>parsePlan</code> is part of the ParserInterface abstraction.</p>","text":""},{"location":"sql/AstBuilder/","title":"AstBuilder \u2014 ANTLR-based SQL Parser","text":"<p><code>AstBuilder</code> converts ANTLR <code>ParseTree</code>s into Catalyst entities using visit callbacks.</p> <p><code>AstBuilder</code> is the only requirement of the AbstractSqlParser abstraction (and used by CatalystSqlParser directly while SparkSqlParser uses SparkSqlAstBuilder instead).</p>"},{"location":"sql/AstBuilder/#sqlbaseparserg4-antlr-grammar","title":"SqlBaseParser.g4 \u2014 ANTLR Grammar <p><code>AstBuilder</code> is an ANTLR <code>AbstractParseTreeVisitor</code> (as <code>SqlBaseParserBaseVisitor</code>) that is generated from the ANTLR grammar of Spark SQL.</p> <p><code>SqlBaseParserBaseVisitor</code> is a ANTLR-specific base class that is generated at build time from the ANTLR grammar of Spark SQL. The Spark SQL grammar is available in the Apache Spark repository at SqlBaseParser.g4.</p> <p><code>SqlBaseParserBaseVisitor</code> is an AbstractParseTreeVisitor in ANTLR.</p>  <p>Spark 3.3.0</p> <p>As of Spark 3.3.0 (SPARK-38378) the ANTLR grammar is in two separate files for the parser and the lexer, <code>SqlBaseParser.g4</code> and <code>SqlBaseLexer.g4</code>, respectively:</p>  <p>By separating the lexer and parser, we will be able to use the full power of ANTLR parser and lexer grammars. e.g. lexer mode. This will give us more flexibility when implementing new SQL features.</p>","text":""},{"location":"sql/AstBuilder/#visit-callbacks","title":"Visit Callbacks","text":""},{"location":"sql/AstBuilder/#visitaddtablecolumns","title":"visitAddTableColumns <p>Creates an AddColumns logical operator</p> <pre><code>ALTER TABLE multipartIdentifier\n  ADD (COLUMN | COLUMNS)\n  '(' columns=qualifiedColTypeWithPositionList ')'\n\nqualifiedColTypeWithPositionList\n    : qualifiedColTypeWithPosition (',' qualifiedColTypeWithPosition)*\n    ;\n\nqualifiedColTypeWithPosition\n    : name=multipartIdentifier dataType (NOT NULL)? commentSpec? colPosition?\n    ;\n\ncommentSpec\n    : COMMENT text\n    ;\n\ncolPosition\n    : position=FIRST | position=AFTER afterCol=errorCapturingIdentifier\n    ;\n</code></pre> <p>ANTLR labeled alternative: <code>#addTableColumns</code></p>","text":""},{"location":"sql/AstBuilder/#visitanalyze","title":"visitAnalyze <p>Creates an AnalyzeColumn or AnalyzeTable logical operators</p> <pre><code>ANALYZE TABLE multipartIdentifier partitionSpec? COMPUTE STATISTICS\n  (NOSCAN | FOR COLUMNS identifierSeq | FOR ALL COLUMNS)?\n\npartitionSpec\n    : PARTITION '(' partitionVal (',' partitionVal)* ')'\n    ;\n\npartitionVal\n    : identifier (EQ constant)?\n    ;\n\nEQ  : '=' | '==';\n</code></pre> <p><code>visitAnalyze</code> creates an AnalyzeColumn when one of the following is used:</p> <ul> <li><code>FOR COLUMNS identifierSeq</code></li> <li><code>FOR ALL COLUMNS</code></li> </ul> <p>ANTLR labeled alternative: <code>#analyze</code></p>","text":""},{"location":"sql/AstBuilder/#visitbucketspec","title":"visitBucketSpec <p>Creates a BucketSpec</p> <pre><code>bucketSpec\n  : CLUSTERED BY '(' identifierList ')'\n    (SORTED BY '(' orderedIdentifierList ')' )?\n    INTO digit BUCKETS\n  ;\n</code></pre> <p>Column ordering must be <code>ASC</code></p>","text":""},{"location":"sql/AstBuilder/#visitcommenttable","title":"visitCommentTable <p>Creates a CommentOnTable logical command</p> <pre><code>COMMENT ON TABLE tableIdentifier IS ('text' | NULL)\n</code></pre> <p>ANTLR labeled alternative: <code>#commentTable</code></p>","text":""},{"location":"sql/AstBuilder/#visitcommonselectqueryclauseplan","title":"visitCommonSelectQueryClausePlan <p>Used in withTransformQuerySpecification and withSelectQuerySpecification</p>","text":""},{"location":"sql/AstBuilder/#visitcreatetable","title":"visitCreateTable <p>Creates a CreateTableAsSelect (for CTAS queries with <code>AS</code> clause) or CreateTable logical operator</p> <pre><code>CREATE TEMPORARY? EXTERNAL? TABLE (IF NOT EXISTS)? [multipartIdentifier]\n  ('(' [colType] (',' [colType])* ')')?\n  (USING [provider])?\n  [createTableClauses]\n  (AS? query)?\n\ncolType\n  : colName dataType (NOT NULL)? (COMMENT [comment])?\n  ;\n\ncreateTableClauses:\n  ((OPTIONS options=propertyList) |\n  (PARTITIONED BY partitioning=partitionFieldList) |\n  skewSpec |\n  bucketSpec |\n  rowFormat |\n  createFileFormat |\n  locationSpec |\n  commentSpec |\n  (TBLPROPERTIES tableProps=propertyList))*\n</code></pre> <p>ANTLR labeled alternative: <code>#createTable</code></p>","text":""},{"location":"sql/AstBuilder/#visitdeletefromtable","title":"visitDeleteFromTable <p>Creates a DeleteFromTable logical command</p> <pre><code>DELETE FROM multipartIdentifier tableAlias whereClause?\n</code></pre> <p>ANTLR labeled alternative: <code>#deleteFromTable</code></p>","text":""},{"location":"sql/AstBuilder/#visitdescriberelation","title":"visitDescribeRelation <p>Creates a <code>DescribeColumnStatement</code> or DescribeRelation</p> <pre><code>(DESC | DESCRIBE) TABLE? option=(EXTENDED | FORMATTED)?\n  multipartIdentifier partitionSpec? describeColName?\n</code></pre> <p>ANTLR labeled alternative: <code>#describeRelation</code></p>","text":""},{"location":"sql/AstBuilder/#visitexists","title":"visitExists <p>Creates an Exists expression</p> <p>ANTLR labeled alternative: <code>#exists</code></p>","text":""},{"location":"sql/AstBuilder/#visitexplain","title":"visitExplain <p>Creates a ExplainCommand</p> <p>ANTLR rule: <code>explain</code></p>","text":""},{"location":"sql/AstBuilder/#visitfirst","title":"visitFirst <p>Creates a First aggregate function expression</p> <pre><code>FIRST '(' expression (IGNORE NULLS)? ')'\n</code></pre> <p>ANTLR labeled alternative: <code>#first</code></p>","text":""},{"location":"sql/AstBuilder/#visitfromclause","title":"visitFromClause <p>Creates a LogicalPlan</p> <pre><code>FROM relation (',' relation)* lateralView* pivotClause?\n</code></pre> <p>Supports multiple comma-separated relations (that all together build a condition-less INNER JOIN) with optional LATERAL VIEW.</p> <p>A relation can be one of the following or a combination thereof:</p> <ul> <li>Table identifier</li> <li>Inline table using <code>VALUES exprs AS tableIdent</code></li> <li>Table-valued function (currently only <code>range</code> is supported)</li> </ul> <p>ANTLR rule: <code>fromClause</code></p>","text":""},{"location":"sql/AstBuilder/#visitfunctioncall","title":"visitFunctionCall <p>Creates one of the following:</p> <ul> <li> <p>UnresolvedFunction for a bare function (with no window specification)</p> </li> <li> <p><code>UnresolvedWindowExpression</code> for a function evaluated in a windowed context with a <code>WindowSpecReference</code></p> </li> <li> <p>WindowExpression for a function over a window</p> </li> </ul> <p>ANTLR rule: <code>functionCall</code></p> <pre><code>import spark.sessionState.sqlParser\n\nscala&gt; sqlParser.parseExpression(\"foo()\")\nres0: org.apache.spark.sql.catalyst.expressions.Expression = 'foo()\n\nscala&gt; sqlParser.parseExpression(\"foo() OVER windowSpecRef\")\nres1: org.apache.spark.sql.catalyst.expressions.Expression = unresolvedwindowexpression('foo(), WindowSpecReference(windowSpecRef))\n\nscala&gt; sqlParser.parseExpression(\"foo() OVER (CLUSTER BY field)\")\nres2: org.apache.spark.sql.catalyst.expressions.Expression = 'foo() windowspecdefinition('field, UnspecifiedFrame)\n</code></pre>","text":""},{"location":"sql/AstBuilder/#visitinlinetable","title":"visitInlineTable <p>Creates a <code>UnresolvedInlineTable</code> unary logical operator (as the child of SubqueryAlias for <code>tableAlias</code>)</p> <pre><code>VALUES expression (',' expression)* tableAlias\n</code></pre> <p><code>expression</code> can be as follows:</p> <ul> <li> <p>CreateNamedStruct expression for multiple-column tables</p> </li> <li> <p>Any Catalyst expression for one-column tables</p> </li> </ul> <p><code>tableAlias</code> can be specified explicitly or defaults to <code>colN</code> for every column (starting from <code>1</code> for <code>N</code>).</p> <p>ANTLR rule: <code>inlineTable</code></p>","text":""},{"location":"sql/AstBuilder/#visitinsertintotable","title":"visitInsertIntoTable <p>Creates a InsertIntoTable (indirectly)</p> <p>A 3-element tuple with a <code>TableIdentifier</code>, optional partition keys and the <code>exists</code> flag disabled</p> <pre><code>INSERT INTO TABLE? tableIdentifier partitionSpec?\n</code></pre> <p>ANTLR labeled alternative: <code>#insertIntoTable</code></p>  <p>Note</p> <p><code>insertIntoTable</code> is part of <code>insertInto</code> that is in turn used only as a helper labeled alternative in singleInsertQuery and multiInsertQueryBody ANTLR rules.</p>","text":""},{"location":"sql/AstBuilder/#visitinsertoverwritetable","title":"visitInsertOverwriteTable <p>Creates a InsertIntoTable (indirectly)</p> <p>A 3-element tuple with a <code>TableIdentifier</code>, optional partition keys and the <code>exists</code> flag</p> <pre><code>INSERT OVERWRITE TABLE tableIdentifier (partitionSpec (IF NOT EXISTS)?)?\n</code></pre> <p>In a way, <code>visitInsertOverwriteTable</code> is simply a more general version of the visitInsertIntoTable with the <code>exists</code> flag on or off based on existence of <code>IF NOT EXISTS</code>. The main difference is that dynamic partitions are used with no <code>IF NOT EXISTS</code>.</p> <p>ANTLR labeled alternative: <code>#insertOverwriteTable</code></p>  <p>Note</p> <p><code>insertIntoTable</code> is part of <code>insertInto</code> that is in turn used only as a helper labeled alternative in singleInsertQuery and multiInsertQueryBody ANTLR rules.</p>","text":""},{"location":"sql/AstBuilder/#visitinterval","title":"visitInterval <p>Creates a Literal expression to represent an interval type</p> <pre><code>INTERVAL (MultiUnitsInterval | UnitToUnitInterval)?\n</code></pre> <p>ANTLR rule: <code>interval</code></p>  <p><code>visitInterval</code> creates a CalendarInterval.</p> <p><code>visitInterval</code> parses <code>UnitToUnitInterval</code> if specified first (and spark.sql.legacy.interval.enabled configuration property turned off).</p> <pre><code>unitToUnitInterval\n    : value from TO to\n    ;\n</code></pre> <p><code>month</code> in <code>to</code> leads to a <code>YearMonthIntervalType</code> while other identifiers lead to a <code>DayTimeIntervalType</code>.</p> <pre><code>INTERVAL '0-0' YEAR TO MONTH        // YearMonthIntervalType\nINTERVAL '0 00:00:00' DAY TO SECOND // DayTimeIntervalType\n</code></pre> <p><code>visitInterval</code> parses <code>MultiUnitsInterval</code> if specified (and spark.sql.legacy.interval.enabled configuration property turned off).</p> <pre><code>multiUnitsInterval\n    : (intervalValue unit)+\n    ;\n</code></pre>","text":""},{"location":"sql/AstBuilder/#visitmergeintotable","title":"visitMergeIntoTable <p>Creates a MergeIntoTable logical command</p> <pre><code>MERGE INTO target targetAlias\nUSING (source | '(' sourceQuery ')') sourceAlias\nON mergeCondition\nmatchedClause*\nnotMatchedClause*\n\nmatchedClause\n  : WHEN MATCHED (AND matchedCond)? THEN matchedAction\n\nnotMatchedClause\n  : WHEN NOT MATCHED (AND notMatchedCond)? THEN notMatchedAction\n\nmatchedAction\n  : DELETE\n  | UPDATE SET *\n  | UPDATE SET assignment (',' assignment)*\n\nnotMatchedAction\n  : INSERT *\n  | INSERT '(' columns ')'\n    VALUES '(' expression (',' expression)* ')'\n</code></pre> <p>Requirements:</p> <ol> <li>There must be at least one <code>WHEN</code> clause</li> <li>When there are more than one <code>MATCHED</code> clauses, only the last <code>MATCHED</code> clause can omit the condition</li> <li>When there are more than one <code>NOT MATCHED</code> clauses, only the last <code>NOT MATCHED</code> clause can omit the condition</li> </ol> <p>ANTLR labeled alternative: <code>#mergeIntoTable</code></p>","text":""},{"location":"sql/AstBuilder/#visitmultiinsertquery","title":"visitMultiInsertQuery <p>Creates a logical operator with a InsertIntoTable (and <code>UnresolvedRelation</code> leaf operator)</p> <pre><code>FROM relation (',' relation)* lateralView*\nINSERT OVERWRITE TABLE ...\n\nFROM relation (',' relation)* lateralView*\nINSERT INTO TABLE? ...\n</code></pre> <p>ANTLR rule: <code>multiInsertQueryBody</code></p>","text":""},{"location":"sql/AstBuilder/#visitnamedexpression","title":"visitNamedExpression <p>Creates one of the following Catalyst expressions:</p> <ul> <li><code>Alias</code> (for a single alias)</li> <li><code>MultiAlias</code> (for a parenthesis enclosed alias list)</li> <li>a bare Expression</li> </ul> <p>ANTLR rule: <code>namedExpression</code></p>","text":""},{"location":"sql/AstBuilder/#visitnamedquery","title":"visitNamedQuery <p>Creates a SubqueryAlias</p>","text":""},{"location":"sql/AstBuilder/#visitprimitivedatatype","title":"visitPrimitiveDataType <p>Creates a primitive DataType from SQL type representation.</p>    SQL Type DataType     <code>boolean</code> BooleanType   <code>tinyint</code>, <code>byte</code> ByteType   <code>smallint</code>, <code>short</code> ShortType   <code>int</code>, <code>integer</code> IntegerType   <code>bigint</code>, <code>long</code> LongType   <code>float</code>, <code>real</code> FloatType   <code>double</code> DoubleType   <code>date</code> DateType   <code>timestamp</code> TimestampType   <code>string</code> StringType   <code>character</code>, <code>char</code> CharType   <code>varchar</code> VarcharType   <code>binary</code> BinaryType   <code>decimal</code>, <code>dec</code>, <code>numeric</code> DecimalType   <code>void</code> NullType   <code>interval</code> CalendarIntervalType","text":""},{"location":"sql/AstBuilder/#visitpredicated","title":"visitPredicated <p>Creates an Expression</p> <p>ANTLR rule: <code>predicated</code></p>","text":""},{"location":"sql/AstBuilder/#visitqueryspecification","title":"visitQuerySpecification <p>Creates <code>OneRowRelation</code> or LogicalPlan</p>  OneRowRelation <p><code>visitQuerySpecification</code> creates a <code>OneRowRelation</code> for a <code>SELECT</code> without a <code>FROM</code> clause.</p> <pre><code>val q = sql(\"select 1\")\nscala&gt; println(q.queryExecution.logical.numberedTreeString)\n00 'Project [unresolvedalias(1, None)]\n01 +- OneRowRelation$\n</code></pre>  <p>ANTLR rule: <code>querySpecification</code></p>","text":""},{"location":"sql/AstBuilder/#visitrelation","title":"visitRelation <p>Creates a LogicalPlan for a <code>FROM</code> clause.</p> <p>ANTLR rule: <code>relation</code></p>","text":""},{"location":"sql/AstBuilder/#visitrenametablecolumn","title":"visitRenameTableColumn <p>Creates a RenameColumn for the following SQL statement:</p> <pre><code>ALTER TABLE table\nRENAME COLUMN from TO to\n</code></pre> <p>ANTLR labeled alternative: <code>#renameTableColumn</code></p>","text":""},{"location":"sql/AstBuilder/#visitrepairtable","title":"visitRepairTable <p>Creates a <code>RepairTable</code> unary logical command for the following SQL statement:</p> <pre><code>MSCK REPAIR TABLE multipartIdentifier\n</code></pre> <p>ANTLR labeled alternative: <code>#repairTable</code></p>","text":""},{"location":"sql/AstBuilder/#visitshowcreatetable","title":"visitShowCreateTable <p>Creates a ShowCreateTable logical command for the following SQL statement:</p> <pre><code>SHOW CREATE TABLE multipartIdentifier (AS SERDE)?\n</code></pre> <p>ANTLR labeled alternative: <code>#showCreateTable</code></p>","text":""},{"location":"sql/AstBuilder/#visitshowcurrentnamespace","title":"visitShowCurrentNamespace <p>Creates a <code>ShowCurrentNamespaceCommand</code> logical command for the following SQL statement:</p> <pre><code>SHOW CURRENT NAMESPACE\n</code></pre> <p>ANTLR labeled alternative: <code>#showCurrentNamespace</code></p>","text":""},{"location":"sql/AstBuilder/#visitshowtblproperties","title":"visitShowTblProperties <p>Creates a ShowTableProperties logical command</p> <pre><code>SHOW TBLPROPERTIES [multi-part table identifier]\n  ('(' [dot-separated table property key] ')')?\n</code></pre> <p>ANTLR labeled alternative: <code>#showTblProperties</code></p>","text":""},{"location":"sql/AstBuilder/#visitshowtables","title":"visitShowTables <p>Creates a ShowTables for the following SQL statement:</p> <pre><code>SHOW TABLES ((FROM | IN) multipartIdentifier)?\n  (LIKE? pattern=STRING)?\n</code></pre> <p>ANTLR labeled alternative: <code>#showTables</code></p>","text":""},{"location":"sql/AstBuilder/#visitsingledatatype","title":"visitSingleDataType <p>Creates a DataType</p> <p>ANTLR rule: <code>singleDataType</code></p>","text":""},{"location":"sql/AstBuilder/#visitsingleexpression","title":"visitSingleExpression <p>Creates an Expression</p> <p>Takes the named expression and relays to visitNamedExpression</p> <p>ANTLR rule: <code>singleExpression</code></p>","text":""},{"location":"sql/AstBuilder/#visitsingleinsertquery","title":"visitSingleInsertQuery <p>Calls withInsertInto (with an <code>InsertIntoContext</code>) for the following SQLs:</p> <pre><code>INSERT OVERWRITE TABLE? multipartIdentifier\n(partitionSpec (IF NOT EXISTS)?)?\nidentifierList?\n</code></pre> <pre><code>INSERT INTO TABLE? multipartIdentifier\npartitionSpec?\n(IF NOT EXISTS)?\nidentifierList?\n</code></pre> <pre><code>INSERT OVERWRITE LOCAL? DIRECTORY path=STRING rowFormat? createFileFormat?\n</code></pre> <pre><code>INSERT OVERWRITE LOCAL? DIRECTORY (path=STRING)?\ntableProvider\n(OPTIONS options=tablePropertyList)?\n</code></pre> <p>ANTLR labeled alternative: <code>#singleInsertQuery</code></p>","text":""},{"location":"sql/AstBuilder/#visitsortitem","title":"visitSortItem <p>Creates a SortOrder unary expression</p> <pre><code>sortItem\n    : expression ordering=(ASC | DESC)? (NULLS nullOrder=(LAST | FIRST))?\n    ;\n\n// queryOrganization\nORDER BY order+=sortItem (',' order+=sortItem)*\nSORT BY sort+=sortItem (',' sort+=sortItem)*\n\n// windowSpec\n(ORDER | SORT) BY sortItem (',' sortItem)*)?\n</code></pre> <p>ANTLR rule: <code>sortItem</code></p>","text":""},{"location":"sql/AstBuilder/#visitsinglestatement","title":"visitSingleStatement <p>Creates a LogicalPlan from a single SQL statement</p> <p>ANTLR rule: <code>singleStatement</code></p>","text":""},{"location":"sql/AstBuilder/#visitstar","title":"visitStar <p>Creates a UnresolvedStar</p> <p>ANTLR labeled alternative: <code>#star</code></p>","text":""},{"location":"sql/AstBuilder/#visitsubqueryexpression","title":"visitSubqueryExpression <p>Creates a ScalarSubquery</p> <p>ANTLR labeled alternative: <code>#subqueryExpression</code></p>","text":""},{"location":"sql/AstBuilder/#visittablevaluedfunction","title":"visitTableValuedFunction <p>Creates a UnresolvedTableValuedFunction</p> <pre><code>relationPrimary\n  :\n  ...\n  | functionTable                                         #tableValuedFunction\n  ;\n\nfunctionTable\n  : funcName '(' (expression (',' expression)*)? ')' tableAlias\n  ;\n</code></pre> <p>ANTLR labeled alternative: <code>#tableValuedFunction</code></p>","text":""},{"location":"sql/AstBuilder/#visitupdatetable","title":"visitUpdateTable <p>Creates an UpdateTable logical operator</p> <pre><code>UPDATE multipartIdentifier tableAlias setClause whereClause?\n</code></pre> <p>ANTLR labeled alternative: <code>#updateTable</code></p>","text":""},{"location":"sql/AstBuilder/#visituse","title":"visitUse <p>Creates a SetCatalogAndNamespace</p> <pre><code>USE NAMESPACE? multipartIdentifier\n</code></pre> <p>ANTLR labeled alternative: <code>#use</code></p>","text":""},{"location":"sql/AstBuilder/#visitwindowdef","title":"visitWindowDef <p>Creates a WindowSpecDefinition</p> <pre><code>windowSpec\n    : '('\n      ( CLUSTER BY partition (',' partition)*\n      | ((PARTITION | DISTRIBUTE) BY partition (',' partition)*)?\n        ((ORDER | SORT) BY sortItem (',' sortItem)*)?)\n      windowFrame?\n      ')'\n    ;\n\nwindowFrame\n    : RANGE start\n    | ROWS start\n    | RANGE BETWEEN start AND end\n    | ROWS BETWEEN start AND end\n    ;\n\n// start and end bounds of windowFrames\nframeBound\n    : UNBOUNDED (PRECEDING | FOLLOWING)\n    | CURRENT ROW\n    | literal (PRECEDING | FOLLOWING)\n    ;\n</code></pre> <p>ANTLR rule: <code>windowDef</code></p>","text":""},{"location":"sql/AstBuilder/#parsing-handlers","title":"Parsing Handlers","text":""},{"location":"sql/AstBuilder/#withaggregationclause","title":"withAggregationClause <p>Creates one of the following logical operators:</p> <ul> <li> <p>GroupingSets for <code>GROUP BY ... GROUPING SETS (...)</code></p> </li> <li> <p>Aggregate for <code>GROUP BY ... (WITH CUBE | WITH ROLLUP)?</code></p> </li> </ul>","text":""},{"location":"sql/AstBuilder/#withcte","title":"withCTE <p>Creates an UnresolvedWith logical operator for Common Table Expressions (in visitQuery and visitDmlStatement)</p> <pre><code>WITH namedQuery (',' namedQuery)*\n\nnamedQuery\n    : name (columnAliases)? AS? '(' query ')'\n    ;\n</code></pre>","text":""},{"location":"sql/AstBuilder/#withfromstatementbody","title":"withFromStatementBody <p>Used in visitFromStatement and visitMultiInsertQuery</p>","text":""},{"location":"sql/AstBuilder/#withgenerate","title":"withGenerate <p>Adds a Generate with a UnresolvedGenerator and join flag enabled for <code>LATERAL VIEW</code> (in <code>SELECT</code> or <code>FROM</code> clauses).</p>","text":""},{"location":"sql/AstBuilder/#withhavingclause","title":"withHavingClause <p>Creates an UnresolvedHaving for the following:</p> <pre><code>HAVING booleanExpression\n</code></pre>","text":""},{"location":"sql/AstBuilder/#withhints","title":"withHints <p>Adds an UnresolvedHint for <code>/*+ hint */</code> in <code>SELECT</code> queries.</p>  <p>Note</p> <p>Note <code>+</code> (plus) between <code>/*</code> and <code>*/</code></p>  <p><code>hint</code> is of the format <code>name</code> or <code>name (param1, param2, ...)</code>.</p> <pre><code>/*+ BROADCAST (table) */\n</code></pre>","text":""},{"location":"sql/AstBuilder/#withinsertinto","title":"withInsertInto <p>Creates one of the following logical operators:</p> <ul> <li>InsertIntoStatement</li> <li>InsertIntoDir</li> </ul> <p>Used in visitMultiInsertQuery and visitSingleInsertQuery</p>","text":""},{"location":"sql/AstBuilder/#withjoinrelations","title":"withJoinRelations <p>Creates one or more Join logical operators for a FROM clause and relation.</p> <p>The following join types are supported:</p> <ul> <li><code>INNER</code> (default)</li> <li><code>CROSS</code></li> <li><code>LEFT</code> (with optional <code>OUTER</code>)</li> <li><code>LEFT SEMI</code></li> <li><code>RIGHT</code> (with optional <code>OUTER</code>)</li> <li><code>FULL</code> (with optional <code>OUTER</code>)</li> <li><code>ANTI</code> (optionally prefixed with <code>LEFT</code>)</li> </ul> <p>The following join criteria are supported:</p> <ul> <li><code>ON booleanExpression</code></li> <li><code>USING '(' identifier (',' identifier)* ')'</code></li> </ul> <p>Joins can be <code>NATURAL</code> (with no join criteria)</p>","text":""},{"location":"sql/AstBuilder/#withqueryspecification","title":"withQuerySpecification <p>Adds a query specification to a logical operator</p> <p>For transform <code>SELECT</code> (with <code>TRANSFORM</code>, <code>MAP</code> or <code>REDUCE</code> qualifiers), <code>withQuerySpecification</code> does...FIXME</p> <p>For regular <code>SELECT</code> (no <code>TRANSFORM</code>, <code>MAP</code> or <code>REDUCE</code> qualifiers), <code>withQuerySpecification</code> adds (in that order):</p> <ol> <li> <p>Generate unary logical operators (if used in the parsed SQL text)</p> </li> <li> <p><code>Filter</code> unary logical plan (if used in the parsed SQL text)</p> </li> <li> <p>GroupingSets or Aggregate unary logical operators (if used in the parsed SQL text)</p> </li> <li> <p><code>Project</code> and/or <code>Filter</code> unary logical operators</p> </li> <li> <p>WithWindowDefinition unary logical operator (if used in the parsed SQL text)</p> </li> <li> <p>UnresolvedHint unary logical operator (if used in the parsed SQL text)</p> </li> </ol>","text":""},{"location":"sql/AstBuilder/#withpredicate","title":"withPredicate <ul> <li> <p><code>NOT? IN '(' query ')'</code> adds an In predicate expression with a ListQuery subquery expression</p> </li> <li> <p><code>NOT? IN '(' expression (',' expression)* ')'</code> adds an In predicate expression</p> </li> </ul>","text":""},{"location":"sql/AstBuilder/#withpivot","title":"withPivot <p>Creates a Pivot unary logical operator for the following SQL clause:</p> <pre><code>PIVOT '(' aggregates FOR pivotColumn IN '(' pivotValue (',' pivotValue)* ')' ')'\n</code></pre> <p>Used in visitFromClause</p>","text":""},{"location":"sql/AstBuilder/#withrepartitionbyexpression","title":"withRepartitionByExpression <p><code>withRepartitionByExpression</code> throws a <code>ParseException</code>:</p> <pre><code>DISTRIBUTE BY is not supported\n</code></pre> <p>Used in withQueryResultClauses</p>","text":""},{"location":"sql/AstBuilder/#withselectqueryspecification","title":"withSelectQuerySpecification <p>Used in withFromStatementBody and visitRegularQuerySpecification</p>","text":""},{"location":"sql/AstBuilder/#withtransformqueryspecification","title":"withTransformQuerySpecification <p>Used in withFromStatementBody and visitTransformQuerySpecification</p>","text":""},{"location":"sql/AstBuilder/#withwindows","title":"withWindows <p>Adds a WithWindowDefinition for window aggregates (given <code>WINDOW</code> definitions).</p> <pre><code>WINDOW identifier AS windowSpec\n  (',' identifier AS windowSpec)*\n</code></pre> <p>Used in withQueryResultClauses and withQuerySpecification</p>","text":""},{"location":"sql/AstBuilder/#parseintervalliteral","title":"parseIntervalLiteral <pre><code>parseIntervalLiteral(\n  ctx: IntervalContext): CalendarInterval\n</code></pre> <p><code>parseIntervalLiteral</code> creates a CalendarInterval (using visitMultiUnitsInterval and visitUnitToUnitInterval).</p> <p><code>parseIntervalLiteral</code> is used when:</p> <ul> <li><code>AstBuilder</code> is requested to visitInterval</li> <li><code>SparkSqlAstBuilder</code> is requested to visitSetTimeZone</li> </ul>","text":""},{"location":"sql/CatalystSqlParser/","title":"CatalystSqlParser","text":"<p><code>CatalystSqlParser</code> is an AbstractSqlParser for DataTypes.</p> <p><code>CatalystSqlParser</code> uses AstBuilder for parsing SQL texts.</p> <pre><code>import org.apache.spark.sql.catalyst.parser.CatalystSqlParser\nimport org.apache.spark.sql.internal.SQLConf\nval catalystSqlParser = new CatalystSqlParser(SQLConf.get)\nscala&gt; :type catalystSqlParser.astBuilder\norg.apache.spark.sql.catalyst.parser.AstBuilder\n</code></pre> <p><code>CatalystSqlParser</code> is used to translate DataTypes from their canonical string representation (e.g. when adding fields to a schema or casting column to a different data type) or StructTypes.</p> <pre><code>import org.apache.spark.sql.types.StructType\nscala&gt; val struct = new StructType().add(\"a\", \"int\")\nstruct: org.apache.spark.sql.types.StructType = StructType(StructField(a,IntegerType,true))\n\nscala&gt; val asInt = expr(\"token = 'hello'\").cast(\"int\")\nasInt: org.apache.spark.sql.Column = CAST((token = hello) AS INT)\n</code></pre> <p>When parsing, you should see INFO messages in the logs:</p> <pre><code>Parsing command: int\n</code></pre> <p>It is also used in <code>HiveClientImpl</code> (when converting columns from Hive to Spark) and in <code>OrcFileOperator</code> (when inferring the schema for ORC files).</p>"},{"location":"sql/CatalystSqlParser/#creating-instance","title":"Creating Instance","text":"<p><code>CatalystSqlParser</code> takes the following to be created:</p> <ul> <li>SQLConf</li> </ul> <p><code>CatalystSqlParser</code> is created when:</p> <ul> <li>SessionCatalog is created</li> </ul>"},{"location":"sql/CatalystSqlParser/#logging","title":"Logging","text":"<p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.catalyst.parser.CatalystSqlParser</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.catalyst.parser.CatalystSqlParser=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"sql/ParserInterface/","title":"ParserInterface","text":"<p><code>ParserInterface</code> is the abstraction of SQL parsers that can convert (parse) textual representation of SQL statements (SQL text) into Spark SQL's relational entities (e.g. Catalyst expressions, logical operators, table and function identifiers, table schema, and data types).</p>"},{"location":"sql/ParserInterface/#accessing-parserinterface","title":"Accessing ParserInterface","text":"<p><code>ParserInterface</code> is available as SessionState.sqlParser.</p> <pre><code>scala&gt; :type spark\norg.apache.spark.sql.SparkSession\n\nscala&gt; :type spark.sessionState.sqlParser\norg.apache.spark.sql.catalyst.parser.ParserInterface\n</code></pre>"},{"location":"sql/ParserInterface/#contract","title":"Contract","text":""},{"location":"sql/ParserInterface/#parsedatatype","title":"parseDataType <pre><code>parseDataType(\n  sqlText: String): DataType\n</code></pre> <p>Creates a DataType from the given SQL text</p>","text":""},{"location":"sql/ParserInterface/#parseexpression","title":"parseExpression <pre><code>parseExpression(\n  sqlText: String): Expression\n</code></pre> <p>Creates an Expression from the given SQL text</p> <p>Used when:</p> <ul> <li><code>Dataset</code> is requested to selectExpr, filter, where</li> <li>expr standard function is used</li> </ul>","text":""},{"location":"sql/ParserInterface/#parsefunctionidentifier","title":"parseFunctionIdentifier <pre><code>parseFunctionIdentifier(\n  sqlText: String): FunctionIdentifier\n</code></pre> <p>Creates a <code>FunctionIdentifier</code> from the given SQL text</p> <p>Used when:</p> <ul> <li> <p><code>SessionCatalog</code> is requested to listFunctions</p> </li> <li> <p><code>CatalogImpl</code> is requested to getFunction and functionExists</p> </li> </ul>","text":""},{"location":"sql/ParserInterface/#parsemultipartidentifier","title":"parseMultipartIdentifier <pre><code>parseMultipartIdentifier(\n  sqlText: String): Seq[String]\n</code></pre> <p>Creates a multi-part identifier from the given SQL text</p> <p>Used when:</p> <ul> <li><code>CatalogV2Implicits</code> utility is requested to <code>parseColumnPath</code></li> <li><code>LogicalExpressions</code> utility is requested to <code>parseReference</code></li> <li> <p><code>DataFrameWriter</code> is requested to insertInto and saveAsTable</p> </li> <li> <p>DataFrameWriterV2 is created (and requested for tableName)</p> </li> <li> <p><code>SparkSession</code> is requested to table</p> </li> </ul>","text":""},{"location":"sql/ParserInterface/#parseplan","title":"parsePlan <pre><code>parsePlan(\n  sqlText: String): LogicalPlan\n</code></pre> <p>Creates a LogicalPlan from the given SQL text</p> <p>Used when:</p> <ul> <li><code>SessionCatalog</code> is requested to fromCatalogTable</li> <li><code>SparkSession</code> is requested to execute a SQL query</li> </ul>","text":""},{"location":"sql/ParserInterface/#parsetableidentifier","title":"parseTableIdentifier <pre><code>parseTableIdentifier(\n  sqlText: String): TableIdentifier\n</code></pre> <p>Creates a <code>TableIdentifier</code> from the given SQL text</p> <p>Used when:</p> <ul> <li> <p><code>DataFrameWriter</code> is requested to insertInto and saveAsTable</p> </li> <li> <p><code>Dataset</code> is requested to createTempViewCommand</p> </li> <li> <p><code>SparkSession</code> is requested to table</p> </li> <li> <p><code>CatalogImpl</code> is requested to listColumns, getTable, tableExists, createTable, recoverPartitions, uncacheTable, and refreshTable</p> </li> <li> <p><code>SessionState</code> is requested to &lt;&gt;","text":""},{"location":"sql/ParserInterface/#parsetableschema","title":"parseTableSchema <pre><code>parseTableSchema(\n  sqlText: String): StructType\n</code></pre> <p>Creates a StructType from the given SQL text</p>","text":""},{"location":"sql/ParserInterface/#implementations","title":"Implementations","text":"<ul> <li>AbstractSqlParser</li> </ul>"},{"location":"sql/SparkSqlAstBuilder/","title":"SparkSqlAstBuilder \u2014 ANTLR-based SQL Parser","text":"<p><code>SparkSqlAstBuilder</code> is an AstBuilder that converts SQL statements into Catalyst expressions, logical plans or table identifiers (using visit callbacks).</p>"},{"location":"sql/SparkSqlAstBuilder/#creating-instance","title":"Creating Instance","text":"<p><code>SparkSqlAstBuilder</code> takes the following to be created:</p> <ul> <li> SQLConf <p><code>SparkSqlAstBuilder</code> is created for SparkSqlParser (which happens when <code>SparkSession</code> is requested for SessionState).</p> <p></p> expr Standard Function <p><code>SparkSqlAstBuilder</code> can also be temporarily created for expr standard function (to create column expressions).</p> <pre><code>val c = expr(\"from_json(value, schema)\")\nscala&gt; :type c\norg.apache.spark.sql.Column\n\nscala&gt; :type c.expr\norg.apache.spark.sql.catalyst.expressions.Expression\n\nscala&gt; println(c.expr.numberedTreeString)\n00 'from_json('value, 'schema)\n01 :- 'value\n02 +- 'schema\n</code></pre>"},{"location":"sql/SparkSqlAstBuilder/#accessing-sparksqlastbuilder","title":"Accessing SparkSqlAstBuilder","text":"<pre><code>scala&gt; :type spark.sessionState.sqlParser\norg.apache.spark.sql.catalyst.parser.ParserInterface\n\nimport org.apache.spark.sql.execution.SparkSqlParser\nval sqlParser = spark.sessionState.sqlParser.asInstanceOf[SparkSqlParser]\n\nscala&gt; :type sqlParser.astBuilder\norg.apache.spark.sql.execution.SparkSqlAstBuilder\n</code></pre>"},{"location":"sql/SparkSqlAstBuilder/#visit-callbacks","title":"Visit Callbacks","text":""},{"location":"sql/SparkSqlAstBuilder/#visitanalyze","title":"visitAnalyze <p>Creates AnalyzeColumnCommand, AnalyzePartitionCommand or AnalyzeTableCommand logical commands.</p> <p>ANTLR labeled alternative: <code>#analyze</code></p>  NOSCAN Identifier <p> <code>visitAnalyze</code> supports <code>NOSCAN</code> identifier only (and reports a <code>ParseException</code> if not used). <p><code>NOSCAN</code> is used for <code>AnalyzePartitionCommand</code> and <code>AnalyzeTableCommand</code> logical commands only.</p>","text":""},{"location":"sql/SparkSqlAstBuilder/#analyzecolumncommand","title":"AnalyzeColumnCommand","text":"<p>AnalyzeColumnCommand logical command for <code>ANALYZE TABLE</code> with <code>FOR COLUMNS</code> clause (but no <code>PARTITION</code> specification)</p> <pre><code>// Seq((0, 0, \"zero\"), (1, 1, \"one\")).toDF(\"id\", \"p1\", \"p2\").write.partitionBy(\"p1\", \"p2\").saveAsTable(\"t1\")\nval sqlText = \"ANALYZE TABLE t1 COMPUTE STATISTICS FOR COLUMNS id, p1\"\nval plan = spark.sql(sqlText).queryExecution.logical\nimport org.apache.spark.sql.execution.command.AnalyzeColumnCommand\nval cmd = plan.asInstanceOf[AnalyzeColumnCommand]\nscala&gt; println(cmd)\nAnalyzeColumnCommand `t1`, [id, p1]\n</code></pre>"},{"location":"sql/SparkSqlAstBuilder/#analyzepartitioncommand","title":"AnalyzePartitionCommand","text":"<p>AnalyzePartitionCommand logical command for <code>ANALYZE TABLE</code> with <code>PARTITION</code> specification (but no <code>FOR COLUMNS</code> clause)</p> <pre><code>// Seq((0, 0, \"zero\"), (1, 1, \"one\")).toDF(\"id\", \"p1\", \"p2\").write.partitionBy(\"p1\", \"p2\").saveAsTable(\"t1\")\nval analyzeTable = \"ANALYZE TABLE t1 PARTITION (p1, p2) COMPUTE STATISTICS\"\nval plan = spark.sql(analyzeTable).queryExecution.logical\nimport org.apache.spark.sql.execution.command.AnalyzePartitionCommand\nval cmd = plan.asInstanceOf[AnalyzePartitionCommand]\nscala&gt; println(cmd)\nAnalyzePartitionCommand `t1`, Map(p1 -&gt; None, p2 -&gt; None), false\n</code></pre>"},{"location":"sql/SparkSqlAstBuilder/#analyzetablecommand","title":"AnalyzeTableCommand","text":"<p>AnalyzeTableCommand logical command for <code>ANALYZE TABLE</code> with neither <code>PARTITION</code> specification nor <code>FOR COLUMNS</code> clause</p> <pre><code>// Seq((0, 0, \"zero\"), (1, 1, \"one\")).toDF(\"id\", \"p1\", \"p2\").write.partitionBy(\"p1\", \"p2\").saveAsTable(\"t1\")\nval sqlText = \"ANALYZE TABLE t1 COMPUTE STATISTICS NOSCAN\"\nval plan = spark.sql(sqlText).queryExecution.logical\nimport org.apache.spark.sql.execution.command.AnalyzeTableCommand\nval cmd = plan.asInstanceOf[AnalyzeTableCommand]\nscala&gt; println(cmd)\nAnalyzeTableCommand `t1`, false\n</code></pre>"},{"location":"sql/SparkSqlAstBuilder/#visitgenericfileformat","title":"visitGenericFileFormat <p>Creates a CatalogStorageFormat with the Hive SerDe for the data source name that can be one of the following (with their Hive-supported variants):</p> <ul> <li><code>sequencefile</code></li> <li><code>rcfile</code></li> <li><code>orc</code></li> <li><code>parquet</code></li> <li><code>textfile</code></li> <li><code>avro</code></li> </ul>","text":""},{"location":"sql/SparkSqlAstBuilder/#visitcachetable","title":"visitCacheTable <p>Creates a CacheTableCommand logical command for <code>CACHE LAZY? TABLE [table] (AS? [query])?</code></p> <p>ANTLR labeled alternative: <code>#cacheTable</code></p>","text":""},{"location":"sql/SparkSqlAstBuilder/#visitcreatehivetable","title":"visitCreateHiveTable <p>Creates a CreateTable</p> <p>ANTLR labeled alternative: <code>#createHiveTable</code></p>","text":""},{"location":"sql/SparkSqlAstBuilder/#visitcreatetable","title":"visitCreateTable <p>Creates CreateTempViewUsing logical operator for <code>CREATE TEMPORARY VIEW &amp;hellip; USING &amp;hellip;</code> or falls back to AstBuilder</p> <p>ANTLR labeled alternative: <code>#createTable</code></p>","text":""},{"location":"sql/SparkSqlAstBuilder/#visitcreateview","title":"visitCreateView <p>Creates a CreateViewCommand for <code>CREATE VIEW AS</code> SQL statement.</p> <pre><code>CREATE [OR REPLACE] [[GLOBAL] TEMPORARY]\nVIEW [IF NOT EXISTS] tableIdentifier\n[identifierCommentList] [COMMENT STRING]\n[PARTITIONED ON identifierList]\n[TBLPROPERTIES tablePropertyList] AS query\n</code></pre> <p>ANTLR labeled alternative: <code>#createView</code></p>","text":""},{"location":"sql/SparkSqlAstBuilder/#visitcreatetempviewusing","title":"visitCreateTempViewUsing <p>Creates a CreateTempViewUsing for <code>CREATE TEMPORARY VIEW &amp;hellip; USING</code></p> <p>ANTLR labeled alternative: <code>#createTempViewUsing</code></p>","text":""},{"location":"sql/SparkSqlAstBuilder/#visitdescribetable","title":"visitDescribeTable <p>Creates DescribeColumnCommand or DescribeTableCommand logical commands.</p> <p>ANTLR labeled alternative: <code>#describeTable</code></p>","text":""},{"location":"sql/SparkSqlAstBuilder/#describecolumncommand","title":"DescribeColumnCommand","text":"<p>DescribeColumnCommand logical command for <code>DESCRIBE TABLE</code> with a single column only (i.e. no <code>PARTITION</code> specification).</p> <pre><code>// Seq((0, 0, \"zero\"), (1, 1, \"one\")).toDF(\"id\", \"p1\", \"p2\").write.partitionBy(\"p1\", \"p2\").saveAsTable(\"t1\")\nval sqlCmd = \"DESC EXTENDED t1 p1\"\nval plan = spark.sql(sqlCmd).queryExecution.logical\nimport org.apache.spark.sql.execution.command.DescribeColumnCommand\nval cmd = plan.asInstanceOf[DescribeColumnCommand]\nscala&gt; println(cmd)\nDescribeColumnCommand `t1`, [p1], true\n</code></pre>"},{"location":"sql/SparkSqlAstBuilder/#describetablecommand","title":"DescribeTableCommand","text":"<p>DescribeTableCommand logical command for all other variants of <code>DESCRIBE TABLE</code> (i.e. no column)</p> <pre><code>// Seq((0, 0, \"zero\"), (1, 1, \"one\")).toDF(\"id\", \"p1\", \"p2\").write.partitionBy(\"p1\", \"p2\").saveAsTable(\"t1\")\nval sqlCmd = \"DESC t1\"\nval plan = spark.sql(sqlCmd).queryExecution.logical\nimport org.apache.spark.sql.execution.command.DescribeTableCommand\nval cmd = plan.asInstanceOf[DescribeTableCommand]\nscala&gt; println(cmd)\nDescribeTableCommand `t1`, false\n</code></pre>"},{"location":"sql/SparkSqlAstBuilder/#visitexplain","title":"visitExplain <p>Creates an ExplainCommand logical command for the following:</p> <pre><code>EXPLAIN (LOGICAL | FORMATTED | EXTENDED | CODEGEN | COST)?\n  statement\n</code></pre>  <p>Operation not allowed: EXPLAIN LOGICAL</p> <p><code>EXPLAIN LOGICAL</code> is currently not supported.</p>  <p>ANTLR labeled alternative: <code>#explain</code></p>","text":""},{"location":"sql/SparkSqlAstBuilder/#visitshowcreatetable","title":"visitShowCreateTable <p>Creates ShowCreateTableCommand logical command for <code>SHOW CREATE TABLE</code> SQL statement.</p> <pre><code>SHOW CREATE TABLE tableIdentifier\n</code></pre> <p>ANTLR labeled alternative: <code>#showCreateTable</code></p>","text":""},{"location":"sql/SparkSqlAstBuilder/#visittruncatetable","title":"visitTruncateTable <p>Creates TruncateTableCommand logical command for <code>TRUNCATE TABLE</code> SQL statement.</p> <pre><code>TRUNCATE TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]\n</code></pre> <p>ANTLR labeled alternative: <code>#truncateTable</code></p>","text":""},{"location":"sql/SparkSqlAstBuilder/#withrepartitionbyexpression","title":"withRepartitionByExpression <pre><code>withRepartitionByExpression(\n  ctx: QueryOrganizationContext,\n  expressions: Seq[Expression],\n  query: LogicalPlan): LogicalPlan\n</code></pre> <p><code>withRepartitionByExpression</code> creates a RepartitionByExpression logical operator (with undefined number of partitions).</p> <p><code>withRepartitionByExpression</code> is part of AstBuilder abstraction.</p>","text":""},{"location":"sql/SparkSqlParser/","title":"SparkSqlParser \u2014 Default SQL Parser","text":"<p><code>SparkSqlParser</code> is a SQL parser to extract Catalyst expressions, plans, table identifiers from SQL texts using SparkSqlAstBuilder (as AstBuilder).</p> <p><code>SparkSqlParser</code> is the initial SQL parser in a <code>SparkSession</code>.</p> <p><code>SparkSqlParser</code> supports variable substitution.</p> <p><code>SparkSqlParser</code> is used to parse table strings into their corresponding table identifiers in the following:</p> <ul> <li><code>table</code> methods in DataFrameReader and SparkSession</li> <li>insertInto and saveAsTable methods of <code>DataFrameWriter</code></li> <li><code>createExternalTable</code> and <code>refreshTable</code> methods of Catalog (and SessionState)</li> </ul>"},{"location":"sql/SparkSqlParser/#creating-instance","title":"Creating Instance","text":"<p><code>SparkSqlParser</code> takes the following to be created:</p> <ul> <li> SQLConf <p><code>SparkSqlParser</code> is created when:</p> <ul> <li> <p><code>BaseSessionStateBuilder</code> is requested for a SQL parser</p> </li> <li> <p>expr standard function is used</p> </li> </ul>"},{"location":"sql/SparkSqlParser/#parsing-command","title":"Parsing Command <pre><code>parse[T](\n  command: String)(\n  toResult: SqlBaseParser =&gt; T): T\n</code></pre> <p><code>parse</code> is part of the AbstractSqlParser abstraction.</p>   <p>Note</p> <p>The only reason for overriding <code>parse</code> method is to allow for VariableSubstitution to substitute variables.</p>  <p><code>parse</code> requests the VariableSubstitution to substitute variables before requesting the default (parent) parser to parse the command.</p>","text":""},{"location":"sql/SparkSqlParser/#sparksqlastbuilder","title":"SparkSqlAstBuilder <p><code>SparkSqlParser</code> uses SparkSqlAstBuilder (as AstBuilder).</p>","text":""},{"location":"sql/SparkSqlParser/#accessing-sparksqlparser","title":"Accessing SparkSqlParser <p><code>SparkSqlParser</code> is available as SessionState.sqlParser (unless...FIXME(note)).</p> <pre><code>import org.apache.spark.sql.SparkSession\nassert(spark.isInstanceOf[SparkSession])\n\nimport org.apache.spark.sql.catalyst.parser.ParserInterface\nval p = spark.sessionState.sqlParser\nassert(p.isInstanceOf[ParserInterface])\n\nimport org.apache.spark.sql.execution.SparkSqlParser\nassert(spark.sessionState.sqlParser.isInstanceOf[SparkSqlParser])\n</code></pre>","text":""},{"location":"sql/SparkSqlParser/#translating-sql-statements-to-logical-operators","title":"Translating SQL Statements to Logical Operators <p><code>SparkSqlParser</code> is used in SparkSession.sql to translate a SQL text to a logical operator.</p>","text":""},{"location":"sql/SparkSqlParser/#translating-sql-statements-to-column-api","title":"Translating SQL Statements to Column API <p><code>SparkSqlParser</code> is used to translate an expression to the corresponding Column in the following:</p> <ul> <li>expr standard function</li> <li>Dataset operators: selectExpr, filter, where</li> </ul> <pre><code>scala&gt; expr(\"token = 'hello'\")\n16/07/07 18:32:53 INFO SparkSqlParser: Parsing command: token = 'hello'\nres0: org.apache.spark.sql.Column = (token = hello)\n</code></pre>","text":""},{"location":"sql/SparkSqlParser/#variable-substitution","title":"Variable Substitution <p><code>SparkSqlParser</code> creates a VariableSubstitution when created.</p> <p>The <code>VariableSubstitution</code> is used while parsing a SQL command.</p>","text":""},{"location":"sql/SparkSqlParser/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.SparkSqlParser</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.SparkSqlParser=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"sql/VariableSubstitution/","title":"VariableSubstitution","text":"<p><code>VariableSubstitution</code> allows for variable substitution in SQL commands (in SparkSqlParser and Spark Thrift Server).</p> <p>Note</p> <p><code>VariableSubstitution</code> is meant for SQL commands since in programming languages there are other means like String Interpolation in Scala.</p>"},{"location":"sql/VariableSubstitution/#demo","title":"Demo","text":"SQLScala <pre><code>SET values = VALUES 1,2,3;\n</code></pre> <pre><code>SELECT * FROM ${values};\n</code></pre> <pre><code>import org.apache.spark.sql.internal.VariableSubstitution\nval substitutor = new VariableSubstitution()\nsubstitutor.substitute(...)\n</code></pre>"},{"location":"sql/VariableSubstitution/#creating-instance","title":"Creating Instance","text":"<p><code>VariableSubstitution</code> takes no arguments to be created.</p> <p><code>VariableSubstitution</code> is created when:</p> <ul> <li><code>SparkSqlParser</code> is created</li> <li><code>SparkExecuteStatementOperation</code> (Spark Thrift Server) is created</li> <li><code>SparkSQLDriver</code> (Spark Thrift Server) is executed</li> </ul>"},{"location":"sql/VariableSubstitution/#configreader","title":"ConfigReader <p><code>VariableSubstitution</code> creates a <code>ConfigReader</code> (Spark Core) when created (for Variable Substitution).</p> <p>This <code>ConfigReader</code> uses the active SQLConf to look up keys (variables) first. It then binds the same variable provider to handle the following prefixes:</p> <ul> <li><code>spark</code></li> <li><code>sparkconf</code></li> <li><code>hivevar</code></li> <li><code>hiveconf</code></li> </ul>  <p>Note</p> <p>By default, the <code>ConfigReader</code> handles the other two prefixes:</p> <ul> <li><code>env</code> (for environment variables)</li> <li><code>system</code> (for Java system properties)</li> </ul> <p>If a reference cannot be resolved, the original string will be retained.</p>","text":""},{"location":"sql/VariableSubstitution/#variable-substitution","title":"Variable Substitution <pre><code>substitute(\n  input: String): String\n</code></pre> <p>With spark.sql.variable.substitute enabled, <code>substitute</code> requests the ConfigReader to substitute variables. Otherwise, <code>substitute</code> does nothing and simply returns the given <code>input</code>.</p> <p><code>substitute</code> is used when:</p> <ul> <li><code>SparkSqlParser</code> is requested to parse a command</li> <li><code>SparkExecuteStatementOperation</code> (Spark Thrift Server) is created</li> <li><code>SparkSQLDriver</code> (Spark Thrift Server) is executed</li> </ul>","text":""},{"location":"subqueries/","title":"Subqueries (Subquery Expressions)","text":"<p>As of Spark 2.0, Spark SQL supports subqueries.</p> <p>A subquery (aka subquery expression) is a query that is nested inside of another query.</p> <p>There are the following kinds of subqueries:</p> <p>. A subquery as a source (inside a SQL <code>FROM</code> clause) . A scalar subquery or a predicate subquery (as a column)</p> <p>Every subquery can also be correlated or uncorrelated.</p> <p>[[scalar-subquery]] A scalar subquery is a structured query that returns a single row and a single column only. Spark SQL uses ScalarSubquery (SubqueryExpression) expression to represent scalar subqueries (while sql/AstBuilder.md#visitSubqueryExpression[parsing a SQL statement]).</p>"},{"location":"subqueries/#source-scala","title":"[source, scala]","text":""},{"location":"subqueries/#fixme-scalarsubquery-in-a-logical-plan","title":"// FIXME: ScalarSubquery in a logical plan","text":"<p>A <code>ScalarSubquery</code> expression appears as scalar-subquery#[exprId] [conditionString] in a logical plan.</p>"},{"location":"subqueries/#source-scala_1","title":"[source, scala]","text":""},{"location":"subqueries/#fixme-name-of-a-scalarsubquery-in-a-logical-plan","title":"// FIXME: Name of a ScalarSubquery in a logical plan","text":"<p>It is said that scalar subqueries should be used very rarely if at all and you should join instead.</p> <p>Spark Analyzer uses ResolveSubquery resolution rule to resolve subqueries and at the end CheckAnalysis.md#checkSubqueryExpression[makes sure that they are valid].</p> <p>Catalyst Optimizer uses the following optimizations for subqueries:</p> <ul> <li> <p>PullupCorrelatedPredicates.md[PullupCorrelatedPredicates] optimization to PullupCorrelatedPredicates.md#rewriteSubQueries[rewrite subqueries] and pull up correlated predicates</p> </li> <li> <p>RewriteCorrelatedScalarSubquery optimization (to constructLeftJoins)</p> </li> </ul> <p>Spark Physical Optimizer uses PlanSubqueries physical optimization to plan queries with scalar subqueries.</p>"},{"location":"table-valued-functions/","title":"Table-Valued Functions","text":"<p>Table-Valued Functions (TFVs) are functions that return a table (as a LogicalPlan) that can be used anywhere that a regular (scalar) table is allowed.</p> <p>Table functions behave similarly to views, but, as functions in general, table functions accept parameters.</p> <p>Google BigQuery Documentation </p> <p>Read up on table-valued functions in the official documentation of Google BigQuery (for a lack of a better documentation).</p> <p>Spark SQL supports Logical Operators and Generator expressions as table-valued functions.</p> Name Logical Operator Generator Expression <code>range</code> <code>Range</code> <code>explode</code> <code>Explode</code> <code>explode_outer</code> <code>Explode</code> <code>inline</code> Inline <code>inline_outer</code> Inline <code>json_tuple</code> <code>JsonTuple</code> <code>posexplode</code> <code>PosExplode</code> <code>posexplode_outer</code> <code>PosExplode</code> <code>stack</code> <code>Stack</code> <p>Custom table-valued functions can be registered using SparkSessionExtensions.</p> <p>Spark SQL uses the SimpleTableFunctionRegistry to manage the built-in table-valued functions.</p>"},{"location":"thrift-server/","title":"Thrift JDBC/ODBC Server -- Spark Thrift Server (STS)","text":"<p>Thrift JDBC/ODBC Server (aka Spark Thrift Server or STS) is Spark SQL's port of Apache Hive's HiveServer2 that allows JDBC/ODBC clients to execute SQL queries over JDBC and ODBC protocols on Apache Spark.</p> <p>With Spark Thrift Server, business users can work with their shiny Business Intelligence (BI) tools, e.g. Tableau or Microsoft Excel, and connect to Apache Spark using the ODBC interface. That brings the in-memory distributed capabilities of Spark SQL's query engine (with all the logical optimizations you surely like very much) to environments that were initially \"disconnected\".</p> <p>Beside, SQL queries in Spark Thrift Server share the same SparkContext that helps further improve performance of SQL queries using the same data sources.</p> <p>Spark Thrift Server is a Spark standalone application that you start using &lt;start-thriftserver.sh&gt;&gt; and stop using &lt;stop-thriftserver.sh&gt;&gt; shell scripts. <p>Spark Thrift Server has its own tab in web UI -- &lt;&gt; available at <code>/sqlserver</code> URL. <p>.Spark Thrift Server's web UI image::images/spark-thriftserver-webui.png[align=\"center\"]</p> <p>Spark Thrift Server can work in &lt;&gt;. <p>Use &lt;&gt; or &lt;&gt; or Spark SQL's &lt;&gt; to connect to Spark Thrift Server through the JDBC interface. <p>Spark Thrift Server extends spark-submit.md[spark-submit]'s command-line options with <code>--hiveconf [prop=value]</code>.</p>"},{"location":"thrift-server/#important","title":"[IMPORTANT]","text":"<p>You have to enable <code>hive-thriftserver</code> build profile to include Spark Thrift Server in your build.</p> <pre><code>./build/mvn -Phadoop-2.7,yarn,mesos,hive,hive-thriftserver -DskipTests clean install\n</code></pre>"},{"location":"thrift-server/#refer-to-variaspark-building-from-sourcesmdhive-thriftserverbuilding-apache-spark-from-sources","title":"Refer to varia/spark-building-from-sources.md#hive-thriftserver[Building Apache Spark from Sources].","text":""},{"location":"thrift-server/#tip","title":"[TIP]","text":"<p>Enable <code>INFO</code> or <code>DEBUG</code> logging levels for <code>org.apache.spark.sql.hive.thriftserver</code> and <code>org.apache.hive.service.server</code> loggers to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.hive.thriftserver=DEBUG\nlog4j.logger.org.apache.hive.service.server=INFO\n</code></pre>"},{"location":"thrift-server/#refer-to-spark-loggingmdlogging","title":"Refer to spark-logging.md[Logging].","text":"<p>=== [[start-thriftserver]] Starting Thrift JDBC/ODBC Server -- <code>start-thriftserver.sh</code></p> <p>You can start Thrift JDBC/ODBC Server using <code>./sbin/start-thriftserver.sh</code> shell script.</p> <p>With <code>INFO</code> logging level enabled, when you execute the script you should see the following INFO messages in the logs:</p> <pre><code>INFO HiveThriftServer2: Started daemon with process name: 16633@japila.local\nINFO HiveThriftServer2: Starting SparkContext\n...\nINFO HiveThriftServer2: HiveThriftServer2 started\n</code></pre> <p>Internally, <code>start-thriftserver.sh</code> script submits <code>org.apache.spark.sql.hive.thriftserver.HiveThriftServer2</code> standalone application for execution (using spark-submit.md[spark-submit]).</p> <pre><code>$ ./bin/spark-submit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2\n</code></pre> <p>TIP: Using the more explicit approach with <code>spark-submit</code> to start Spark Thrift Server could be easier to trace execution by seeing the logs printed out to the standard output and hence terminal directly.</p> <p>=== [[beeline]] Using Beeline JDBC Client to Connect to Spark Thrift Server</p> <p><code>beeline</code> is a command-line tool that allows you to access Spark Thrift Server using the JDBC interface on command line. It is included in the Spark distribution in <code>bin</code> directory.</p> <pre><code>$ ./bin/beeline\nBeeline version 1.2.1.spark2 by Apache Hive\nbeeline&gt;\n</code></pre> <p>You can connect to Spark Thrift Server using <code>connect</code> command as follows:</p> <pre><code>beeline&gt; !connect jdbc:hive2://localhost:10000\n</code></pre> <p>When connecting in non-secure mode, simply enter the username on your machine and a blank password.</p> <pre><code>beeline&gt; !connect jdbc:hive2://localhost:10000\nConnecting to jdbc:hive2://localhost:10000\nEnter username for jdbc:hive2://localhost:10000: jacek\nEnter password for jdbc:hive2://localhost:10000: [press ENTER]\nConnected to: Spark SQL (version 2.3.0)\nDriver: Hive JDBC (version 1.2.1.spark2)\nTransaction isolation: TRANSACTION_REPEATABLE_READ\n0: jdbc:hive2://localhost:10000&gt;\n</code></pre> <p>Once connected, you can send SQL queries (as if Spark SQL were a JDBC-compliant database).</p> <pre><code>0: jdbc:hive2://localhost:10000&gt; show databases;\n+---------------+--+\n| databaseName  |\n+---------------+--+\n| default       |\n+---------------+--+\n1 row selected (0.074 seconds)\n</code></pre> <p>=== [[SQuirreL-SQL-Client]] Connecting to Spark Thrift Server using SQuirreL SQL Client 3.7.1</p> <p>Spark Thrift Server allows for remote access to Spark SQL using JDBC protocol.</p> <p>NOTE: This section was tested with SQuirreL SQL Client 3.7.1 (<code>squirrelsql-3.7.1-standard.zip</code>) on Mac OS X.</p> <p>SQuirreL SQL Client is a Java SQL client for JDBC-compliant databases.</p> <p>Run the client using <code>java -jar squirrel-sql.jar</code>.</p> <p>.SQuirreL SQL Client image::images/spark-thriftserver-squirrel.png[align=\"center\"]</p> <p>You first have to configure a JDBC driver for Spark Thrift Server. Spark Thrift Server uses <code>org.spark-project.hive:hive-jdbc:1.2.1.spark2</code> dependency that is the JDBC driver (that also downloads transitive dependencies).</p> <p>TIP: The Hive JDBC Driver, i.e. <code>hive-jdbc-1.2.1.spark2.jar</code> and other jar files are in <code>jars</code> directory of the Apache Spark distribution (or <code>assembly/target/scala-2.11/jars</code> for local builds).</p> <p>.SQuirreL SQL Client's Connection Parameters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Parameter | Description | Name | Spark Thrift Server | Example URL | <code>jdbc:hive2://localhost:10000</code> | Extra Class Path | All the jar files of your Spark distribution | Class Name | <code>org.apache.hive.jdbc.HiveDriver</code> |===</p> <p>.Adding Hive JDBC Driver in SQuirreL SQL Client image::images/spark-thriftserver-squirrel-adddriver.png[align=\"center\"]</p> <p>With the Hive JDBC Driver defined, you can connect to Spark SQL Thrift Server.</p> <p>.Adding Hive JDBC Driver in SQuirreL SQL Client image::images/spark-thriftserver-squirrel-addalias.png[align=\"center\"]</p> <p>Since you did not specify the database to use, Spark SQL's <code>default</code> is used.</p> <p>.SQuirreL SQL Client Connected to Spark Thrift Server (Metadata Tab) image::images/spark-thriftserver-squirrel-metadata.png[align=\"center\"]</p> <p>Below is <code>show tables</code> SQL query in SQuirrel SQL Client executed in Spark SQL through Spark Thrift Server.</p> <p>.<code>show tables</code> SQL Query in SQuirrel SQL Client using Spark Thrift Server image::images/spark-thriftserver-squirrel-show-tables.png[align=\"center\"]</p> <p>=== [[dataframereader]] Using Spark SQL's DataSource API to Connect to Spark Thrift Server</p> <p>What might seem a quite artificial setup at first is accessing Spark Thrift Server using Spark SQL's spark-sql-datasource-api.md[DataSource API], i.e. DataFrameReader.jdbc.</p>"},{"location":"thrift-server/#tip_1","title":"[TIP]","text":"<p>When executed in <code>local</code> mode, Spark Thrift Server and <code>spark-shell</code> will try to access the same Hive Warehouse's directory that will inevitably lead to an error.</p> <p>Use StaticSQLConf.md#spark.sql.warehouse.dir[spark.sql.warehouse.dir] to point to another directory for <code>spark-shell</code>.</p> <pre><code>./bin/spark-shell --conf spark.sql.warehouse.dir=/tmp/spark-warehouse\n</code></pre>"},{"location":"thrift-server/#you-should-also-not-share-the-same-home-directory-between-them-since-metastore_db-becomes-an-issue","title":"You should also not share the same home directory between them since <code>metastore_db</code> becomes an issue.","text":"<p><pre><code>// Inside spark-shell\n// Paste in :paste mode\nval df = spark\n.read\n.option(\"url\", \"jdbc:hive2://localhost:10000\") // &lt;1&gt;\n.option(\"dbtable\", \"people\") // &lt;2&gt;\n.format(\"jdbc\")\n.load\n</code></pre> &lt;1&gt; Connect to Spark Thrift Server at localhost on port 10000 &lt;2&gt; Use <code>people</code> table. It assumes that <code>people</code> table is available.</p> <p>=== [[webui]][[ThriftServerTab]] <code>ThriftServerTab</code> -- web UI's Tab for Spark Thrift Server</p> <p><code>ThriftServerTab</code> is...FIXME</p> <p>CAUTION: FIXME Elaborate</p> <p>=== [[stop-thriftserver]] Stopping Thrift JDBC/ODBC Server -- <code>stop-thriftserver.sh</code></p> <p>You can stop a running instance of Thrift JDBC/ODBC Server using <code>./sbin/stop-thriftserver.sh</code> shell script.</p> <p>With <code>DEBUG</code> logging level enabled, you should see the following messages in the logs:</p> <pre><code>ERROR HiveThriftServer2: RECEIVED SIGNAL TERM\nDEBUG SparkSQLEnv: Shutting down Spark SQL Environment\nINFO HiveServer2: Shutting down HiveServer2\nINFO BlockManager: BlockManager stopped\nINFO SparkContext: Successfully stopped SparkContext\n</code></pre> <p>TIP: You can also send <code>SIGTERM</code> signal to the process of Thrift JDBC/ODBC Server, i.e. <code>kill [PID]</code> that triggers the same sequence of shutdown steps as <code>stop-thriftserver.sh</code>.</p> <p>=== [[transport-mode]] Transport Mode</p> <p>Spark Thrift Server can be configured to listen in two modes (aka transport modes):</p> <ol> <li> <p>Binary mode -- clients should send thrift requests in binary</p> </li> <li> <p>HTTP mode -- clients send thrift requests over HTTP.</p> </li> </ol> <p>You can control the transport modes using <code>HIVE_SERVER2_TRANSPORT_MODE=http</code> or <code>hive.server2.transport.mode</code> (default: <code>binary</code>). It can be <code>binary</code> (default) or <code>http</code>.</p> <p>=== [[main]] <code>main</code> method</p> <p>Thrift JDBC/ODBC Server is a Spark standalone application that you...</p> <p>CAUTION: FIXME</p> <p>=== [[HiveThriftServer2Listener]] HiveThriftServer2Listener</p> <p>CAUTION: FIXME</p>"},{"location":"thrift-server/SparkSQLEnv/","title":"SparkSQLEnv","text":"<p>CAUTION: FIXME</p>"},{"location":"transactional-writes/","title":"Transactional Writes in File-Based Connectors","text":"<p>File-based connectors use spark.sql.sources.commitProtocolClass configuration property for the class that is responsible for transactional writes.</p>"},{"location":"transactional-writes/#logging","title":"Logging","text":"<p>Enable the following loggers:</p> <ul> <li>FileFormatWriter</li> <li>ParquetUtils</li> <li>SQLHadoopMapReduceCommitProtocol</li> </ul>"},{"location":"transactional-writes/#learn-more","title":"Learn More","text":"<ol> <li>Transactional Writes to Cloud Storage on Databricks</li> </ol>"},{"location":"transactional-writes/SQLHadoopMapReduceCommitProtocol/","title":"SQLHadoopMapReduceCommitProtocol","text":"<p><code>SQLHadoopMapReduceCommitProtocol</code> is a <code>HadoopMapReduceCommitProtocol</code> (Spark Core) that allows for a custom user-defined Hadoop OutputCommitter based on spark.sql.sources.outputCommitterClass configuration property.</p> <p><code>SQLHadoopMapReduceCommitProtocol</code> is the default value of spark.sql.sources.commitProtocolClass configuration property.</p> <p><code>SQLHadoopMapReduceCommitProtocol</code> is <code>Serializable</code>.</p>"},{"location":"transactional-writes/SQLHadoopMapReduceCommitProtocol/#creating-instance","title":"Creating Instance","text":"<p><code>SQLHadoopMapReduceCommitProtocol</code> takes the following to be created:</p> <ul> <li> Job ID <li> Path <li>dynamicPartitionOverwrite</li>"},{"location":"transactional-writes/SQLHadoopMapReduceCommitProtocol/#dynamicPartitionOverwrite","title":"dynamicPartitionOverwrite","text":"<p><code>SQLHadoopMapReduceCommitProtocol</code> can be given <code>dynamicPartitionOverwrite</code> flag when created. Unless given, <code>dynamicPartitionOverwrite</code> is disabled (<code>false</code>).</p>"},{"location":"transactional-writes/SQLHadoopMapReduceCommitProtocol/#setupCommitter","title":"Setting Up OutputCommitter","text":"HadoopMapReduceCommitProtocol <pre><code>setupCommitter(\ncontext: TaskAttemptContext): OutputCommitter\n</code></pre> <p><code>setupCommitter</code> is part of the <code>HadoopMapReduceCommitProtocol</code> (Spark Core) abstraction.</p> <p><code>setupCommitter</code> allows specifying a custom user-defined Hadoop OutputCommitter based on spark.sql.sources.outputCommitterClass configuration property (in the Hadoop Configuration of the given Hadoop TaskAttemptContext).</p> <p><code>setupCommitter</code> takes the default parent <code>OutputCommitter</code> (for the given Hadoop TaskAttemptContext) unless spark.sql.sources.outputCommitterClass configuration property is defined (that overrides the parent's <code>OutputCommitter</code>).</p> <p>If spark.sql.sources.outputCommitterClass is defined, <code>setupCommitter</code> prints out the following INFO message to the logs:</p> <pre><code>Using user defined output committer class [className]\n</code></pre> <p>In the end, <code>setupCommitter</code> prints out the following INFO message to the logs (and returns the <code>OutputCommitter</code>):</p> <pre><code>Using output committer class [className]\n</code></pre>"},{"location":"transactional-writes/SQLHadoopMapReduceCommitProtocol/#logging","title":"Logging","text":"<p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>logger.SQLHadoopMapReduceCommitProtocol.name = org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\nlogger.SQLHadoopMapReduceCommitProtocol.level = all\n</code></pre> <p>Refer to Logging.</p>"},{"location":"tungsten/","title":"Tungsten Execution Backend (Project Tungsten)","text":"<p>The goal of Project Tungsten is to improve Spark execution by optimizing Spark jobs for CPU and memory efficiency (as opposed to network and disk I/O which are considered fast enough).</p> <p>Tungsten focuses on the hardware architecture of the platform Spark runs on (including but not limited to JVM, LLVM, GPU, NVRAM) by offering the following optimization features:</p> <ol> <li> <p>Off-Heap Memory Management using binary in-memory data representation aka Tungsten row format and managing memory explicitly</p> </li> <li> <p>Cache Locality which is about cache-aware computations with cache-aware layout for high cache hit rates</p> </li> <li> <p>Whole-Stage Code Generation (aka CodeGen)</p> </li> </ol> <p>Project Tungsten uses <code>sun.misc.unsafe</code> API for direct memory access to bypass the JVM in order to avoid garbage collection.</p> <p></p>"},{"location":"tungsten/#off-heap-memory-management","title":"Off-Heap Memory Management","text":"<p>Project Tungsten aims at substantially reducing the usage of JVM objects (and therefore JVM garbage collection) by introducing its own off-heap binary memory management. Instead of working with Java objects, Tungsten uses <code>sun.misc.Unsafe</code> to manipulate raw memory.</p> <p>Tungsten uses the compact storage format called UnsafeRow for data representation that further reduces memory footprint.</p> <p>Since Datasets have known schema, Tungsten properly and in a more compact and efficient way lays out the objects on its own. That brings benefits similar to using extensions written in low-level and hardware-aware languages like C or assembler.</p> <p>It is possible immediately with the data being already serialized (that further reduces or completely avoids serialization between JVM object representation and Spark's internal one).</p>"},{"location":"tungsten/#cache-locality","title":"Cache Locality","text":"<p>Tungsten uses algorithms and cache-aware data structures that exploit the physical machine caches at different levels - L1, L2, L3.</p>"},{"location":"tungsten/#whole-stage-java-code-generation","title":"Whole-Stage Java Code Generation","text":"<p>Tungsten does code generation at compile time and generates JVM bytecode to access Tungsten-managed memory structures that gives a very fast access. It uses the Janino compiler that is a super-small, super-fast Java compiler.</p> <p>Note</p> <p>The code generation was tracked under SPARK-8159 Improve expression function coverage (Spark 1.5).</p>"},{"location":"tungsten/#further-reading-and-watching","title":"Further Reading and Watching","text":"<ul> <li> <p>Project Tungsten: Bringing Spark Closer to Bare Metal</p> </li> <li> <p>(video) From DataFrames to Tungsten: A Peek into Spark's Future by Reynold Xin (Databricks)</p> </li> <li> <p>(video) Deep Dive into Project Tungsten: Bringing Spark Closer to Bare Metal by Josh Rosen (Databricks)</p> </li> </ul>"},{"location":"tungsten/UnsafeRowSerializerInstance/","title":"UnsafeRowSerializerInstance","text":"<p><code>UnsafeRowSerializerInstance</code> is...FIXME</p>"},{"location":"types/","title":"Data Types","text":"<p>DataType is the base type of all the data types in Spark SQL.</p> <p>Built-in data types belong to the <code>org.apache.spark.sql.types</code> package.</p> <p>A schema is the description of the structure of your data (which together create a Dataset in Spark SQL). It can be implicit (and inferred at runtime) or explicit (and known at compile time).</p> <pre><code>val df = Seq((0, s\"\"\"hello\\tworld\"\"\"), (1, \"two  spaces inside\")).toDF(\"label\", \"sentence\")\n\nscala&gt; df.printSchema\nroot\n |-- label: integer (nullable = false)\n |-- sentence: string (nullable = true)\n\nscala&gt; df.schema\nres0: org.apache.spark.sql.types.StructType = StructType(StructField(label,IntegerType,false), StructField(sentence,StringType,true))\n\nscala&gt; df.schema(\"label\").dataType\nres1: org.apache.spark.sql.types.DataType = IntegerType\n</code></pre> <p>A schema is described using StructType which is a collection of StructFields (that in turn are tuples of names, types, and <code>nullability</code> classifier).</p> <pre><code>import org.apache.spark.sql.types.{IntegerType, StringType}\nval schemaTyped = new StructType()\n.add(\"a\", IntegerType)\n.add(\"b\", StringType)\n</code></pre> <p>DataTypes utility can be used to create data types.</p> <pre><code>import org.apache.spark.sql.types.DataTypes._\nval schemaWithMap = StructType(\n  StructField(\"map\", createMapType(LongType, StringType), false) :: Nil)\n</code></pre> <p>CatalystSqlParser is responsible for parsing data types.</p> <p>Encoder allows describing the schema in a type-safe manner.</p> <pre><code>import org.apache.spark.sql.Encoders\n\nscala&gt; Encoders.INT.schema.printTreeString\nroot\n |-- value: integer (nullable = true)\n\nscala&gt; Encoders.product[(String, java.sql.Timestamp)].schema.printTreeString\nroot\n|-- _1: string (nullable = true)\n|-- _2: timestamp (nullable = true)\n\ncase class Person(id: Long, name: String)\nscala&gt; Encoders.product[Person].schema.printTreeString\nroot\n |-- id: long (nullable = false)\n |-- name: string (nullable = true)\n</code></pre> <p><code>StructType</code> gives printTreeString that makes presenting the schema more user-friendly.</p> <pre><code>scala&gt; schemaTyped.printTreeString\nroot\n |-- a: integer (nullable = true)\n |-- b: string (nullable = true)\n\nscala&gt; schemaWithMap.printTreeString\nroot\n|-- map: map (nullable = false)\n|    |-- key: long\n|    |-- value: string (valueContainsNull = true)\n\n// You can use prettyJson method on any DataType\nscala&gt; println(schema1.prettyJson)\n{\n \"type\" : \"struct\",\n \"fields\" : [ {\n   \"name\" : \"a\",\n   \"type\" : \"integer\",\n   \"nullable\" : true,\n   \"metadata\" : { }\n }, {\n   \"name\" : \"b\",\n   \"type\" : \"string\",\n   \"nullable\" : true,\n   \"metadata\" : { }\n } ]\n}\n</code></pre> <pre><code>import org.apache.spark.sql.types.StructType\nval schemaUntyped = new StructType()\n.add(\"a\", \"int\")\n.add(\"b\", \"string\")\n</code></pre> <pre><code>// alternatively using Schema DSL\nval schemaUntyped_2 = new StructType()\n.add($\"a\".int)\n.add($\"b\".string)\n</code></pre>"},{"location":"types/AbstractDataType/","title":"AbstractDataType","text":"<p><code>AbstractDataType</code> is...FIXME</p>"},{"location":"types/ArrayType/","title":"ArrayType","text":"<p><code>ArrayType</code> is a DataType.</p>"},{"location":"types/ArrayType/#creating-instance","title":"Creating Instance","text":"<p><code>ArrayType</code> takes the following to be created:</p> <ul> <li> DataType of the elements <li> <code>containsNull</code> flag"},{"location":"types/ArrayType/#defaultsize","title":"defaultSize <pre><code>defaultSize: Int\n</code></pre> <p><code>defaultSize</code>\u00a0is part of the DataType abstraction.</p> <p><code>defaultSize</code> is the defaultSize of the elementType.</p>","text":""},{"location":"types/ArrayType/#jsonvalue","title":"jsonValue <pre><code>jsonValue: JValue\n</code></pre> <p><code>jsonValue</code>\u00a0is part of the DataType abstraction.</p> <p><code>jsonValue</code>...FIXME</p>","text":""},{"location":"types/ArrayType/#simple-representation","title":"Simple Representation <pre><code>simpleString: String\n</code></pre> <p><code>simpleString</code>\u00a0is part of the DataType abstraction.</p> <p><code>simpleString</code> is the following:</p> <pre><code>array&lt;[elementType]&gt;\n</code></pre>","text":""},{"location":"types/ArrayType/#catalog-representation","title":"Catalog Representation <pre><code>catalogString: String\n</code></pre> <p><code>catalogString</code>\u00a0is part of the DataType abstraction.</p> <p><code>catalogString</code> is the following:</p> <pre><code>array&lt;[elementType]&gt;\n</code></pre>","text":""},{"location":"types/ArrayType/#sql-representation","title":"SQL Representation <pre><code>sql: String\n</code></pre> <p><code>sql</code>\u00a0is part of the DataType abstraction.</p> <p><code>sql</code> is the following:</p> <pre><code>ARRAY&lt;[elementType]&gt;\n</code></pre>","text":""},{"location":"types/AtomicType/","title":"AtomicType","text":"<p><code>AtomicType</code>\u00a0is an extension of the DataType abstraction for atomic types.</p>"},{"location":"types/AtomicType/#internaltype","title":"InternalType <pre><code>abstract class AtomicType extends DataType {\n  type InternalType\n}\n</code></pre>","text":""},{"location":"types/AtomicType/#contract","title":"Contract","text":""},{"location":"types/AtomicType/#typetag","title":"TypeTag <pre><code>tag: TypeTag[InternalType]\n</code></pre> <p><code>TypeTag</code> (Scala)</p>","text":""},{"location":"types/AtomicType/#ordering","title":"Ordering <pre><code>ordering: Ordering[InternalType]\n</code></pre>","text":""},{"location":"types/AtomicType/#implementations","title":"Implementations <ul> <li><code>BinaryType</code></li> <li><code>BooleanType</code></li> <li><code>CharType</code></li> <li> <code>DateType</code> <li> <code>NumericType</code> <li><code>StringType</code></li> <li> <code>TimestampType</code> <li><code>VarcharType</code></li>","text":""},{"location":"types/AtomicType/#extractor","title":"Extractor <pre><code>unapply(\n  e: Expression): Boolean\n</code></pre> <p><code>unapply</code> is <code>true</code> when the data type of the input ../expressions/Expression is an <code>AtomicType</code>.</p> <p><code>unapply</code> allows pattern matching in Scala against <code>AtomicType</code> for expressions.</p>","text":""},{"location":"types/CalendarInterval/","title":"CalendarInterval","text":"<p><code>CalendarInterval</code> represents a calendar interval.</p>"},{"location":"types/CalendarInterval/#creating-instance","title":"Creating Instance","text":"<p><code>CalendarInterval</code> takes the following to be created:</p> <ul> <li> Months <li> Days <li> Microseconds <p><code>CalendarInterval</code> is created when:</p> <ul> <li><code>CALENDAR_INTERVAL</code> utility is used to <code>extract</code> a <code>CalendarInterval</code> from a <code>ByteBuffer</code></li> <li><code>ColumnVector</code> is requested to getInterval</li> <li><code>IntervalUtils</code> utilities are used</li> <li><code>DateTimeUtils</code> utility is used to <code>subtractDates</code></li> <li><code>UnsafeRow</code> is requested to getInterval</li> <li><code>UnsafeArrayData</code> is requested to <code>getInterval</code></li> <li><code>Literal</code> utility is used to create the default value for CalendarIntervalType</li> <li><code>TemporalSequenceImpl</code> is requested for the <code>defaultStep</code></li> </ul>"},{"location":"types/CalendarInterval/#examples","title":"Examples","text":"<pre><code>0 seconds\n5 years\n2 months\n10 days\n2 hours\n1 minute\n</code></pre>"},{"location":"types/DataType/","title":"DataType","text":"<p><code>DataType</code> is an extension of the AbstractDataType abstraction for data types in Spark SQL.</p>"},{"location":"types/DataType/#contract","title":"Contract","text":""},{"location":"types/DataType/#asnullable","title":"asNullable <pre><code>asNullable: DataType\n</code></pre>","text":""},{"location":"types/DataType/#default-size","title":"Default Size <pre><code>defaultSize: Int\n</code></pre> <p>Default size of a value of this data type</p> <p>Used when:</p> <ul> <li>ResolveGroupingAnalytics logical resolution is executed</li> <li><code>CommandUtils</code> is used to statExprs</li> <li><code>JoinEstimation</code> is used to estimateInnerOuterJoin</li> <li>others</li> </ul>","text":""},{"location":"types/DataType/#implementations","title":"Implementations","text":"<ul> <li>ArrayType</li> <li>AtomicType</li> <li>StructType</li> <li>others</li> </ul>"},{"location":"types/StructField/","title":"StructField","text":"<p><code>StructField</code> is a named field of a StructType.</p>"},{"location":"types/StructField/#creating-instance","title":"Creating Instance","text":"<p><code>StructField</code> takes the following to be created:</p> <ul> <li> Field Name <li> DataType <li> <code>nullable</code> flag (default: <code>true</code>) <li> <code>Metadata</code> (default: <code>Metadata.empty</code>)"},{"location":"types/StructField/#converting-to-ddl-format","title":"Converting to DDL Format <pre><code>toDDL: String\n</code></pre> <p><code>toDDL</code> uses the sql format of the DataType and the comment for conversion:</p> <pre><code>[name] [sql][comment]\n</code></pre>  <p><code>toDDL</code>\u00a0is used when:</p> <ul> <li><code>StructType</code> is requested to toDDL</li> <li>ShowCreateTableCommand, <code>ShowCreateTableAsSerdeCommand</code> logical commands are executed</li> </ul> <p><code>toDDL</code> gives a text in the format:</p> <pre><code>[quoted name] [dataType][optional comment]\n</code></pre> <p><code>toDDL</code> is used when:</p> <ul> <li><code>StructType</code> is requested to convert itself to DDL format</li> <li>ShowCreateTableCommand logical command is executed</li> </ul>","text":""},{"location":"types/StructField/#comment","title":"Comment <pre><code>getComment(): Option[String]\n</code></pre> <p><code>getComment</code> is the value of the <code>comment</code> key in the Metadata (if defined).</p>","text":""},{"location":"types/StructField/#ddl-comment","title":"DDL Comment <pre><code>getDDLComment: String\n</code></pre> <p><code>getDDLComment</code>...FIXME</p>","text":""},{"location":"types/StructField/#demo","title":"Demo <pre><code>import org.apache.spark.sql.types.{LongType, StructField}\nval f = StructField(\n    name = \"id\",\n    dataType = LongType,\n    nullable = false)\n  .withComment(\"this is a comment\")\n</code></pre> <pre><code>scala&gt; println(f)\nStructField(id,LongType,false)\n</code></pre> <pre><code>scala&gt; println(f.toDDL)\n`id` BIGINT COMMENT 'this is a comment'\n</code></pre>","text":""},{"location":"types/StructType/","title":"StructType","text":"<p><code>StructType</code> is a recursive DataType with fields and being a collection of fields itself.</p> <p><code>StructType</code> is used to define a schema.</p>"},{"location":"types/StructType/#creating-instance","title":"Creating Instance","text":"<p><code>StructType</code> takes the following to be created:</p> <ul> <li> StructFields"},{"location":"types/StructType/#seqstructfield","title":"Seq[StructField] <p>Not only does <code>StructType</code> has fields but is also a collection of StructFields (<code>Seq[StructField]</code>).</p> <p>All things <code>Seq</code> (Scala) apply equally here.</p> <pre><code>scala&gt; schemaTyped.foreach(println)\nStructField(a,IntegerType,true)\nStructField(b,StringType,true)\n</code></pre>","text":""},{"location":"types/StructType/#sql-representation","title":"SQL Representation <p><code>StructType</code> uses <code>STRUCT&lt;...&gt;</code> for SQL representation (in query plans or SQL statements).</p>","text":""},{"location":"types/StructType/#catalog-representation","title":"Catalog Representation <p><code>StructType</code> uses <code>struct&lt;...&gt;</code> for catalog representation.</p>","text":""},{"location":"types/StructType/#demo","title":"Demo <pre><code>// Generating a schema from a case class\n// Because we're all properly lazy\ncase class Person(id: Long, name: String)\nimport org.apache.spark.sql.Encoders\nval schema = Encoders.product[Person].schema\nscala&gt; println(schema.toDDL)\n`id` BIGINT,`name` STRING\n</code></pre> <pre><code>scala&gt; schemaTyped.simpleString\nres0: String = struct&lt;a:int,b:string&gt;\n\nscala&gt; schemaTyped.catalogString\nres1: String = struct&lt;a:int,b:string&gt;\n\nscala&gt; schemaTyped.sql\nres2: String = STRUCT&lt;`a`: INT, `b`: STRING&gt;\n</code></pre>","text":""},{"location":"types/UserDefinedType/","title":"UserDefinedType","text":"<p><code>UserDefinedType[UserType]</code> is an extension of the DataType abstraction for user-defined data types.</p>","tags":["DeveloperApi"]},{"location":"types/UserDefinedType/#contract","title":"Contract","text":"","tags":["DeveloperApi"]},{"location":"types/UserDefinedType/#deserialize","title":"deserialize <pre><code>deserialize(\n  datum: Any): UserType\n</code></pre> <p>Used when:</p> <ul> <li><code>UDTConverter</code> is requested to <code>toScala</code></li> <li><code>Cast</code> expression is requested to <code>castToString</code></li> <li><code>BaseScriptTransformationExec</code> is requested to <code>outputFieldWriters</code></li> </ul>","text":"","tags":["DeveloperApi"]},{"location":"types/UserDefinedType/#serialize","title":"serialize <pre><code>serialize(\n  obj: UserType): Any\n</code></pre> <p>Used when:</p> <ul> <li><code>Row</code> is requested to toJson</li> <li><code>UDTConverter</code> is requested to <code>toCatalystImpl</code></li> </ul>","text":"","tags":["DeveloperApi"]},{"location":"types/UserDefinedType/#sqltype","title":"sqlType <pre><code>sqlType: DataType\n</code></pre> <p>The underlying storage DataType</p>","text":"","tags":["DeveloperApi"]},{"location":"types/UserDefinedType/#userclass","title":"userClass <pre><code>userClass: Class[UserType]\n</code></pre>","text":"","tags":["DeveloperApi"]},{"location":"types/UserDefinedType/#implementations","title":"Implementations","text":"<ul> <li><code>PythonUserDefinedType</code></li> <li><code>MatrixUDT</code> (Spark MLlib)</li> <li><code>VectorUDT</code> (Spark MLlib)</li> </ul>","tags":["DeveloperApi"]},{"location":"ui/","title":"SQL / DataFrame UI","text":"<p>Structured queries can be monitored using web UI that attaches the following two pages:</p> <ul> <li>AllExecutionsPage</li> <li>ExecutionPage</li> </ul>"},{"location":"ui/AllExecutionsPage/","title":"AllExecutionsPage","text":"<p><code>AllExecutionsPage</code> is...FIXME</p>"},{"location":"ui/ExecutionPage/","title":"ExecutionPage","text":"<p><code>ExecutionPage</code> is...FIXME</p>"},{"location":"ui/SQLAppStatusListener/","title":"SQLAppStatusListener","text":"<p><code>SQLAppStatusListener</code> is a <code>SparkListener</code> (Spark Core).</p>"},{"location":"ui/SQLAppStatusListener/#creating-instance","title":"Creating Instance","text":"<p><code>SQLAppStatusListener</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> (Spark Core) <li> <code>ElementTrackingStore</code> (Spark Core) <li> <code>live</code> flag <p><code>SQLAppStatusListener</code> is created\u00a0when:</p> <ul> <li><code>SharedState</code> is created (and initializes a SQLAppStatusStore)</li> <li><code>SQLHistoryServerPlugin</code> is requested to create <code>SparkListener</code>s</li> </ul>"},{"location":"ui/SQLAppStatusStore/","title":"SQLAppStatusStore","text":"<p><code>SQLAppStatusStore</code> is...FIXME</p>"},{"location":"ui/SQLTab/","title":"SQLTab","text":""},{"location":"ui/SparkListenerSQLExecutionEnd/","title":"SparkListenerSQLExecutionEnd","text":"<p><code>SparkListenerSQLExecutionEnd</code> is a <code>SparkListenerEvent</code> (Spark Core).</p> <p><code>SparkListenerSQLExecutionEnd</code> is posted (to an event bus) to announce that <code>SQLExecution</code> has completed executing a structured query.</p>"},{"location":"ui/SparkListenerSQLExecutionEnd/#creating-instance","title":"Creating Instance","text":"<p><code>SparkListenerSQLExecutionEnd</code> takes the following to be created:</p> <ul> <li>Execution ID</li> <li>Timestamp</li> </ul> <p><code>SparkListenerSQLExecutionEnd</code> is created when:</p> <ul> <li><code>SQLExecution</code> is requested to withNewExecutionId</li> </ul>"},{"location":"ui/SparkListenerSQLExecutionEnd/#execution-id","title":"Execution ID <p><code>SparkListenerSQLExecutionEnd</code> is given <code>executionId</code> when created.</p> <p>The execution ID is the next available execution ID when <code>SQLExecution</code> is requested to withNewExecutionId.</p>","text":""},{"location":"ui/SparkListenerSQLExecutionEnd/#timestamp","title":"Timestamp <p><code>SparkListenerSQLExecutionEnd</code> is given a timestamp when created.</p> <p>The timestamp is the time when <code>SQLExecution</code> has finished withNewExecutionId.</p>","text":""},{"location":"ui/SparkListenerSQLExecutionEnd/#sparklisteneronotherevent","title":"SparkListener.onOtherEvent","text":"<p><code>SparkListenerSQLExecutionEnd</code> can be intercepted using <code>SparkListener.onOtherEvent</code> (Spark Core).</p>"},{"location":"ui/SparkListenerSQLExecutionEnd/#sparklisteners","title":"SparkListeners","text":"<p>The following <code>SparkListener</code>s intercepts <code>SparkListenerSQLExecutionEnd</code>s:</p> <ul> <li><code>SQLEventFilterBuilder</code></li> <li>SQLAppStatusListener</li> </ul>"},{"location":"ui/SparkListenerSQLExecutionEnd/#queryexecutionlisteners","title":"QueryExecutionListeners","text":"<p><code>SparkListenerSQLExecutionEnd</code> is posted to QueryExecutionListeners using ExecutionListenerBus.</p>"},{"location":"vectorized-decoding/","title":"Vectorized Parquet Decoding (Reader)","text":"<p>Vectorized Parquet Decoding (Vectorized Parquet Reader) allows for reading datasets in parquet format in batches, i.e. rows are decoded in batches. That aims at improving memory locality and cache utilization.</p> <p>Quoting SPARK-12854 Vectorize Parquet reader:</p> <p>The parquet encodings are largely designed to decode faster in batches, column by column. This can speed up the decoding considerably.</p> <p>Vectorized Parquet Decoding is used exclusively when <code>ParquetFileFormat</code> is requested for a data reader when spark.sql.parquet.enableVectorizedReader property is enabled (<code>true</code>) and the read schema uses AtomicTypes data types only.</p> <p>Vectorized Parquet Decoding uses VectorizedParquetRecordReader for vectorized decoding (and ParquetReadSupport otherwise).</p>"},{"location":"vectorized-decoding/ColumnVector/","title":"ColumnVector","text":"<p><code>ColumnVector</code> is an abstraction of in-memory columnar data (vectors) with elements of a given DataType.</p> <p><code>ColumnVector</code> is expected to be reused during the entire data loading process, to avoid allocating memory again and again.</p> <p><code>ColumnVector</code> is meant to maximize CPU efficiency but not to minimize storage footprint. Implementations should prefer computing efficiency over storage efficiency when design the format. Since it is expected to reuse the ColumnVector instance while loading data, the storage footprint is negligible.</p>"},{"location":"vectorized-decoding/ColumnVector/#implementations","title":"Implementations","text":"<ul> <li><code>ArrowColumnVector</code></li> <li><code>ConstantColumnVector</code></li> <li><code>OrcColumnVector</code></li> <li>WritableColumnVector</li> </ul>"},{"location":"vectorized-decoding/ColumnVector/#creating-instance","title":"Creating Instance","text":"<p><code>ColumnVector</code> takes the following to be created:</p> <ul> <li> DataType <p>Abstract Class</p> <p><code>ColumnVector</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete ColumnVectors.</p>"},{"location":"vectorized-decoding/OffHeapColumnVector/","title":"OffHeapColumnVector","text":"<p><code>OffHeapColumnVector</code> is a concrete WritableColumnVector.</p>"},{"location":"vectorized-decoding/OnHeapColumnVector/","title":"OnHeapColumnVector","text":"<p><code>OnHeapColumnVector</code> is a concrete WritableColumnVector.</p>"},{"location":"vectorized-decoding/WritableColumnVector/","title":"WritableColumnVector","text":"<p><code>WritableColumnVector</code> is an extension of the ColumnVector abstraction for writable in-memory columnar vectors.</p> <p><code>WritableColumnVector</code> is used to allocate ColumnVectors for VectorizedParquetRecordReader.</p>"},{"location":"vectorized-decoding/WritableColumnVector/#contract-subset","title":"Contract (Subset)","text":""},{"location":"vectorized-decoding/WritableColumnVector/#reserveinternal","title":"reserveInternal <pre><code>void reserveInternal(\n  int capacity)\n</code></pre> <p>Used when:</p> <ul> <li>OffHeapColumnVector and OnHeapColumnVector are created</li> <li><code>WritableColumnVector</code> is requested to reserve</li> </ul>","text":""},{"location":"vectorized-decoding/WritableColumnVector/#reservenewcolumn","title":"reserveNewColumn <pre><code>WritableColumnVector reserveNewColumn(\n  int capacity,\n  DataType type)\n</code></pre> <p>Used when:</p> <ul> <li><code>WritableColumnVector</code> is created or requested to reserveDictionaryIds</li> </ul>","text":""},{"location":"vectorized-decoding/WritableColumnVector/#implementations","title":"Implementations","text":"<ul> <li>OffHeapColumnVector</li> <li>OnHeapColumnVector</li> </ul>"},{"location":"vectorized-decoding/WritableColumnVector/#creating-instance","title":"Creating Instance","text":"<p><code>WritableColumnVector</code> takes the following to be created:</p> <ul> <li> Capacity (number of rows to hold in a vector) <li> Data type of the rows stored <p>Abstract Class</p> <p><code>WritableColumnVector</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete WritableColumnVectors.</p>"},{"location":"vectorized-decoding/WritableColumnVector/#reserveadditional","title":"reserveAdditional <pre><code>void reserveAdditional(\n  int additionalCapacity)\n</code></pre> <p><code>reserveAdditional</code>...FIXME</p>  <p><code>reserveAdditional</code> is used when:</p> <ul> <li><code>VectorizedRleValuesReader</code> is requested to <code>readValues</code></li> </ul>","text":""},{"location":"vectorized-decoding/WritableColumnVector/#reservedictionaryids","title":"reserveDictionaryIds <pre><code>WritableColumnVector reserveDictionaryIds(\n  int capacity)\n</code></pre> <p><code>reserveDictionaryIds</code>...FIXME</p>  <p><code>reserveDictionaryIds</code> is used when:</p> <ul> <li><code>VectorizedColumnReader</code> is requested to readBatch</li> <li><code>DictionaryEncoding.Decoder</code> is requested to <code>decompress</code></li> </ul>","text":""},{"location":"vectorized-decoding/WritableColumnVector/#reserve","title":"reserve <pre><code>void reserve(\n  int requiredCapacity)\n</code></pre> <p><code>reserve</code>...FIXME</p>  <p><code>reserve</code> is used when:</p> <ul> <li><code>ParquetColumnVector</code> is requested to <code>assembleCollection</code> and <code>assembleStruct</code></li> <li><code>WritableColumnVector</code> is requested to reserveAdditional, reserveDictionaryIds and all the <code>append</code>s</li> </ul>","text":""},{"location":"vectorized-decoding/WritableColumnVector/#reset","title":"reset <pre><code>void reset()\n</code></pre> <p><code>reset</code> does nothing (noop) when either isConstant or isAllNull is enabled.</p> <p><code>reset</code>...FIXME</p>  <p><code>reset</code> is used when:</p> <ul> <li><code>ParquetColumnVector</code> is requested to <code>reset</code></li> <li><code>VectorizedDeltaByteArrayReader</code> is requested to <code>skipBinary</code></li> <li>OffHeapColumnVector and OnHeapColumnVector are created</li> <li><code>WritableColumnVector</code> is requested to reserveDictionaryIds</li> <li><code>RowToColumnarExec</code> physical operator is requested to doExecuteColumnar</li> </ul>","text":""},{"location":"vectorized-query-execution/","title":"Vectorized Query Execution","text":"<p>Vectorized Query Execution is...FIXME</p> <p>Vectorized Query Execution starts with Columnar Scan.</p>"},{"location":"vectorized-query-execution/ColumnarBatch/","title":"ColumnarBatch","text":"<p><code>ColumnarBatch</code> allows to work with multiple ColumnVectors as a row-wise table for Columnar Scan and Vectorized Query Execution.</p>","tags":["DeveloperApi"]},{"location":"vectorized-query-execution/ColumnarBatch/#creating-instance","title":"Creating Instance","text":"<p><code>ColumnarBatch</code> takes the following to be created:</p> <ul> <li> ColumnVectors <li>Number of Rows</li> <p><code>ColumnarBatch</code> immediately creates an internal ColumnarBatchRow.</p> <p><code>ColumnarBatch</code> is created when:</p> <ul> <li><code>RowToColumnarExec</code> physical operator is requested to doExecuteColumnar</li> <li>InMemoryTableScanExec leaf physical operator is requested for a RDD[ColumnarBatch]</li> <li><code>OrcColumnarBatchReader</code> is requested to <code>initBatch</code></li> <li><code>VectorizedParquetRecordReader</code> is requested to init a batch</li> <li>others (PySpark and SparkR)</li> </ul>","tags":["DeveloperApi"]},{"location":"vectorized-query-execution/ColumnarBatch/#columnarbatchrow","title":"ColumnarBatchRow <p><code>ColumnarBatch</code> creates a <code>ColumnarBatchRow</code> when created.</p>","text":"","tags":["DeveloperApi"]},{"location":"vectorized-query-execution/ColumnarBatch/#number-of-rows","title":"Number of Rows <pre><code>int numRows\n</code></pre> <p><code>ColumnarBatch</code> is given the number of rows (<code>numRows</code>) when created.</p> <p><code>numRows</code> can also be (re)set using setNumRows (and is often used to reset a <code>ColumnarBatch</code> to <code>0</code> before the end value is set).</p> <p><code>numRows</code> is available using numRows accessor.</p> <p>Used when:</p> <ul> <li>rowIterator</li> <li>getRow</li> </ul>","text":"","tags":["DeveloperApi"]},{"location":"vectorized-query-execution/ColumnarBatch/#numrows","title":"numRows <pre><code>int numRows()\n</code></pre> <p><code>numRows</code> returns the number of rows.</p>  <p><code>numRows</code> is used when:</p> <ul> <li><code>FileScanRDD</code> is requested to compute a partition</li> <li><code>ColumnarToRowExec</code> physical operator is requested to execute</li> <li><code>InMemoryTableScanExec</code> physical operator is requested for the columnarInputRDD</li> <li><code>MetricsBatchIterator</code> is requested for <code>next</code> (ColumnarBatch)</li> <li>DataSourceV2ScanExecBase and FileSourceScanExec physical operators are requested to <code>doExecuteColumnar</code></li> <li>others (PySpark)</li> </ul>","text":"","tags":["DeveloperApi"]},{"location":"vectorized-query-execution/ColumnarBatch/#setnumrows","title":"setNumRows <pre><code>void setNumRows(\n  int numRows)\n</code></pre> <p><code>setNumRows</code> sets the setNumRows registry to the given <code>numRows</code>.</p>  <p><code>setNumRows</code> is used when:</p> <ul> <li><code>OrcColumnarBatchReader</code> is requested to <code>nextBatch</code></li> <li><code>VectorizedParquetRecordReader</code> is requested to nextBatch</li> <li><code>RowToColumnarExec</code> physical operator is requested to doExecuteColumnar</li> <li><code>InMemoryTableScanExec</code> physical operator is requested for the columnarInputRDD (and uses <code>DefaultCachedBatchSerializer</code> to <code>convertCachedBatchToColumnarBatch</code>)</li> <li>others (PySpark and SparkR)</li> </ul>","text":"","tags":["DeveloperApi"]},{"location":"vectorized-query-execution/ColumnarBatch/#rowiterator","title":"rowIterator <pre><code>Iterator&lt;InternalRow&gt; rowIterator()\n</code></pre> <p><code>rowIterator</code>...FIXME</p>  <p><code>rowIterator</code> is used when:</p> <ul> <li><code>SparkResult</code> is requested for <code>iterator</code></li> <li>ColumnarToRowExec physical operator is executed</li> <li>others (SparkR and PySpark)</li> </ul>","text":"","tags":["DeveloperApi"]},{"location":"vectorized-query-execution/ColumnarBatch/#demo","title":"Demo <pre><code>import org.apache.spark.sql.types._\nval schema = new StructType()\n  .add(\"intCol\", IntegerType)\n  .add(\"doubleCol\", DoubleType)\n  .add(\"intCol2\", IntegerType)\n  .add(\"string\", BinaryType)\n\nval capacity = 4 * 1024 // 4k\nimport org.apache.spark.memory.MemoryMode\nimport org.apache.spark.sql.execution.vectorized.OnHeapColumnVector\nval columns = schema.fields.map { field =&gt;\n  new OnHeapColumnVector(capacity, field.dataType)\n}\n\nimport org.apache.spark.sql.vectorized.ColumnarBatch\nval batch = new ColumnarBatch(columns.toArray)\n\n// Add a row [1, 1.1, NULL]\ncolumns(0).putInt(0, 1)\ncolumns(1).putDouble(0, 1.1)\ncolumns(2).putNull(0)\ncolumns(3).putByteArray(0, \"Hello\".getBytes(java.nio.charset.StandardCharsets.UTF_8))\nbatch.setNumRows(1)\n\nassert(batch.getRow(0).numFields == 4)\n</code></pre>","text":"","tags":["DeveloperApi"]},{"location":"whole-stage-code-generation/","title":"Whole-Stage Java Code Generation","text":"<p>Whole-Stage Java Code Generation (Whole-Stage CodeGen) is a physical query optimization in Spark SQL that fuses multiple physical operators (as a subtree of plans that support code generation) together into a single Java function.</p> <p>Whole-Stage Java Code Generation improves the execution performance of a query by collapsing a query tree into a single optimized function that eliminates virtual function calls and leverages CPU registers for intermediate data.</p> <p>Note</p> <p>Whole-Stage Code Generation is used by some modern massively parallel processing (MPP) databases to achieve a better query execution performance.</p> <p>See Efficiently Compiling Efficient Query Plans for Modern Hardware (PDF).</p>"},{"location":"whole-stage-code-generation/#columnar-execution","title":"Columnar Execution <p>The Whole-Stage Code Generation framework is row-based.</p> <p>The input RDDs of the physical operators of a whole-stage pipeline are all <code>RDD[InternalRow]</code>s. The output RDD of a whole-stage pipeline is also an <code>RDD[InternalRow]</code>. However, the input to a whole-stage code gen stage can be columnar (<code>RDD[ColumnarBatch]</code>).</p> <p>If a physical operator supports columnar execution, it can't at the same time support whole-stage-codegen.</p>","text":""},{"location":"whole-stage-code-generation/#codegensupport-physical-operators","title":"CodegenSupport Physical Operators <p>Physical operators that support code generation extend CodegenSupport (and keep supportCodegen flag enabled).</p>","text":""},{"location":"whole-stage-code-generation/#objecttype","title":"ObjectType <p>Whole-Stage Java Code Generation does not support (skips) physical operators that produce a domain object (the DataType of the output expression is ObjectType) as domain objects cannot be written into an UnsafeRow.</p>","text":""},{"location":"whole-stage-code-generation/#aggregatecodegensupport","title":"AggregateCodegenSupport <p>For aggregation, Whole-Stage Code Generation is supported by AggregateCodegenSupport physical operators (HashAggregateExec and SortAggregateExec) and only when there are no ImperativeAggregates (supportCodegen).</p> <p>In other words, Whole-Stage Code Generation will only be used for aggreation for DeclarativeAggregate and <code>TypedAggregateExpression</code> expressions.</p>","text":""},{"location":"whole-stage-code-generation/#fast-driver-local-collecttake-paths","title":"Fast Driver-Local Collect/Take Paths <p>The following physical operators cannot be a root of WholeStageCodegen to support the fast driver-local collect/take paths:</p> <ul> <li>LocalTableScanExec</li> <li><code>CommandResultExec</code></li> </ul>","text":""},{"location":"whole-stage-code-generation/#wholestagecodegenexec-physical-operator","title":"WholeStageCodegenExec Physical Operator <p>WholeStageCodegenExec physical operator</p>","text":""},{"location":"whole-stage-code-generation/#janino","title":"Janino <p>Janino is used to compile a Java source code into a Java class at runtime.</p>","text":""},{"location":"whole-stage-code-generation/#debugging-query-execution","title":"Debugging Query Execution <p>Debugging Query Execution facility allows deep dive into the whole-stage code generation.</p> <pre><code>val q = spark.range(10).where('id === 4)\n</code></pre> <pre><code>scala&gt; q.queryExecution.debug.codegen\nFound 1 WholeStageCodegen subtrees.\n== Subtree 1 / 1 ==\n*(1) Filter (id#3L = 4)\n+- *(1) Range (0, 10, step=1, splits=8)\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n...\n</code></pre> <pre><code>val q = spark.range(10).where('id === 4)\nimport org.apache.spark.sql.execution.debug._\nscala&gt; q.debugCodegen()\nFound 1 WholeStageCodegen subtrees.\n== Subtree 1 / 1 ==\n*(1) Filter (id#0L = 4)\n+- *(1) Range (0, 10, step=1, splits=8)\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n...\n</code></pre>","text":""},{"location":"whole-stage-code-generation/#collapsecodegenstages-physical-preparation-rule","title":"CollapseCodegenStages Physical Preparation Rule <p>At query execution planning, CollapseCodegenStages physical preparation rule finds the physical query plans that support codegen and collapses them together as a WholeStageCodegen (possibly with InputAdapter in-between for physical operators with no support for Java code generation).</p> <p><code>CollapseCodegenStages</code> is part of the sequence of physical preparation rules QueryExecution.preparations that will be applied in order to the physical plan before execution.</p>","text":""},{"location":"whole-stage-code-generation/#debugcodegen","title":"debugCodegen <p>debugCodegen or QueryExecution.debug.codegen methods allow to access the generated Java source code for a structured query.</p> <p>As of Spark 3.0.0, <code>debugCodegen</code> prints Java bytecode statistics of generated classes (and compiled by Janino).</p> <pre><code>import org.apache.spark.sql.execution.debug._\nval q = \"SELECT sum(v) FROM VALUES(1) t(v)\"\nscala&gt; sql(q).debugCodegen\nFound 2 WholeStageCodegen subtrees.\n== Subtree 1 / 2 (maxMethodCodeSize:124; maxConstantPoolSize:130(0.20% used); numInnerClasses:0) ==\n...\n== Subtree 2 / 2 (maxMethodCodeSize:139; maxConstantPoolSize:137(0.21% used); numInnerClasses:0) ==\n</code></pre>","text":""},{"location":"whole-stage-code-generation/#sparksqlcodegenwholestage","title":"spark.sql.codegen.wholeStage <p>Whole-Stage Code Generation is controlled by spark.sql.codegen.wholeStage Spark internal property.</p> <p>Whole-Stage Code Generation is on by default.</p> <pre><code>assert(spark.sessionState.conf.wholeStageEnabled)\n</code></pre>","text":""},{"location":"whole-stage-code-generation/#code-generation-paths","title":"Code Generation Paths <p>Code generation paths were coined in this commit.</p>  <p>Tip</p> <p>Learn more in SPARK-12795 Whole stage codegen.</p>","text":""},{"location":"whole-stage-code-generation/#non-whole-stage-codegen-path","title":"Non-Whole-Stage-Codegen Path","text":""},{"location":"whole-stage-code-generation/#produce-path","title":"Produce Path","text":"<p>Whole-stage-codegen \"produce\" path</p> <p>A physical operator with CodegenSupport can generate Java source code to process the rows from input RDDs.</p>"},{"location":"whole-stage-code-generation/#consume-path","title":"Consume Path","text":"<p>Whole-stage-codegen \"consume\" path</p>"},{"location":"whole-stage-code-generation/#benchmarkwholestagecodegen","title":"BenchmarkWholeStageCodegen <p><code>BenchmarkWholeStageCodegen</code> class provides a benchmark to measure whole stage codegen performance.</p> <p>You can execute it using the command:</p> <pre><code>build/sbt 'sql/testOnly *BenchmarkWholeStageCodegen'\n</code></pre>  <p>Note</p> <p>You need to un-ignore tests in <code>BenchmarkWholeStageCodegen</code> by replacing <code>ignore</code> with <code>test</code>.</p>  <pre><code>$ build/sbt 'sql/testOnly *BenchmarkWholeStageCodegen'\n...\nRunning benchmark: range/limit/sum\n  Running case: range/limit/sum codegen=false\n22:55:23.028 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n  Running case: range/limit/sum codegen=true\n\nJava HotSpot(TM) 64-Bit Server VM 1.8.0_77-b03 on Mac OS X 10.10.5\nIntel(R) Core(TM) i7-4870HQ CPU @ 2.50GHz\n\nrange/limit/sum:                    Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-------------------------------------------------------------------------------------------\nrange/limit/sum codegen=false             376 /  433       1394.5           0.7       1.0X\nrange/limit/sum codegen=true              332 /  388       1581.3           0.6       1.1X\n\n[info] - range/limit/sum (10 seconds, 74 milliseconds)\n</code></pre>","text":""},{"location":"whole-stage-code-generation/Block/","title":"Block","text":"<p><code>Block</code> is an extension of the TreeNode abstraction for nodes that represent a block of Java code.</p>"},{"location":"whole-stage-code-generation/Block/#implementations","title":"Implementations","text":""},{"location":"whole-stage-code-generation/Block/#codeblock","title":"CodeBlock","text":""},{"location":"whole-stage-code-generation/Block/#emptyblock","title":"EmptyBlock","text":""},{"location":"whole-stage-code-generation/Block/#textual-representation","title":"Textual Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code>...FIXME</p> <p><code>toString</code> is part of Java's <code>java.lang.Object</code> abstraction.</p>","text":""},{"location":"whole-stage-code-generation/BufferedRowIterator/","title":"BufferedRowIterator","text":"<p><code>BufferedRowIterator</code> is an abstraction of iterators that are code-generated at execution time in Whole-Stage Java Code Generation (using WholeStageCodegenExec unary physical operator).</p>"},{"location":"whole-stage-code-generation/BufferedRowIterator/#contract","title":"Contract","text":""},{"location":"whole-stage-code-generation/BufferedRowIterator/#initializing","title":"Initializing <pre><code>void init(\n  int index,\n  Iterator&lt;InternalRow&gt;[] iters)\n</code></pre> <p>Used when <code>WholeStageCodegenExec</code> unary physical operator is executed</p>","text":""},{"location":"whole-stage-code-generation/BufferedRowIterator/#processing-next-row","title":"Processing Next Row <pre><code>void processNext()\n</code></pre> <p>Used when <code>BufferedRowIterator</code> is requested to hasNext</p>","text":""},{"location":"whole-stage-code-generation/BufferedRowIterator/#hasnext","title":"hasNext <pre><code>boolean hasNext()\n</code></pre> <p><code>hasNext</code> processNext when there are no rows in the currentRows buffer.</p> <p><code>hasNext</code> is <code>true</code> when the currentRows buffer has got any element. Otherwise, <code>hasNext</code> is <code>false</code>.</p> <p><code>hasNext</code>\u00a0is used when <code>WholeStageCodegenExec</code> unary physical operator is executed (to generate an <code>RDD[InternalRow]</code> to <code>mapPartitionsWithIndex</code> over rows from up to two input RDDs).</p>","text":""},{"location":"whole-stage-code-generation/BufferedRowIterator/#currentrows-buffer","title":"currentRows Buffer <pre><code>LinkedList&lt;InternalRow&gt; currentRows\n</code></pre> <p><code>currentRows</code> is an internal buffer of InternalRow:</p> <ul> <li>A new row is added when appending a row</li> <li>A row is removed when requested to next</li> </ul> <p><code>currentRows</code>\u00a0is used for hasNext and shouldStop.</p>","text":""},{"location":"whole-stage-code-generation/BufferedRowIterator/#appending-row","title":"Appending Row <pre><code>void append(\n  InternalRow row)\n</code></pre> <p><code>append</code> simply adds the given InternalRow to the end of the currentRows buffer (thus the name append).</p> <p><code>append</code>\u00a0is used...FIXME</p>","text":""},{"location":"whole-stage-code-generation/CodeGenerator/","title":"CodeGenerator","text":"<p><code>CodeGenerator</code> is an abstraction of JVM bytecode generators for expression evaluation.</p> <p>The Scala definition of this abstract class is as follows:</p> <pre><code>CodeGenerator[InType &lt;: AnyRef, OutType &lt;: AnyRef]\n</code></pre>"},{"location":"whole-stage-code-generation/CodeGenerator/#contract","title":"Contract","text":""},{"location":"whole-stage-code-generation/CodeGenerator/#bind","title":"bind <pre><code>bind(\n  in: InType,\n  inputSchema: Seq[Attribute]): InType\n</code></pre> <p>Used when:</p> <ul> <li><code>CodeGenerator</code> is requested to generate</li> </ul>","text":""},{"location":"whole-stage-code-generation/CodeGenerator/#canonicalize","title":"canonicalize <pre><code>canonicalize(\n  in: InType): InType\n</code></pre> <p>Used when:</p> <ul> <li><code>CodeGenerator</code> is requested to generate</li> </ul>","text":""},{"location":"whole-stage-code-generation/CodeGenerator/#create","title":"create <pre><code>create(\n  in: InType): OutType\n</code></pre> <p>Used when:</p> <ul> <li><code>CodeGenerator</code> is requested to generate</li> </ul>","text":""},{"location":"whole-stage-code-generation/CodeGenerator/#implementations","title":"Implementations","text":"<ul> <li>GenerateColumnAccessor</li> <li>GenerateMutableProjection</li> <li>GenerateOrdering</li> <li>GeneratePredicate</li> <li>GenerateSafeProjection</li> <li>GenerateUnsafeProjection</li> <li><code>GenerateUnsafeRowJoiner</code></li> </ul>"},{"location":"whole-stage-code-generation/CodeGenerator/#cache","title":"cache <pre><code>cache: Cache[CodeAndComment, (GeneratedClass, ByteCodeStats)]\n</code></pre> <p><code>CodeGenerator</code> creates a cache of generated classes when loaded (as an Scala object).</p> <p>When requested to look up a non-existent <code>CodeAndComment</code>, <code>cache</code> doCompile, updates <code>CodegenMetrics</code> and prints out the following INFO message to the logs:</p> <pre><code>Code generated in [timeMs] ms\n</code></pre> <p><code>cache</code> allows for up to spark.sql.codegen.cache.maxEntries pairs.</p> <p><code>cache</code> is used when:</p> <ul> <li><code>CodeGenerator</code> is requested to compile a Java source code.</li> </ul>","text":""},{"location":"whole-stage-code-generation/CodeGenerator/#compiling-java-code","title":"Compiling Java Code <pre><code>compile(\n  code: CodeAndComment): (GeneratedClass, ByteCodeStats)\n</code></pre> <p><code>compile</code> looks the given <code>CodeAndComment</code> up in the cache.</p>  <p><code>compile</code> is used when:</p> <ul> <li><code>GenerateMutableProjection</code> is requested to create a MutableProjection</li> <li><code>GenerateOrdering</code> is requested to create a BaseOrdering</li> <li><code>GeneratePredicate</code> is requested to create a BasePredicate</li> <li><code>GenerateSafeProjection</code> is requested to create a Projection</li> <li><code>GenerateUnsafeProjection</code> is requested to create an UnsafeProjection</li> <li><code>GenerateUnsafeRowJoiner</code> is requested to <code>create</code> an <code>UnsafeRowJoiner</code></li> <li><code>WholeStageCodegenExec</code> is requested to doExecute</li> <li><code>GenerateColumnAccessor</code> is requested to create a ColumnarIterator</li> <li><code>debug</code> utility is used to <code>codegenStringSeq</code></li> </ul>","text":""},{"location":"whole-stage-code-generation/CodeGenerator/#generate","title":"generate <pre><code>generate(\n  expressions: InType): OutType\ngenerate(\n  expressions: InType,\n  inputSchema: Seq[Attribute]): OutType // (1)!\n</code></pre> <ol> <li>Binds the input expressions to the given input schema</li> </ol> <p><code>generate</code> creates a class for the input <code>expressions</code> (after canonicalization).</p> <p><code>generate</code> is used when:</p> <ul> <li><code>Serializer</code> (of ExpressionEncoder) is requested to <code>apply</code></li> <li><code>RowOrdering</code> utility is used to createCodeGeneratedObject</li> <li><code>SafeProjection</code> utility is used to <code>createCodeGeneratedObject</code></li> <li><code>LazilyGeneratedOrdering</code> is requested for <code>generatedOrdering</code></li> <li><code>ObjectOperator</code> utility is used to <code>deserializeRowToObject</code> and <code>serializeObjectToRow</code></li> <li><code>ComplexTypedAggregateExpression</code> is requested for <code>inputRowToObj</code> and <code>bufferRowToObject</code></li> <li><code>DefaultCachedBatchSerializer</code> is requested to <code>convertCachedBatchToInternalRow</code></li> </ul>","text":""},{"location":"whole-stage-code-generation/CodeGenerator/#creating-codegencontext","title":"Creating CodegenContext <pre><code>newCodeGenContext(): CodegenContext\n</code></pre> <p><code>newCodeGenContext</code> creates a new CodegenContext.</p> <p><code>newCodeGenContext</code> is used when:</p> <ul> <li><code>GenerateMutableProjection</code> is requested to create a MutableProjection</li> <li><code>GenerateOrdering</code> is requested to create a BaseOrdering</li> <li><code>GeneratePredicate</code> utility is used to create a BasePredicate</li> <li><code>GenerateSafeProjection</code> is requested to create a Projection</li> <li><code>GenerateUnsafeProjection</code> utility is used to create an UnsafeProjection</li> <li><code>GenerateColumnAccessor</code> is requested to create a ColumnarIterator</li> </ul>","text":""},{"location":"whole-stage-code-generation/CodeGenerator/#docompile","title":"doCompile <pre><code>doCompile(\n  code: CodeAndComment): (GeneratedClass, ByteCodeStats)\n</code></pre> <p><code>doCompile</code> creates a <code>ClassBodyEvaluator</code> (Janino).</p> <p><code>doCompile</code> requests the <code>ClassBodyEvaluator</code> to use <code>org.apache.spark.sql.catalyst.expressions.GeneratedClass</code> as the name of the generated class and sets some default imports (to be included in the generated class).</p> <p><code>doCompile</code> requests the <code>ClassBodyEvaluator</code> to use <code>GeneratedClass</code> as a superclass of the generated class (for passing extra <code>references</code> objects into the generated class).</p> <pre><code>abstract class GeneratedClass {\n  def generate(references: Array[Any]): Any\n}\n</code></pre> <p><code>doCompile</code> prints out the following DEBUG message to the logs (with the given <code>code</code>):</p> <pre><code>[formatted code]\n</code></pre> <p><code>doCompile</code> requests the <code>ClassBodyEvaluator</code> to cook (read, scan, parse and compile Java tokens) the source code and gets the bytecode statistics:</p> <ul> <li>max method bytecode size</li> <li>max constant pool size</li> <li>number of inner classes</li> </ul> <p><code>doCompile</code> updates <code>CodeGenerator</code> code-gen metrics.</p> <p>In the end, <code>doCompile</code> returns the <code>GeneratedClass</code> instance and bytecode statistics.</p>  <p><code>doCompile</code> is used when:</p> <ul> <li><code>CodeGenerator</code> is requested to look up a code (in the cache)</li> </ul>","text":""},{"location":"whole-stage-code-generation/CodeGenerator/#logging","title":"Logging <p><code>CodeGenerator</code> is an abstract class and logging is configured using the logger of the implementations.</p>","text":""},{"location":"whole-stage-code-generation/CodegenContext/","title":"CodegenContext","text":"<p><code>CodegenContext</code> is a context for Whole-Stage Java Code Generation to track objects (that could be passed into generated Java code).</p>"},{"location":"whole-stage-code-generation/CodegenContext/#creating-instance","title":"Creating Instance","text":"<p><code>CodegenContext</code> takes no arguments to be created.</p> <p><code>CodegenContext</code> is created when:</p> <ul> <li><code>CodegenContext</code> is requested for a new CodegenContext</li> <li><code>GenerateUnsafeRowJoiner</code> utility is used to create a <code>UnsafeRowJoiner</code></li> <li><code>WholeStageCodegenExec</code> unary physical operator is requested for a Java source code for the child operator (when <code>WholeStageCodegenExec</code> is executed)</li> </ul>"},{"location":"whole-stage-code-generation/CodegenContext/#newcodegencontext","title":"newCodeGenContext <pre><code>newCodeGenContext(): CodegenContext\n</code></pre> <p><code>newCodeGenContext</code> creates a new CodegenContext.</p> <p><code>newCodeGenContext</code> is used when:</p> <ul> <li><code>GenerateMutableProjection</code> utility is used to create a MutableProjection</li> <li><code>GenerateOrdering</code> utility is used to create a BaseOrdering</li> <li><code>GeneratePredicate</code> utility is used to create a BaseOrdering</li> <li><code>GenerateSafeProjection</code> utility is used to create a Projection</li> <li><code>GenerateUnsafeProjection</code> utility is used to create an UnsafeProjection</li> <li><code>GenerateColumnAccessor</code> utility is used to create a ColumnarIterator</li> </ul>","text":""},{"location":"whole-stage-code-generation/CodegenContext/#references","title":"references <pre><code>references: mutable.ArrayBuffer[Any]\n</code></pre> <p><code>CodegenContext</code> uses <code>references</code> collection for objects that could be passed into generated class.</p> <p>A new reference is added:</p> <ul> <li>addReferenceObj</li> <li><code>CodegenFallback</code> is requested to doGenCode</li> </ul> <p>Used when:</p> <ul> <li><code>WholeStageCodegenExec</code> unary physical operator is requested to doExecute</li> <li><code>GenerateMutableProjection</code> utility is used to create a MutableProjection</li> <li><code>GenerateOrdering</code> utility is used to create a BaseOrdering</li> <li><code>GeneratePredicate</code> utility is used to create a BaseOrdering</li> <li><code>GenerateSafeProjection</code> utility is used to create a Projection</li> <li><code>GenerateUnsafeProjection</code> utility is used to create an UnsafeProjection</li> </ul>","text":""},{"location":"whole-stage-code-generation/CodegenContext/#addreferenceobj","title":"addReferenceObj <pre><code>addReferenceObj(\n  objName: String,\n  obj: Any,\n  className: String = null): String\n</code></pre> <p><code>addReferenceObj</code> adds the given <code>obj</code> to the references registry and returns the following code text:</p> <pre><code>(([clsName]) references[[idx]] /* [objName] */)\n</code></pre> <p><code>addReferenceObj</code> is used when:</p> <ul> <li><code>AvroDataToCatalyst</code> is requested to doGenCode</li> <li><code>CatalystDataToAvro</code> is requested to doGenCode</li> <li><code>CastBase</code> is requested to <code>castToStringCode</code>, <code>castToDateCode</code>, <code>castToTimestampCode</code> and <code>castToTimestampNTZCode</code></li> <li>Catalyst <code>Expression</code>s are requested to doGenCode</li> <li><code>BroadcastHashJoinExec</code> physical operator is requested to prepareBroadcast</li> <li><code>BroadcastNestedLoopJoinExec</code> physical operator is requested to prepareBroadcast</li> <li><code>ShuffledHashJoinExec</code> physical operator is requested to prepareRelation</li> <li><code>SortExec</code> physical operator is requested to doProduce</li> <li><code>SortMergeJoinExec</code> physical operator is requested to doProduce</li> <li><code>HashAggregateExec</code> physical operator is requested to doProduceWithKeys</li> <li><code>CodegenSupport</code> is requested to metricTerm</li> <li><code>HashMapGenerator</code> is requested to initializeAggregateHashMap</li> </ul>","text":""},{"location":"whole-stage-code-generation/CodegenContext/#generateexpressions","title":"generateExpressions <pre><code>generateExpressions(\n  expressions: Seq[Expression],\n  doSubexpressionElimination: Boolean = false): Seq[ExprCode]\n</code></pre> <p><code>generateExpressions</code> generates a Java source code for Code-Generated Evaluation of multiple Catalyst Expressions (with optional subexpression elimination).</p>  <p>With the given <code>doSubexpressionElimination</code> enabled, <code>generateExpressions</code> subexpressionElimination (with the given <code>expressions</code>).</p> <p>In the end, <code>generateExpressions</code> requests every Expression (in the given <code>expressions</code>) for a Java source code for code-generated (non-interpreted) expression evaluation.</p>  <p><code>generateExpressions</code> is used when:</p> <ul> <li><code>GenerateMutableProjection</code> is requested to create a MutableProjection</li> <li><code>GeneratePredicate</code> is requested to create</li> <li><code>GenerateUnsafeProjection</code> is requested to create</li> <li><code>HashAggregateExec</code> physical operator is requested for a Java source code for whole-stage consume path with grouping keys</li> </ul>","text":""},{"location":"whole-stage-code-generation/CodegenContext/#subexpressionelimination","title":"subexpressionElimination <pre><code>subexpressionElimination(\n  expressions: Seq[Expression]): Unit\n</code></pre> <p><code>subexpressionElimination</code>...FIXME</p>","text":""},{"location":"whole-stage-code-generation/CodegenContext/#subexpressioneliminationforwholestagecodegen","title":"subexpressionEliminationForWholeStageCodegen <pre><code>subexpressionEliminationForWholeStageCodegen(\n  expressions: Seq[Expression]): SubExprCodes\n</code></pre> <p><code>subexpressionEliminationForWholeStageCodegen</code>...FIXME</p>  <p><code>subexpressionEliminationForWholeStageCodegen</code> is used when:</p> <ul> <li><code>ProjectExec</code> is requested to doConsume</li> <li><code>AggregateCodegenSupport</code> is requested to doConsumeWithoutKeys</li> <li><code>HashAggregateExec</code> is requested to doConsumeWithKeys</li> </ul>","text":""},{"location":"whole-stage-code-generation/CodegenContext/#demo","title":"Demo","text":""},{"location":"whole-stage-code-generation/CodegenContext/#adding-state","title":"Adding State <pre><code>import org.apache.spark.sql.catalyst.expressions.codegen._\nval ctx = new CodegenContext\n\nval input = ctx.addMutableState(\n  \"scala.collection.Iterator\",\n  \"input\",\n  v =&gt; s\"$v = inputs[0];\")\n</code></pre>","text":""},{"location":"whole-stage-code-generation/CodegenContext/#codegencontextsubexpressionelimination","title":"CodegenContext.subexpressionElimination <pre><code>import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext\nval ctx = new CodegenContext\n\n// Use Catalyst DSL\nimport org.apache.spark.sql.catalyst.dsl.expressions._\nval expressions = \"hello\".expr.as(\"world\") :: \"hello\".expr.as(\"world\") :: Nil\n\n// FIXME Use a real-life query to extract the expressions\n\n// CodegenContext.subexpressionElimination (where the elimination all happens) is a private method\n// It is used exclusively in CodegenContext.generateExpressions which is public\n// and does the elimination when it is enabled\n\n// Note the doSubexpressionElimination flag is on\n// Triggers the subexpressionElimination private method\nctx.generateExpressions(expressions, doSubexpressionElimination = true)\n\n// subexpressionElimination private method uses ctx.equivalentExpressions\nval commonExprs = ctx.equivalentExpressions.getAllEquivalentExprs\n\nassert(commonExprs.length &gt; 0, \"No common expressions found\")\n</code></pre>","text":""},{"location":"whole-stage-code-generation/GenerateColumnAccessor/","title":"GenerateColumnAccessor","text":"<p><code>GenerateColumnAccessor</code> is a CodeGenerator.</p>"},{"location":"whole-stage-code-generation/GenerateColumnAccessor/#creating-columnariterator","title":"Creating ColumnarIterator <pre><code>create(\n  columnTypes: Seq[DataType]): ColumnarIterator\n</code></pre> <p><code>create</code> is part of the CodeGenerator abstraction.</p> <p><code>create</code>...FIXME</p>","text":""},{"location":"whole-stage-code-generation/GenerateMutableProjection/","title":"GenerateMutableProjection","text":"<p>=== [[create]] Creating MutableProjection -- <code>create</code> Internal Method</p>"},{"location":"whole-stage-code-generation/GenerateMutableProjection/#source-scala","title":"[source, scala]","text":"<p>create(   expressions: Seq[Expression],   useSubexprElimination: Boolean): MutableProjection</p> <p><code>create</code>...FIXME</p> <p>NOTE: <code>create</code> is used when...FIXME</p>"},{"location":"whole-stage-code-generation/GenerateOrdering/","title":"GenerateOrdering","text":"<p>=== [[create]] Creating BaseOrdering -- <code>create</code> Method</p>"},{"location":"whole-stage-code-generation/GenerateOrdering/#source-scala","title":"[source, scala]","text":"<p>create(ordering: Seq[SortOrder]): BaseOrdering create(schema: StructType): BaseOrdering</p> <p><code>create</code> is part of the CodeGenerator abstraction.</p> <p><code>create</code>...FIXME</p> <p>=== [[genComparisons]] <code>genComparisons</code> Method</p>"},{"location":"whole-stage-code-generation/GenerateOrdering/#source-scala_1","title":"[source, scala]","text":""},{"location":"whole-stage-code-generation/GenerateOrdering/#gencomparisonsctx-codegencontext-schema-structtype-string","title":"genComparisons(ctx: CodegenContext, schema: StructType): String","text":"<p><code>genComparisons</code>...FIXME</p> <p>NOTE: <code>genComparisons</code> is used when...FIXME</p>"},{"location":"whole-stage-code-generation/GeneratePredicate/","title":"GeneratePredicate","text":"<p>=== [[create]] Creating Predicate -- <code>create</code> Method</p>"},{"location":"whole-stage-code-generation/GeneratePredicate/#source-scala","title":"[source, scala]","text":""},{"location":"whole-stage-code-generation/GeneratePredicate/#createpredicate-expression-predicate","title":"create(predicate: Expression): Predicate","text":"<p><code>create</code> is part of the CodeGenerator abstraction.</p> <p><code>create</code>...FIXME</p>"},{"location":"whole-stage-code-generation/GenerateSafeProjection/","title":"GenerateSafeProjection","text":"<p><code>GenerateSafeProjection</code> utility is a CodeGenerator.</p> <pre><code>CodeGenerator[Seq[Expression], Projection]\n</code></pre> <p><code>GenerateSafeProjection</code> is used when:</p> <ul> <li><code>SafeProjection</code> utility is used to <code>createCodeGeneratedObject</code></li> <li><code>DeserializeToObjectExec</code> physical operator is executed</li> <li><code>ObjectOperator</code> utility is used to <code>deserializeRowToObject</code></li> <li><code>ComplexTypedAggregateExpression</code> is requested for <code>inputRowToObj</code> and <code>bufferRowToObject</code></li> </ul>"},{"location":"whole-stage-code-generation/GenerateSafeProjection/#creating-projection","title":"Creating Projection <pre><code>create(\n  expressions: Seq[Expression]): Projection\n</code></pre> <p><code>create</code>...FIXME</p> <p><code>create</code> is part of the CodeGenerator abstraction.</p>","text":""},{"location":"whole-stage-code-generation/GenerateUnsafeProjection/","title":"GenerateUnsafeProjection","text":"<p><code>GenerateUnsafeProjection</code> is a CodeGenerator to generate the bytecode for creating an UnsafeProjection from the given Expressions (i.e. <code>CodeGenerator[Seq[Expression], UnsafeProjection]</code>).</p> <pre><code>GenerateUnsafeProjection: Seq[Expression] =&gt; UnsafeProjection\n</code></pre>"},{"location":"whole-stage-code-generation/GenerateUnsafeProjection/#binding-expressions-to-schema","title":"Binding Expressions to Schema <pre><code>bind(\n  in: Seq[Expression],\n  inputSchema: Seq[Attribute]): Seq[Expression]\n</code></pre> <p><code>bind</code> binds the given <code>in</code> expressions to the given <code>inputSchema</code>.</p> <p><code>bind</code> is part of the CodeGenerator abstraction.</p>","text":""},{"location":"whole-stage-code-generation/GenerateUnsafeProjection/#creating-unsafeprojection-for-expressions","title":"Creating UnsafeProjection (for Expressions) <pre><code>create(\n  references: Seq[Expression]): UnsafeProjection // (1)!\ncreate(\n  expressions: Seq[Expression],\n  subexpressionEliminationEnabled: Boolean): UnsafeProjection\n</code></pre> <ol> <li><code>subexpressionEliminationEnabled</code> flag is <code>false</code></li> </ol> <p><code>create</code> creates a new CodegenContext.</p> <p><code>create</code>...FIXME</p> <p><code>create</code> is part of the CodeGenerator abstraction.</p>","text":""},{"location":"whole-stage-code-generation/GenerateUnsafeProjection/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j2.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"whole-stage-code-generation/GenerateUnsafeProjection/#review-me","title":"Review Me <p>=== [[generate]] Generating UnsafeProjection -- <code>generate</code> Method</p>","text":""},{"location":"whole-stage-code-generation/GenerateUnsafeProjection/#source-scala","title":"[source, scala] <p>generate(   expressions: Seq[Expression],   subexpressionEliminationEnabled: Boolean): UnsafeProjection</p>  <p><code>generate</code> &lt;&gt; the input <code>expressions</code> followed by &lt;&gt; for the expressions. <p><code>generate</code> is used when:</p> <ul> <li> <p><code>UnsafeProjection</code> factory object is requested for a UnsafeProjection</p> </li> <li> <p><code>ExpressionEncoder</code> is requested to initialize the internal UnsafeProjection</p> </li> <li> <p><code>FileFormat</code> is requested to build a data reader with partition column values appended</p> </li> <li> <p><code>OrcFileFormat</code> is requested to <code>buildReaderWithPartitionValues</code></p> </li> <li> <p><code>ParquetFileFormat</code> is requested to build a data reader with partition column values appended</p> </li> <li> <p><code>GroupedIterator</code> is requested for <code>keyProjection</code></p> </li> <li> <p><code>ObjectOperator</code> is requested to <code>serializeObjectToRow</code></p> </li> <li> <p>(Spark MLlib) <code>LibSVMFileFormat</code> is requested to <code>buildReader</code></p> </li> <li> <p>(Spark Structured Streaming) <code>StateStoreRestoreExec</code>, <code>StateStoreSaveExec</code> and <code>StreamingDeduplicateExec</code> are requested to execute</p> </li> </ul> <p>=== [[canonicalize]] <code>canonicalize</code> Method</p>","text":""},{"location":"whole-stage-code-generation/GenerateUnsafeProjection/#source-scala_1","title":"[source, scala]","text":""},{"location":"whole-stage-code-generation/GenerateUnsafeProjection/#canonicalizein-seqexpression-seqexpression","title":"canonicalize(in: Seq[Expression]): Seq[Expression] <p><code>canonicalize</code> removes unnecessary <code>Alias</code> expressions.</p> <p>Internally, <code>canonicalize</code> uses <code>ExpressionCanonicalizer</code> rule executor (that in turn uses just one <code>CleanExpressions</code> expression rule).</p> <p>=== [[create]] Generating JVM Bytecode For UnsafeProjection For Given Expressions (With Optional Subexpression Elimination) -- <code>create</code> Method</p>","text":""},{"location":"whole-stage-code-generation/GenerateUnsafeProjection/#source-scala_2","title":"[source, scala] <p>create(   expressions: Seq[Expression],   subexpressionEliminationEnabled: Boolean): UnsafeProjection create(references: Seq[Expression]): UnsafeProjection // &lt;1&gt;</p>  <p>&lt;1&gt; Calls the former <code>create</code> with <code>subexpressionEliminationEnabled</code> flag off</p> <p><code>create</code> first creates a CodegenContext and an &lt;&gt; for the input <code>expressions</code>. <p><code>create</code> creates a code body with <code>public java.lang.Object generate(Object[] references)</code> method that creates a <code>SpecificUnsafeProjection</code>.</p>","text":""},{"location":"whole-stage-code-generation/GenerateUnsafeProjection/#source-java","title":"[source, java] <p>public java.lang.Object generate(Object[] references) {   return new SpecificUnsafeProjection(references); }</p> <p>class SpecificUnsafeProjection extends UnsafeProjection {   ... }</p>  <p><code>create</code> creates a <code>CodeAndComment</code> with the code body and comment placeholders.</p> <p>You should see the following DEBUG message in the logs:</p> <pre><code>DEBUG GenerateUnsafeProjection: code for [expressions]:\n[code]\n</code></pre>","text":""},{"location":"whole-stage-code-generation/GenerateUnsafeProjection/#tip","title":"[TIP]","text":"<p>Enable <code>DEBUG</code> logging level for <code>org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator</code> logger to see the message above.</p> <pre><code>log4j.logger.org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator=DEBUG\n</code></pre>"},{"location":"whole-stage-code-generation/GenerateUnsafeProjection/#see-codegenerator","title":"See CodeGenerator.","text":"<p><code>create</code> requests <code>CodeGenerator</code> to compile the Java source code to JVM bytecode (using Janino).</p> <p><code>create</code> requests <code>CodegenContext</code> for references and requests the compiled class to create a <code>SpecificUnsafeProjection</code> for the input references that in the end is the final UnsafeProjection.</p> <p>(Single-argument) <code>create</code> is part of the CodeGenerator abstraction.</p> <p>=== [[createCode]] Creating ExprCode for Expressions (With Optional Subexpression Elimination) -- <code>createCode</code> Method</p>"},{"location":"whole-stage-code-generation/GenerateUnsafeProjection/#source-scala_3","title":"[source, scala] <p>createCode(   ctx: CodegenContext,   expressions: Seq[Expression],   useSubexprElimination: Boolean = false): ExprCode</p>  <p><code>createCode</code> requests the input <code>CodegenContext</code> to generate a Java source code for code-generated evaluation of every expression in the input <code>expressions</code>.</p> <p><code>createCode</code>...FIXME</p>","text":""},{"location":"whole-stage-code-generation/GenerateUnsafeProjection/#source-scala_4","title":"[source, scala] <p>import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext val ctx = new CodegenContext</p> <p>// Use Catalyst DSL import org.apache.spark.sql.catalyst.dsl.expressions._ val expressions = \"hello\".expr.as(\"world\") :: \"hello\".expr.as(\"world\") :: Nil</p> <p>import org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection val eval = GenerateUnsafeProjection.createCode(ctx, expressions, useSubexprElimination = true)</p> <p>scala&gt; println(eval.code)</p> <pre><code>    mutableStateArray1[0].reset();\n\n    mutableStateArray2[0].write(0, ((UTF8String) references[0] /* literal */));\n\n\n        mutableStateArray2[0].write(1, ((UTF8String) references[1] /* literal */));\n    mutableStateArray[0].setTotalSize(mutableStateArray1[0].totalSize());\n</code></pre> <p>scala&gt; println(eval.value) mutableStateArray[0]</p>  <p><code>createCode</code> is used when:</p> <ul> <li> <p><code>CreateNamedStructUnsafe</code> is requested to generate a Java source code</p> </li> <li> <p><code>GenerateUnsafeProjection</code> is requested to create a UnsafeProjection</p> </li> <li> <p><code>CodegenSupport</code> is requested to prepareRowVar (to generate a Java source code to consume generated columns or row from a physical operator)</p> </li> <li> <p><code>HashAggregateExec</code> is requested to doProduceWithKeys and doConsumeWithKeys</p> </li> <li> <p><code>BroadcastHashJoinExec</code> is requested to genStreamSideJoinKey (when generating the Java source code for joins)</p> </li> </ul>","text":""},{"location":"window-functions/","title":"Window Functions","text":"<p>From the official documentation of PostgreSQL:</p> <p>Window functions provide the ability to perform calculations across sets of rows that are related to the current query row.</p> <p>Since window functions are a subset of standard functions in Spark SQL, they generate a value for every group of rows that are associated with the current row by some relation.</p> <p>Window functions require a window specification (WindowSpec) that defines which rows are included in a window (frame, i.e. the set of rows that are associated with the current row by some relation).</p> <p>Window utility is used to create a <code>WindowSpec</code> to be refined further using Window operators.</p> <pre><code>import org.apache.spark.sql.expressions.Window\nval byHTokens = Window.partitionBy('token startsWith \"h\")\n</code></pre> <pre><code>import org.apache.spark.sql.expressions.WindowSpec\nassert(byHTokens.isInstanceOf[WindowSpec])\n</code></pre> <pre><code>import org.apache.spark.sql.expressions.Window\nval windowSpec = Window\n.partitionBy($\"orderId\")\n.orderBy($\"time\")\n</code></pre> <p>With a <code>WindowSpec</code> defined, Column.over operator is used to associate the <code>WindowSpec</code> with aggregate or window functions.</p> <pre><code>import org.apache.spark.sql.functions.rank\nrank.over(byHTokens)\n</code></pre> <pre><code>import org.apache.spark.sql.functions.first\nfirst.over(windowSpec)\n</code></pre>"},{"location":"window-functions/#limitations","title":"Limitations","text":"<p><code>WindowSpecDefinition</code> expression enforces the following requirements on WindowFrames:</p> <ol> <li>No <code>UnspecifiedFrame</code>s are allowed (and should be resolved during analysis)</li> <li>A range window frame cannot be used in an unordered window specification.</li> <li>A range window frame with value boundaries cannot be used in a window specification with multiple order by expressions</li> <li>The data type in the order specification ought to match the data type of the range frame</li> </ol>"},{"location":"window-functions/AggregateProcessor/","title":"AggregateProcessor","text":"<p><code>AggregateProcessor</code> is used by WindowExecBase unary physical operators when requested for windowFrameExpressionFactoryPairs to create the following WindowFunctionFrames (for supported AGGREGATE functions):</p> <ul> <li><code>SlidingWindowFunctionFrame</code></li> <li><code>UnboundedFollowingWindowFunctionFrame</code></li> <li><code>UnboundedPrecedingWindowFunctionFrame</code></li> <li><code>UnboundedWindowFunctionFrame</code></li> </ul>"},{"location":"window-functions/AggregateProcessor/#supported-aggregate-functions","title":"Supported AGGREGATE Functions","text":"<p><code>AggregateProcessor</code> is used for the following <code>AGGREGATE</code> functions:</p> <ul> <li>AggregateExpression</li> <li>AggregateWindowFunction</li> <li>OffsetWindowFunction with <code>RowFrame</code> with (<code>UnboundedPreceding</code>, non-<code>CurrentRow</code>) frame</li> </ul>"},{"location":"window-functions/AggregateProcessor/#creating-instance","title":"Creating Instance","text":"<p><code>AggregateProcessor</code> takes the following to be created:</p> <ul> <li> Buffer Schema (<code>Array[AttributeReference]</code>) <li> Initial <code>MutableProjection</code> <li> Update <code>MutableProjection</code> <li> Evaluate <code>MutableProjection</code> <li> ImperativeAggregate <li> <code>trackPartitionSize</code> flag <p><code>AggregateProcessor</code> is created using apply factory.</p>"},{"location":"window-functions/AggregateProcessor/#creating-aggregateprocessor-instance","title":"Creating AggregateProcessor Instance <pre><code>apply(\n  functions: Array[Expression],\n  ordinal: Int,\n  inputAttributes: Seq[Attribute],\n  newMutableProjection: (Seq[Expression], Seq[Attribute]) =&gt; MutableProjection\n): AggregateProcessor\n</code></pre> <p><code>apply</code> creates an AggregateProcessor.</p> <p><code>apply</code> is used when:</p> <ul> <li><code>WindowExecBase</code> unary physical operator is requested for windowFrameExpressionFactoryPairs</li> </ul>","text":""},{"location":"window-functions/AggregateProcessor/#evaluate","title":"evaluate <pre><code>evaluate(\n  target: InternalRow): Unit\n</code></pre> <p><code>evaluate</code>...FIXME</p> <p><code>evaluate</code> is used when:</p> <ul> <li><code>SlidingWindowFunctionFrame</code> is requested to <code>write</code></li> <li><code>UnboundedFollowingWindowFunctionFrame</code> is requested to <code>write</code></li> <li><code>UnboundedPrecedingWindowFunctionFrame</code> is requested to <code>write</code></li> <li><code>UnboundedWindowFunctionFrame</code> is requested to <code>prepare</code></li> </ul>","text":""},{"location":"window-functions/AggregateProcessor/#initialize","title":"initialize <pre><code>initialize(\n  size: Int): Unit\n</code></pre> <p><code>initialize</code>...FIXME</p> <p><code>initialize</code> is used when:</p> <ul> <li><code>SlidingWindowFunctionFrame</code> is requested to <code>write</code></li> <li><code>UnboundedFollowingWindowFunctionFrame</code> is requested to <code>write</code></li> <li><code>UnboundedPrecedingWindowFunctionFrame</code> is requested to <code>prepare</code></li> <li><code>UnboundedWindowFunctionFrame</code> is requested to <code>prepare</code></li> </ul>","text":""},{"location":"window-functions/AggregateProcessor/#update","title":"update <pre><code>update(\n  input: InternalRow): Unit\n</code></pre> <p><code>update</code>...FIXME</p> <p><code>update</code> is used when:</p> <ul> <li><code>SlidingWindowFunctionFrame</code> is requested to <code>write</code></li> <li><code>UnboundedFollowingWindowFunctionFrame</code> is requested to <code>write</code></li> <li><code>UnboundedPrecedingWindowFunctionFrame</code> is requested to <code>write</code></li> <li><code>UnboundedWindowFunctionFrame</code> is requested to <code>prepare</code></li> </ul>","text":""},{"location":"window-functions/RangeFrame/","title":"RangeFrame","text":"<p><code>RangeFrame</code> is a <code>FrameType</code> of WindowSpecs with the following:</p> <ol> <li>rangeBetween operator or RANGE BETWEEN SQL clause</li> <li>Unspecified frame with an order specification</li> </ol>"},{"location":"window-functions/RangeFrame/#cumedist-expression","title":"CumeDist Expression <p>CumeDist window function expression requires a <code>RangeFrame</code> with UnboundedPreceding and CurrentRow.</p> <p>It is because <code>CUME_DIST</code> must return the same value for equal values in the partition.</p>","text":""},{"location":"window-functions/RangeFrame/#unspecifiedframe-with-order-specification-and-resolvewindowframe","title":"UnspecifiedFrame with Order Specification and ResolveWindowFrame <p><code>RangeFrame</code> with UnboundedPreceding and CurrentRow is assumed (by ResolveWindowFrame logical resolution rule) for ordered window specifications (WindowSpecDefinitions with <code>UnspecifiedFrame</code> but a non-empty order specification).</p>","text":""},{"location":"window-functions/RangeFrame/#inputtype","title":"inputType <pre><code>inputType: AbstractDataType\n</code></pre> <p><code>inputType</code> can be any of the following numeric and interval data types:</p> <ul> <li><code>NumericType</code></li> <li><code>CalendarIntervalType</code></li> <li><code>DayTimeIntervalType</code></li> <li><code>YearMonthIntervalType</code></li> </ul> <p><code>inputType</code> is part of the <code>FrameType</code> abstraction.</p>","text":""},{"location":"window-functions/RangeFrame/#sql","title":"sql <pre><code>sql: String\n</code></pre> <p><code>sql</code> is <code>RANGE</code>.</p> <p><code>sql</code> is part of the <code>FrameType</code> abstraction.</p>","text":""},{"location":"window-functions/Window/","title":"Window Utility","text":"<p><code>Window</code> utility allows to define a WindowSpec.</p>"},{"location":"window-functions/Window/#frame-boundaries","title":"Frame Boundaries","text":"Numeric Value Frame Boundary <code>Long.MinValue</code> Window.unboundedPreceding <code>0</code> Window.currentRow <code>Long.MaxValue</code> Window.unboundedFollowing"},{"location":"window-functions/WindowFunctionFrame/","title":"WindowFunctionFrame","text":"<p><code>WindowFunctionFrame</code> is an abstraction of window frames.</p>"},{"location":"window-functions/WindowFunctionFrame/#contract","title":"Contract","text":""},{"location":"window-functions/WindowFunctionFrame/#currentlowerbound","title":"currentLowerBound <pre><code>currentLowerBound(): Int\n</code></pre> <p>Used when:</p> <ul> <li><code>WindowInPandasExec</code> (PySpark) physical operator is requested to <code>doExecute</code></li> </ul>","text":""},{"location":"window-functions/WindowFunctionFrame/#currentupperbound","title":"currentUpperBound <pre><code>currentUpperBound(): Int\n</code></pre> <p>Used when:</p> <ul> <li><code>WindowInPandasExec</code> (PySpark) physical operator is requested to <code>doExecute</code></li> </ul>","text":""},{"location":"window-functions/WindowFunctionFrame/#prepare","title":"prepare <pre><code>prepare(\n  rows: ExternalAppendOnlyUnsafeRowArray): Unit\n</code></pre> <p>Prepares the frame (with the given ExternalAppendOnlyUnsafeRowArray)</p> <p>Used when:</p> <ul> <li><code>WindowExec</code> physical operator is requested to doExecute</li> <li><code>WindowInPandasExec</code> (PySpark) physical operator is requested to <code>doExecute</code></li> </ul>","text":""},{"location":"window-functions/WindowFunctionFrame/#write","title":"write <pre><code>write(\n  index: Int,\n  current: InternalRow): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>WindowExec</code> physical operator is requested to doExecute</li> <li><code>WindowInPandasExec</code> (PySpark) physical operator is requested to <code>doExecute</code></li> </ul>","text":""},{"location":"window-functions/WindowFunctionFrame/#implementations","title":"Implementations","text":"<ul> <li><code>OffsetWindowFunctionFrameBase</code></li> <li><code>SlidingWindowFunctionFrame</code></li> <li><code>UnboundedFollowingWindowFunctionFrame</code></li> <li><code>UnboundedPrecedingWindowFunctionFrame</code></li> </ul>"},{"location":"window-functions/WindowFunctionFrame/#unboundedwindowfunctionframe","title":"UnboundedWindowFunctionFrame <p><code>UnboundedWindowFunctionFrame</code> is a WindowFunctionFrame that gives the same value for every row in a partition.</p> <p><code>UnboundedWindowFunctionFrame</code> is created for AggregateFunctions (in AggregateExpressions) or AggregateWindowFunctions with no frame defined (i.e. no <code>rowsBetween</code> or <code>rangeBetween</code>) that boils down to using the WindowExec.md#entire-partition-frame[entire partition frame].</p>","text":""},{"location":"window-functions/WindowSpec/","title":"WindowSpec","text":"<p><code>WindowSpec</code> defines a window specification for Window Functions.</p>"},{"location":"window-functions/WindowSpec/#creating-instance","title":"Creating Instance","text":"<p><code>WindowSpec</code> takes the following to be created:</p> <ul> <li>Partition Specification</li> <li>Order Specification</li> <li>Frame Specification</li> </ul> <p><code>WindowSpec</code> is created when:</p> <ul> <li><code>Window</code> utility is used to create a default WindowSpec</li> <li><code>WindowSpec</code> is requested to partitionBy, orderBy, rowsBetween, rangeBetween</li> </ul>"},{"location":"window-functions/WindowSpec/#partition-specification","title":"Partition Specification <p>Partition specification are Expressions that define which rows are in the same partition. With no partition defined, all rows belong to a single partition.</p>","text":""},{"location":"window-functions/WindowSpec/#order-specification","title":"Order Specification <p>Order specification are SortOrders that define how rows are ordered in a partition that in turn defines the position of a row in a partition.</p> <p>The ordering could be ascending (<code>ASC</code> in SQL or <code>asc</code> in Scala) or descending (<code>DESC</code> or <code>desc</code>).</p>","text":""},{"location":"window-functions/WindowSpec/#frame-specification","title":"Frame Specification <p>Frame specification is a <code>WindowFrame</code> that defines the rows to include in the frame for the current row, based on their relative position to the current row.</p> <p>Frame specification is defined using rowsBetween and rangeBetween operators.</p> <p>For example, \"the three rows preceding the current row to the current row\" describes a frame including the current input row and three rows appearing before the current row.</p> <p><code>Window</code> utility defines special Frame Boundaries.</p>","text":""},{"location":"window-functions/WindowSpec/#rowsbetween","title":"rowsBetween <pre><code>rowsBetween(\n  start: Long,\n  end: Long): WindowSpec\n</code></pre> <p><code>rowsBetween</code> creates a WindowSpec with a <code>SpecifiedWindowFrame</code> boundary frame (of <code>RowFrame</code> type) from the given<code>start</code> and <code>end</code> (both inclusive).</p>","text":""},{"location":"window-functions/WindowSpec/#rangebetween","title":"rangeBetween <pre><code>rangeBetween(\n  start: Long,\n  end: Long): WindowSpec\n</code></pre> <p><code>rangeBetween</code> creates a WindowSpec with a <code>SpecifiedWindowFrame</code> boundary frame (of RangeFrame type) from the given<code>start</code> and <code>end</code> (both inclusive).</p>","text":""},{"location":"developer-api/","title":"Developer API","text":""},{"location":"developer-api/#developerapi","title":"DeveloperApi","text":"<ul> <li>SparkSessionExtensions</li> <li>CachedBatch</li> <li>CachedBatchSerializer</li> <li>SimpleMetricsCachedBatchSerializer</li> <li>UserDefinedType</li> <li>ColumnarBatch</li> </ul>"}]}