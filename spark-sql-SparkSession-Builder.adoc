== [[Builder]] Builder -- Building SparkSession using Fluent API

`Builder` is the fluent API to build a fully-configured link:spark-sql-SparkSession.adoc[SparkSession].

.Builder Methods
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| <<getOrCreate, getOrCreate>>
| Gets the current link:spark-sql-SparkSession.adoc[SparkSession] or creates a new one.

| <<enableHiveSupport, enableHiveSupport>>
| Enables Hive support
|===

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
  .appName("My Spark Application")  // optional and will be autogenerated if not specified
  .master("local[*]")               // avoid hardcoding the deployment environment
  .enableHiveSupport()              // self-explanatory, isn't it?
  .getOrCreate
----

You can use the fluent design pattern to set the various properties of a `SparkSession` that opens a session to Spark SQL.

NOTE: You can have multiple ``SparkSession``s in a single Spark application for different link:spark-sql-SparkSession.adoc#catalog[data catalogs] (through relational entities).

=== [[getOrCreate]] `getOrCreate` Method

CAUTION: FIXME

=== [[config]] `config` Method

CAUTION: FIXME

=== [[enableHiveSupport]] Enabling Hive Support -- `enableHiveSupport` Method

[source, scala]
----
enableHiveSupport(): Builder
----

`enableHiveSupport` enables Hive support, i.e. running structured queries on Hive tables (and a persistent Hive metastore, support for Hive serdes and Hive user-defined functions).

[NOTE]
====
You do *not* need any existing Hive installation to use Spark's Hive support. `SparkSession` context will automatically create `metastore_db` in the current directory of a Spark application and a directory configured by link:spark-sql-StaticSQLConf.adoc#spark.sql.warehouse.dir[spark.sql.warehouse.dir].

Refer to link:spark-sql-SharedState.adoc[SharedState].
====

Internally, `enableHiveSupport` makes sure that the Hive classes are on CLASSPATH, i.e. Spark SQL's `org.apache.hadoop.hive.conf.HiveConf`, and sets link:spark-sql-StaticSQLConf.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation] internal configuration property to `hive`.
