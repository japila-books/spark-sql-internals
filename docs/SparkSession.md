# SparkSession &mdash; The Entry Point to Spark SQL

`SparkSession` is the entry point to Spark SQL. It is one of the first objects created in a Spark SQL application.

`SparkSession` is [created](#creating-instance) using the [SparkSession.builder](#builder) method.

```scala
import org.apache.spark.sql.SparkSession
val spark = SparkSession.builder
  .appName("My Spark Application")  // optional and will be autogenerated if not specified
  .master("local[*]")               // only for demo and testing purposes, use spark-submit instead
  .enableHiveSupport()              // self-explanatory, isn't it?
  .config("spark.sql.warehouse.dir", "target/spark-warehouse")
  .withExtensions { extensions =>
    extensions.injectResolutionRule { session =>
      ...
    }
    extensions.injectOptimizerRule { session =>
      ...
    }
  }
  .getOrCreate
```

`SparkSession` is a namespace of relational entities (e.g. databases, tables). A Spark SQL application could use many `SparkSessions` to keep the relational entities separate logically in [metadata catalogs](#catalog).

!!! note "SparkSession in spark-shell"
    `spark` object in `spark-shell` (the instance of `SparkSession` that is auto-created) has Hive support enabled.

    In order to disable the pre-configured Hive support in the `spark` object, use [spark.sql.catalogImplementation](StaticSQLConf.md#spark.sql.catalogImplementation) internal configuration property with `in-memory` value (that uses [InMemoryCatalog](InMemoryCatalog.md) external catalog instead).

    ```text
    $ spark-shell --conf spark.sql.catalogImplementation=in-memory
    ```

## Creating Instance

`SparkSession` takes the following to be created:

* <span id="sparkContext"> `SparkContext`
* <span id="existingSharedState"> Existing [SharedState](SharedState.md) (if given)
* <span id="parentSessionState"> Parent [SessionState](SessionState.md) (if given)
* <span id="extensions"> [SparkSessionExtensions](SparkSessionExtensions.md)

`SparkSession` is created when:

* `SparkSession.Builder` is requested to [getOrCreate](SparkSession-Builder.md#getOrCreate)
* Indirectly using [newSession](#newSession) or [cloneSession](#cloneSession)

## <span id="sessionState"> SessionState

```scala
sessionState: SessionState
```

`sessionState` is the current [SessionState](SessionState.md).

Internally, `sessionState` <<SessionState.md#clone, clones>> the optional <<parentSessionState, parent SessionState>> (if given when <<creating-instance, creating the SparkSession>>) or <<instantiateSessionState, creates a new SessionState>> using <<BaseSessionStateBuilder.md#, BaseSessionStateBuilder>> as defined by <<StaticSQLConf.md#spark.sql.catalogImplementation, spark.sql.catalogImplementation>> configuration property:

* *in-memory* (default) for SessionStateBuilder.md[org.apache.spark.sql.internal.SessionStateBuilder]
* *hive* for hive/HiveSessionStateBuilder.md[org.apache.spark.sql.hive.HiveSessionStateBuilder]

## <span id="newSession"> Creating New SparkSession

```scala
newSession(): SparkSession
```

`newSession` creates a new `SparkSession` with an undefined parent [SessionState](#parentSessionState) and (re)using the following:

* [SparkContext](#sparkContext)
* [SharedState](#sharedState)
* [SparkSessionExtensions](#extensions)

!!! note "SparkSession.newSession and SparkSession.cloneSession"
    `SparkSession.newSession` uses no parent [SessionState](#parentSessionState) while [SparkSession.cloneSession](#cloneSession) (re)uses [SessionState](#sessionState).

## <span id="cloneSession"> Cloning SparkSession

```scala
cloneSession(): SparkSession
```

`cloneSession`...FIXME

`cloneSession` is used when:

* `AdaptiveSparkPlanHelper` is requested to `getOrCloneSessionWithAqeOff`
* `StreamExecution` (Spark Structured Streaming) is created

## <span id="builder"> Creating SparkSession Using Builder Pattern

```scala
builder(): Builder
```

`builder` is an object method that creates a new [Builder](SparkSession-Builder.md) to build a `SparkSession` using a _fluent API_.

```scala
import org.apache.spark.sql.SparkSession
val builder = SparkSession.builder
```

TIP: Read about https://en.wikipedia.org/wiki/Fluent_interface[Fluent interface] design pattern in Wikipedia, the free encyclopedia.

## <span id="version"> Spark Version

```scala
version: String
```

`version` returns the version of Apache Spark in use.

Internally, `version` uses `spark.SPARK_VERSION` value that is the `version` property in `spark-version-info.properties` properties file on CLASSPATH.

## <span id="emptyDataset"> Creating Empty Dataset (Given Encoder)

```scala
emptyDataset[T: Encoder]: Dataset[T]
```

`emptyDataset` creates an empty [Dataset](Dataset.md) (assuming that future records being of type `T`).

```text
scala> val strings = spark.emptyDataset[String]
strings: org.apache.spark.sql.Dataset[String] = [value: string]

scala> strings.printSchema
root
 |-- value: string (nullable = true)
```

`emptyDataset` creates a [LocalRelation](logical-operators/LocalRelation.md) logical operator.

## <span id="createDataset"> Creating Dataset from Local Collections or RDDs

```scala
createDataset[T : Encoder](
  data: RDD[T]): Dataset[T]
createDataset[T : Encoder](
  data: Seq[T]): Dataset[T]
```

`createDataset` creates a [Dataset](Dataset.md) from a local Scala collection, i.e. `Seq[T]`, Java's `List[T]`, or a distributed `RDD[T]`.

```text
scala> val one = spark.createDataset(Seq(1))
one: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> one.show
+-----+
|value|
+-----+
|    1|
+-----+
```

`createDataset` creates logical operators:

* [LocalRelation](logical-operators/LocalRelation.md) for the input `data` collection
* [LogicalRDD](logical-operators/LogicalRDD.md) for the input `RDD[T]`


!!! tip "implicits object"
    You may want to consider [implicits](implicits.md) object and `toDS` method instead.

    ```text
    val spark: SparkSession = ...
    import spark.implicits._

    scala> val one = Seq(1).toDS
    one: org.apache.spark.sql.Dataset[Int] = [value: int]
    ```

Internally, `createDataset` first looks up the implicit [ExpressionEncoder](ExpressionEncoder.md) in scope to access the ``AttributeReference``s (of the [schema](types/index.md)).

The expression encoder is then used to map elements (of the input `Seq[T]`) into a collection of [InternalRow](InternalRow.md)s. With the references and rows, `createDataset` returns a Dataset.md[Dataset] with a LocalRelation.md[`LocalRelation` logical query plan].

## <span id="sql"> Executing SQL Queries (SQL Mode)

```scala
sql(
  sqlText: String): DataFrame
```

`sql` creates a [QueryPlanningTracker](QueryPlanningTracker.md) to [measure](QueryPlanningTracker.md#measurePhase) executing the following in [parsing](QueryPlanningTracker.md#PARSING) phase:

* `sql` requests the [SessionState](#sessionState) for the [ParserInterface](SessionState.md#sqlParser) to [parse](sql/ParserInterface.md#parsePlan) the given `sqlText` SQL statement (that gives a [LogicalPlan](logical-operators/LogicalPlan.md))

In the end, `sql` [creates a DataFrame](Dataset.md#ofRows) with the following:

* This `SparkSession`
* The `LogicalPlan`
* The `QueryPlanningTracker`

## <span id="udf"> Accessing UDFRegistration

```scala
udf: UDFRegistration
```

`udf` attribute is [UDFRegistration](user-defined-functions/UDFRegistration.md) (for registering [user-defined functions](spark-sql-udfs.md) for SQL-based queries).

```text
val spark: SparkSession = ...
spark.udf.register("myUpper", (s: String) => s.toUpperCase)

val strs = ('a' to 'c').map(_.toString).toDS
strs.registerTempTable("strs")

scala> sql("SELECT *, myUpper(value) UPPER FROM strs").show
+-----+-----+
|value|UPPER|
+-----+-----+
|    a|    A|
|    b|    B|
|    c|    C|
+-----+-----+
```

Internally, it is simply an alias for [SessionState.udfRegistration](SessionState.md#udfRegistration).

## <span id="table"> Loading Data From Table

```scala
table(
  multipartIdentifier: Seq[String]): DataFrame
table(
  tableName: String): DataFrame
table(
  tableIdent: TableIdentifier): DataFrame
```

`table` creates a [DataFrame](DataFrame.md) for the input `tableName` table.

!!! note
    [baseRelationToDataFrame](#baseRelationToDataFrame) acts as a mechanism to plug `BaseRelation` object hierarchy in into adoc[LogicalPlan](logical-operators/LogicalPlan.md) object hierarchy that `SparkSession` uses to bridge them.

```text
scala> spark.catalog.tableExists("t1")
res1: Boolean = true

// t1 exists in the catalog
// let's load it
val t1 = spark.table("t1")
```

## Catalog

```scala
catalog: Catalog
```

`catalog` creates a [CatalogImpl](CatalogImpl.md) when first accessed.

??? note "lazy value"
    `catalog` is a Scala lazy value which is computed once when accessed and cached afterwards.

## <span id="read"> DataFrameReader

```scala
read: DataFrameReader
```

`read` gives [DataFrameReader](DataFrameReader.md) to load data from external data sources and load it into a `DataFrame`.

```scala
val spark: SparkSession = ... // create instance
val dfReader: DataFrameReader = spark.read
```

## <span id="conf"> Runtime Configuration

```scala
conf: RuntimeConfig
```

`conf` returns the current [RuntimeConfig](RuntimeConfig.md).

Internally, `conf` creates a [RuntimeConfig](RuntimeConfig.md) (when requested the very first time and cached afterwards) with the [SQLConf](SessionState.md#conf) (of the [SessionState](#sessionState)).

## <span id="experimentalMethods"> ExperimentalMethods

```scala
experimental: ExperimentalMethods
```

`experimentalMethods` is an extension point with [ExperimentalMethods](ExperimentalMethods.md) that is a per-session collection of extra strategies and ``Rule[LogicalPlan]``s.

`experimental` is used in [SparkPlanner](SparkPlanner.md) and [SparkOptimizer](SparkOptimizer.md).

## <span id="baseRelationToDataFrame"> Create DataFrame for BaseRelation

```scala
baseRelationToDataFrame(
  baseRelation: BaseRelation): DataFrame
```

Internally, `baseRelationToDataFrame` creates a [DataFrame](DataFrame.md) from the input [BaseRelation](BaseRelation.md) wrapped inside [LogicalRelation](logical-operators/LogicalRelation.md).

`baseRelationToDataFrame` is used when:

* `DataFrameReader` is requested to load data from [data source](DataFrameReader.md#load) or [JDBC table](DataFrameReader.md#jdbc)
* `TextInputCSVDataSource` creates a base `Dataset` (of Strings)
* `TextInputJsonDataSource` creates a base `Dataset` (of Strings)

## <span id="instantiateSessionState"> Creating SessionState

```scala
instantiateSessionState(
  className: String,
  sparkSession: SparkSession): SessionState
```

`instantiateSessionState` finds the `className` that is then used to [create](BaseSessionStateBuilder.md#creating-instance) and [build](BaseSessionStateBuilder.md#build) a `BaseSessionStateBuilder`.

`instantiateSessionState` may report an `IllegalArgumentException` while instantiating the class of a `SessionState`:

```text
Error while instantiating '[className]'
```

`instantiateSessionState` is used when `SparkSession` is requested for [SessionState](#sessionState) (based on [spark.sql.catalogImplementation](StaticSQLConf.md#spark.sql.catalogImplementation) configuration property).

## <span id="sessionStateClassName"> sessionStateClassName

```scala
sessionStateClassName(
  conf: SparkConf): String
```

`sessionStateClassName` gives the name of the class of the [SessionState](SessionState.md) per [spark.sql.catalogImplementation](StaticSQLConf.md#spark.sql.catalogImplementation), i.e.

* [org.apache.spark.sql.hive.HiveSessionStateBuilder](hive/HiveSessionStateBuilder.md) for `hive`
* [org.apache.spark.sql.internal.SessionStateBuilder](SessionStateBuilder.md) for `in-memory`

`sessionStateClassName` is used when `SparkSession` is requested for the [SessionState](#sessionState) (and one is not available yet).

## <span id="internalCreateDataFrame"> Creating DataFrame From RDD Of Internal Binary Rows and Schema

```scala
internalCreateDataFrame(
  catalystRows: RDD[InternalRow],
  schema: StructType,
  isStreaming: Boolean = false): DataFrame
```

`internalCreateDataFrame` creates a [DataFrame](Dataset.md#ofRows) with [LogicalRDD](logical-operators/LogicalRDD.md).

`internalCreateDataFrame` is used when:

* `DataFrameReader` is requested to create a DataFrame from Dataset of [JSONs](DataFrameReader.md#json) or [CSVs](DataFrameReader.md#csv)

* `SparkSession` is requested to [create a DataFrame from RDD of rows](#createDataFrame)

* [InsertIntoDataSourceCommand](logical-operators/InsertIntoDataSourceCommand.md) logical command is executed

## <span id="listenerManager"> ExecutionListenerManager

```scala
listenerManager: ExecutionListenerManager
```

[ExecutionListenerManager](ExecutionListenerManager.md)

## <span id="sharedState"> SharedState

```scala
sharedState: SharedState
```

[SharedState](SharedState.md)

## <span id="time"> Measuring Duration of Executing Code Block

```scala
time[T](f: => T): T
```

`time` executes a code block and prints out (to standard output) the time taken to execute it

## <span id="applyExtensions"> Applying SparkSessionExtensions

```scala
applyExtensions(
  extensionConfClassNames: Seq[String],
  extensions: SparkSessionExtensions): SparkSessionExtensions
```

`applyExtensions` uses the given `extensionConfClassNames` as the names of the extension classes.

For every extension class, `applyExtensions` instantiates one by one passing in the given [SparkSessionExtensions](SparkSessionExtensions.md).

!!! note
    The given `SparkSessionExtensions` is mutated in-place.

In the end, `applyExtensions` returns the given `SparkSessionExtensions`.

---

In case of `ClassCastException`, `ClassNotFoundException` or `NoClassDefFoundError`, `applyExtensions` prints out the following WARN message to the logs:

```text
Cannot use [extensionConfClassName] to configure session extensions.
```

---

`applyExtensions` is used when:

* `SparkSession.Builder` is requested to [get active or create a new SparkSession instance](SparkSession-Builder.md#getOrCreate)
* `SparkSession` is [created](#creating-instance) (from a `SparkContext`)

## <span id="leafNodeDefaultParallelism"> Default Parallelism of Leaf Nodes

```scala
leafNodeDefaultParallelism: Int
```

`leafNodeDefaultParallelism` is the value of [spark.sql.leafNodeDefaultParallelism](configuration-properties.md#spark.sql.leafNodeDefaultParallelism) if defined or `SparkContext.defaultParallelism` ([Spark Core]({{ book.spark_core }}/SparkContext#defaultParallelism)).

---

`leafNodeDefaultParallelism`Â is used when:

* [SparkSession.range](SparkSession.md#range) operator is used
* `RangeExec` leaf physical operator is [created](physical-operators/RangeExec.md#numSlices)
* `CommandResultExec` physical operator is requested for the `RDD[InternalRow]`
* `LocalTableScanExec` physical operator is requested for the [RDD](physical-operators/LocalTableScanExec.md#rdd)
* `FilePartition` is requested for [maxSplitBytes](files/FilePartition.md#maxSplitBytes)
