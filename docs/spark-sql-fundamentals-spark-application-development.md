# Fundamentals of Spark SQL Application Development

Development of a Spark SQL application requires the following steps:

1. Setting up Development Environment (IntelliJ IDEA, Scala and sbt)
1. Specifying Library Dependencies
1. Creating [SparkSession](SparkSession.md)
1. [Loading Data](DataFrameReader.md) from Data Sources
1. Processing Data Using [Dataset API](spark-sql-dataset-operators.md)
1. [Saving Data](DataFrameWriter.md) to Persistent Storage
1. Deploying Spark Application to Cluster (using `spark-submit`)
